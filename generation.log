2025-02-11 07:50:43,721 - 
Starting step 0
2025-02-11 07:50:43,721 - Current_ids device: cuda:0
2025-02-11 07:50:43,721 - Current_ids dtype: torch.int64
2025-02-11 07:50:43,904 - Model output complete
2025-02-11 07:50:43,904 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:43,904 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:43,904 - Next token logits device: cuda:0
2025-02-11 07:50:43,905 - Entered do_sample
2025-02-11 07:50:43,910 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:43,916 - Probs max: 0.50341796875
2025-02-11 07:50:43,948 - Pre-cat
2025-02-11 07:50:43,948 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:43,951 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:43,951 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:43,951 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:43,951 - Step 0: Generated next token
2025-02-11 07:50:43,952 - Step 0: Updated current_ids
2025-02-11 07:50:43,952 - Step 0: Decoded token text:  The
2025-02-11 07:50:43,952 - Step 0: Updated current_phrase
2025-02-11 07:50:43,957 - Step 0: Created step_acts
2025-02-11 07:50:43,957 - Step 0: Added to generation_acts
2025-02-11 07:50:43,958 - Step 0: Updated generated_texts
2025-02-11 07:50:43,958 - Step 0: Updated recent_tokens
2025-02-11 07:50:43,958 - Step 0: Decoded current text
2025-02-11 07:50:43,958 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:43,958 - 
Starting step 1
2025-02-11 07:50:43,958 - Current_ids device: cuda:0
2025-02-11 07:50:43,958 - Current_ids dtype: torch.int64
2025-02-11 07:50:43,982 - Model output complete
2025-02-11 07:50:43,982 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:43,982 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:43,982 - Next token logits device: cuda:0
2025-02-11 07:50:43,982 - Entered do_sample
2025-02-11 07:50:43,983 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:43,988 - Probs max: 0.83837890625
2025-02-11 07:50:43,989 - Pre-cat
2025-02-11 07:50:43,989 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:50:43,990 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:43,990 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:43,990 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:43,990 - Step 1: Generated next token
2025-02-11 07:50:43,990 - Step 1: Updated current_ids
2025-02-11 07:50:43,991 - Step 1: Decoded token text:  ball
2025-02-11 07:50:43,991 - Step 1: Updated current_phrase
2025-02-11 07:50:43,991 - Step 1: Created step_acts
2025-02-11 07:50:43,991 - Step 1: Added to generation_acts
2025-02-11 07:50:43,991 - Step 1: Updated recent_tokens
2025-02-11 07:50:43,992 - Step 1: Decoded current text
2025-02-11 07:50:43,992 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:43,992 - 
Starting step 2
2025-02-11 07:50:43,993 - Current_ids device: cuda:0
2025-02-11 07:50:43,993 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,014 - Model output complete
2025-02-11 07:50:44,014 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:44,014 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,014 - Next token logits device: cuda:0
2025-02-11 07:50:44,014 - Entered do_sample
2025-02-11 07:50:44,014 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,020 - Probs max: 0.583984375
2025-02-11 07:50:44,021 - Pre-cat
2025-02-11 07:50:44,021 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:50:44,022 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:44,023 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:44,023 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,023 - Step 2: Generated next token
2025-02-11 07:50:44,023 - Step 2: Updated current_ids
2025-02-11 07:50:44,023 - Step 2: Decoded token text:  will
2025-02-11 07:50:44,023 - Step 2: Updated current_phrase
2025-02-11 07:50:44,023 - Step 2: Created step_acts
2025-02-11 07:50:44,023 - Step 2: Added to generation_acts
2025-02-11 07:50:44,024 - Step 2: Updated recent_tokens
2025-02-11 07:50:44,025 - Step 2: Decoded current text
2025-02-11 07:50:44,025 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:44,025 - 
Starting step 3
2025-02-11 07:50:44,025 - Current_ids device: cuda:0
2025-02-11 07:50:44,025 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,049 - Model output complete
2025-02-11 07:50:44,049 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:44,049 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,049 - Next token logits device: cuda:0
2025-02-11 07:50:44,049 - Entered do_sample
2025-02-11 07:50:44,049 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,054 - Probs max: 0.56884765625
2025-02-11 07:50:44,055 - Pre-cat
2025-02-11 07:50:44,055 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686]],
       device='cuda:0')
2025-02-11 07:50:44,057 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:50:44,057 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:44,057 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,057 - Step 3: Generated next token
2025-02-11 07:50:44,057 - Step 3: Updated current_ids
2025-02-11 07:50:44,057 - Step 3: Decoded token text:  hit
2025-02-11 07:50:44,057 - Step 3: Updated current_phrase
2025-02-11 07:50:44,058 - Step 3: Created step_acts
2025-02-11 07:50:44,058 - Step 3: Added to generation_acts
2025-02-11 07:50:44,058 - Step 3: Updated recent_tokens
2025-02-11 07:50:44,059 - Step 3: Decoded current text
2025-02-11 07:50:44,060 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:44,060 - 
Starting step 4
2025-02-11 07:50:44,060 - Current_ids device: cuda:0
2025-02-11 07:50:44,060 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,083 - Model output complete
2025-02-11 07:50:44,083 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:44,083 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,083 - Next token logits device: cuda:0
2025-02-11 07:50:44,083 - Entered do_sample
2025-02-11 07:50:44,083 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,089 - Probs max: 0.99609375
2025-02-11 07:50:44,090 - Pre-cat
2025-02-11 07:50:44,090 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201]],
       device='cuda:0')
2025-02-11 07:50:44,092 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:44,092 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:44,092 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,092 - Step 4: Generated next token
2025-02-11 07:50:44,092 - Step 4: Updated current_ids
2025-02-11 07:50:44,093 - Step 4: Decoded token text:  the
2025-02-11 07:50:44,093 - Step 4: Updated current_phrase
2025-02-11 07:50:44,093 - Step 4: Created step_acts
2025-02-11 07:50:44,093 - Step 4: Added to generation_acts
2025-02-11 07:50:44,093 - Step 4: Updated recent_tokens
2025-02-11 07:50:44,095 - Step 4: Decoded current text
2025-02-11 07:50:44,095 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:44,095 - 
Starting step 5
2025-02-11 07:50:44,095 - Current_ids device: cuda:0
2025-02-11 07:50:44,095 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,120 - Model output complete
2025-02-11 07:50:44,120 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:44,120 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,120 - Next token logits device: cuda:0
2025-02-11 07:50:44,120 - Entered do_sample
2025-02-11 07:50:44,121 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,124 - Probs max: 0.99853515625
2025-02-11 07:50:44,125 - Pre-cat
2025-02-11 07:50:44,125 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279]],
       device='cuda:0')
2025-02-11 07:50:44,128 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:44,128 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:44,128 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,128 - Step 5: Generated next token
2025-02-11 07:50:44,128 - Step 5: Updated current_ids
2025-02-11 07:50:44,129 - Step 5: Decoded token text:  wall
2025-02-11 07:50:44,129 - Step 5: Updated current_phrase
2025-02-11 07:50:44,129 - Step 5: Created step_acts
2025-02-11 07:50:44,129 - Step 5: Added to generation_acts
2025-02-11 07:50:44,130 - Step 5: Updated generated_texts
2025-02-11 07:50:44,131 - Step 5: Updated recent_tokens
2025-02-11 07:50:44,131 - Step 5: Decoded current text
2025-02-11 07:50:44,131 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:44,131 - 
Starting step 6
2025-02-11 07:50:44,131 - Current_ids device: cuda:0
2025-02-11 07:50:44,131 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,157 - Model output complete
2025-02-11 07:50:44,157 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:44,157 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,157 - Next token logits device: cuda:0
2025-02-11 07:50:44,157 - Entered do_sample
2025-02-11 07:50:44,157 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,160 - Probs max: 0.376708984375
2025-02-11 07:50:44,161 - Pre-cat
2025-02-11 07:50:44,161 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002]], device='cuda:0')
2025-02-11 07:50:44,163 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:50:44,164 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:44,164 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,164 - Step 6: Generated next token
2025-02-11 07:50:44,164 - Step 6: Updated current_ids
2025-02-11 07:50:44,164 - Step 6: Decoded token text:  and
2025-02-11 07:50:44,164 - Step 6: Updated current_phrase
2025-02-11 07:50:44,165 - Step 6: Created step_acts
2025-02-11 07:50:44,165 - Step 6: Added to generation_acts
2025-02-11 07:50:44,165 - Step 6: Updated recent_tokens
2025-02-11 07:50:44,166 - Step 6: Decoded current text
2025-02-11 07:50:44,166 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:44,166 - 
Starting step 7
2025-02-11 07:50:44,166 - Current_ids device: cuda:0
2025-02-11 07:50:44,166 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,202 - Model output complete
2025-02-11 07:50:44,202 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:44,202 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,202 - Next token logits device: cuda:0
2025-02-11 07:50:44,202 - Entered do_sample
2025-02-11 07:50:44,202 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,204 - Probs max: 0.416748046875
2025-02-11 07:50:44,205 - Pre-cat
2025-02-11 07:50:44,205 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323]], device='cuda:0')
2025-02-11 07:50:44,208 - Next token: tensor([[1221]], device='cuda:0')
2025-02-11 07:50:44,209 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:44,209 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,209 - Step 7: Generated next token
2025-02-11 07:50:44,209 - Step 7: Updated current_ids
2025-02-11 07:50:44,209 - Step 7: Decoded token text:  then
2025-02-11 07:50:44,209 - Step 7: Updated current_phrase
2025-02-11 07:50:44,210 - Step 7: Created step_acts
2025-02-11 07:50:44,210 - Step 7: Added to generation_acts
2025-02-11 07:50:44,210 - Step 7: Updated recent_tokens
2025-02-11 07:50:44,211 - Step 7: Decoded current text
2025-02-11 07:50:44,211 - Step 7: Incremented consecutive_fillers to 1
2025-02-11 07:50:44,211 - 
Starting step 8
2025-02-11 07:50:44,211 - Current_ids device: cuda:0
2025-02-11 07:50:44,211 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,233 - Model output complete
2025-02-11 07:50:44,233 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:44,234 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,234 - Next token logits device: cuda:0
2025-02-11 07:50:44,234 - Entered do_sample
2025-02-11 07:50:44,234 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,237 - Probs max: 0.27587890625
2025-02-11 07:50:44,237 - Pre-cat
2025-02-11 07:50:44,237 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,   1221]], device='cuda:0')
2025-02-11 07:50:44,239 - Next token: tensor([[7069]], device='cuda:0')
2025-02-11 07:50:44,239 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:44,239 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,240 - Step 8: Generated next token
2025-02-11 07:50:44,240 - Step 8: Updated current_ids
2025-02-11 07:50:44,240 - Step 8: Decoded token text:  immediately
2025-02-11 07:50:44,240 - Step 8: Updated current_phrase
2025-02-11 07:50:44,240 - Step 8: Created step_acts
2025-02-11 07:50:44,240 - Step 8: Added to generation_acts
2025-02-11 07:50:44,241 - Step 8: Updated recent_tokens
2025-02-11 07:50:44,242 - Step 8: Decoded current text
2025-02-11 07:50:44,242 - Step 8: Incremented consecutive_fillers to 2
2025-02-11 07:50:44,242 - 
Starting step 9
2025-02-11 07:50:44,242 - Current_ids device: cuda:0
2025-02-11 07:50:44,242 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,265 - Model output complete
2025-02-11 07:50:44,265 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:44,265 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,265 - Next token logits device: cuda:0
2025-02-11 07:50:44,265 - Entered do_sample
2025-02-11 07:50:44,265 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,269 - Probs max: 0.40869140625
2025-02-11 07:50:44,270 - Pre-cat
2025-02-11 07:50:44,270 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,   1221,   7069]], device='cuda:0')
2025-02-11 07:50:44,271 - Next token: tensor([[2936]], device='cuda:0')
2025-02-11 07:50:44,272 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:44,272 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,272 - Step 9: Generated next token
2025-02-11 07:50:44,272 - Step 9: Updated current_ids
2025-02-11 07:50:44,272 - Step 9: Decoded token text:  stop
2025-02-11 07:50:44,272 - Step 9: Updated current_phrase
2025-02-11 07:50:44,273 - Step 9: Created step_acts
2025-02-11 07:50:44,273 - Step 9: Added to generation_acts
2025-02-11 07:50:44,273 - Step 9: Updated recent_tokens
2025-02-11 07:50:44,274 - Step 9: Decoded current text
2025-02-11 07:50:44,274 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:44,274 - 
Starting step 10
2025-02-11 07:50:44,274 - Current_ids device: cuda:0
2025-02-11 07:50:44,274 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,298 - Model output complete
2025-02-11 07:50:44,298 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:44,298 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,298 - Next token logits device: cuda:0
2025-02-11 07:50:44,298 - Entered do_sample
2025-02-11 07:50:44,298 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,301 - Probs max: 0.623046875
2025-02-11 07:50:44,301 - Pre-cat
2025-02-11 07:50:44,301 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,   1221,   7069,   2936]], device='cuda:0')
2025-02-11 07:50:44,304 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:44,304 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:44,304 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,304 - Step 10: Generated next token
2025-02-11 07:50:44,304 - Step 10: Updated current_ids
2025-02-11 07:50:44,305 - Step 10: Decoded token text: .
2025-02-11 07:50:44,305 - Step 10: Updated current_phrase
2025-02-11 07:50:44,305 - Step 10: Created step_acts
2025-02-11 07:50:44,305 - Step 10: Added to generation_acts
2025-02-11 07:50:44,306 - Step 10: Updated generated_texts
2025-02-11 07:50:44,307 - Step 10: Updated recent_tokens
2025-02-11 07:50:44,307 - Step 10: Found phrase end token
2025-02-11 07:50:44,307 - Step 10: Updated recent_phrases
2025-02-11 07:50:44,307 - Step 10: Decoded current text
2025-02-11 07:50:44,307 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:44,307 - 
Starting step 11
2025-02-11 07:50:44,307 - Current_ids device: cuda:0
2025-02-11 07:50:44,307 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,329 - Model output complete
2025-02-11 07:50:44,329 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:44,329 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,329 - Next token logits device: cuda:0
2025-02-11 07:50:44,329 - Entered do_sample
2025-02-11 07:50:44,330 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,333 - Probs max: 0.296142578125
2025-02-11 07:50:44,334 - Pre-cat
2025-02-11 07:50:44,334 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,   1221,   7069,   2936,     13]], device='cuda:0')
2025-02-11 07:50:44,336 - Next token: tensor([[2055]], device='cuda:0')
2025-02-11 07:50:44,337 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:44,337 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,337 - Step 11: Generated next token
2025-02-11 07:50:44,337 - Step 11: Updated current_ids
2025-02-11 07:50:44,337 - Step 11: Decoded token text:  So
2025-02-11 07:50:44,337 - Step 11: Updated current_phrase
2025-02-11 07:50:44,337 - Step 11: Created step_acts
2025-02-11 07:50:44,337 - Step 11: Added to generation_acts
2025-02-11 07:50:44,338 - Step 11: Updated recent_tokens
2025-02-11 07:50:44,339 - Step 11: Decoded current text
2025-02-11 07:50:44,339 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:44,339 - 
Starting step 12
2025-02-11 07:50:44,339 - Current_ids device: cuda:0
2025-02-11 07:50:44,339 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,362 - Model output complete
2025-02-11 07:50:44,362 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:44,362 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,362 - Next token logits device: cuda:0
2025-02-11 07:50:44,362 - Entered do_sample
2025-02-11 07:50:44,362 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,365 - Probs max: 0.625
2025-02-11 07:50:44,366 - Pre-cat
2025-02-11 07:50:44,366 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,   1221,   7069,   2936,     13,   2055]],
       device='cuda:0')
2025-02-11 07:50:44,368 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:44,368 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:44,368 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,368 - Step 12: Generated next token
2025-02-11 07:50:44,369 - Step 12: Updated current_ids
2025-02-11 07:50:44,369 - Step 12: Decoded token text:  the
2025-02-11 07:50:44,369 - Step 12: Updated current_phrase
2025-02-11 07:50:44,369 - Step 12: Created step_acts
2025-02-11 07:50:44,369 - Step 12: Added to generation_acts
2025-02-11 07:50:44,369 - Step 12: Updated recent_tokens
2025-02-11 07:50:44,371 - Step 12: Decoded current text
2025-02-11 07:50:44,371 - Step 12: Incremented consecutive_fillers to 1
2025-02-11 07:50:44,371 - 
Starting step 13
2025-02-11 07:50:44,371 - Current_ids device: cuda:0
2025-02-11 07:50:44,371 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,394 - Model output complete
2025-02-11 07:50:44,394 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:44,394 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,394 - Next token logits device: cuda:0
2025-02-11 07:50:44,394 - Entered do_sample
2025-02-11 07:50:44,394 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,397 - Probs max: 0.497314453125
2025-02-11 07:50:44,397 - Pre-cat
2025-02-11 07:50:44,397 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,   1221,   7069,   2936,     13,   2055,    279]],
       device='cuda:0')
2025-02-11 07:50:44,400 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:44,400 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:44,400 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,400 - Step 13: Generated next token
2025-02-11 07:50:44,400 - Step 13: Updated current_ids
2025-02-11 07:50:44,401 - Step 13: Decoded token text:  wall
2025-02-11 07:50:44,401 - Step 13: Updated current_phrase
2025-02-11 07:50:44,401 - Step 13: Created step_acts
2025-02-11 07:50:44,401 - Step 13: Added to generation_acts
2025-02-11 07:50:44,401 - Step 13: Updated recent_tokens
2025-02-11 07:50:44,402 - Step 13: Decoded current text
2025-02-11 07:50:44,402 - Step 13: Incremented consecutive_fillers to 2
2025-02-11 07:50:44,403 - 
Starting step 14
2025-02-11 07:50:44,403 - Current_ids device: cuda:0
2025-02-11 07:50:44,403 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,424 - Model output complete
2025-02-11 07:50:44,424 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:50:44,424 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,424 - Next token logits device: cuda:0
2025-02-11 07:50:44,424 - Entered do_sample
2025-02-11 07:50:44,425 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,428 - Probs max: 0.8095703125
2025-02-11 07:50:44,429 - Pre-cat
2025-02-11 07:50:44,429 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,   1221,   7069,   2936,     13,   2055,    279,   7002]],
       device='cuda:0')
2025-02-11 07:50:44,431 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:44,432 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:50:44,432 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,432 - Step 14: Generated next token
2025-02-11 07:50:44,432 - Step 14: Updated current_ids
2025-02-11 07:50:44,432 - Step 14: Decoded token text:  will
2025-02-11 07:50:44,432 - Step 14: Updated current_phrase
2025-02-11 07:50:44,432 - Step 14: Created step_acts
2025-02-11 07:50:44,433 - Step 14: Added to generation_acts
2025-02-11 07:50:44,433 - Step 14: Updated recent_tokens
2025-02-11 07:50:44,434 - Step 14: Decoded current text
2025-02-11 07:50:44,434 - Step 14: Incremented consecutive_fillers to 3
2025-02-11 07:50:44,532 - 
Starting step 0
2025-02-11 07:50:44,532 - Current_ids device: cuda:0
2025-02-11 07:50:44,532 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,570 - Model output complete
2025-02-11 07:50:44,570 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:44,570 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,570 - Next token logits device: cuda:0
2025-02-11 07:50:44,570 - Entered do_sample
2025-02-11 07:50:44,570 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,573 - Probs max: 0.50341796875
2025-02-11 07:50:44,574 - Pre-cat
2025-02-11 07:50:44,574 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:44,576 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:44,577 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:44,577 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,577 - Step 0: Generated next token
2025-02-11 07:50:44,577 - Step 0: Updated current_ids
2025-02-11 07:50:44,577 - Step 0: Decoded token text:  The
2025-02-11 07:50:44,577 - Step 0: Updated current_phrase
2025-02-11 07:50:44,578 - Step 0: Created step_acts
2025-02-11 07:50:44,578 - Step 0: Added to generation_acts
2025-02-11 07:50:44,579 - Step 0: Updated generated_texts
2025-02-11 07:50:44,579 - Step 0: Updated recent_tokens
2025-02-11 07:50:44,579 - Step 0: Decoded current text
2025-02-11 07:50:44,579 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:44,579 - 
Starting step 1
2025-02-11 07:50:44,579 - Current_ids device: cuda:0
2025-02-11 07:50:44,580 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,602 - Model output complete
2025-02-11 07:50:44,602 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:44,603 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,603 - Next token logits device: cuda:0
2025-02-11 07:50:44,603 - Entered do_sample
2025-02-11 07:50:44,603 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,606 - Probs max: 0.83837890625
2025-02-11 07:50:44,607 - Pre-cat
2025-02-11 07:50:44,607 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:50:44,608 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:44,608 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:44,609 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,609 - Step 1: Generated next token
2025-02-11 07:50:44,609 - Step 1: Updated current_ids
2025-02-11 07:50:44,609 - Step 1: Decoded token text:  ball
2025-02-11 07:50:44,609 - Step 1: Updated current_phrase
2025-02-11 07:50:44,609 - Step 1: Created step_acts
2025-02-11 07:50:44,609 - Step 1: Added to generation_acts
2025-02-11 07:50:44,609 - Step 1: Updated recent_tokens
2025-02-11 07:50:44,611 - Step 1: Decoded current text
2025-02-11 07:50:44,611 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:44,611 - 
Starting step 2
2025-02-11 07:50:44,611 - Current_ids device: cuda:0
2025-02-11 07:50:44,611 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,633 - Model output complete
2025-02-11 07:50:44,633 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:44,633 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,633 - Next token logits device: cuda:0
2025-02-11 07:50:44,633 - Entered do_sample
2025-02-11 07:50:44,634 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,636 - Probs max: 0.583984375
2025-02-11 07:50:44,637 - Pre-cat
2025-02-11 07:50:44,637 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:50:44,639 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:44,640 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:44,640 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,640 - Step 2: Generated next token
2025-02-11 07:50:44,640 - Step 2: Updated current_ids
2025-02-11 07:50:44,640 - Step 2: Decoded token text:  will
2025-02-11 07:50:44,640 - Step 2: Updated current_phrase
2025-02-11 07:50:44,641 - Step 2: Created step_acts
2025-02-11 07:50:44,641 - Step 2: Added to generation_acts
2025-02-11 07:50:44,641 - Step 2: Updated recent_tokens
2025-02-11 07:50:44,642 - Step 2: Decoded current text
2025-02-11 07:50:44,642 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:44,642 - 
Starting step 3
2025-02-11 07:50:44,642 - Current_ids device: cuda:0
2025-02-11 07:50:44,642 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,692 - Model output complete
2025-02-11 07:50:44,692 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:44,692 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,692 - Next token logits device: cuda:0
2025-02-11 07:50:44,692 - Entered do_sample
2025-02-11 07:50:44,692 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,694 - Probs max: 0.56884765625
2025-02-11 07:50:44,696 - Pre-cat
2025-02-11 07:50:44,696 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686]],
       device='cuda:0')
2025-02-11 07:50:44,699 - Next token: tensor([[537]], device='cuda:0')
2025-02-11 07:50:44,699 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:44,699 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,699 - Step 3: Generated next token
2025-02-11 07:50:44,699 - Step 3: Updated current_ids
2025-02-11 07:50:44,700 - Step 3: Decoded token text:  not
2025-02-11 07:50:44,700 - Step 3: Updated current_phrase
2025-02-11 07:50:44,700 - Step 3: Created step_acts
2025-02-11 07:50:44,700 - Step 3: Added to generation_acts
2025-02-11 07:50:44,700 - Step 3: Updated recent_tokens
2025-02-11 07:50:44,702 - Step 3: Decoded current text
2025-02-11 07:50:44,702 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:44,702 - 
Starting step 4
2025-02-11 07:50:44,702 - Current_ids device: cuda:0
2025-02-11 07:50:44,702 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,725 - Model output complete
2025-02-11 07:50:44,725 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:44,725 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,725 - Next token logits device: cuda:0
2025-02-11 07:50:44,725 - Entered do_sample
2025-02-11 07:50:44,725 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,728 - Probs max: 0.496826171875
2025-02-11 07:50:44,728 - Pre-cat
2025-02-11 07:50:44,729 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537]],
       device='cuda:0')
2025-02-11 07:50:44,730 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:50:44,730 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:44,731 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,731 - Step 4: Generated next token
2025-02-11 07:50:44,731 - Step 4: Updated current_ids
2025-02-11 07:50:44,731 - Step 4: Decoded token text:  hit
2025-02-11 07:50:44,731 - Step 4: Updated current_phrase
2025-02-11 07:50:44,731 - Step 4: Created step_acts
2025-02-11 07:50:44,731 - Step 4: Added to generation_acts
2025-02-11 07:50:44,731 - Step 4: Updated recent_tokens
2025-02-11 07:50:44,733 - Step 4: Decoded current text
2025-02-11 07:50:44,733 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:44,733 - 
Starting step 5
2025-02-11 07:50:44,733 - Current_ids device: cuda:0
2025-02-11 07:50:44,733 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,761 - Model output complete
2025-02-11 07:50:44,761 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:44,761 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,761 - Next token logits device: cuda:0
2025-02-11 07:50:44,761 - Entered do_sample
2025-02-11 07:50:44,761 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,764 - Probs max: 0.998046875
2025-02-11 07:50:44,765 - Pre-cat
2025-02-11 07:50:44,765 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201]],
       device='cuda:0')
2025-02-11 07:50:44,767 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:44,767 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:44,767 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,767 - Step 5: Generated next token
2025-02-11 07:50:44,767 - Step 5: Updated current_ids
2025-02-11 07:50:44,767 - Step 5: Decoded token text:  the
2025-02-11 07:50:44,768 - Step 5: Updated current_phrase
2025-02-11 07:50:44,768 - Step 5: Created step_acts
2025-02-11 07:50:44,768 - Step 5: Added to generation_acts
2025-02-11 07:50:44,769 - Step 5: Updated generated_texts
2025-02-11 07:50:44,769 - Step 5: Updated recent_tokens
2025-02-11 07:50:44,770 - Step 5: Decoded current text
2025-02-11 07:50:44,770 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:44,770 - 
Starting step 6
2025-02-11 07:50:44,770 - Current_ids device: cuda:0
2025-02-11 07:50:44,770 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,792 - Model output complete
2025-02-11 07:50:44,792 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:44,792 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,792 - Next token logits device: cuda:0
2025-02-11 07:50:44,793 - Entered do_sample
2025-02-11 07:50:44,793 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,795 - Probs max: 0.99951171875
2025-02-11 07:50:44,796 - Pre-cat
2025-02-11 07:50:44,796 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279]], device='cuda:0')
2025-02-11 07:50:44,798 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:44,799 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:44,799 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,799 - Step 6: Generated next token
2025-02-11 07:50:44,799 - Step 6: Updated current_ids
2025-02-11 07:50:44,799 - Step 6: Decoded token text:  wall
2025-02-11 07:50:44,799 - Step 6: Updated current_phrase
2025-02-11 07:50:44,800 - Step 6: Created step_acts
2025-02-11 07:50:44,800 - Step 6: Added to generation_acts
2025-02-11 07:50:44,800 - Step 6: Updated recent_tokens
2025-02-11 07:50:44,801 - Step 6: Decoded current text
2025-02-11 07:50:44,801 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:44,801 - 
Starting step 7
2025-02-11 07:50:44,801 - Current_ids device: cuda:0
2025-02-11 07:50:44,801 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,824 - Model output complete
2025-02-11 07:50:44,824 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:44,824 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,824 - Next token logits device: cuda:0
2025-02-11 07:50:44,824 - Entered do_sample
2025-02-11 07:50:44,824 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,827 - Probs max: 0.54638671875
2025-02-11 07:50:44,827 - Pre-cat
2025-02-11 07:50:44,827 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002]], device='cuda:0')
2025-02-11 07:50:44,830 - Next token: tensor([[421]], device='cuda:0')
2025-02-11 07:50:44,831 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:44,831 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,831 - Step 7: Generated next token
2025-02-11 07:50:44,831 - Step 7: Updated current_ids
2025-02-11 07:50:44,831 - Step 7: Decoded token text:  if
2025-02-11 07:50:44,831 - Step 7: Updated current_phrase
2025-02-11 07:50:44,831 - Step 7: Created step_acts
2025-02-11 07:50:44,831 - Step 7: Added to generation_acts
2025-02-11 07:50:44,832 - Step 7: Updated recent_tokens
2025-02-11 07:50:44,833 - Step 7: Decoded current text
2025-02-11 07:50:44,833 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:44,833 - 
Starting step 8
2025-02-11 07:50:44,833 - Current_ids device: cuda:0
2025-02-11 07:50:44,833 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,855 - Model output complete
2025-02-11 07:50:44,856 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:44,856 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,856 - Next token logits device: cuda:0
2025-02-11 07:50:44,856 - Entered do_sample
2025-02-11 07:50:44,856 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,858 - Probs max: 0.80517578125
2025-02-11 07:50:44,859 - Pre-cat
2025-02-11 07:50:44,859 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    421]], device='cuda:0')
2025-02-11 07:50:44,862 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:44,862 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:44,862 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,862 - Step 8: Generated next token
2025-02-11 07:50:44,862 - Step 8: Updated current_ids
2025-02-11 07:50:44,863 - Step 8: Decoded token text:  the
2025-02-11 07:50:44,863 - Step 8: Updated current_phrase
2025-02-11 07:50:44,863 - Step 8: Created step_acts
2025-02-11 07:50:44,863 - Step 8: Added to generation_acts
2025-02-11 07:50:44,863 - Step 8: Updated recent_tokens
2025-02-11 07:50:44,864 - Step 8: Decoded current text
2025-02-11 07:50:44,864 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:44,864 - 
Starting step 9
2025-02-11 07:50:44,865 - Current_ids device: cuda:0
2025-02-11 07:50:44,865 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,888 - Model output complete
2025-02-11 07:50:44,888 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:44,888 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,888 - Next token logits device: cuda:0
2025-02-11 07:50:44,888 - Entered do_sample
2025-02-11 07:50:44,888 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,891 - Probs max: 0.7568359375
2025-02-11 07:50:44,892 - Pre-cat
2025-02-11 07:50:44,892 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    421,    279]], device='cuda:0')
2025-02-11 07:50:44,895 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:50:44,895 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:44,895 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,895 - Step 9: Generated next token
2025-02-11 07:50:44,895 - Step 9: Updated current_ids
2025-02-11 07:50:44,896 - Step 9: Decoded token text:  speed
2025-02-11 07:50:44,896 - Step 9: Updated current_phrase
2025-02-11 07:50:44,896 - Step 9: Created step_acts
2025-02-11 07:50:44,896 - Step 9: Added to generation_acts
2025-02-11 07:50:44,896 - Step 9: Updated recent_tokens
2025-02-11 07:50:44,897 - Step 9: Decoded current text
2025-02-11 07:50:44,898 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:44,898 - 
Starting step 10
2025-02-11 07:50:44,898 - Current_ids device: cuda:0
2025-02-11 07:50:44,898 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,919 - Model output complete
2025-02-11 07:50:44,919 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:44,920 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,920 - Next token logits device: cuda:0
2025-02-11 07:50:44,920 - Entered do_sample
2025-02-11 07:50:44,920 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,923 - Probs max: 0.814453125
2025-02-11 07:50:44,924 - Pre-cat
2025-02-11 07:50:44,924 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    421,    279,   4628]], device='cuda:0')
2025-02-11 07:50:44,926 - Next token: tensor([[315]], device='cuda:0')
2025-02-11 07:50:44,927 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:44,927 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,927 - Step 10: Generated next token
2025-02-11 07:50:44,927 - Step 10: Updated current_ids
2025-02-11 07:50:44,927 - Step 10: Decoded token text:  of
2025-02-11 07:50:44,927 - Step 10: Updated current_phrase
2025-02-11 07:50:44,928 - Step 10: Created step_acts
2025-02-11 07:50:44,928 - Step 10: Added to generation_acts
2025-02-11 07:50:44,929 - Step 10: Updated generated_texts
2025-02-11 07:50:44,929 - Step 10: Updated recent_tokens
2025-02-11 07:50:44,929 - Step 10: Decoded current text
2025-02-11 07:50:44,929 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:44,929 - 
Starting step 11
2025-02-11 07:50:44,929 - Current_ids device: cuda:0
2025-02-11 07:50:44,930 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,952 - Model output complete
2025-02-11 07:50:44,952 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:44,952 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,952 - Next token logits device: cuda:0
2025-02-11 07:50:44,952 - Entered do_sample
2025-02-11 07:50:44,952 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,955 - Probs max: 0.99658203125
2025-02-11 07:50:44,956 - Pre-cat
2025-02-11 07:50:44,956 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    421,    279,   4628,    315]], device='cuda:0')
2025-02-11 07:50:44,959 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:44,959 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:44,959 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,959 - Step 11: Generated next token
2025-02-11 07:50:44,959 - Step 11: Updated current_ids
2025-02-11 07:50:44,960 - Step 11: Decoded token text:  the
2025-02-11 07:50:44,960 - Step 11: Updated current_phrase
2025-02-11 07:50:44,960 - Step 11: Created step_acts
2025-02-11 07:50:44,960 - Step 11: Added to generation_acts
2025-02-11 07:50:44,960 - Step 11: Updated recent_tokens
2025-02-11 07:50:44,961 - Step 11: Decoded current text
2025-02-11 07:50:44,962 - Step 11: Incremented consecutive_fillers to 1
2025-02-11 07:50:44,962 - 
Starting step 12
2025-02-11 07:50:44,962 - Current_ids device: cuda:0
2025-02-11 07:50:44,962 - Current_ids dtype: torch.int64
2025-02-11 07:50:44,985 - Model output complete
2025-02-11 07:50:44,985 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:44,985 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,985 - Next token logits device: cuda:0
2025-02-11 07:50:44,985 - Entered do_sample
2025-02-11 07:50:44,985 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:44,988 - Probs max: 0.7275390625
2025-02-11 07:50:44,988 - Pre-cat
2025-02-11 07:50:44,989 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    421,    279,   4628,    315,    279]],
       device='cuda:0')
2025-02-11 07:50:44,991 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:44,991 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:44,991 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:44,991 - Step 12: Generated next token
2025-02-11 07:50:44,991 - Step 12: Updated current_ids
2025-02-11 07:50:44,991 - Step 12: Decoded token text:  wall
2025-02-11 07:50:44,991 - Step 12: Updated current_phrase
2025-02-11 07:50:44,992 - Step 12: Created step_acts
2025-02-11 07:50:44,992 - Step 12: Added to generation_acts
2025-02-11 07:50:44,992 - Step 12: Updated recent_tokens
2025-02-11 07:50:44,993 - Step 12: Decoded current text
2025-02-11 07:50:44,993 - Step 12: Incremented consecutive_fillers to 2
2025-02-11 07:50:44,993 - 
Starting step 13
2025-02-11 07:50:44,993 - Current_ids device: cuda:0
2025-02-11 07:50:44,993 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,017 - Model output complete
2025-02-11 07:50:45,017 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:45,017 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,017 - Next token logits device: cuda:0
2025-02-11 07:50:45,017 - Entered do_sample
2025-02-11 07:50:45,017 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,020 - Probs max: 0.98583984375
2025-02-11 07:50:45,021 - Pre-cat
2025-02-11 07:50:45,021 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    421,    279,   4628,    315,    279,   7002]],
       device='cuda:0')
2025-02-11 07:50:45,023 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:45,023 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:45,023 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,023 - Step 13: Generated next token
2025-02-11 07:50:45,024 - Step 13: Updated current_ids
2025-02-11 07:50:45,024 - Step 13: Decoded token text:  is
2025-02-11 07:50:45,024 - Step 13: Updated current_phrase
2025-02-11 07:50:45,024 - Step 13: Created step_acts
2025-02-11 07:50:45,024 - Step 13: Added to generation_acts
2025-02-11 07:50:45,024 - Step 13: Updated recent_tokens
2025-02-11 07:50:45,026 - Step 13: Decoded current text
2025-02-11 07:50:45,026 - Step 13: Incremented consecutive_fillers to 3
2025-02-11 07:50:45,127 - 
Starting step 0
2025-02-11 07:50:45,127 - Current_ids device: cuda:0
2025-02-11 07:50:45,127 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,153 - Model output complete
2025-02-11 07:50:45,153 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:45,153 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,153 - Next token logits device: cuda:0
2025-02-11 07:50:45,153 - Entered do_sample
2025-02-11 07:50:45,153 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,155 - Probs max: 0.50341796875
2025-02-11 07:50:45,156 - Pre-cat
2025-02-11 07:50:45,156 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:45,157 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:45,158 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:45,158 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,158 - Step 0: Generated next token
2025-02-11 07:50:45,158 - Step 0: Updated current_ids
2025-02-11 07:50:45,158 - Step 0: Decoded token text:  The
2025-02-11 07:50:45,158 - Step 0: Updated current_phrase
2025-02-11 07:50:45,158 - Step 0: Created step_acts
2025-02-11 07:50:45,159 - Step 0: Added to generation_acts
2025-02-11 07:50:45,160 - Step 0: Updated generated_texts
2025-02-11 07:50:45,160 - Step 0: Updated recent_tokens
2025-02-11 07:50:45,160 - Step 0: Decoded current text
2025-02-11 07:50:45,160 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:45,160 - 
Starting step 1
2025-02-11 07:50:45,160 - Current_ids device: cuda:0
2025-02-11 07:50:45,160 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,184 - Model output complete
2025-02-11 07:50:45,184 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:45,184 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,184 - Next token logits device: cuda:0
2025-02-11 07:50:45,184 - Entered do_sample
2025-02-11 07:50:45,184 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,187 - Probs max: 0.83837890625
2025-02-11 07:50:45,188 - Pre-cat
2025-02-11 07:50:45,188 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:50:45,189 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:45,190 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:45,190 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,190 - Step 1: Generated next token
2025-02-11 07:50:45,190 - Step 1: Updated current_ids
2025-02-11 07:50:45,190 - Step 1: Decoded token text:  ball
2025-02-11 07:50:45,190 - Step 1: Updated current_phrase
2025-02-11 07:50:45,190 - Step 1: Created step_acts
2025-02-11 07:50:45,190 - Step 1: Added to generation_acts
2025-02-11 07:50:45,190 - Step 1: Updated recent_tokens
2025-02-11 07:50:45,192 - Step 1: Decoded current text
2025-02-11 07:50:45,192 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:45,192 - 
Starting step 2
2025-02-11 07:50:45,192 - Current_ids device: cuda:0
2025-02-11 07:50:45,192 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,214 - Model output complete
2025-02-11 07:50:45,214 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:45,214 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,214 - Next token logits device: cuda:0
2025-02-11 07:50:45,214 - Entered do_sample
2025-02-11 07:50:45,214 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,218 - Probs max: 0.583984375
2025-02-11 07:50:45,218 - Pre-cat
2025-02-11 07:50:45,218 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:50:45,220 - Next token: tensor([[646]], device='cuda:0')
2025-02-11 07:50:45,220 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:45,220 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,220 - Step 2: Generated next token
2025-02-11 07:50:45,220 - Step 2: Updated current_ids
2025-02-11 07:50:45,220 - Step 2: Decoded token text:  can
2025-02-11 07:50:45,220 - Step 2: Updated current_phrase
2025-02-11 07:50:45,221 - Step 2: Created step_acts
2025-02-11 07:50:45,221 - Step 2: Added to generation_acts
2025-02-11 07:50:45,221 - Step 2: Updated recent_tokens
2025-02-11 07:50:45,222 - Step 2: Decoded current text
2025-02-11 07:50:45,222 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:45,222 - 
Starting step 3
2025-02-11 07:50:45,222 - Current_ids device: cuda:0
2025-02-11 07:50:45,222 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,245 - Model output complete
2025-02-11 07:50:45,245 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:45,245 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,245 - Next token logits device: cuda:0
2025-02-11 07:50:45,245 - Entered do_sample
2025-02-11 07:50:45,245 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,248 - Probs max: 0.45263671875
2025-02-11 07:50:45,248 - Pre-cat
2025-02-11 07:50:45,249 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646]],
       device='cuda:0')
2025-02-11 07:50:45,250 - Next token: tensor([[944]], device='cuda:0')
2025-02-11 07:50:45,250 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:45,250 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,250 - Step 3: Generated next token
2025-02-11 07:50:45,250 - Step 3: Updated current_ids
2025-02-11 07:50:45,251 - Step 3: Decoded token text: 't
2025-02-11 07:50:45,251 - Step 3: Updated current_phrase
2025-02-11 07:50:45,251 - Step 3: Created step_acts
2025-02-11 07:50:45,251 - Step 3: Added to generation_acts
2025-02-11 07:50:45,251 - Step 3: Updated recent_tokens
2025-02-11 07:50:45,252 - Step 3: Decoded current text
2025-02-11 07:50:45,252 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:45,252 - 
Starting step 4
2025-02-11 07:50:45,253 - Current_ids device: cuda:0
2025-02-11 07:50:45,253 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,275 - Model output complete
2025-02-11 07:50:45,275 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:45,275 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,275 - Next token logits device: cuda:0
2025-02-11 07:50:45,276 - Entered do_sample
2025-02-11 07:50:45,276 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,279 - Probs max: 0.515625
2025-02-11 07:50:45,281 - Pre-cat
2025-02-11 07:50:45,281 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944]],
       device='cuda:0')
2025-02-11 07:50:45,284 - Next token: tensor([[5545]], device='cuda:0')
2025-02-11 07:50:45,284 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:45,284 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,284 - Step 4: Generated next token
2025-02-11 07:50:45,285 - Step 4: Updated current_ids
2025-02-11 07:50:45,285 - Step 4: Decoded token text:  reach
2025-02-11 07:50:45,285 - Step 4: Updated current_phrase
2025-02-11 07:50:45,285 - Step 4: Created step_acts
2025-02-11 07:50:45,286 - Step 4: Added to generation_acts
2025-02-11 07:50:45,286 - Step 4: Updated recent_tokens
2025-02-11 07:50:45,287 - Step 4: Decoded current text
2025-02-11 07:50:45,287 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:45,287 - 
Starting step 5
2025-02-11 07:50:45,287 - Current_ids device: cuda:0
2025-02-11 07:50:45,287 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,330 - Model output complete
2025-02-11 07:50:45,330 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:45,330 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,330 - Next token logits device: cuda:0
2025-02-11 07:50:45,330 - Entered do_sample
2025-02-11 07:50:45,331 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,333 - Probs max: 0.99560546875
2025-02-11 07:50:45,333 - Pre-cat
2025-02-11 07:50:45,333 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545]],
       device='cuda:0')
2025-02-11 07:50:45,335 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:45,335 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:45,335 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,335 - Step 5: Generated next token
2025-02-11 07:50:45,335 - Step 5: Updated current_ids
2025-02-11 07:50:45,335 - Step 5: Decoded token text:  the
2025-02-11 07:50:45,335 - Step 5: Updated current_phrase
2025-02-11 07:50:45,336 - Step 5: Created step_acts
2025-02-11 07:50:45,336 - Step 5: Added to generation_acts
2025-02-11 07:50:45,337 - Step 5: Updated generated_texts
2025-02-11 07:50:45,337 - Step 5: Updated recent_tokens
2025-02-11 07:50:45,337 - Step 5: Decoded current text
2025-02-11 07:50:45,337 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:45,337 - 
Starting step 6
2025-02-11 07:50:45,338 - Current_ids device: cuda:0
2025-02-11 07:50:45,338 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,360 - Model output complete
2025-02-11 07:50:45,360 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:45,360 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,360 - Next token logits device: cuda:0
2025-02-11 07:50:45,360 - Entered do_sample
2025-02-11 07:50:45,360 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,364 - Probs max: 0.9990234375
2025-02-11 07:50:45,364 - Pre-cat
2025-02-11 07:50:45,364 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279]], device='cuda:0')
2025-02-11 07:50:45,366 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:45,366 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:45,366 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,366 - Step 6: Generated next token
2025-02-11 07:50:45,366 - Step 6: Updated current_ids
2025-02-11 07:50:45,366 - Step 6: Decoded token text:  wall
2025-02-11 07:50:45,366 - Step 6: Updated current_phrase
2025-02-11 07:50:45,367 - Step 6: Created step_acts
2025-02-11 07:50:45,367 - Step 6: Added to generation_acts
2025-02-11 07:50:45,367 - Step 6: Updated recent_tokens
2025-02-11 07:50:45,368 - Step 6: Decoded current text
2025-02-11 07:50:45,368 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:45,368 - 
Starting step 7
2025-02-11 07:50:45,368 - Current_ids device: cuda:0
2025-02-11 07:50:45,368 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,391 - Model output complete
2025-02-11 07:50:45,392 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:45,392 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,392 - Next token logits device: cuda:0
2025-02-11 07:50:45,392 - Entered do_sample
2025-02-11 07:50:45,392 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,395 - Probs max: 0.8115234375
2025-02-11 07:50:45,395 - Pre-cat
2025-02-11 07:50:45,396 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002]], device='cuda:0')
2025-02-11 07:50:45,397 - Next token: tensor([[1576]], device='cuda:0')
2025-02-11 07:50:45,397 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:45,397 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,397 - Step 7: Generated next token
2025-02-11 07:50:45,397 - Step 7: Updated current_ids
2025-02-11 07:50:45,398 - Step 7: Decoded token text:  because
2025-02-11 07:50:45,398 - Step 7: Updated current_phrase
2025-02-11 07:50:45,398 - Step 7: Created step_acts
2025-02-11 07:50:45,398 - Step 7: Added to generation_acts
2025-02-11 07:50:45,398 - Step 7: Updated recent_tokens
2025-02-11 07:50:45,399 - Step 7: Decoded current text
2025-02-11 07:50:45,399 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:45,399 - 
Starting step 8
2025-02-11 07:50:45,399 - Current_ids device: cuda:0
2025-02-11 07:50:45,400 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,422 - Model output complete
2025-02-11 07:50:45,423 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:45,423 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,423 - Next token logits device: cuda:0
2025-02-11 07:50:45,423 - Entered do_sample
2025-02-11 07:50:45,423 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,426 - Probs max: 0.5009765625
2025-02-11 07:50:45,427 - Pre-cat
2025-02-11 07:50:45,427 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576]], device='cuda:0')
2025-02-11 07:50:45,428 - Next token: tensor([[432]], device='cuda:0')
2025-02-11 07:50:45,428 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:45,429 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,429 - Step 8: Generated next token
2025-02-11 07:50:45,429 - Step 8: Updated current_ids
2025-02-11 07:50:45,429 - Step 8: Decoded token text:  it
2025-02-11 07:50:45,429 - Step 8: Updated current_phrase
2025-02-11 07:50:45,429 - Step 8: Created step_acts
2025-02-11 07:50:45,429 - Step 8: Added to generation_acts
2025-02-11 07:50:45,429 - Step 8: Updated recent_tokens
2025-02-11 07:50:45,431 - Step 8: Decoded current text
2025-02-11 07:50:45,431 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:45,431 - 
Starting step 9
2025-02-11 07:50:45,431 - Current_ids device: cuda:0
2025-02-11 07:50:45,431 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,453 - Model output complete
2025-02-11 07:50:45,453 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:45,453 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,454 - Next token logits device: cuda:0
2025-02-11 07:50:45,454 - Entered do_sample
2025-02-11 07:50:45,454 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,458 - Probs max: 0.7900390625
2025-02-11 07:50:45,458 - Pre-cat
2025-02-11 07:50:45,458 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432]], device='cuda:0')
2025-02-11 07:50:45,460 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:50:45,460 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:45,460 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,460 - Step 9: Generated next token
2025-02-11 07:50:45,460 - Step 9: Updated current_ids
2025-02-11 07:50:45,461 - Step 9: Decoded token text: 's
2025-02-11 07:50:45,461 - Step 9: Updated current_phrase
2025-02-11 07:50:45,461 - Step 9: Created step_acts
2025-02-11 07:50:45,461 - Step 9: Added to generation_acts
2025-02-11 07:50:45,461 - Step 9: Updated recent_tokens
2025-02-11 07:50:45,462 - Step 9: Decoded current text
2025-02-11 07:50:45,462 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:45,462 - 
Starting step 10
2025-02-11 07:50:45,462 - Current_ids device: cuda:0
2025-02-11 07:50:45,463 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,490 - Model output complete
2025-02-11 07:50:45,490 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:45,490 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,490 - Next token logits device: cuda:0
2025-02-11 07:50:45,490 - Entered do_sample
2025-02-11 07:50:45,490 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,493 - Probs max: 0.28857421875
2025-02-11 07:50:45,494 - Pre-cat
2025-02-11 07:50:45,494 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594]], device='cuda:0')
2025-02-11 07:50:45,496 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:45,496 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:45,496 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,496 - Step 10: Generated next token
2025-02-11 07:50:45,496 - Step 10: Updated current_ids
2025-02-11 07:50:45,496 - Step 10: Decoded token text:  very
2025-02-11 07:50:45,496 - Step 10: Updated current_phrase
2025-02-11 07:50:45,496 - Step 10: Created step_acts
2025-02-11 07:50:45,497 - Step 10: Added to generation_acts
2025-02-11 07:50:45,498 - Step 10: Updated generated_texts
2025-02-11 07:50:45,498 - Step 10: Updated recent_tokens
2025-02-11 07:50:45,498 - Step 10: Decoded current text
2025-02-11 07:50:45,498 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:45,498 - 
Starting step 11
2025-02-11 07:50:45,498 - Current_ids device: cuda:0
2025-02-11 07:50:45,498 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,522 - Model output complete
2025-02-11 07:50:45,522 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:45,522 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,522 - Next token logits device: cuda:0
2025-02-11 07:50:45,522 - Entered do_sample
2025-02-11 07:50:45,522 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,525 - Probs max: 0.98486328125
2025-02-11 07:50:45,526 - Pre-cat
2025-02-11 07:50:45,526 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602]], device='cuda:0')
2025-02-11 07:50:45,527 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:45,528 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:45,528 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,528 - Step 11: Generated next token
2025-02-11 07:50:45,528 - Step 11: Updated current_ids
2025-02-11 07:50:45,528 - Step 11: Decoded token text:  fast
2025-02-11 07:50:45,528 - Step 11: Updated current_phrase
2025-02-11 07:50:45,528 - Step 11: Created step_acts
2025-02-11 07:50:45,528 - Step 11: Added to generation_acts
2025-02-11 07:50:45,529 - Step 11: Updated recent_tokens
2025-02-11 07:50:45,530 - Step 11: Decoded current text
2025-02-11 07:50:45,530 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:45,530 - 
Starting step 12
2025-02-11 07:50:45,530 - Current_ids device: cuda:0
2025-02-11 07:50:45,530 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,556 - Model output complete
2025-02-11 07:50:45,556 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:45,556 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,556 - Next token logits device: cuda:0
2025-02-11 07:50:45,556 - Entered do_sample
2025-02-11 07:50:45,556 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,558 - Probs max: 0.50537109375
2025-02-11 07:50:45,559 - Pre-cat
2025-02-11 07:50:45,559 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937]],
       device='cuda:0')
2025-02-11 07:50:45,561 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:45,561 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:45,561 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,561 - Step 12: Generated next token
2025-02-11 07:50:45,561 - Step 12: Updated current_ids
2025-02-11 07:50:45,561 - Step 12: Decoded token text: ,
2025-02-11 07:50:45,561 - Step 12: Updated current_phrase
2025-02-11 07:50:45,562 - Step 12: Created step_acts
2025-02-11 07:50:45,562 - Step 12: Added to generation_acts
2025-02-11 07:50:45,562 - Step 12: Updated recent_tokens
2025-02-11 07:50:45,563 - Step 12: Found phrase end token
2025-02-11 07:50:45,564 - Step 12: Updated recent_phrases
2025-02-11 07:50:45,564 - Step 12: Decoded current text
2025-02-11 07:50:45,564 - Step 12: Reset consecutive_fillers
2025-02-11 07:50:45,564 - 
Starting step 13
2025-02-11 07:50:45,564 - Current_ids device: cuda:0
2025-02-11 07:50:45,564 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,588 - Model output complete
2025-02-11 07:50:45,588 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:45,588 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,588 - Next token logits device: cuda:0
2025-02-11 07:50:45,588 - Entered do_sample
2025-02-11 07:50:45,588 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,591 - Probs max: 0.74755859375
2025-02-11 07:50:45,591 - Pre-cat
2025-02-11 07:50:45,592 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11]],
       device='cuda:0')
2025-02-11 07:50:45,593 - Next token: tensor([[714]], device='cuda:0')
2025-02-11 07:50:45,593 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:45,593 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,594 - Step 13: Generated next token
2025-02-11 07:50:45,594 - Step 13: Updated current_ids
2025-02-11 07:50:45,594 - Step 13: Decoded token text:  but
2025-02-11 07:50:45,594 - Step 13: Updated current_phrase
2025-02-11 07:50:45,594 - Step 13: Created step_acts
2025-02-11 07:50:45,594 - Step 13: Added to generation_acts
2025-02-11 07:50:45,594 - Step 13: Updated recent_tokens
2025-02-11 07:50:45,596 - Step 13: Decoded current text
2025-02-11 07:50:45,596 - Step 13: Reset consecutive_fillers
2025-02-11 07:50:45,596 - 
Starting step 14
2025-02-11 07:50:45,596 - Current_ids device: cuda:0
2025-02-11 07:50:45,596 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,619 - Model output complete
2025-02-11 07:50:45,619 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:50:45,619 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,619 - Next token logits device: cuda:0
2025-02-11 07:50:45,619 - Entered do_sample
2025-02-11 07:50:45,619 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,622 - Probs max: 0.921875
2025-02-11 07:50:45,623 - Pre-cat
2025-02-11 07:50:45,623 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714]],
       device='cuda:0')
2025-02-11 07:50:45,625 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:45,625 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:50:45,625 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,625 - Step 14: Generated next token
2025-02-11 07:50:45,625 - Step 14: Updated current_ids
2025-02-11 07:50:45,625 - Step 14: Decoded token text:  the
2025-02-11 07:50:45,626 - Step 14: Updated current_phrase
2025-02-11 07:50:45,626 - Step 14: Created step_acts
2025-02-11 07:50:45,626 - Step 14: Added to generation_acts
2025-02-11 07:50:45,626 - Step 14: Updated recent_tokens
2025-02-11 07:50:45,628 - Step 14: Decoded current text
2025-02-11 07:50:45,628 - Step 14: Reset consecutive_fillers
2025-02-11 07:50:45,628 - 
Starting step 15
2025-02-11 07:50:45,628 - Current_ids device: cuda:0
2025-02-11 07:50:45,628 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,652 - Model output complete
2025-02-11 07:50:45,652 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:50:45,653 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,653 - Next token logits device: cuda:0
2025-02-11 07:50:45,653 - Entered do_sample
2025-02-11 07:50:45,653 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,655 - Probs max: 0.9609375
2025-02-11 07:50:45,656 - Pre-cat
2025-02-11 07:50:45,656 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279]], device='cuda:0')
2025-02-11 07:50:45,657 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:45,658 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:50:45,658 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,658 - Step 15: Generated next token
2025-02-11 07:50:45,658 - Step 15: Updated current_ids
2025-02-11 07:50:45,658 - Step 15: Decoded token text:  wall
2025-02-11 07:50:45,658 - Step 15: Updated current_phrase
2025-02-11 07:50:45,659 - Step 15: Created step_acts
2025-02-11 07:50:45,659 - Step 15: Added to generation_acts
2025-02-11 07:50:45,661 - Step 15: Updated generated_texts
2025-02-11 07:50:45,661 - Step 15: Updated recent_tokens
2025-02-11 07:50:45,661 - Step 15: Decoded current text
2025-02-11 07:50:45,661 - Step 15: Reset consecutive_fillers
2025-02-11 07:50:45,661 - 
Starting step 16
2025-02-11 07:50:45,662 - Current_ids device: cuda:0
2025-02-11 07:50:45,662 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,710 - Model output complete
2025-02-11 07:50:45,711 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:50:45,711 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,711 - Next token logits device: cuda:0
2025-02-11 07:50:45,711 - Entered do_sample
2025-02-11 07:50:45,711 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,713 - Probs max: 0.85693359375
2025-02-11 07:50:45,714 - Pre-cat
2025-02-11 07:50:45,714 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002]], device='cuda:0')
2025-02-11 07:50:45,716 - Next token: tensor([[646]], device='cuda:0')
2025-02-11 07:50:45,716 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:50:45,716 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,716 - Step 16: Generated next token
2025-02-11 07:50:45,716 - Step 16: Updated current_ids
2025-02-11 07:50:45,716 - Step 16: Decoded token text:  can
2025-02-11 07:50:45,716 - Step 16: Updated current_phrase
2025-02-11 07:50:45,717 - Step 16: Created step_acts
2025-02-11 07:50:45,717 - Step 16: Added to generation_acts
2025-02-11 07:50:45,717 - Step 16: Updated recent_tokens
2025-02-11 07:50:45,718 - Step 16: Decoded current text
2025-02-11 07:50:45,718 - Step 16: Reset consecutive_fillers
2025-02-11 07:50:45,726 - Step 16: Calculated unique_ratio: 0.8125
2025-02-11 07:50:45,726 - 
Starting step 17
2025-02-11 07:50:45,726 - Current_ids device: cuda:0
2025-02-11 07:50:45,726 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,750 - Model output complete
2025-02-11 07:50:45,750 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:50:45,750 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,750 - Next token logits device: cuda:0
2025-02-11 07:50:45,750 - Entered do_sample
2025-02-11 07:50:45,750 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,753 - Probs max: 0.97216796875
2025-02-11 07:50:45,753 - Pre-cat
2025-02-11 07:50:45,753 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646]], device='cuda:0')
2025-02-11 07:50:45,756 - Next token: tensor([[944]], device='cuda:0')
2025-02-11 07:50:45,756 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:50:45,756 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,756 - Step 17: Generated next token
2025-02-11 07:50:45,756 - Step 17: Updated current_ids
2025-02-11 07:50:45,756 - Step 17: Decoded token text: 't
2025-02-11 07:50:45,757 - Step 17: Updated current_phrase
2025-02-11 07:50:45,757 - Step 17: Created step_acts
2025-02-11 07:50:45,757 - Step 17: Added to generation_acts
2025-02-11 07:50:45,757 - Step 17: Updated recent_tokens
2025-02-11 07:50:45,758 - Step 17: Decoded current text
2025-02-11 07:50:45,758 - Step 17: Reset consecutive_fillers
2025-02-11 07:50:45,759 - Step 17: Calculated unique_ratio: 0.75
2025-02-11 07:50:45,759 - 
Starting step 18
2025-02-11 07:50:45,759 - Current_ids device: cuda:0
2025-02-11 07:50:45,759 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,781 - Model output complete
2025-02-11 07:50:45,781 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:50:45,781 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,782 - Next token logits device: cuda:0
2025-02-11 07:50:45,782 - Entered do_sample
2025-02-11 07:50:45,782 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,785 - Probs max: 0.6083984375
2025-02-11 07:50:45,785 - Pre-cat
2025-02-11 07:50:45,785 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944]], device='cuda:0')
2025-02-11 07:50:45,788 - Next token: tensor([[387]], device='cuda:0')
2025-02-11 07:50:45,789 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:50:45,789 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,789 - Step 18: Generated next token
2025-02-11 07:50:45,789 - Step 18: Updated current_ids
2025-02-11 07:50:45,789 - Step 18: Decoded token text:  be
2025-02-11 07:50:45,789 - Step 18: Updated current_phrase
2025-02-11 07:50:45,789 - Step 18: Created step_acts
2025-02-11 07:50:45,790 - Step 18: Added to generation_acts
2025-02-11 07:50:45,790 - Step 18: Updated recent_tokens
2025-02-11 07:50:45,791 - Step 18: Decoded current text
2025-02-11 07:50:45,791 - Step 18: Reset consecutive_fillers
2025-02-11 07:50:45,791 - Step 18: Calculated unique_ratio: 0.8125
2025-02-11 07:50:45,791 - 
Starting step 19
2025-02-11 07:50:45,791 - Current_ids device: cuda:0
2025-02-11 07:50:45,791 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,820 - Model output complete
2025-02-11 07:50:45,820 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:50:45,820 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,820 - Next token logits device: cuda:0
2025-02-11 07:50:45,820 - Entered do_sample
2025-02-11 07:50:45,820 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,822 - Probs max: 0.63232421875
2025-02-11 07:50:45,823 - Pre-cat
2025-02-11 07:50:45,823 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944,    387]], device='cuda:0')
2025-02-11 07:50:45,825 - Next token: tensor([[8643]], device='cuda:0')
2025-02-11 07:50:45,825 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:50:45,825 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,825 - Step 19: Generated next token
2025-02-11 07:50:45,825 - Step 19: Updated current_ids
2025-02-11 07:50:45,826 - Step 19: Decoded token text:  reached
2025-02-11 07:50:45,826 - Step 19: Updated current_phrase
2025-02-11 07:50:45,826 - Step 19: Created step_acts
2025-02-11 07:50:45,826 - Step 19: Added to generation_acts
2025-02-11 07:50:45,826 - Step 19: Updated recent_tokens
2025-02-11 07:50:45,827 - Step 19: Decoded current text
2025-02-11 07:50:45,828 - Step 19: Reset consecutive_fillers
2025-02-11 07:50:45,828 - Step 19: Calculated unique_ratio: 0.875
2025-02-11 07:50:45,828 - 
Starting step 20
2025-02-11 07:50:45,828 - Current_ids device: cuda:0
2025-02-11 07:50:45,828 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,851 - Model output complete
2025-02-11 07:50:45,851 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:50:45,851 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,851 - Next token logits device: cuda:0
2025-02-11 07:50:45,851 - Entered do_sample
2025-02-11 07:50:45,851 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,855 - Probs max: 0.415283203125
2025-02-11 07:50:45,855 - Pre-cat
2025-02-11 07:50:45,855 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944,    387,   8643]], device='cuda:0')
2025-02-11 07:50:45,858 - Next token: tensor([[1576]], device='cuda:0')
2025-02-11 07:50:45,858 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:50:45,858 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,858 - Step 20: Generated next token
2025-02-11 07:50:45,859 - Step 20: Updated current_ids
2025-02-11 07:50:45,859 - Step 20: Decoded token text:  because
2025-02-11 07:50:45,859 - Step 20: Updated current_phrase
2025-02-11 07:50:45,859 - Step 20: Created step_acts
2025-02-11 07:50:45,859 - Step 20: Added to generation_acts
2025-02-11 07:50:45,861 - Step 20: Updated generated_texts
2025-02-11 07:50:45,861 - Step 20: Updated recent_tokens
2025-02-11 07:50:45,861 - Step 20: Decoded current text
2025-02-11 07:50:45,861 - Step 20: Reset consecutive_fillers
2025-02-11 07:50:45,861 - Step 20: Calculated unique_ratio: 0.8125
2025-02-11 07:50:45,861 - 
Starting step 21
2025-02-11 07:50:45,861 - Current_ids device: cuda:0
2025-02-11 07:50:45,861 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,886 - Model output complete
2025-02-11 07:50:45,887 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:50:45,887 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,887 - Next token logits device: cuda:0
2025-02-11 07:50:45,887 - Entered do_sample
2025-02-11 07:50:45,887 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,889 - Probs max: 0.865234375
2025-02-11 07:50:45,890 - Pre-cat
2025-02-11 07:50:45,890 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944,    387,   8643,   1576]],
       device='cuda:0')
2025-02-11 07:50:45,892 - Next token: tensor([[432]], device='cuda:0')
2025-02-11 07:50:45,892 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:50:45,893 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,893 - Step 21: Generated next token
2025-02-11 07:50:45,893 - Step 21: Updated current_ids
2025-02-11 07:50:45,893 - Step 21: Decoded token text:  it
2025-02-11 07:50:45,893 - Step 21: Updated current_phrase
2025-02-11 07:50:45,893 - Step 21: Created step_acts
2025-02-11 07:50:45,893 - Step 21: Added to generation_acts
2025-02-11 07:50:45,893 - Step 21: Updated recent_tokens
2025-02-11 07:50:45,895 - Step 21: Decoded current text
2025-02-11 07:50:45,895 - Step 21: Reset consecutive_fillers
2025-02-11 07:50:45,895 - Step 21: Calculated unique_ratio: 0.8125
2025-02-11 07:50:45,895 - 
Starting step 22
2025-02-11 07:50:45,895 - Current_ids device: cuda:0
2025-02-11 07:50:45,895 - Current_ids dtype: torch.int64
2025-02-11 07:50:45,970 - Model output complete
2025-02-11 07:50:45,970 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:50:45,970 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,970 - Next token logits device: cuda:0
2025-02-11 07:50:45,970 - Entered do_sample
2025-02-11 07:50:45,970 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:45,972 - Probs max: 0.98681640625
2025-02-11 07:50:45,973 - Pre-cat
2025-02-11 07:50:45,973 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944,    387,   8643,   1576,    432]],
       device='cuda:0')
2025-02-11 07:50:45,975 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:50:45,976 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:50:45,976 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:45,976 - Step 22: Generated next token
2025-02-11 07:50:45,976 - Step 22: Updated current_ids
2025-02-11 07:50:45,976 - Step 22: Decoded token text: 's
2025-02-11 07:50:45,976 - Step 22: Updated current_phrase
2025-02-11 07:50:45,976 - Step 22: Created step_acts
2025-02-11 07:50:45,976 - Step 22: Added to generation_acts
2025-02-11 07:50:45,976 - Step 22: Updated recent_tokens
2025-02-11 07:50:45,978 - Step 22: Decoded current text
2025-02-11 07:50:45,978 - Step 22: Reset consecutive_fillers
2025-02-11 07:50:45,978 - Step 22: Calculated unique_ratio: 0.8125
2025-02-11 07:50:45,978 - 
Starting step 23
2025-02-11 07:50:45,979 - Current_ids device: cuda:0
2025-02-11 07:50:45,979 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,002 - Model output complete
2025-02-11 07:50:46,002 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:50:46,002 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,002 - Next token logits device: cuda:0
2025-02-11 07:50:46,002 - Entered do_sample
2025-02-11 07:50:46,002 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,005 - Probs max: 0.8935546875
2025-02-11 07:50:46,006 - Pre-cat
2025-02-11 07:50:46,006 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944,    387,   8643,   1576,    432,    594]],
       device='cuda:0')
2025-02-11 07:50:46,007 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:46,008 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:50:46,008 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,008 - Step 23: Generated next token
2025-02-11 07:50:46,008 - Step 23: Updated current_ids
2025-02-11 07:50:46,008 - Step 23: Decoded token text:  very
2025-02-11 07:50:46,008 - Step 23: Updated current_phrase
2025-02-11 07:50:46,008 - Step 23: Created step_acts
2025-02-11 07:50:46,009 - Step 23: Added to generation_acts
2025-02-11 07:50:46,009 - Step 23: Updated recent_tokens
2025-02-11 07:50:46,010 - Step 23: Decoded current text
2025-02-11 07:50:46,010 - Step 23: Reset consecutive_fillers
2025-02-11 07:50:46,011 - Step 23: Calculated unique_ratio: 0.8125
2025-02-11 07:50:46,011 - 
Starting step 24
2025-02-11 07:50:46,011 - Current_ids device: cuda:0
2025-02-11 07:50:46,011 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,036 - Model output complete
2025-02-11 07:50:46,036 - Logits shape: torch.Size([1, 55, 151936])
2025-02-11 07:50:46,036 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,036 - Next token logits device: cuda:0
2025-02-11 07:50:46,036 - Entered do_sample
2025-02-11 07:50:46,036 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,038 - Probs max: 0.78466796875
2025-02-11 07:50:46,039 - Pre-cat
2025-02-11 07:50:46,039 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944,    387,   8643,   1576,    432,    594,
           1602]], device='cuda:0')
2025-02-11 07:50:46,043 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:46,043 - Current_ids shape: torch.Size([1, 55])
2025-02-11 07:50:46,043 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,043 - Step 24: Generated next token
2025-02-11 07:50:46,043 - Step 24: Updated current_ids
2025-02-11 07:50:46,044 - Step 24: Decoded token text:  fast
2025-02-11 07:50:46,044 - Step 24: Updated current_phrase
2025-02-11 07:50:46,044 - Step 24: Created step_acts
2025-02-11 07:50:46,044 - Step 24: Added to generation_acts
2025-02-11 07:50:46,044 - Step 24: Updated recent_tokens
2025-02-11 07:50:46,046 - Step 24: Decoded current text
2025-02-11 07:50:46,046 - Step 24: Reset consecutive_fillers
2025-02-11 07:50:46,046 - Step 24: Calculated unique_ratio: 0.8125
2025-02-11 07:50:46,046 - 
Starting step 25
2025-02-11 07:50:46,046 - Current_ids device: cuda:0
2025-02-11 07:50:46,046 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,071 - Model output complete
2025-02-11 07:50:46,071 - Logits shape: torch.Size([1, 56, 151936])
2025-02-11 07:50:46,072 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,072 - Next token logits device: cuda:0
2025-02-11 07:50:46,072 - Entered do_sample
2025-02-11 07:50:46,072 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,075 - Probs max: 0.76123046875
2025-02-11 07:50:46,076 - Pre-cat
2025-02-11 07:50:46,076 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944,    387,   8643,   1576,    432,    594,
           1602,   4937]], device='cuda:0')
2025-02-11 07:50:46,078 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:46,078 - Current_ids shape: torch.Size([1, 56])
2025-02-11 07:50:46,078 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,078 - Step 25: Generated next token
2025-02-11 07:50:46,079 - Step 25: Updated current_ids
2025-02-11 07:50:46,079 - Step 25: Decoded token text: .
2025-02-11 07:50:46,079 - Step 25: Updated current_phrase
2025-02-11 07:50:46,079 - Step 25: Created step_acts
2025-02-11 07:50:46,079 - Step 25: Added to generation_acts
2025-02-11 07:50:46,081 - Step 25: Updated generated_texts
2025-02-11 07:50:46,081 - Step 25: Updated recent_tokens
2025-02-11 07:50:46,081 - Step 25: Found phrase end token
2025-02-11 07:50:46,081 - Step 25: Updated recent_phrases
2025-02-11 07:50:46,081 - Step 25: Calculated similarity: 0.6666666666666666
2025-02-11 07:50:46,081 - Step 25: Decoded current text
2025-02-11 07:50:46,081 - Step 25: Reset consecutive_fillers
2025-02-11 07:50:46,081 - Step 25: Calculated unique_ratio: 0.875
2025-02-11 07:50:46,081 - 
Starting step 26
2025-02-11 07:50:46,081 - Current_ids device: cuda:0
2025-02-11 07:50:46,082 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,107 - Model output complete
2025-02-11 07:50:46,107 - Logits shape: torch.Size([1, 57, 151936])
2025-02-11 07:50:46,107 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,107 - Next token logits device: cuda:0
2025-02-11 07:50:46,108 - Entered do_sample
2025-02-11 07:50:46,108 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,110 - Probs max: 0.412109375
2025-02-11 07:50:46,111 - Pre-cat
2025-02-11 07:50:46,111 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944,    387,   8643,   1576,    432,    594,
           1602,   4937,     13]], device='cuda:0')
2025-02-11 07:50:46,114 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:46,114 - Current_ids shape: torch.Size([1, 57])
2025-02-11 07:50:46,114 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,114 - Step 26: Generated next token
2025-02-11 07:50:46,114 - Step 26: Updated current_ids
2025-02-11 07:50:46,114 - Step 26: Decoded token text:  The
2025-02-11 07:50:46,114 - Step 26: Updated current_phrase
2025-02-11 07:50:46,115 - Step 26: Created step_acts
2025-02-11 07:50:46,115 - Step 26: Added to generation_acts
2025-02-11 07:50:46,115 - Step 26: Updated recent_tokens
2025-02-11 07:50:46,116 - Step 26: Decoded current text
2025-02-11 07:50:46,116 - Step 26: Reset consecutive_fillers
2025-02-11 07:50:46,117 - Step 26: Calculated unique_ratio: 0.9375
2025-02-11 07:50:46,117 - 
Starting step 27
2025-02-11 07:50:46,117 - Current_ids device: cuda:0
2025-02-11 07:50:46,117 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,143 - Model output complete
2025-02-11 07:50:46,143 - Logits shape: torch.Size([1, 58, 151936])
2025-02-11 07:50:46,143 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,143 - Next token logits device: cuda:0
2025-02-11 07:50:46,143 - Entered do_sample
2025-02-11 07:50:46,143 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,146 - Probs max: 0.47900390625
2025-02-11 07:50:46,146 - Pre-cat
2025-02-11 07:50:46,146 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944,    387,   8643,   1576,    432,    594,
           1602,   4937,     13,    576]], device='cuda:0')
2025-02-11 07:50:46,148 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:50:46,149 - Current_ids shape: torch.Size([1, 58])
2025-02-11 07:50:46,149 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,149 - Step 27: Generated next token
2025-02-11 07:50:46,149 - Step 27: Updated current_ids
2025-02-11 07:50:46,149 - Step 27: Decoded token text:  speed
2025-02-11 07:50:46,149 - Step 27: Updated current_phrase
2025-02-11 07:50:46,149 - Step 27: Created step_acts
2025-02-11 07:50:46,150 - Step 27: Added to generation_acts
2025-02-11 07:50:46,150 - Step 27: Updated recent_tokens
2025-02-11 07:50:46,151 - Step 27: Decoded current text
2025-02-11 07:50:46,151 - Step 27: Reset consecutive_fillers
2025-02-11 07:50:46,151 - Step 27: Calculated unique_ratio: 1.0
2025-02-11 07:50:46,151 - 
Starting step 28
2025-02-11 07:50:46,151 - Current_ids device: cuda:0
2025-02-11 07:50:46,151 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,175 - Model output complete
2025-02-11 07:50:46,175 - Logits shape: torch.Size([1, 59, 151936])
2025-02-11 07:50:46,175 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,175 - Next token logits device: cuda:0
2025-02-11 07:50:46,175 - Entered do_sample
2025-02-11 07:50:46,176 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,178 - Probs max: 0.88037109375
2025-02-11 07:50:46,179 - Pre-cat
2025-02-11 07:50:46,179 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944,    387,   8643,   1576,    432,    594,
           1602,   4937,     13,    576,   4628]], device='cuda:0')
2025-02-11 07:50:46,181 - Next token: tensor([[315]], device='cuda:0')
2025-02-11 07:50:46,181 - Current_ids shape: torch.Size([1, 59])
2025-02-11 07:50:46,182 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,182 - Step 28: Generated next token
2025-02-11 07:50:46,182 - Step 28: Updated current_ids
2025-02-11 07:50:46,182 - Step 28: Decoded token text:  of
2025-02-11 07:50:46,182 - Step 28: Updated current_phrase
2025-02-11 07:50:46,182 - Step 28: Created step_acts
2025-02-11 07:50:46,182 - Step 28: Added to generation_acts
2025-02-11 07:50:46,182 - Step 28: Updated recent_tokens
2025-02-11 07:50:46,184 - Step 28: Decoded current text
2025-02-11 07:50:46,184 - Step 28: Reset consecutive_fillers
2025-02-11 07:50:46,184 - Step 28: Calculated unique_ratio: 1.0
2025-02-11 07:50:46,184 - 
Starting step 29
2025-02-11 07:50:46,184 - Current_ids device: cuda:0
2025-02-11 07:50:46,184 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,209 - Model output complete
2025-02-11 07:50:46,209 - Logits shape: torch.Size([1, 60, 151936])
2025-02-11 07:50:46,209 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,209 - Next token logits device: cuda:0
2025-02-11 07:50:46,209 - Entered do_sample
2025-02-11 07:50:46,209 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,212 - Probs max: 0.95947265625
2025-02-11 07:50:46,213 - Pre-cat
2025-02-11 07:50:46,213 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944,    387,   8643,   1576,    432,    594,
           1602,   4937,     13,    576,   4628,    315]], device='cuda:0')
2025-02-11 07:50:46,215 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:46,215 - Current_ids shape: torch.Size([1, 60])
2025-02-11 07:50:46,215 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,215 - Step 29: Generated next token
2025-02-11 07:50:46,216 - Step 29: Updated current_ids
2025-02-11 07:50:46,216 - Step 29: Decoded token text:  the
2025-02-11 07:50:46,216 - Step 29: Updated current_phrase
2025-02-11 07:50:46,216 - Step 29: Created step_acts
2025-02-11 07:50:46,216 - Step 29: Added to generation_acts
2025-02-11 07:50:46,216 - Step 29: Updated recent_tokens
2025-02-11 07:50:46,218 - Step 29: Decoded current text
2025-02-11 07:50:46,218 - Step 29: Incremented consecutive_fillers to 1
2025-02-11 07:50:46,218 - Step 29: Calculated unique_ratio: 0.9375
2025-02-11 07:50:46,218 - 
Starting step 30
2025-02-11 07:50:46,219 - Current_ids device: cuda:0
2025-02-11 07:50:46,219 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,242 - Model output complete
2025-02-11 07:50:46,242 - Logits shape: torch.Size([1, 61, 151936])
2025-02-11 07:50:46,242 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,242 - Next token logits device: cuda:0
2025-02-11 07:50:46,242 - Entered do_sample
2025-02-11 07:50:46,242 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,245 - Probs max: 0.70458984375
2025-02-11 07:50:46,246 - Pre-cat
2025-02-11 07:50:46,246 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944,    387,   8643,   1576,    432,    594,
           1602,   4937,     13,    576,   4628,    315,    279]],
       device='cuda:0')
2025-02-11 07:50:46,248 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:46,248 - Current_ids shape: torch.Size([1, 61])
2025-02-11 07:50:46,248 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,249 - Step 30: Generated next token
2025-02-11 07:50:46,249 - Step 30: Updated current_ids
2025-02-11 07:50:46,249 - Step 30: Decoded token text:  ball
2025-02-11 07:50:46,249 - Step 30: Updated current_phrase
2025-02-11 07:50:46,249 - Step 30: Created step_acts
2025-02-11 07:50:46,250 - Step 30: Added to generation_acts
2025-02-11 07:50:46,251 - Step 30: Updated generated_texts
2025-02-11 07:50:46,252 - Step 30: Updated recent_tokens
2025-02-11 07:50:46,252 - Step 30: Decoded current text
2025-02-11 07:50:46,252 - Step 30: Incremented consecutive_fillers to 2
2025-02-11 07:50:46,252 - Step 30: Calculated unique_ratio: 1.0
2025-02-11 07:50:46,252 - 
Starting step 31
2025-02-11 07:50:46,252 - Current_ids device: cuda:0
2025-02-11 07:50:46,252 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,274 - Model output complete
2025-02-11 07:50:46,274 - Logits shape: torch.Size([1, 62, 151936])
2025-02-11 07:50:46,274 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,274 - Next token logits device: cuda:0
2025-02-11 07:50:46,274 - Entered do_sample
2025-02-11 07:50:46,274 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,279 - Probs max: 0.5244140625
2025-02-11 07:50:46,279 - Pre-cat
2025-02-11 07:50:46,279 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    646,    944,   5545,
            279,   7002,   1576,    432,    594,   1602,   4937,     11,    714,
            279,   7002,    646,    944,    387,   8643,   1576,    432,    594,
           1602,   4937,     13,    576,   4628,    315,    279,   4935]],
       device='cuda:0')
2025-02-11 07:50:46,283 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:50:46,283 - Current_ids shape: torch.Size([1, 62])
2025-02-11 07:50:46,283 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,283 - Step 31: Generated next token
2025-02-11 07:50:46,284 - Step 31: Updated current_ids
2025-02-11 07:50:46,284 - Step 31: Decoded token text:  and
2025-02-11 07:50:46,284 - Step 31: Updated current_phrase
2025-02-11 07:50:46,284 - Step 31: Created step_acts
2025-02-11 07:50:46,284 - Step 31: Added to generation_acts
2025-02-11 07:50:46,284 - Step 31: Updated recent_tokens
2025-02-11 07:50:46,286 - Step 31: Decoded current text
2025-02-11 07:50:46,286 - Step 31: Incremented consecutive_fillers to 3
2025-02-11 07:50:46,387 - 
Starting step 0
2025-02-11 07:50:46,388 - Current_ids device: cuda:0
2025-02-11 07:50:46,388 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,415 - Model output complete
2025-02-11 07:50:46,415 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:46,416 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,416 - Next token logits device: cuda:0
2025-02-11 07:50:46,416 - Entered do_sample
2025-02-11 07:50:46,416 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,418 - Probs max: 0.50341796875
2025-02-11 07:50:46,419 - Pre-cat
2025-02-11 07:50:46,419 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:46,421 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:46,421 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:46,421 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,421 - Step 0: Generated next token
2025-02-11 07:50:46,422 - Step 0: Updated current_ids
2025-02-11 07:50:46,422 - Step 0: Decoded token text:  The
2025-02-11 07:50:46,422 - Step 0: Updated current_phrase
2025-02-11 07:50:46,422 - Step 0: Created step_acts
2025-02-11 07:50:46,422 - Step 0: Added to generation_acts
2025-02-11 07:50:46,423 - Step 0: Updated generated_texts
2025-02-11 07:50:46,424 - Step 0: Updated recent_tokens
2025-02-11 07:50:46,424 - Step 0: Decoded current text
2025-02-11 07:50:46,424 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:46,424 - 
Starting step 1
2025-02-11 07:50:46,424 - Current_ids device: cuda:0
2025-02-11 07:50:46,424 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,453 - Model output complete
2025-02-11 07:50:46,453 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:46,453 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,453 - Next token logits device: cuda:0
2025-02-11 07:50:46,453 - Entered do_sample
2025-02-11 07:50:46,453 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,456 - Probs max: 0.83837890625
2025-02-11 07:50:46,457 - Pre-cat
2025-02-11 07:50:46,457 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:50:46,459 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:46,459 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:46,459 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,459 - Step 1: Generated next token
2025-02-11 07:50:46,460 - Step 1: Updated current_ids
2025-02-11 07:50:46,460 - Step 1: Decoded token text:  ball
2025-02-11 07:50:46,460 - Step 1: Updated current_phrase
2025-02-11 07:50:46,460 - Step 1: Created step_acts
2025-02-11 07:50:46,460 - Step 1: Added to generation_acts
2025-02-11 07:50:46,460 - Step 1: Updated recent_tokens
2025-02-11 07:50:46,462 - Step 1: Decoded current text
2025-02-11 07:50:46,462 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:46,462 - 
Starting step 2
2025-02-11 07:50:46,462 - Current_ids device: cuda:0
2025-02-11 07:50:46,462 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,487 - Model output complete
2025-02-11 07:50:46,487 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:46,487 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,487 - Next token logits device: cuda:0
2025-02-11 07:50:46,487 - Entered do_sample
2025-02-11 07:50:46,487 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,489 - Probs max: 0.583984375
2025-02-11 07:50:46,491 - Pre-cat
2025-02-11 07:50:46,491 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:50:46,493 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:46,493 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:46,493 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,493 - Step 2: Generated next token
2025-02-11 07:50:46,494 - Step 2: Updated current_ids
2025-02-11 07:50:46,494 - Step 2: Decoded token text:  will
2025-02-11 07:50:46,494 - Step 2: Updated current_phrase
2025-02-11 07:50:46,494 - Step 2: Created step_acts
2025-02-11 07:50:46,494 - Step 2: Added to generation_acts
2025-02-11 07:50:46,494 - Step 2: Updated recent_tokens
2025-02-11 07:50:46,496 - Step 2: Decoded current text
2025-02-11 07:50:46,496 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:46,496 - 
Starting step 3
2025-02-11 07:50:46,496 - Current_ids device: cuda:0
2025-02-11 07:50:46,496 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,521 - Model output complete
2025-02-11 07:50:46,521 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:46,522 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,522 - Next token logits device: cuda:0
2025-02-11 07:50:46,522 - Entered do_sample
2025-02-11 07:50:46,522 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,524 - Probs max: 0.56884765625
2025-02-11 07:50:46,525 - Pre-cat
2025-02-11 07:50:46,525 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686]],
       device='cuda:0')
2025-02-11 07:50:46,527 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:50:46,527 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:46,527 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,527 - Step 3: Generated next token
2025-02-11 07:50:46,527 - Step 3: Updated current_ids
2025-02-11 07:50:46,527 - Step 3: Decoded token text:  hit
2025-02-11 07:50:46,527 - Step 3: Updated current_phrase
2025-02-11 07:50:46,528 - Step 3: Created step_acts
2025-02-11 07:50:46,528 - Step 3: Added to generation_acts
2025-02-11 07:50:46,528 - Step 3: Updated recent_tokens
2025-02-11 07:50:46,529 - Step 3: Decoded current text
2025-02-11 07:50:46,529 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:46,529 - 
Starting step 4
2025-02-11 07:50:46,529 - Current_ids device: cuda:0
2025-02-11 07:50:46,529 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,559 - Model output complete
2025-02-11 07:50:46,559 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:46,559 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,559 - Next token logits device: cuda:0
2025-02-11 07:50:46,559 - Entered do_sample
2025-02-11 07:50:46,559 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,562 - Probs max: 0.99609375
2025-02-11 07:50:46,563 - Pre-cat
2025-02-11 07:50:46,563 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201]],
       device='cuda:0')
2025-02-11 07:50:46,566 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:46,566 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:46,566 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,566 - Step 4: Generated next token
2025-02-11 07:50:46,566 - Step 4: Updated current_ids
2025-02-11 07:50:46,566 - Step 4: Decoded token text:  the
2025-02-11 07:50:46,566 - Step 4: Updated current_phrase
2025-02-11 07:50:46,567 - Step 4: Created step_acts
2025-02-11 07:50:46,567 - Step 4: Added to generation_acts
2025-02-11 07:50:46,567 - Step 4: Updated recent_tokens
2025-02-11 07:50:46,568 - Step 4: Decoded current text
2025-02-11 07:50:46,569 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:46,569 - 
Starting step 5
2025-02-11 07:50:46,569 - Current_ids device: cuda:0
2025-02-11 07:50:46,569 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,607 - Model output complete
2025-02-11 07:50:46,607 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:46,607 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,607 - Next token logits device: cuda:0
2025-02-11 07:50:46,607 - Entered do_sample
2025-02-11 07:50:46,608 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,609 - Probs max: 0.99853515625
2025-02-11 07:50:46,611 - Pre-cat
2025-02-11 07:50:46,611 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279]],
       device='cuda:0')
2025-02-11 07:50:46,613 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:46,614 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:46,614 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,614 - Step 5: Generated next token
2025-02-11 07:50:46,614 - Step 5: Updated current_ids
2025-02-11 07:50:46,615 - Step 5: Decoded token text:  wall
2025-02-11 07:50:46,615 - Step 5: Updated current_phrase
2025-02-11 07:50:46,615 - Step 5: Created step_acts
2025-02-11 07:50:46,615 - Step 5: Added to generation_acts
2025-02-11 07:50:46,617 - Step 5: Updated generated_texts
2025-02-11 07:50:46,617 - Step 5: Updated recent_tokens
2025-02-11 07:50:46,618 - Step 5: Decoded current text
2025-02-11 07:50:46,618 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:46,618 - 
Starting step 6
2025-02-11 07:50:46,618 - Current_ids device: cuda:0
2025-02-11 07:50:46,618 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,656 - Model output complete
2025-02-11 07:50:46,656 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:46,656 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,656 - Next token logits device: cuda:0
2025-02-11 07:50:46,656 - Entered do_sample
2025-02-11 07:50:46,656 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,658 - Probs max: 0.376708984375
2025-02-11 07:50:46,659 - Pre-cat
2025-02-11 07:50:46,659 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002]], device='cuda:0')
2025-02-11 07:50:46,661 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:46,662 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:46,662 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,662 - Step 6: Generated next token
2025-02-11 07:50:46,662 - Step 6: Updated current_ids
2025-02-11 07:50:46,662 - Step 6: Decoded token text:  very
2025-02-11 07:50:46,662 - Step 6: Updated current_phrase
2025-02-11 07:50:46,663 - Step 6: Created step_acts
2025-02-11 07:50:46,663 - Step 6: Added to generation_acts
2025-02-11 07:50:46,663 - Step 6: Updated recent_tokens
2025-02-11 07:50:46,664 - Step 6: Decoded current text
2025-02-11 07:50:46,665 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:46,665 - 
Starting step 7
2025-02-11 07:50:46,665 - Current_ids device: cuda:0
2025-02-11 07:50:46,665 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,691 - Model output complete
2025-02-11 07:50:46,691 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:46,692 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,692 - Next token logits device: cuda:0
2025-02-11 07:50:46,692 - Entered do_sample
2025-02-11 07:50:46,692 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,694 - Probs max: 0.7685546875
2025-02-11 07:50:46,695 - Pre-cat
2025-02-11 07:50:46,695 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602]], device='cuda:0')
2025-02-11 07:50:46,698 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:46,699 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:46,699 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,699 - Step 7: Generated next token
2025-02-11 07:50:46,699 - Step 7: Updated current_ids
2025-02-11 07:50:46,699 - Step 7: Decoded token text:  fast
2025-02-11 07:50:46,699 - Step 7: Updated current_phrase
2025-02-11 07:50:46,700 - Step 7: Created step_acts
2025-02-11 07:50:46,700 - Step 7: Added to generation_acts
2025-02-11 07:50:46,701 - Step 7: Updated recent_tokens
2025-02-11 07:50:46,702 - Step 7: Decoded current text
2025-02-11 07:50:46,702 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:46,702 - 
Starting step 8
2025-02-11 07:50:46,702 - Current_ids device: cuda:0
2025-02-11 07:50:46,702 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,746 - Model output complete
2025-02-11 07:50:46,746 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:46,746 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,746 - Next token logits device: cuda:0
2025-02-11 07:50:46,746 - Entered do_sample
2025-02-11 07:50:46,747 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,748 - Probs max: 0.36474609375
2025-02-11 07:50:46,749 - Pre-cat
2025-02-11 07:50:46,750 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:50:46,752 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:46,752 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:46,752 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,752 - Step 8: Generated next token
2025-02-11 07:50:46,752 - Step 8: Updated current_ids
2025-02-11 07:50:46,753 - Step 8: Decoded token text: .
2025-02-11 07:50:46,753 - Step 8: Updated current_phrase
2025-02-11 07:50:46,753 - Step 8: Created step_acts
2025-02-11 07:50:46,753 - Step 8: Added to generation_acts
2025-02-11 07:50:46,754 - Step 8: Updated recent_tokens
2025-02-11 07:50:46,755 - Step 8: Found phrase end token
2025-02-11 07:50:46,755 - Step 8: Updated recent_phrases
2025-02-11 07:50:46,755 - Step 8: Decoded current text
2025-02-11 07:50:46,755 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:46,755 - 
Starting step 9
2025-02-11 07:50:46,755 - Current_ids device: cuda:0
2025-02-11 07:50:46,755 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,802 - Model output complete
2025-02-11 07:50:46,802 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:46,802 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,802 - Next token logits device: cuda:0
2025-02-11 07:50:46,802 - Entered do_sample
2025-02-11 07:50:46,803 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,805 - Probs max: 0.68603515625
2025-02-11 07:50:46,807 - Pre-cat
2025-02-11 07:50:46,807 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13]], device='cuda:0')
2025-02-11 07:50:46,810 - Next token: tensor([[425]], device='cuda:0')
2025-02-11 07:50:46,811 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:46,811 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,811 - Step 9: Generated next token
2025-02-11 07:50:46,811 - Step 9: Updated current_ids
2025-02-11 07:50:46,811 - Step 9: Decoded token text:  B
2025-02-11 07:50:46,811 - Step 9: Updated current_phrase
2025-02-11 07:50:46,812 - Step 9: Created step_acts
2025-02-11 07:50:46,812 - Step 9: Added to generation_acts
2025-02-11 07:50:46,812 - Step 9: Updated recent_tokens
2025-02-11 07:50:46,813 - Step 9: Decoded current text
2025-02-11 07:50:46,813 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:46,813 - 
Starting step 10
2025-02-11 07:50:46,814 - Current_ids device: cuda:0
2025-02-11 07:50:46,814 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,845 - Model output complete
2025-02-11 07:50:46,845 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:46,845 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,845 - Next token logits device: cuda:0
2025-02-11 07:50:46,845 - Entered do_sample
2025-02-11 07:50:46,846 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,848 - Probs max: 1.0
2025-02-11 07:50:46,849 - Pre-cat
2025-02-11 07:50:46,849 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425]], device='cuda:0')
2025-02-11 07:50:46,851 - Next token: tensor([[25]], device='cuda:0')
2025-02-11 07:50:46,851 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:46,851 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,851 - Step 10: Generated next token
2025-02-11 07:50:46,851 - Step 10: Updated current_ids
2025-02-11 07:50:46,851 - Step 10: Decoded token text: :
2025-02-11 07:50:46,852 - Step 10: Updated current_phrase
2025-02-11 07:50:46,852 - Step 10: Created step_acts
2025-02-11 07:50:46,852 - Step 10: Added to generation_acts
2025-02-11 07:50:46,853 - Step 10: Updated generated_texts
2025-02-11 07:50:46,853 - Step 10: Updated recent_tokens
2025-02-11 07:50:46,853 - Step 10: Found phrase end token
2025-02-11 07:50:46,853 - Step 10: Updated recent_phrases
2025-02-11 07:50:46,853 - Step 10: Calculated similarity: 0.0
2025-02-11 07:50:46,854 - Step 10: Decoded current text
2025-02-11 07:50:46,854 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:46,854 - 
Starting step 11
2025-02-11 07:50:46,854 - Current_ids device: cuda:0
2025-02-11 07:50:46,854 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,879 - Model output complete
2025-02-11 07:50:46,879 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:46,880 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,880 - Next token logits device: cuda:0
2025-02-11 07:50:46,880 - Entered do_sample
2025-02-11 07:50:46,880 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,882 - Probs max: 0.994140625
2025-02-11 07:50:46,883 - Pre-cat
2025-02-11 07:50:46,883 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25]], device='cuda:0')
2025-02-11 07:50:46,884 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:46,885 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:46,885 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,885 - Step 11: Generated next token
2025-02-11 07:50:46,885 - Step 11: Updated current_ids
2025-02-11 07:50:46,885 - Step 11: Decoded token text:  The
2025-02-11 07:50:46,885 - Step 11: Updated current_phrase
2025-02-11 07:50:46,886 - Step 11: Created step_acts
2025-02-11 07:50:46,886 - Step 11: Added to generation_acts
2025-02-11 07:50:46,886 - Step 11: Updated recent_tokens
2025-02-11 07:50:46,888 - Step 11: Decoded current text
2025-02-11 07:50:46,888 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:46,888 - 
Starting step 12
2025-02-11 07:50:46,888 - Current_ids device: cuda:0
2025-02-11 07:50:46,888 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,914 - Model output complete
2025-02-11 07:50:46,914 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:46,914 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,915 - Next token logits device: cuda:0
2025-02-11 07:50:46,915 - Entered do_sample
2025-02-11 07:50:46,915 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,917 - Probs max: 0.900390625
2025-02-11 07:50:46,918 - Pre-cat
2025-02-11 07:50:46,918 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25,    576]],
       device='cuda:0')
2025-02-11 07:50:46,921 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:46,921 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:46,921 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,922 - Step 12: Generated next token
2025-02-11 07:50:46,922 - Step 12: Updated current_ids
2025-02-11 07:50:46,922 - Step 12: Decoded token text:  ball
2025-02-11 07:50:46,922 - Step 12: Updated current_phrase
2025-02-11 07:50:46,922 - Step 12: Created step_acts
2025-02-11 07:50:46,922 - Step 12: Added to generation_acts
2025-02-11 07:50:46,923 - Step 12: Updated recent_tokens
2025-02-11 07:50:46,924 - Step 12: Decoded current text
2025-02-11 07:50:46,924 - Step 12: Reset consecutive_fillers
2025-02-11 07:50:46,924 - 
Starting step 13
2025-02-11 07:50:46,924 - Current_ids device: cuda:0
2025-02-11 07:50:46,924 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,954 - Model output complete
2025-02-11 07:50:46,954 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:46,954 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,954 - Next token logits device: cuda:0
2025-02-11 07:50:46,954 - Entered do_sample
2025-02-11 07:50:46,954 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,956 - Probs max: 0.9755859375
2025-02-11 07:50:46,958 - Pre-cat
2025-02-11 07:50:46,958 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25,    576,   4935]],
       device='cuda:0')
2025-02-11 07:50:46,961 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:46,961 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:46,961 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:46,961 - Step 13: Generated next token
2025-02-11 07:50:46,962 - Step 13: Updated current_ids
2025-02-11 07:50:46,962 - Step 13: Decoded token text:  will
2025-02-11 07:50:46,962 - Step 13: Updated current_phrase
2025-02-11 07:50:46,962 - Step 13: Created step_acts
2025-02-11 07:50:46,962 - Step 13: Added to generation_acts
2025-02-11 07:50:46,962 - Step 13: Updated recent_tokens
2025-02-11 07:50:46,964 - Step 13: Decoded current text
2025-02-11 07:50:46,964 - Step 13: Reset consecutive_fillers
2025-02-11 07:50:46,965 - 
Starting step 14
2025-02-11 07:50:46,965 - Current_ids device: cuda:0
2025-02-11 07:50:46,965 - Current_ids dtype: torch.int64
2025-02-11 07:50:46,996 - Model output complete
2025-02-11 07:50:46,996 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:50:46,996 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,996 - Next token logits device: cuda:0
2025-02-11 07:50:46,996 - Entered do_sample
2025-02-11 07:50:46,996 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:46,999 - Probs max: 0.7890625
2025-02-11 07:50:47,000 - Pre-cat
2025-02-11 07:50:47,000 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25,    576,   4935,    686]],
       device='cuda:0')
2025-02-11 07:50:47,002 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:50:47,003 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:50:47,003 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,003 - Step 14: Generated next token
2025-02-11 07:50:47,003 - Step 14: Updated current_ids
2025-02-11 07:50:47,003 - Step 14: Decoded token text:  hit
2025-02-11 07:50:47,003 - Step 14: Updated current_phrase
2025-02-11 07:50:47,004 - Step 14: Created step_acts
2025-02-11 07:50:47,004 - Step 14: Added to generation_acts
2025-02-11 07:50:47,004 - Step 14: Updated recent_tokens
2025-02-11 07:50:47,005 - Step 14: Decoded current text
2025-02-11 07:50:47,005 - Step 14: Reset consecutive_fillers
2025-02-11 07:50:47,005 - 
Starting step 15
2025-02-11 07:50:47,005 - Current_ids device: cuda:0
2025-02-11 07:50:47,005 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,027 - Model output complete
2025-02-11 07:50:47,027 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:50:47,027 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,027 - Next token logits device: cuda:0
2025-02-11 07:50:47,028 - Entered do_sample
2025-02-11 07:50:47,028 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,031 - Probs max: 0.99609375
2025-02-11 07:50:47,032 - Pre-cat
2025-02-11 07:50:47,032 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25,    576,   4935,    686,
           4201]], device='cuda:0')
2025-02-11 07:50:47,034 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:47,035 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:50:47,035 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,035 - Step 15: Generated next token
2025-02-11 07:50:47,035 - Step 15: Updated current_ids
2025-02-11 07:50:47,035 - Step 15: Decoded token text:  the
2025-02-11 07:50:47,035 - Step 15: Updated current_phrase
2025-02-11 07:50:47,035 - Step 15: Created step_acts
2025-02-11 07:50:47,035 - Step 15: Added to generation_acts
2025-02-11 07:50:47,037 - Step 15: Updated generated_texts
2025-02-11 07:50:47,037 - Step 15: Updated recent_tokens
2025-02-11 07:50:47,037 - Step 15: Decoded current text
2025-02-11 07:50:47,037 - Step 15: Reset consecutive_fillers
2025-02-11 07:50:47,037 - 
Starting step 16
2025-02-11 07:50:47,037 - Current_ids device: cuda:0
2025-02-11 07:50:47,037 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,060 - Model output complete
2025-02-11 07:50:47,061 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:50:47,061 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,061 - Next token logits device: cuda:0
2025-02-11 07:50:47,061 - Entered do_sample
2025-02-11 07:50:47,061 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,063 - Probs max: 0.99951171875
2025-02-11 07:50:47,064 - Pre-cat
2025-02-11 07:50:47,065 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25,    576,   4935,    686,
           4201,    279]], device='cuda:0')
2025-02-11 07:50:47,066 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:47,067 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:50:47,067 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,067 - Step 16: Generated next token
2025-02-11 07:50:47,067 - Step 16: Updated current_ids
2025-02-11 07:50:47,067 - Step 16: Decoded token text:  wall
2025-02-11 07:50:47,067 - Step 16: Updated current_phrase
2025-02-11 07:50:47,068 - Step 16: Created step_acts
2025-02-11 07:50:47,068 - Step 16: Added to generation_acts
2025-02-11 07:50:47,068 - Step 16: Updated recent_tokens
2025-02-11 07:50:47,069 - Step 16: Decoded current text
2025-02-11 07:50:47,069 - Step 16: Reset consecutive_fillers
2025-02-11 07:50:47,069 - Step 16: Calculated unique_ratio: 0.6875
2025-02-11 07:50:47,069 - 
Starting step 17
2025-02-11 07:50:47,069 - Current_ids device: cuda:0
2025-02-11 07:50:47,070 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,094 - Model output complete
2025-02-11 07:50:47,094 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:50:47,094 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,094 - Next token logits device: cuda:0
2025-02-11 07:50:47,094 - Entered do_sample
2025-02-11 07:50:47,094 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,097 - Probs max: 0.453125
2025-02-11 07:50:47,098 - Pre-cat
2025-02-11 07:50:47,098 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25,    576,   4935,    686,
           4201,    279,   7002]], device='cuda:0')
2025-02-11 07:50:47,100 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:47,100 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:50:47,100 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,100 - Step 17: Generated next token
2025-02-11 07:50:47,100 - Step 17: Updated current_ids
2025-02-11 07:50:47,100 - Step 17: Decoded token text:  very
2025-02-11 07:50:47,101 - Step 17: Updated current_phrase
2025-02-11 07:50:47,101 - Step 17: Created step_acts
2025-02-11 07:50:47,101 - Step 17: Added to generation_acts
2025-02-11 07:50:47,101 - Step 17: Updated recent_tokens
2025-02-11 07:50:47,102 - Step 17: Decoded current text
2025-02-11 07:50:47,102 - Step 17: Reset consecutive_fillers
2025-02-11 07:50:47,103 - Step 17: Calculated unique_ratio: 0.6875
2025-02-11 07:50:47,103 - 
Starting step 18
2025-02-11 07:50:47,103 - Current_ids device: cuda:0
2025-02-11 07:50:47,103 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,126 - Model output complete
2025-02-11 07:50:47,126 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:50:47,126 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,126 - Next token logits device: cuda:0
2025-02-11 07:50:47,126 - Entered do_sample
2025-02-11 07:50:47,126 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,129 - Probs max: 0.60546875
2025-02-11 07:50:47,130 - Pre-cat
2025-02-11 07:50:47,130 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25,    576,   4935,    686,
           4201,    279,   7002,   1602]], device='cuda:0')
2025-02-11 07:50:47,132 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:47,132 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:50:47,132 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,133 - Step 18: Generated next token
2025-02-11 07:50:47,133 - Step 18: Updated current_ids
2025-02-11 07:50:47,133 - Step 18: Decoded token text:  fast
2025-02-11 07:50:47,133 - Step 18: Updated current_phrase
2025-02-11 07:50:47,133 - Step 18: Created step_acts
2025-02-11 07:50:47,133 - Step 18: Added to generation_acts
2025-02-11 07:50:47,134 - Step 18: Updated recent_tokens
2025-02-11 07:50:47,135 - Step 18: Decoded current text
2025-02-11 07:50:47,135 - Step 18: Reset consecutive_fillers
2025-02-11 07:50:47,135 - Step 18: Calculated unique_ratio: 0.6875
2025-02-11 07:50:47,136 - 
Starting step 19
2025-02-11 07:50:47,136 - Current_ids device: cuda:0
2025-02-11 07:50:47,136 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,177 - Model output complete
2025-02-11 07:50:47,177 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:50:47,177 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,177 - Next token logits device: cuda:0
2025-02-11 07:50:47,177 - Entered do_sample
2025-02-11 07:50:47,177 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,179 - Probs max: 0.52783203125
2025-02-11 07:50:47,182 - Pre-cat
2025-02-11 07:50:47,182 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25,    576,   4935,    686,
           4201,    279,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:50:47,186 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:47,187 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:50:47,187 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,187 - Step 19: Generated next token
2025-02-11 07:50:47,187 - Step 19: Updated current_ids
2025-02-11 07:50:47,187 - Step 19: Decoded token text: ,
2025-02-11 07:50:47,187 - Step 19: Updated current_phrase
2025-02-11 07:50:47,188 - Step 19: Created step_acts
2025-02-11 07:50:47,188 - Step 19: Added to generation_acts
2025-02-11 07:50:47,188 - Step 19: Updated recent_tokens
2025-02-11 07:50:47,189 - Step 19: Found phrase end token
2025-02-11 07:50:47,189 - Step 19: Updated recent_phrases
2025-02-11 07:50:47,189 - Step 19: Calculated similarity: 0.0
2025-02-11 07:50:47,190 - Step 19: Decoded current text
2025-02-11 07:50:47,190 - Step 19: Reset consecutive_fillers
2025-02-11 07:50:47,191 - Step 19: Calculated unique_ratio: 0.75
2025-02-11 07:50:47,191 - 
Starting step 20
2025-02-11 07:50:47,191 - Current_ids device: cuda:0
2025-02-11 07:50:47,191 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,256 - Model output complete
2025-02-11 07:50:47,256 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:50:47,256 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,256 - Next token logits device: cuda:0
2025-02-11 07:50:47,256 - Entered do_sample
2025-02-11 07:50:47,256 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,259 - Probs max: 0.935546875
2025-02-11 07:50:47,260 - Pre-cat
2025-02-11 07:50:47,260 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25,    576,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:50:47,262 - Next token: tensor([[714]], device='cuda:0')
2025-02-11 07:50:47,262 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:50:47,262 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,262 - Step 20: Generated next token
2025-02-11 07:50:47,262 - Step 20: Updated current_ids
2025-02-11 07:50:47,262 - Step 20: Decoded token text:  but
2025-02-11 07:50:47,262 - Step 20: Updated current_phrase
2025-02-11 07:50:47,263 - Step 20: Created step_acts
2025-02-11 07:50:47,263 - Step 20: Added to generation_acts
2025-02-11 07:50:47,264 - Step 20: Updated generated_texts
2025-02-11 07:50:47,264 - Step 20: Updated recent_tokens
2025-02-11 07:50:47,265 - Step 20: Decoded current text
2025-02-11 07:50:47,265 - Step 20: Reset consecutive_fillers
2025-02-11 07:50:47,265 - Step 20: Calculated unique_ratio: 0.8125
2025-02-11 07:50:47,265 - 
Starting step 21
2025-02-11 07:50:47,265 - Current_ids device: cuda:0
2025-02-11 07:50:47,265 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,289 - Model output complete
2025-02-11 07:50:47,290 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:50:47,290 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,290 - Next token logits device: cuda:0
2025-02-11 07:50:47,290 - Entered do_sample
2025-02-11 07:50:47,290 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,292 - Probs max: 0.857421875
2025-02-11 07:50:47,293 - Pre-cat
2025-02-11 07:50:47,293 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25,    576,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11,    714]],
       device='cuda:0')
2025-02-11 07:50:47,295 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:47,295 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:50:47,295 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,295 - Step 21: Generated next token
2025-02-11 07:50:47,295 - Step 21: Updated current_ids
2025-02-11 07:50:47,296 - Step 21: Decoded token text:  the
2025-02-11 07:50:47,296 - Step 21: Updated current_phrase
2025-02-11 07:50:47,296 - Step 21: Created step_acts
2025-02-11 07:50:47,296 - Step 21: Added to generation_acts
2025-02-11 07:50:47,296 - Step 21: Updated recent_tokens
2025-02-11 07:50:47,297 - Step 21: Decoded current text
2025-02-11 07:50:47,298 - Step 21: Reset consecutive_fillers
2025-02-11 07:50:47,298 - Step 21: Calculated unique_ratio: 0.8125
2025-02-11 07:50:47,298 - 
Starting step 22
2025-02-11 07:50:47,298 - Current_ids device: cuda:0
2025-02-11 07:50:47,298 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,331 - Model output complete
2025-02-11 07:50:47,331 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:50:47,331 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,331 - Next token logits device: cuda:0
2025-02-11 07:50:47,331 - Entered do_sample
2025-02-11 07:50:47,331 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,334 - Probs max: 0.9501953125
2025-02-11 07:50:47,335 - Pre-cat
2025-02-11 07:50:47,335 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25,    576,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11,    714,    279]],
       device='cuda:0')
2025-02-11 07:50:47,337 - Next token: tensor([[1112]], device='cuda:0')
2025-02-11 07:50:47,338 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:50:47,338 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,338 - Step 22: Generated next token
2025-02-11 07:50:47,338 - Step 22: Updated current_ids
2025-02-11 07:50:47,338 - Step 22: Decoded token text: ...
2025-02-11 07:50:47,338 - Step 22: Updated current_phrase
2025-02-11 07:50:47,339 - Step 22: Created step_acts
2025-02-11 07:50:47,339 - Step 22: Added to generation_acts
2025-02-11 07:50:47,339 - Step 22: Updated recent_tokens
2025-02-11 07:50:47,340 - Step 22: Found phrase end token
2025-02-11 07:50:47,340 - Step 22: Updated recent_phrases
2025-02-11 07:50:47,340 - Step 22: Calculated similarity: 0.0
2025-02-11 07:50:47,341 - Step 22: Decoded current text
2025-02-11 07:50:47,341 - Step 22: Reset consecutive_fillers
2025-02-11 07:50:47,341 - Step 22: Calculated unique_ratio: 0.875
2025-02-11 07:50:47,341 - 
Starting step 23
2025-02-11 07:50:47,341 - Current_ids device: cuda:0
2025-02-11 07:50:47,341 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,365 - Model output complete
2025-02-11 07:50:47,365 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:50:47,365 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,365 - Next token logits device: cuda:0
2025-02-11 07:50:47,365 - Entered do_sample
2025-02-11 07:50:47,365 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,368 - Probs max: 0.210693359375
2025-02-11 07:50:47,369 - Pre-cat
2025-02-11 07:50:47,369 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25,    576,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11,    714,    279,   1112]],
       device='cuda:0')
2025-02-11 07:50:47,371 - Next token: tensor([[320]], device='cuda:0')
2025-02-11 07:50:47,372 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:50:47,372 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,372 - Step 23: Generated next token
2025-02-11 07:50:47,372 - Step 23: Updated current_ids
2025-02-11 07:50:47,372 - Step 23: Decoded token text:  (
2025-02-11 07:50:47,372 - Step 23: Updated current_phrase
2025-02-11 07:50:47,372 - Step 23: Created step_acts
2025-02-11 07:50:47,372 - Step 23: Added to generation_acts
2025-02-11 07:50:47,372 - Step 23: Updated recent_tokens
2025-02-11 07:50:47,374 - Step 23: Decoded current text
2025-02-11 07:50:47,374 - Step 23: Reset consecutive_fillers
2025-02-11 07:50:47,374 - Step 23: Calculated unique_ratio: 0.9375
2025-02-11 07:50:47,374 - 
Starting step 24
2025-02-11 07:50:47,374 - Current_ids device: cuda:0
2025-02-11 07:50:47,374 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,397 - Model output complete
2025-02-11 07:50:47,397 - Logits shape: torch.Size([1, 55, 151936])
2025-02-11 07:50:47,397 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,397 - Next token logits device: cuda:0
2025-02-11 07:50:47,397 - Entered do_sample
2025-02-11 07:50:47,398 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,401 - Probs max: 0.384521484375
2025-02-11 07:50:47,402 - Pre-cat
2025-02-11 07:50:47,402 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,   1602,   4937,     13,    425,     25,    576,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11,    714,    279,   1112,
            320]], device='cuda:0')
2025-02-11 07:50:47,404 - Next token: tensor([[3798]], device='cuda:0')
2025-02-11 07:50:47,404 - Current_ids shape: torch.Size([1, 55])
2025-02-11 07:50:47,405 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,405 - Step 24: Generated next token
2025-02-11 07:50:47,405 - Step 24: Updated current_ids
2025-02-11 07:50:47,405 - Step 24: Decoded token text: Options
2025-02-11 07:50:47,405 - Step 24: Updated current_phrase
2025-02-11 07:50:47,405 - Step 24: Created step_acts
2025-02-11 07:50:47,405 - Step 24: Added to generation_acts
2025-02-11 07:50:47,405 - Step 24: Updated recent_tokens
2025-02-11 07:50:47,407 - Step 24: Decoded current text
2025-02-11 07:50:47,407 - Step 24: Reset consecutive_fillers
2025-02-11 07:50:47,407 - Step 24: Calculated unique_ratio: 0.9375
2025-02-11 07:50:47,407 - 
Starting step 25
2025-02-11 07:50:47,407 - Current_ids device: cuda:0
2025-02-11 07:50:47,407 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,429 - Model output complete
2025-02-11 07:50:47,430 - Logits shape: torch.Size([1, 56, 151936])
2025-02-11 07:50:47,430 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,430 - Next token logits device: cuda:0
2025-02-11 07:50:47,430 - Entered do_sample
2025-02-11 07:50:47,430 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,434 - Probs max: 0.18017578125
2025-02-11 07:50:47,535 - 
Starting step 0
2025-02-11 07:50:47,536 - Current_ids device: cuda:0
2025-02-11 07:50:47,536 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,565 - Model output complete
2025-02-11 07:50:47,565 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:47,565 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,565 - Next token logits device: cuda:0
2025-02-11 07:50:47,565 - Entered do_sample
2025-02-11 07:50:47,565 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,568 - Probs max: 0.50341796875
2025-02-11 07:50:47,569 - Pre-cat
2025-02-11 07:50:47,569 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:47,570 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:47,570 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:47,570 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,571 - Step 0: Generated next token
2025-02-11 07:50:47,571 - Step 0: Updated current_ids
2025-02-11 07:50:47,571 - Step 0: Decoded token text:  The
2025-02-11 07:50:47,571 - Step 0: Updated current_phrase
2025-02-11 07:50:47,571 - Step 0: Created step_acts
2025-02-11 07:50:47,571 - Step 0: Added to generation_acts
2025-02-11 07:50:47,572 - Step 0: Updated generated_texts
2025-02-11 07:50:47,573 - Step 0: Updated recent_tokens
2025-02-11 07:50:47,573 - Step 0: Decoded current text
2025-02-11 07:50:47,573 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:47,573 - 
Starting step 1
2025-02-11 07:50:47,573 - Current_ids device: cuda:0
2025-02-11 07:50:47,573 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,597 - Model output complete
2025-02-11 07:50:47,598 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:47,598 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,598 - Next token logits device: cuda:0
2025-02-11 07:50:47,598 - Entered do_sample
2025-02-11 07:50:47,598 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,600 - Probs max: 0.83837890625
2025-02-11 07:50:47,601 - Pre-cat
2025-02-11 07:50:47,601 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:50:47,602 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:47,602 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:47,602 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,602 - Step 1: Generated next token
2025-02-11 07:50:47,603 - Step 1: Updated current_ids
2025-02-11 07:50:47,603 - Step 1: Decoded token text:  ball
2025-02-11 07:50:47,603 - Step 1: Updated current_phrase
2025-02-11 07:50:47,603 - Step 1: Created step_acts
2025-02-11 07:50:47,603 - Step 1: Added to generation_acts
2025-02-11 07:50:47,603 - Step 1: Updated recent_tokens
2025-02-11 07:50:47,605 - Step 1: Decoded current text
2025-02-11 07:50:47,605 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:47,605 - 
Starting step 2
2025-02-11 07:50:47,605 - Current_ids device: cuda:0
2025-02-11 07:50:47,605 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,630 - Model output complete
2025-02-11 07:50:47,630 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:47,630 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,630 - Next token logits device: cuda:0
2025-02-11 07:50:47,630 - Entered do_sample
2025-02-11 07:50:47,630 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,633 - Probs max: 0.583984375
2025-02-11 07:50:47,633 - Pre-cat
2025-02-11 07:50:47,633 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:50:47,635 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:47,635 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:47,635 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,635 - Step 2: Generated next token
2025-02-11 07:50:47,635 - Step 2: Updated current_ids
2025-02-11 07:50:47,635 - Step 2: Decoded token text:  will
2025-02-11 07:50:47,635 - Step 2: Updated current_phrase
2025-02-11 07:50:47,636 - Step 2: Created step_acts
2025-02-11 07:50:47,636 - Step 2: Added to generation_acts
2025-02-11 07:50:47,636 - Step 2: Updated recent_tokens
2025-02-11 07:50:47,637 - Step 2: Decoded current text
2025-02-11 07:50:47,637 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:47,637 - 
Starting step 3
2025-02-11 07:50:47,637 - Current_ids device: cuda:0
2025-02-11 07:50:47,637 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,660 - Model output complete
2025-02-11 07:50:47,660 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:47,660 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,660 - Next token logits device: cuda:0
2025-02-11 07:50:47,660 - Entered do_sample
2025-02-11 07:50:47,660 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,663 - Probs max: 0.56884765625
2025-02-11 07:50:47,663 - Pre-cat
2025-02-11 07:50:47,663 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686]],
       device='cuda:0')
2025-02-11 07:50:47,665 - Next token: tensor([[537]], device='cuda:0')
2025-02-11 07:50:47,665 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:47,665 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,665 - Step 3: Generated next token
2025-02-11 07:50:47,665 - Step 3: Updated current_ids
2025-02-11 07:50:47,665 - Step 3: Decoded token text:  not
2025-02-11 07:50:47,665 - Step 3: Updated current_phrase
2025-02-11 07:50:47,666 - Step 3: Created step_acts
2025-02-11 07:50:47,666 - Step 3: Added to generation_acts
2025-02-11 07:50:47,666 - Step 3: Updated recent_tokens
2025-02-11 07:50:47,667 - Step 3: Decoded current text
2025-02-11 07:50:47,667 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:47,667 - 
Starting step 4
2025-02-11 07:50:47,667 - Current_ids device: cuda:0
2025-02-11 07:50:47,667 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,691 - Model output complete
2025-02-11 07:50:47,691 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:47,691 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,691 - Next token logits device: cuda:0
2025-02-11 07:50:47,691 - Entered do_sample
2025-02-11 07:50:47,691 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,693 - Probs max: 0.496826171875
2025-02-11 07:50:47,694 - Pre-cat
2025-02-11 07:50:47,694 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537]],
       device='cuda:0')
2025-02-11 07:50:47,696 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:50:47,696 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:47,696 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,696 - Step 4: Generated next token
2025-02-11 07:50:47,696 - Step 4: Updated current_ids
2025-02-11 07:50:47,696 - Step 4: Decoded token text:  hit
2025-02-11 07:50:47,696 - Step 4: Updated current_phrase
2025-02-11 07:50:47,697 - Step 4: Created step_acts
2025-02-11 07:50:47,697 - Step 4: Added to generation_acts
2025-02-11 07:50:47,697 - Step 4: Updated recent_tokens
2025-02-11 07:50:47,698 - Step 4: Decoded current text
2025-02-11 07:50:47,698 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:47,698 - 
Starting step 5
2025-02-11 07:50:47,698 - Current_ids device: cuda:0
2025-02-11 07:50:47,698 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,721 - Model output complete
2025-02-11 07:50:47,721 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:47,721 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,721 - Next token logits device: cuda:0
2025-02-11 07:50:47,722 - Entered do_sample
2025-02-11 07:50:47,722 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,724 - Probs max: 0.998046875
2025-02-11 07:50:47,725 - Pre-cat
2025-02-11 07:50:47,725 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201]],
       device='cuda:0')
2025-02-11 07:50:47,726 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:47,726 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:47,727 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,727 - Step 5: Generated next token
2025-02-11 07:50:47,727 - Step 5: Updated current_ids
2025-02-11 07:50:47,727 - Step 5: Decoded token text:  the
2025-02-11 07:50:47,727 - Step 5: Updated current_phrase
2025-02-11 07:50:47,727 - Step 5: Created step_acts
2025-02-11 07:50:47,727 - Step 5: Added to generation_acts
2025-02-11 07:50:47,728 - Step 5: Updated generated_texts
2025-02-11 07:50:47,729 - Step 5: Updated recent_tokens
2025-02-11 07:50:47,729 - Step 5: Decoded current text
2025-02-11 07:50:47,729 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:47,729 - 
Starting step 6
2025-02-11 07:50:47,729 - Current_ids device: cuda:0
2025-02-11 07:50:47,729 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,754 - Model output complete
2025-02-11 07:50:47,754 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:47,754 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,754 - Next token logits device: cuda:0
2025-02-11 07:50:47,754 - Entered do_sample
2025-02-11 07:50:47,755 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,757 - Probs max: 0.99951171875
2025-02-11 07:50:47,757 - Pre-cat
2025-02-11 07:50:47,757 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279]], device='cuda:0')
2025-02-11 07:50:47,759 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:47,760 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:47,760 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,760 - Step 6: Generated next token
2025-02-11 07:50:47,760 - Step 6: Updated current_ids
2025-02-11 07:50:47,760 - Step 6: Decoded token text:  wall
2025-02-11 07:50:47,760 - Step 6: Updated current_phrase
2025-02-11 07:50:47,760 - Step 6: Created step_acts
2025-02-11 07:50:47,760 - Step 6: Added to generation_acts
2025-02-11 07:50:47,760 - Step 6: Updated recent_tokens
2025-02-11 07:50:47,762 - Step 6: Decoded current text
2025-02-11 07:50:47,762 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:47,762 - 
Starting step 7
2025-02-11 07:50:47,762 - Current_ids device: cuda:0
2025-02-11 07:50:47,762 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,784 - Model output complete
2025-02-11 07:50:47,785 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:47,785 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,785 - Next token logits device: cuda:0
2025-02-11 07:50:47,785 - Entered do_sample
2025-02-11 07:50:47,785 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,787 - Probs max: 0.54638671875
2025-02-11 07:50:47,788 - Pre-cat
2025-02-11 07:50:47,788 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002]], device='cuda:0')
2025-02-11 07:50:47,790 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:50:47,790 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:47,790 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,790 - Step 7: Generated next token
2025-02-11 07:50:47,790 - Step 7: Updated current_ids
2025-02-11 07:50:47,791 - Step 7: Decoded token text:  and
2025-02-11 07:50:47,791 - Step 7: Updated current_phrase
2025-02-11 07:50:47,791 - Step 7: Created step_acts
2025-02-11 07:50:47,791 - Step 7: Added to generation_acts
2025-02-11 07:50:47,791 - Step 7: Updated recent_tokens
2025-02-11 07:50:47,792 - Step 7: Decoded current text
2025-02-11 07:50:47,792 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:47,792 - 
Starting step 8
2025-02-11 07:50:47,792 - Current_ids device: cuda:0
2025-02-11 07:50:47,792 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,816 - Model output complete
2025-02-11 07:50:47,816 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:47,816 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,816 - Next token logits device: cuda:0
2025-02-11 07:50:47,816 - Entered do_sample
2025-02-11 07:50:47,816 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,819 - Probs max: 0.58642578125
2025-02-11 07:50:47,819 - Pre-cat
2025-02-11 07:50:47,819 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    323]], device='cuda:0')
2025-02-11 07:50:47,821 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:47,821 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:47,821 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,821 - Step 8: Generated next token
2025-02-11 07:50:47,821 - Step 8: Updated current_ids
2025-02-11 07:50:47,822 - Step 8: Decoded token text:  will
2025-02-11 07:50:47,822 - Step 8: Updated current_phrase
2025-02-11 07:50:47,822 - Step 8: Created step_acts
2025-02-11 07:50:47,822 - Step 8: Added to generation_acts
2025-02-11 07:50:47,822 - Step 8: Updated recent_tokens
2025-02-11 07:50:47,823 - Step 8: Decoded current text
2025-02-11 07:50:47,823 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:47,823 - 
Starting step 9
2025-02-11 07:50:47,824 - Current_ids device: cuda:0
2025-02-11 07:50:47,824 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,848 - Model output complete
2025-02-11 07:50:47,848 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:47,848 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,848 - Next token logits device: cuda:0
2025-02-11 07:50:47,848 - Entered do_sample
2025-02-11 07:50:47,849 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,851 - Probs max: 0.3232421875
2025-02-11 07:50:47,851 - Pre-cat
2025-02-11 07:50:47,851 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    323,    686]], device='cuda:0')
2025-02-11 07:50:47,853 - Next token: tensor([[537]], device='cuda:0')
2025-02-11 07:50:47,853 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:47,853 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,853 - Step 9: Generated next token
2025-02-11 07:50:47,853 - Step 9: Updated current_ids
2025-02-11 07:50:47,854 - Step 9: Decoded token text:  not
2025-02-11 07:50:47,854 - Step 9: Updated current_phrase
2025-02-11 07:50:47,854 - Step 9: Created step_acts
2025-02-11 07:50:47,854 - Step 9: Added to generation_acts
2025-02-11 07:50:47,854 - Step 9: Updated recent_tokens
2025-02-11 07:50:47,855 - Step 9: Decoded current text
2025-02-11 07:50:47,855 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:47,856 - 
Starting step 10
2025-02-11 07:50:47,856 - Current_ids device: cuda:0
2025-02-11 07:50:47,856 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,880 - Model output complete
2025-02-11 07:50:47,880 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:47,880 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,880 - Next token logits device: cuda:0
2025-02-11 07:50:47,880 - Entered do_sample
2025-02-11 07:50:47,880 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,882 - Probs max: 0.37109375
2025-02-11 07:50:47,883 - Pre-cat
2025-02-11 07:50:47,883 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    323,    686,    537]], device='cuda:0')
2025-02-11 07:50:47,884 - Next token: tensor([[3271]], device='cuda:0')
2025-02-11 07:50:47,885 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:47,885 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,885 - Step 10: Generated next token
2025-02-11 07:50:47,885 - Step 10: Updated current_ids
2025-02-11 07:50:47,885 - Step 10: Decoded token text:  move
2025-02-11 07:50:47,885 - Step 10: Updated current_phrase
2025-02-11 07:50:47,885 - Step 10: Created step_acts
2025-02-11 07:50:47,885 - Step 10: Added to generation_acts
2025-02-11 07:50:47,887 - Step 10: Updated generated_texts
2025-02-11 07:50:47,887 - Step 10: Updated recent_tokens
2025-02-11 07:50:47,887 - Step 10: Decoded current text
2025-02-11 07:50:47,887 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:47,887 - 
Starting step 11
2025-02-11 07:50:47,887 - Current_ids device: cuda:0
2025-02-11 07:50:47,887 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,911 - Model output complete
2025-02-11 07:50:47,911 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:47,911 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,911 - Next token logits device: cuda:0
2025-02-11 07:50:47,911 - Entered do_sample
2025-02-11 07:50:47,912 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,914 - Probs max: 0.43310546875
2025-02-11 07:50:47,915 - Pre-cat
2025-02-11 07:50:47,915 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    323,    686,    537,   3271]], device='cuda:0')
2025-02-11 07:50:47,917 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:47,917 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:47,917 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,917 - Step 11: Generated next token
2025-02-11 07:50:47,917 - Step 11: Updated current_ids
2025-02-11 07:50:47,917 - Step 11: Decoded token text: .
2025-02-11 07:50:47,918 - Step 11: Updated current_phrase
2025-02-11 07:50:47,918 - Step 11: Created step_acts
2025-02-11 07:50:47,918 - Step 11: Added to generation_acts
2025-02-11 07:50:47,918 - Step 11: Updated recent_tokens
2025-02-11 07:50:47,919 - Step 11: Found phrase end token
2025-02-11 07:50:47,919 - Step 11: Updated recent_phrases
2025-02-11 07:50:47,919 - Step 11: Decoded current text
2025-02-11 07:50:47,920 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:47,920 - 
Starting step 12
2025-02-11 07:50:47,920 - Current_ids device: cuda:0
2025-02-11 07:50:47,920 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,944 - Model output complete
2025-02-11 07:50:47,945 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:47,945 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,945 - Next token logits device: cuda:0
2025-02-11 07:50:47,945 - Entered do_sample
2025-02-11 07:50:47,945 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,947 - Probs max: 0.2464599609375
2025-02-11 07:50:47,948 - Pre-cat
2025-02-11 07:50:47,948 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    323,    686,    537,   3271,     13]],
       device='cuda:0')
2025-02-11 07:50:47,949 - Next token: tensor([[2055]], device='cuda:0')
2025-02-11 07:50:47,949 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:47,950 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,950 - Step 12: Generated next token
2025-02-11 07:50:47,950 - Step 12: Updated current_ids
2025-02-11 07:50:47,950 - Step 12: Decoded token text:  So
2025-02-11 07:50:47,950 - Step 12: Updated current_phrase
2025-02-11 07:50:47,950 - Step 12: Created step_acts
2025-02-11 07:50:47,950 - Step 12: Added to generation_acts
2025-02-11 07:50:47,950 - Step 12: Updated recent_tokens
2025-02-11 07:50:47,952 - Step 12: Decoded current text
2025-02-11 07:50:47,952 - Step 12: Reset consecutive_fillers
2025-02-11 07:50:47,952 - 
Starting step 13
2025-02-11 07:50:47,952 - Current_ids device: cuda:0
2025-02-11 07:50:47,952 - Current_ids dtype: torch.int64
2025-02-11 07:50:47,976 - Model output complete
2025-02-11 07:50:47,976 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:47,977 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,977 - Next token logits device: cuda:0
2025-02-11 07:50:47,977 - Entered do_sample
2025-02-11 07:50:47,977 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:47,979 - Probs max: 0.654296875
2025-02-11 07:50:47,980 - Pre-cat
2025-02-11 07:50:47,980 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    323,    686,    537,   3271,     13,   2055]],
       device='cuda:0')
2025-02-11 07:50:47,982 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:47,982 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:47,982 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:47,982 - Step 13: Generated next token
2025-02-11 07:50:47,982 - Step 13: Updated current_ids
2025-02-11 07:50:47,982 - Step 13: Decoded token text:  the
2025-02-11 07:50:47,982 - Step 13: Updated current_phrase
2025-02-11 07:50:47,983 - Step 13: Created step_acts
2025-02-11 07:50:47,983 - Step 13: Added to generation_acts
2025-02-11 07:50:47,983 - Step 13: Updated recent_tokens
2025-02-11 07:50:47,984 - Step 13: Decoded current text
2025-02-11 07:50:47,984 - Step 13: Incremented consecutive_fillers to 1
2025-02-11 07:50:47,984 - 
Starting step 14
2025-02-11 07:50:47,984 - Current_ids device: cuda:0
2025-02-11 07:50:47,984 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,007 - Model output complete
2025-02-11 07:50:48,007 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:50:48,007 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,007 - Next token logits device: cuda:0
2025-02-11 07:50:48,007 - Entered do_sample
2025-02-11 07:50:48,007 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,011 - Probs max: 0.74560546875
2025-02-11 07:50:48,011 - Pre-cat
2025-02-11 07:50:48,012 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    323,    686,    537,   3271,     13,   2055,    279]],
       device='cuda:0')
2025-02-11 07:50:48,013 - Next token: tensor([[4226]], device='cuda:0')
2025-02-11 07:50:48,014 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:50:48,014 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,014 - Step 14: Generated next token
2025-02-11 07:50:48,014 - Step 14: Updated current_ids
2025-02-11 07:50:48,014 - Step 14: Decoded token text:  answer
2025-02-11 07:50:48,014 - Step 14: Updated current_phrase
2025-02-11 07:50:48,014 - Step 14: Created step_acts
2025-02-11 07:50:48,014 - Step 14: Added to generation_acts
2025-02-11 07:50:48,014 - Step 14: Updated recent_tokens
2025-02-11 07:50:48,016 - Step 14: Decoded current text
2025-02-11 07:50:48,016 - Step 14: Incremented consecutive_fillers to 2
2025-02-11 07:50:48,016 - 
Starting step 15
2025-02-11 07:50:48,016 - Current_ids device: cuda:0
2025-02-11 07:50:48,016 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,039 - Model output complete
2025-02-11 07:50:48,039 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:50:48,039 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,039 - Next token logits device: cuda:0
2025-02-11 07:50:48,039 - Entered do_sample
2025-02-11 07:50:48,040 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,042 - Probs max: 0.93701171875
2025-02-11 07:50:48,043 - Pre-cat
2025-02-11 07:50:48,043 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    537,   4201,
            279,   7002,    323,    686,    537,   3271,     13,   2055,    279,
           4226]], device='cuda:0')
2025-02-11 07:50:48,045 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:48,045 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:50:48,045 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,045 - Step 15: Generated next token
2025-02-11 07:50:48,045 - Step 15: Updated current_ids
2025-02-11 07:50:48,045 - Step 15: Decoded token text:  is
2025-02-11 07:50:48,045 - Step 15: Updated current_phrase
2025-02-11 07:50:48,046 - Step 15: Created step_acts
2025-02-11 07:50:48,046 - Step 15: Added to generation_acts
2025-02-11 07:50:48,047 - Step 15: Updated generated_texts
2025-02-11 07:50:48,047 - Step 15: Updated recent_tokens
2025-02-11 07:50:48,048 - Step 15: Decoded current text
2025-02-11 07:50:48,048 - Step 15: Incremented consecutive_fillers to 3
2025-02-11 07:50:48,162 - 
Starting step 0
2025-02-11 07:50:48,162 - Current_ids device: cuda:0
2025-02-11 07:50:48,162 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,195 - Model output complete
2025-02-11 07:50:48,196 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:48,196 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,196 - Next token logits device: cuda:0
2025-02-11 07:50:48,196 - Entered do_sample
2025-02-11 07:50:48,196 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,198 - Probs max: 0.50341796875
2025-02-11 07:50:48,199 - Pre-cat
2025-02-11 07:50:48,200 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:48,201 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:50:48,202 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:48,202 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,202 - Step 0: Generated next token
2025-02-11 07:50:48,202 - Step 0: Updated current_ids
2025-02-11 07:50:48,202 - Step 0: Decoded token text:  If
2025-02-11 07:50:48,202 - Step 0: Updated current_phrase
2025-02-11 07:50:48,203 - Step 0: Created step_acts
2025-02-11 07:50:48,203 - Step 0: Added to generation_acts
2025-02-11 07:50:48,204 - Step 0: Updated generated_texts
2025-02-11 07:50:48,204 - Step 0: Updated recent_tokens
2025-02-11 07:50:48,205 - Step 0: Decoded current text
2025-02-11 07:50:48,205 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:48,205 - 
Starting step 1
2025-02-11 07:50:48,205 - Current_ids device: cuda:0
2025-02-11 07:50:48,205 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,236 - Model output complete
2025-02-11 07:50:48,236 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:48,236 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,236 - Next token logits device: cuda:0
2025-02-11 07:50:48,236 - Entered do_sample
2025-02-11 07:50:48,237 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,239 - Probs max: 0.7099609375
2025-02-11 07:50:48,239 - Pre-cat
2025-02-11 07:50:48,239 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:50:48,241 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:48,241 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:48,241 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,241 - Step 1: Generated next token
2025-02-11 07:50:48,241 - Step 1: Updated current_ids
2025-02-11 07:50:48,241 - Step 1: Decoded token text:  the
2025-02-11 07:50:48,242 - Step 1: Updated current_phrase
2025-02-11 07:50:48,242 - Step 1: Created step_acts
2025-02-11 07:50:48,242 - Step 1: Added to generation_acts
2025-02-11 07:50:48,242 - Step 1: Updated recent_tokens
2025-02-11 07:50:48,243 - Step 1: Decoded current text
2025-02-11 07:50:48,243 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:48,243 - 
Starting step 2
2025-02-11 07:50:48,244 - Current_ids device: cuda:0
2025-02-11 07:50:48,244 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,269 - Model output complete
2025-02-11 07:50:48,269 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:48,269 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,269 - Next token logits device: cuda:0
2025-02-11 07:50:48,269 - Entered do_sample
2025-02-11 07:50:48,269 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,272 - Probs max: 0.76123046875
2025-02-11 07:50:48,272 - Pre-cat
2025-02-11 07:50:48,272 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279]], device='cuda:0')
2025-02-11 07:50:48,274 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:48,274 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:48,274 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,274 - Step 2: Generated next token
2025-02-11 07:50:48,274 - Step 2: Updated current_ids
2025-02-11 07:50:48,274 - Step 2: Decoded token text:  ball
2025-02-11 07:50:48,274 - Step 2: Updated current_phrase
2025-02-11 07:50:48,275 - Step 2: Created step_acts
2025-02-11 07:50:48,275 - Step 2: Added to generation_acts
2025-02-11 07:50:48,275 - Step 2: Updated recent_tokens
2025-02-11 07:50:48,277 - Step 2: Decoded current text
2025-02-11 07:50:48,277 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:48,277 - 
Starting step 3
2025-02-11 07:50:48,277 - Current_ids device: cuda:0
2025-02-11 07:50:48,277 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,309 - Model output complete
2025-02-11 07:50:48,310 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:48,310 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,310 - Next token logits device: cuda:0
2025-02-11 07:50:48,310 - Entered do_sample
2025-02-11 07:50:48,310 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,312 - Probs max: 0.9921875
2025-02-11 07:50:48,313 - Pre-cat
2025-02-11 07:50:48,313 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935]],
       device='cuda:0')
2025-02-11 07:50:48,316 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:48,316 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:48,316 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,316 - Step 3: Generated next token
2025-02-11 07:50:48,316 - Step 3: Updated current_ids
2025-02-11 07:50:48,316 - Step 3: Decoded token text:  is
2025-02-11 07:50:48,317 - Step 3: Updated current_phrase
2025-02-11 07:50:48,317 - Step 3: Created step_acts
2025-02-11 07:50:48,317 - Step 3: Added to generation_acts
2025-02-11 07:50:48,317 - Step 3: Updated recent_tokens
2025-02-11 07:50:48,318 - Step 3: Decoded current text
2025-02-11 07:50:48,318 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:48,319 - 
Starting step 4
2025-02-11 07:50:48,319 - Current_ids device: cuda:0
2025-02-11 07:50:48,319 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,345 - Model output complete
2025-02-11 07:50:48,345 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:48,345 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,345 - Next token logits device: cuda:0
2025-02-11 07:50:48,345 - Entered do_sample
2025-02-11 07:50:48,346 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,348 - Probs max: 0.94921875
2025-02-11 07:50:48,348 - Pre-cat
2025-02-11 07:50:48,348 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374]],
       device='cuda:0')
2025-02-11 07:50:48,350 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:50:48,350 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:48,350 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,350 - Step 4: Generated next token
2025-02-11 07:50:48,350 - Step 4: Updated current_ids
2025-02-11 07:50:48,350 - Step 4: Decoded token text:  thrown
2025-02-11 07:50:48,351 - Step 4: Updated current_phrase
2025-02-11 07:50:48,351 - Step 4: Created step_acts
2025-02-11 07:50:48,351 - Step 4: Added to generation_acts
2025-02-11 07:50:48,351 - Step 4: Updated recent_tokens
2025-02-11 07:50:48,352 - Step 4: Decoded current text
2025-02-11 07:50:48,352 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:48,352 - 
Starting step 5
2025-02-11 07:50:48,352 - Current_ids device: cuda:0
2025-02-11 07:50:48,352 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,377 - Model output complete
2025-02-11 07:50:48,378 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:48,378 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,378 - Next token logits device: cuda:0
2025-02-11 07:50:48,378 - Entered do_sample
2025-02-11 07:50:48,378 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,380 - Probs max: 0.7861328125
2025-02-11 07:50:48,381 - Pre-cat
2025-02-11 07:50:48,381 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:50:48,382 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:50:48,382 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:48,382 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,382 - Step 5: Generated next token
2025-02-11 07:50:48,383 - Step 5: Updated current_ids
2025-02-11 07:50:48,383 - Step 5: Decoded token text:  at
2025-02-11 07:50:48,383 - Step 5: Updated current_phrase
2025-02-11 07:50:48,383 - Step 5: Created step_acts
2025-02-11 07:50:48,383 - Step 5: Added to generation_acts
2025-02-11 07:50:48,385 - Step 5: Updated generated_texts
2025-02-11 07:50:48,385 - Step 5: Updated recent_tokens
2025-02-11 07:50:48,385 - Step 5: Decoded current text
2025-02-11 07:50:48,385 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:48,385 - 
Starting step 6
2025-02-11 07:50:48,385 - Current_ids device: cuda:0
2025-02-11 07:50:48,385 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,428 - Model output complete
2025-02-11 07:50:48,428 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:48,428 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,428 - Next token logits device: cuda:0
2025-02-11 07:50:48,428 - Entered do_sample
2025-02-11 07:50:48,429 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,431 - Probs max: 0.8837890625
2025-02-11 07:50:48,432 - Pre-cat
2025-02-11 07:50:48,432 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:50:48,436 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:48,437 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:48,437 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,437 - Step 6: Generated next token
2025-02-11 07:50:48,437 - Step 6: Updated current_ids
2025-02-11 07:50:48,437 - Step 6: Decoded token text:  a
2025-02-11 07:50:48,437 - Step 6: Updated current_phrase
2025-02-11 07:50:48,438 - Step 6: Created step_acts
2025-02-11 07:50:48,438 - Step 6: Added to generation_acts
2025-02-11 07:50:48,438 - Step 6: Updated recent_tokens
2025-02-11 07:50:48,439 - Step 6: Decoded current text
2025-02-11 07:50:48,439 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:48,439 - 
Starting step 7
2025-02-11 07:50:48,439 - Current_ids device: cuda:0
2025-02-11 07:50:48,439 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,466 - Model output complete
2025-02-11 07:50:48,466 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:48,466 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,466 - Next token logits device: cuda:0
2025-02-11 07:50:48,466 - Entered do_sample
2025-02-11 07:50:48,466 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,469 - Probs max: 0.98974609375
2025-02-11 07:50:48,470 - Pre-cat
2025-02-11 07:50:48,470 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:50:48,472 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:48,472 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:48,472 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,472 - Step 7: Generated next token
2025-02-11 07:50:48,472 - Step 7: Updated current_ids
2025-02-11 07:50:48,472 - Step 7: Decoded token text:  wall
2025-02-11 07:50:48,472 - Step 7: Updated current_phrase
2025-02-11 07:50:48,473 - Step 7: Created step_acts
2025-02-11 07:50:48,473 - Step 7: Added to generation_acts
2025-02-11 07:50:48,473 - Step 7: Updated recent_tokens
2025-02-11 07:50:48,474 - Step 7: Decoded current text
2025-02-11 07:50:48,474 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:48,474 - 
Starting step 8
2025-02-11 07:50:48,474 - Current_ids device: cuda:0
2025-02-11 07:50:48,474 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,497 - Model output complete
2025-02-11 07:50:48,497 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:48,497 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,498 - Next token logits device: cuda:0
2025-02-11 07:50:48,498 - Entered do_sample
2025-02-11 07:50:48,498 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,501 - Probs max: 0.99462890625
2025-02-11 07:50:48,503 - Pre-cat
2025-02-11 07:50:48,503 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:50:48,507 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:48,507 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:48,507 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,507 - Step 8: Generated next token
2025-02-11 07:50:48,507 - Step 8: Updated current_ids
2025-02-11 07:50:48,508 - Step 8: Decoded token text:  very
2025-02-11 07:50:48,508 - Step 8: Updated current_phrase
2025-02-11 07:50:48,508 - Step 8: Created step_acts
2025-02-11 07:50:48,508 - Step 8: Added to generation_acts
2025-02-11 07:50:48,509 - Step 8: Updated recent_tokens
2025-02-11 07:50:48,510 - Step 8: Decoded current text
2025-02-11 07:50:48,510 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:48,510 - 
Starting step 9
2025-02-11 07:50:48,510 - Current_ids device: cuda:0
2025-02-11 07:50:48,510 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,540 - Model output complete
2025-02-11 07:50:48,540 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:48,540 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,541 - Next token logits device: cuda:0
2025-02-11 07:50:48,541 - Entered do_sample
2025-02-11 07:50:48,541 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,543 - Probs max: 0.99755859375
2025-02-11 07:50:48,544 - Pre-cat
2025-02-11 07:50:48,544 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:50:48,545 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:48,546 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:48,546 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,546 - Step 9: Generated next token
2025-02-11 07:50:48,546 - Step 9: Updated current_ids
2025-02-11 07:50:48,546 - Step 9: Decoded token text:  fast
2025-02-11 07:50:48,546 - Step 9: Updated current_phrase
2025-02-11 07:50:48,546 - Step 9: Created step_acts
2025-02-11 07:50:48,546 - Step 9: Added to generation_acts
2025-02-11 07:50:48,547 - Step 9: Updated recent_tokens
2025-02-11 07:50:48,549 - Step 9: Decoded current text
2025-02-11 07:50:48,549 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:48,549 - 
Starting step 10
2025-02-11 07:50:48,549 - Current_ids device: cuda:0
2025-02-11 07:50:48,549 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,573 - Model output complete
2025-02-11 07:50:48,573 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:48,573 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,573 - Next token logits device: cuda:0
2025-02-11 07:50:48,573 - Entered do_sample
2025-02-11 07:50:48,573 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,576 - Probs max: 0.99853515625
2025-02-11 07:50:48,577 - Pre-cat
2025-02-11 07:50:48,577 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:50:48,579 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:48,579 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:48,580 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,580 - Step 10: Generated next token
2025-02-11 07:50:48,580 - Step 10: Updated current_ids
2025-02-11 07:50:48,580 - Step 10: Decoded token text: ,
2025-02-11 07:50:48,580 - Step 10: Updated current_phrase
2025-02-11 07:50:48,580 - Step 10: Created step_acts
2025-02-11 07:50:48,581 - Step 10: Added to generation_acts
2025-02-11 07:50:48,582 - Step 10: Updated generated_texts
2025-02-11 07:50:48,582 - Step 10: Updated recent_tokens
2025-02-11 07:50:48,582 - Step 10: Found phrase end token
2025-02-11 07:50:48,582 - Step 10: Updated recent_phrases
2025-02-11 07:50:48,582 - Step 10: Decoded current text
2025-02-11 07:50:48,583 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:48,583 - 
Starting step 11
2025-02-11 07:50:48,583 - Current_ids device: cuda:0
2025-02-11 07:50:48,583 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,606 - Model output complete
2025-02-11 07:50:48,606 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:48,607 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,607 - Next token logits device: cuda:0
2025-02-11 07:50:48,607 - Entered do_sample
2025-02-11 07:50:48,607 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,609 - Probs max: 0.66357421875
2025-02-11 07:50:48,610 - Pre-cat
2025-02-11 07:50:48,610 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:50:48,612 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:48,613 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:48,613 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,613 - Step 11: Generated next token
2025-02-11 07:50:48,613 - Step 11: Updated current_ids
2025-02-11 07:50:48,613 - Step 11: Decoded token text:  the
2025-02-11 07:50:48,613 - Step 11: Updated current_phrase
2025-02-11 07:50:48,614 - Step 11: Created step_acts
2025-02-11 07:50:48,614 - Step 11: Added to generation_acts
2025-02-11 07:50:48,614 - Step 11: Updated recent_tokens
2025-02-11 07:50:48,615 - Step 11: Decoded current text
2025-02-11 07:50:48,615 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:48,615 - 
Starting step 12
2025-02-11 07:50:48,615 - Current_ids device: cuda:0
2025-02-11 07:50:48,615 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,649 - Model output complete
2025-02-11 07:50:48,649 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:48,649 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,649 - Next token logits device: cuda:0
2025-02-11 07:50:48,649 - Entered do_sample
2025-02-11 07:50:48,649 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,651 - Probs max: 0.556640625
2025-02-11 07:50:48,652 - Pre-cat
2025-02-11 07:50:48,652 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279]],
       device='cuda:0')
2025-02-11 07:50:48,655 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:48,655 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:48,655 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,655 - Step 12: Generated next token
2025-02-11 07:50:48,656 - Step 12: Updated current_ids
2025-02-11 07:50:48,656 - Step 12: Decoded token text:  ball
2025-02-11 07:50:48,656 - Step 12: Updated current_phrase
2025-02-11 07:50:48,656 - Step 12: Created step_acts
2025-02-11 07:50:48,656 - Step 12: Added to generation_acts
2025-02-11 07:50:48,656 - Step 12: Updated recent_tokens
2025-02-11 07:50:48,658 - Step 12: Decoded current text
2025-02-11 07:50:48,658 - Step 12: Reset consecutive_fillers
2025-02-11 07:50:48,658 - 
Starting step 13
2025-02-11 07:50:48,658 - Current_ids device: cuda:0
2025-02-11 07:50:48,658 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,684 - Model output complete
2025-02-11 07:50:48,684 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:48,684 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,684 - Next token logits device: cuda:0
2025-02-11 07:50:48,684 - Entered do_sample
2025-02-11 07:50:48,684 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,686 - Probs max: 0.71337890625
2025-02-11 07:50:48,687 - Pre-cat
2025-02-11 07:50:48,687 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935]],
       device='cuda:0')
2025-02-11 07:50:48,689 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:48,690 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:48,690 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,690 - Step 13: Generated next token
2025-02-11 07:50:48,690 - Step 13: Updated current_ids
2025-02-11 07:50:48,690 - Step 13: Decoded token text:  will
2025-02-11 07:50:48,690 - Step 13: Updated current_phrase
2025-02-11 07:50:48,691 - Step 13: Created step_acts
2025-02-11 07:50:48,691 - Step 13: Added to generation_acts
2025-02-11 07:50:48,691 - Step 13: Updated recent_tokens
2025-02-11 07:50:48,693 - Step 13: Decoded current text
2025-02-11 07:50:48,693 - Step 13: Reset consecutive_fillers
2025-02-11 07:50:48,693 - 
Starting step 14
2025-02-11 07:50:48,693 - Current_ids device: cuda:0
2025-02-11 07:50:48,693 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,716 - Model output complete
2025-02-11 07:50:48,717 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:50:48,717 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,717 - Next token logits device: cuda:0
2025-02-11 07:50:48,717 - Entered do_sample
2025-02-11 07:50:48,717 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,719 - Probs max: 0.37744140625
2025-02-11 07:50:48,720 - Pre-cat
2025-02-11 07:50:48,720 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686]],
       device='cuda:0')
2025-02-11 07:50:48,722 - Next token: tensor([[614]], device='cuda:0')
2025-02-11 07:50:48,722 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:50:48,722 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,722 - Step 14: Generated next token
2025-02-11 07:50:48,722 - Step 14: Updated current_ids
2025-02-11 07:50:48,723 - Step 14: Decoded token text:  have
2025-02-11 07:50:48,723 - Step 14: Updated current_phrase
2025-02-11 07:50:48,723 - Step 14: Created step_acts
2025-02-11 07:50:48,723 - Step 14: Added to generation_acts
2025-02-11 07:50:48,724 - Step 14: Updated recent_tokens
2025-02-11 07:50:48,725 - Step 14: Decoded current text
2025-02-11 07:50:48,726 - Step 14: Reset consecutive_fillers
2025-02-11 07:50:48,726 - 
Starting step 15
2025-02-11 07:50:48,726 - Current_ids device: cuda:0
2025-02-11 07:50:48,726 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,749 - Model output complete
2025-02-11 07:50:48,749 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:50:48,750 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,750 - Next token logits device: cuda:0
2025-02-11 07:50:48,750 - Entered do_sample
2025-02-11 07:50:48,750 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,752 - Probs max: 0.84521484375
2025-02-11 07:50:48,753 - Pre-cat
2025-02-11 07:50:48,753 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614]], device='cuda:0')
2025-02-11 07:50:48,756 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:48,756 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:50:48,756 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,756 - Step 15: Generated next token
2025-02-11 07:50:48,756 - Step 15: Updated current_ids
2025-02-11 07:50:48,756 - Step 15: Decoded token text:  a
2025-02-11 07:50:48,756 - Step 15: Updated current_phrase
2025-02-11 07:50:48,757 - Step 15: Created step_acts
2025-02-11 07:50:48,757 - Step 15: Added to generation_acts
2025-02-11 07:50:48,758 - Step 15: Updated generated_texts
2025-02-11 07:50:48,758 - Step 15: Updated recent_tokens
2025-02-11 07:50:48,759 - Step 15: Decoded current text
2025-02-11 07:50:48,759 - Step 15: Reset consecutive_fillers
2025-02-11 07:50:48,759 - 
Starting step 16
2025-02-11 07:50:48,759 - Current_ids device: cuda:0
2025-02-11 07:50:48,759 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,781 - Model output complete
2025-02-11 07:50:48,781 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:50:48,782 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,782 - Next token logits device: cuda:0
2025-02-11 07:50:48,782 - Entered do_sample
2025-02-11 07:50:48,782 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,785 - Probs max: 0.59033203125
2025-02-11 07:50:48,785 - Pre-cat
2025-02-11 07:50:48,785 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264]], device='cuda:0')
2025-02-11 07:50:48,788 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:48,789 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:50:48,789 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,789 - Step 16: Generated next token
2025-02-11 07:50:48,789 - Step 16: Updated current_ids
2025-02-11 07:50:48,789 - Step 16: Decoded token text:  very
2025-02-11 07:50:48,789 - Step 16: Updated current_phrase
2025-02-11 07:50:48,789 - Step 16: Created step_acts
2025-02-11 07:50:48,790 - Step 16: Added to generation_acts
2025-02-11 07:50:48,790 - Step 16: Updated recent_tokens
2025-02-11 07:50:48,791 - Step 16: Decoded current text
2025-02-11 07:50:48,791 - Step 16: Reset consecutive_fillers
2025-02-11 07:50:48,791 - Step 16: Calculated unique_ratio: 0.75
2025-02-11 07:50:48,791 - 
Starting step 17
2025-02-11 07:50:48,791 - Current_ids device: cuda:0
2025-02-11 07:50:48,791 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,815 - Model output complete
2025-02-11 07:50:48,815 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:50:48,815 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,815 - Next token logits device: cuda:0
2025-02-11 07:50:48,815 - Entered do_sample
2025-02-11 07:50:48,815 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,817 - Probs max: 0.498291015625
2025-02-11 07:50:48,818 - Pre-cat
2025-02-11 07:50:48,819 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602]], device='cuda:0')
2025-02-11 07:50:48,821 - Next token: tensor([[1550]], device='cuda:0')
2025-02-11 07:50:48,822 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:50:48,822 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,822 - Step 17: Generated next token
2025-02-11 07:50:48,822 - Step 17: Updated current_ids
2025-02-11 07:50:48,822 - Step 17: Decoded token text:  high
2025-02-11 07:50:48,822 - Step 17: Updated current_phrase
2025-02-11 07:50:48,823 - Step 17: Created step_acts
2025-02-11 07:50:48,823 - Step 17: Added to generation_acts
2025-02-11 07:50:48,823 - Step 17: Updated recent_tokens
2025-02-11 07:50:48,824 - Step 17: Decoded current text
2025-02-11 07:50:48,824 - Step 17: Reset consecutive_fillers
2025-02-11 07:50:48,824 - Step 17: Calculated unique_ratio: 0.8125
2025-02-11 07:50:48,824 - 
Starting step 18
2025-02-11 07:50:48,825 - Current_ids device: cuda:0
2025-02-11 07:50:48,825 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,849 - Model output complete
2025-02-11 07:50:48,850 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:50:48,850 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,850 - Next token logits device: cuda:0
2025-02-11 07:50:48,850 - Entered do_sample
2025-02-11 07:50:48,850 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,853 - Probs max: 0.67236328125
2025-02-11 07:50:48,854 - Pre-cat
2025-02-11 07:50:48,854 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550]], device='cuda:0')
2025-02-11 07:50:48,856 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:50:48,856 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:50:48,856 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,856 - Step 18: Generated next token
2025-02-11 07:50:48,856 - Step 18: Updated current_ids
2025-02-11 07:50:48,856 - Step 18: Decoded token text:  speed
2025-02-11 07:50:48,857 - Step 18: Updated current_phrase
2025-02-11 07:50:48,857 - Step 18: Created step_acts
2025-02-11 07:50:48,857 - Step 18: Added to generation_acts
2025-02-11 07:50:48,857 - Step 18: Updated recent_tokens
2025-02-11 07:50:48,858 - Step 18: Decoded current text
2025-02-11 07:50:48,858 - Step 18: Reset consecutive_fillers
2025-02-11 07:50:48,859 - Step 18: Calculated unique_ratio: 0.875
2025-02-11 07:50:48,859 - 
Starting step 19
2025-02-11 07:50:48,859 - Current_ids device: cuda:0
2025-02-11 07:50:48,859 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,884 - Model output complete
2025-02-11 07:50:48,884 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:50:48,884 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,884 - Next token logits device: cuda:0
2025-02-11 07:50:48,884 - Entered do_sample
2025-02-11 07:50:48,884 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,887 - Probs max: 0.611328125
2025-02-11 07:50:48,887 - Pre-cat
2025-02-11 07:50:48,888 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628]], device='cuda:0')
2025-02-11 07:50:48,891 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:48,891 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:50:48,891 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,891 - Step 19: Generated next token
2025-02-11 07:50:48,891 - Step 19: Updated current_ids
2025-02-11 07:50:48,891 - Step 19: Decoded token text: ,
2025-02-11 07:50:48,891 - Step 19: Updated current_phrase
2025-02-11 07:50:48,892 - Step 19: Created step_acts
2025-02-11 07:50:48,892 - Step 19: Added to generation_acts
2025-02-11 07:50:48,892 - Step 19: Updated recent_tokens
2025-02-11 07:50:48,893 - Step 19: Found phrase end token
2025-02-11 07:50:48,893 - Step 19: Updated recent_phrases
2025-02-11 07:50:48,893 - Step 19: Calculated similarity: 0.5
2025-02-11 07:50:48,894 - Step 19: Decoded current text
2025-02-11 07:50:48,894 - Step 19: Reset consecutive_fillers
2025-02-11 07:50:48,894 - Step 19: Calculated unique_ratio: 0.8125
2025-02-11 07:50:48,894 - 
Starting step 20
2025-02-11 07:50:48,894 - Current_ids device: cuda:0
2025-02-11 07:50:48,894 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,922 - Model output complete
2025-02-11 07:50:48,922 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:50:48,922 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,922 - Next token logits device: cuda:0
2025-02-11 07:50:48,922 - Entered do_sample
2025-02-11 07:50:48,922 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,924 - Probs max: 0.380615234375
2025-02-11 07:50:48,926 - Pre-cat
2025-02-11 07:50:48,926 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11]], device='cuda:0')
2025-02-11 07:50:48,931 - Next token: tensor([[714]], device='cuda:0')
2025-02-11 07:50:48,931 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:50:48,931 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,932 - Step 20: Generated next token
2025-02-11 07:50:48,932 - Step 20: Updated current_ids
2025-02-11 07:50:48,932 - Step 20: Decoded token text:  but
2025-02-11 07:50:48,932 - Step 20: Updated current_phrase
2025-02-11 07:50:48,933 - Step 20: Created step_acts
2025-02-11 07:50:48,933 - Step 20: Added to generation_acts
2025-02-11 07:50:48,934 - Step 20: Updated generated_texts
2025-02-11 07:50:48,934 - Step 20: Updated recent_tokens
2025-02-11 07:50:48,935 - Step 20: Decoded current text
2025-02-11 07:50:48,935 - Step 20: Reset consecutive_fillers
2025-02-11 07:50:48,935 - Step 20: Calculated unique_ratio: 0.8125
2025-02-11 07:50:48,935 - 
Starting step 21
2025-02-11 07:50:48,935 - Current_ids device: cuda:0
2025-02-11 07:50:48,935 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,960 - Model output complete
2025-02-11 07:50:48,960 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:50:48,960 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,960 - Next token logits device: cuda:0
2025-02-11 07:50:48,960 - Entered do_sample
2025-02-11 07:50:48,960 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,964 - Probs max: 0.84375
2025-02-11 07:50:48,964 - Pre-cat
2025-02-11 07:50:48,964 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714]],
       device='cuda:0')
2025-02-11 07:50:48,967 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:48,967 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:50:48,967 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:48,967 - Step 21: Generated next token
2025-02-11 07:50:48,967 - Step 21: Updated current_ids
2025-02-11 07:50:48,968 - Step 21: Decoded token text:  the
2025-02-11 07:50:48,968 - Step 21: Updated current_phrase
2025-02-11 07:50:48,968 - Step 21: Created step_acts
2025-02-11 07:50:48,968 - Step 21: Added to generation_acts
2025-02-11 07:50:48,968 - Step 21: Updated recent_tokens
2025-02-11 07:50:48,969 - Step 21: Decoded current text
2025-02-11 07:50:48,969 - Step 21: Reset consecutive_fillers
2025-02-11 07:50:48,970 - Step 21: Calculated unique_ratio: 0.75
2025-02-11 07:50:48,970 - 
Starting step 22
2025-02-11 07:50:48,970 - Current_ids device: cuda:0
2025-02-11 07:50:48,970 - Current_ids dtype: torch.int64
2025-02-11 07:50:48,994 - Model output complete
2025-02-11 07:50:48,994 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:50:48,994 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,994 - Next token logits device: cuda:0
2025-02-11 07:50:48,994 - Entered do_sample
2025-02-11 07:50:48,995 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:48,998 - Probs max: 0.90625
2025-02-11 07:50:48,998 - Pre-cat
2025-02-11 07:50:48,998 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279]],
       device='cuda:0')
2025-02-11 07:50:49,001 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:49,001 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:50:49,001 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,001 - Step 22: Generated next token
2025-02-11 07:50:49,001 - Step 22: Updated current_ids
2025-02-11 07:50:49,001 - Step 22: Decoded token text:  wall
2025-02-11 07:50:49,001 - Step 22: Updated current_phrase
2025-02-11 07:50:49,002 - Step 22: Created step_acts
2025-02-11 07:50:49,002 - Step 22: Added to generation_acts
2025-02-11 07:50:49,002 - Step 22: Updated recent_tokens
2025-02-11 07:50:49,003 - Step 22: Decoded current text
2025-02-11 07:50:49,003 - Step 22: Reset consecutive_fillers
2025-02-11 07:50:49,003 - Step 22: Calculated unique_ratio: 0.75
2025-02-11 07:50:49,003 - 
Starting step 23
2025-02-11 07:50:49,003 - Current_ids device: cuda:0
2025-02-11 07:50:49,004 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,027 - Model output complete
2025-02-11 07:50:49,028 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:50:49,028 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,028 - Next token logits device: cuda:0
2025-02-11 07:50:49,028 - Entered do_sample
2025-02-11 07:50:49,028 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,030 - Probs max: 0.36376953125
2025-02-11 07:50:49,031 - Pre-cat
2025-02-11 07:50:49,031 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002]],
       device='cuda:0')
2025-02-11 07:50:49,033 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:49,033 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:50:49,034 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,034 - Step 23: Generated next token
2025-02-11 07:50:49,034 - Step 23: Updated current_ids
2025-02-11 07:50:49,034 - Step 23: Decoded token text:  will
2025-02-11 07:50:49,034 - Step 23: Updated current_phrase
2025-02-11 07:50:49,034 - Step 23: Created step_acts
2025-02-11 07:50:49,034 - Step 23: Added to generation_acts
2025-02-11 07:50:49,034 - Step 23: Updated recent_tokens
2025-02-11 07:50:49,036 - Step 23: Decoded current text
2025-02-11 07:50:49,036 - Step 23: Reset consecutive_fillers
2025-02-11 07:50:49,036 - Step 23: Calculated unique_ratio: 0.75
2025-02-11 07:50:49,036 - 
Starting step 24
2025-02-11 07:50:49,036 - Current_ids device: cuda:0
2025-02-11 07:50:49,036 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,061 - Model output complete
2025-02-11 07:50:49,061 - Logits shape: torch.Size([1, 55, 151936])
2025-02-11 07:50:49,061 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,061 - Next token logits device: cuda:0
2025-02-11 07:50:49,061 - Entered do_sample
2025-02-11 07:50:49,061 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,063 - Probs max: 0.65625
2025-02-11 07:50:49,064 - Pre-cat
2025-02-11 07:50:49,064 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686]], device='cuda:0')
2025-02-11 07:50:49,066 - Next token: tensor([[614]], device='cuda:0')
2025-02-11 07:50:49,067 - Current_ids shape: torch.Size([1, 55])
2025-02-11 07:50:49,067 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,067 - Step 24: Generated next token
2025-02-11 07:50:49,067 - Step 24: Updated current_ids
2025-02-11 07:50:49,067 - Step 24: Decoded token text:  have
2025-02-11 07:50:49,067 - Step 24: Updated current_phrase
2025-02-11 07:50:49,067 - Step 24: Created step_acts
2025-02-11 07:50:49,067 - Step 24: Added to generation_acts
2025-02-11 07:50:49,067 - Step 24: Updated recent_tokens
2025-02-11 07:50:49,069 - Step 24: Decoded current text
2025-02-11 07:50:49,069 - Step 24: Reset consecutive_fillers
2025-02-11 07:50:49,069 - Step 24: Calculated unique_ratio: 0.75
2025-02-11 07:50:49,069 - 
Starting step 25
2025-02-11 07:50:49,069 - Current_ids device: cuda:0
2025-02-11 07:50:49,069 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,093 - Model output complete
2025-02-11 07:50:49,093 - Logits shape: torch.Size([1, 56, 151936])
2025-02-11 07:50:49,093 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,093 - Next token logits device: cuda:0
2025-02-11 07:50:49,093 - Entered do_sample
2025-02-11 07:50:49,094 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,096 - Probs max: 0.92333984375
2025-02-11 07:50:49,097 - Pre-cat
2025-02-11 07:50:49,097 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614]], device='cuda:0')
2025-02-11 07:50:49,099 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:49,099 - Current_ids shape: torch.Size([1, 56])
2025-02-11 07:50:49,099 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,099 - Step 25: Generated next token
2025-02-11 07:50:49,099 - Step 25: Updated current_ids
2025-02-11 07:50:49,100 - Step 25: Decoded token text:  a
2025-02-11 07:50:49,100 - Step 25: Updated current_phrase
2025-02-11 07:50:49,100 - Step 25: Created step_acts
2025-02-11 07:50:49,100 - Step 25: Added to generation_acts
2025-02-11 07:50:49,101 - Step 25: Updated generated_texts
2025-02-11 07:50:49,101 - Step 25: Updated recent_tokens
2025-02-11 07:50:49,102 - Step 25: Decoded current text
2025-02-11 07:50:49,102 - Step 25: Reset consecutive_fillers
2025-02-11 07:50:49,102 - Step 25: Calculated unique_ratio: 0.6875
2025-02-11 07:50:49,102 - 
Starting step 26
2025-02-11 07:50:49,102 - Current_ids device: cuda:0
2025-02-11 07:50:49,102 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,128 - Model output complete
2025-02-11 07:50:49,128 - Logits shape: torch.Size([1, 57, 151936])
2025-02-11 07:50:49,128 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,128 - Next token logits device: cuda:0
2025-02-11 07:50:49,128 - Entered do_sample
2025-02-11 07:50:49,128 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,130 - Probs max: 0.90673828125
2025-02-11 07:50:49,131 - Pre-cat
2025-02-11 07:50:49,131 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264]], device='cuda:0')
2025-02-11 07:50:49,133 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:49,133 - Current_ids shape: torch.Size([1, 57])
2025-02-11 07:50:49,133 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,133 - Step 26: Generated next token
2025-02-11 07:50:49,133 - Step 26: Updated current_ids
2025-02-11 07:50:49,134 - Step 26: Decoded token text:  very
2025-02-11 07:50:49,134 - Step 26: Updated current_phrase
2025-02-11 07:50:49,134 - Step 26: Created step_acts
2025-02-11 07:50:49,134 - Step 26: Added to generation_acts
2025-02-11 07:50:49,134 - Step 26: Updated recent_tokens
2025-02-11 07:50:49,136 - Step 26: Decoded current text
2025-02-11 07:50:49,136 - Step 26: Reset consecutive_fillers
2025-02-11 07:50:49,136 - Step 26: Calculated unique_ratio: 0.6875
2025-02-11 07:50:49,136 - 
Starting step 27
2025-02-11 07:50:49,136 - Current_ids device: cuda:0
2025-02-11 07:50:49,136 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,161 - Model output complete
2025-02-11 07:50:49,161 - Logits shape: torch.Size([1, 58, 151936])
2025-02-11 07:50:49,161 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,161 - Next token logits device: cuda:0
2025-02-11 07:50:49,161 - Entered do_sample
2025-02-11 07:50:49,161 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,164 - Probs max: 0.91796875
2025-02-11 07:50:49,164 - Pre-cat
2025-02-11 07:50:49,164 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602]], device='cuda:0')
2025-02-11 07:50:49,166 - Next token: tensor([[1550]], device='cuda:0')
2025-02-11 07:50:49,167 - Current_ids shape: torch.Size([1, 58])
2025-02-11 07:50:49,167 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,167 - Step 27: Generated next token
2025-02-11 07:50:49,167 - Step 27: Updated current_ids
2025-02-11 07:50:49,167 - Step 27: Decoded token text:  high
2025-02-11 07:50:49,167 - Step 27: Updated current_phrase
2025-02-11 07:50:49,167 - Step 27: Created step_acts
2025-02-11 07:50:49,167 - Step 27: Added to generation_acts
2025-02-11 07:50:49,167 - Step 27: Updated recent_tokens
2025-02-11 07:50:49,169 - Step 27: Decoded current text
2025-02-11 07:50:49,169 - Step 27: Reset consecutive_fillers
2025-02-11 07:50:49,169 - Step 27: Calculated unique_ratio: 0.6875
2025-02-11 07:50:49,169 - 
Starting step 28
2025-02-11 07:50:49,169 - Current_ids device: cuda:0
2025-02-11 07:50:49,169 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,192 - Model output complete
2025-02-11 07:50:49,192 - Logits shape: torch.Size([1, 59, 151936])
2025-02-11 07:50:49,192 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,193 - Next token logits device: cuda:0
2025-02-11 07:50:49,193 - Entered do_sample
2025-02-11 07:50:49,193 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,196 - Probs max: 0.9755859375
2025-02-11 07:50:49,196 - Pre-cat
2025-02-11 07:50:49,196 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550]], device='cuda:0')
2025-02-11 07:50:49,198 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:50:49,199 - Current_ids shape: torch.Size([1, 59])
2025-02-11 07:50:49,199 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,199 - Step 28: Generated next token
2025-02-11 07:50:49,199 - Step 28: Updated current_ids
2025-02-11 07:50:49,199 - Step 28: Decoded token text:  speed
2025-02-11 07:50:49,199 - Step 28: Updated current_phrase
2025-02-11 07:50:49,199 - Step 28: Created step_acts
2025-02-11 07:50:49,199 - Step 28: Added to generation_acts
2025-02-11 07:50:49,200 - Step 28: Updated recent_tokens
2025-02-11 07:50:49,201 - Step 28: Decoded current text
2025-02-11 07:50:49,201 - Step 28: Reset consecutive_fillers
2025-02-11 07:50:49,201 - Step 28: Calculated unique_ratio: 0.625
2025-02-11 07:50:49,202 - 
Starting step 29
2025-02-11 07:50:49,202 - Current_ids device: cuda:0
2025-02-11 07:50:49,202 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,246 - Model output complete
2025-02-11 07:50:49,246 - Logits shape: torch.Size([1, 60, 151936])
2025-02-11 07:50:49,246 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,246 - Next token logits device: cuda:0
2025-02-11 07:50:49,247 - Entered do_sample
2025-02-11 07:50:49,247 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,249 - Probs max: 0.7138671875
2025-02-11 07:50:49,252 - Pre-cat
2025-02-11 07:50:49,253 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628]], device='cuda:0')
2025-02-11 07:50:49,258 - Next token: tensor([[438]], device='cuda:0')
2025-02-11 07:50:49,259 - Current_ids shape: torch.Size([1, 60])
2025-02-11 07:50:49,259 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,259 - Step 29: Generated next token
2025-02-11 07:50:49,259 - Step 29: Updated current_ids
2025-02-11 07:50:49,260 - Step 29: Decoded token text:  as
2025-02-11 07:50:49,260 - Step 29: Updated current_phrase
2025-02-11 07:50:49,260 - Step 29: Created step_acts
2025-02-11 07:50:49,260 - Step 29: Added to generation_acts
2025-02-11 07:50:49,260 - Step 29: Updated recent_tokens
2025-02-11 07:50:49,262 - Step 29: Decoded current text
2025-02-11 07:50:49,262 - Step 29: Reset consecutive_fillers
2025-02-11 07:50:49,262 - Step 29: Calculated unique_ratio: 0.6875
2025-02-11 07:50:49,262 - 
Starting step 30
2025-02-11 07:50:49,263 - Current_ids device: cuda:0
2025-02-11 07:50:49,263 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,295 - Model output complete
2025-02-11 07:50:49,296 - Logits shape: torch.Size([1, 61, 151936])
2025-02-11 07:50:49,296 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,296 - Next token logits device: cuda:0
2025-02-11 07:50:49,296 - Entered do_sample
2025-02-11 07:50:49,296 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,299 - Probs max: 0.99755859375
2025-02-11 07:50:49,300 - Pre-cat
2025-02-11 07:50:49,300 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438]],
       device='cuda:0')
2025-02-11 07:50:49,302 - Next token: tensor([[1632]], device='cuda:0')
2025-02-11 07:50:49,302 - Current_ids shape: torch.Size([1, 61])
2025-02-11 07:50:49,302 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,302 - Step 30: Generated next token
2025-02-11 07:50:49,303 - Step 30: Updated current_ids
2025-02-11 07:50:49,303 - Step 30: Decoded token text:  well
2025-02-11 07:50:49,303 - Step 30: Updated current_phrase
2025-02-11 07:50:49,303 - Step 30: Created step_acts
2025-02-11 07:50:49,303 - Step 30: Added to generation_acts
2025-02-11 07:50:49,305 - Step 30: Updated generated_texts
2025-02-11 07:50:49,305 - Step 30: Updated recent_tokens
2025-02-11 07:50:49,305 - Step 30: Decoded current text
2025-02-11 07:50:49,305 - Step 30: Reset consecutive_fillers
2025-02-11 07:50:49,305 - Step 30: Calculated unique_ratio: 0.75
2025-02-11 07:50:49,305 - 
Starting step 31
2025-02-11 07:50:49,305 - Current_ids device: cuda:0
2025-02-11 07:50:49,305 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,328 - Model output complete
2025-02-11 07:50:49,328 - Logits shape: torch.Size([1, 62, 151936])
2025-02-11 07:50:49,328 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,328 - Next token logits device: cuda:0
2025-02-11 07:50:49,328 - Entered do_sample
2025-02-11 07:50:49,328 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,332 - Probs max: 0.85693359375
2025-02-11 07:50:49,333 - Pre-cat
2025-02-11 07:50:49,333 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438,   1632]],
       device='cuda:0')
2025-02-11 07:50:49,335 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:49,336 - Current_ids shape: torch.Size([1, 62])
2025-02-11 07:50:49,336 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,336 - Step 31: Generated next token
2025-02-11 07:50:49,336 - Step 31: Updated current_ids
2025-02-11 07:50:49,336 - Step 31: Decoded token text: .
2025-02-11 07:50:49,336 - Step 31: Updated current_phrase
2025-02-11 07:50:49,337 - Step 31: Created step_acts
2025-02-11 07:50:49,337 - Step 31: Added to generation_acts
2025-02-11 07:50:49,337 - Step 31: Updated recent_tokens
2025-02-11 07:50:49,338 - Step 31: Found phrase end token
2025-02-11 07:50:49,338 - Step 31: Updated recent_phrases
2025-02-11 07:50:49,338 - Step 31: Calculated similarity: 0.75
2025-02-11 07:50:49,338 - Step 31: Decoded current text
2025-02-11 07:50:49,338 - Step 31: Reset consecutive_fillers
2025-02-11 07:50:49,339 - Step 31: Calculated unique_ratio: 0.8125
2025-02-11 07:50:49,339 - 
Starting step 32
2025-02-11 07:50:49,339 - Current_ids device: cuda:0
2025-02-11 07:50:49,339 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,362 - Model output complete
2025-02-11 07:50:49,363 - Logits shape: torch.Size([1, 63, 151936])
2025-02-11 07:50:49,363 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,363 - Next token logits device: cuda:0
2025-02-11 07:50:49,363 - Entered do_sample
2025-02-11 07:50:49,363 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,366 - Probs max: 0.388671875
2025-02-11 07:50:49,367 - Pre-cat
2025-02-11 07:50:49,367 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438,   1632,     13]],
       device='cuda:0')
2025-02-11 07:50:49,369 - Next token: tensor([[2055]], device='cuda:0')
2025-02-11 07:50:49,370 - Current_ids shape: torch.Size([1, 63])
2025-02-11 07:50:49,370 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,370 - Step 32: Generated next token
2025-02-11 07:50:49,370 - Step 32: Updated current_ids
2025-02-11 07:50:49,370 - Step 32: Decoded token text:  So
2025-02-11 07:50:49,370 - Step 32: Updated current_phrase
2025-02-11 07:50:49,370 - Step 32: Created step_acts
2025-02-11 07:50:49,370 - Step 32: Added to generation_acts
2025-02-11 07:50:49,370 - Step 32: Updated recent_tokens
2025-02-11 07:50:49,372 - Step 32: Decoded current text
2025-02-11 07:50:49,372 - Step 32: Reset consecutive_fillers
2025-02-11 07:50:49,372 - Step 32: Calculated unique_ratio: 0.875
2025-02-11 07:50:49,372 - 
Starting step 33
2025-02-11 07:50:49,372 - Current_ids device: cuda:0
2025-02-11 07:50:49,372 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,403 - Model output complete
2025-02-11 07:50:49,403 - Logits shape: torch.Size([1, 64, 151936])
2025-02-11 07:50:49,403 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,403 - Next token logits device: cuda:0
2025-02-11 07:50:49,403 - Entered do_sample
2025-02-11 07:50:49,403 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,406 - Probs max: 0.8505859375
2025-02-11 07:50:49,406 - Pre-cat
2025-02-11 07:50:49,406 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438,   1632,     13,
           2055]], device='cuda:0')
2025-02-11 07:50:49,410 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:49,410 - Current_ids shape: torch.Size([1, 64])
2025-02-11 07:50:49,410 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,410 - Step 33: Generated next token
2025-02-11 07:50:49,410 - Step 33: Updated current_ids
2025-02-11 07:50:49,411 - Step 33: Decoded token text: ,
2025-02-11 07:50:49,411 - Step 33: Updated current_phrase
2025-02-11 07:50:49,411 - Step 33: Created step_acts
2025-02-11 07:50:49,411 - Step 33: Added to generation_acts
2025-02-11 07:50:49,411 - Step 33: Updated recent_tokens
2025-02-11 07:50:49,413 - Step 33: Found phrase end token
2025-02-11 07:50:49,413 - Step 33: Updated recent_phrases
2025-02-11 07:50:49,413 - Step 33: Calculated similarity: 0.0
2025-02-11 07:50:49,413 - Step 33: Decoded current text
2025-02-11 07:50:49,413 - Step 33: Reset consecutive_fillers
2025-02-11 07:50:49,413 - Step 33: Calculated unique_ratio: 0.875
2025-02-11 07:50:49,413 - 
Starting step 34
2025-02-11 07:50:49,414 - Current_ids device: cuda:0
2025-02-11 07:50:49,414 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,437 - Model output complete
2025-02-11 07:50:49,437 - Logits shape: torch.Size([1, 65, 151936])
2025-02-11 07:50:49,437 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,437 - Next token logits device: cuda:0
2025-02-11 07:50:49,437 - Entered do_sample
2025-02-11 07:50:49,438 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,442 - Probs max: 0.85546875
2025-02-11 07:50:49,442 - Pre-cat
2025-02-11 07:50:49,442 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438,   1632,     13,
           2055,     11]], device='cuda:0')
2025-02-11 07:50:49,445 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:49,445 - Current_ids shape: torch.Size([1, 65])
2025-02-11 07:50:49,445 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,445 - Step 34: Generated next token
2025-02-11 07:50:49,445 - Step 34: Updated current_ids
2025-02-11 07:50:49,445 - Step 34: Decoded token text:  the
2025-02-11 07:50:49,445 - Step 34: Updated current_phrase
2025-02-11 07:50:49,446 - Step 34: Created step_acts
2025-02-11 07:50:49,446 - Step 34: Added to generation_acts
2025-02-11 07:50:49,446 - Step 34: Updated recent_tokens
2025-02-11 07:50:49,447 - Step 34: Decoded current text
2025-02-11 07:50:49,447 - Step 34: Reset consecutive_fillers
2025-02-11 07:50:49,448 - Step 34: Calculated unique_ratio: 0.875
2025-02-11 07:50:49,448 - 
Starting step 35
2025-02-11 07:50:49,448 - Current_ids device: cuda:0
2025-02-11 07:50:49,448 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,474 - Model output complete
2025-02-11 07:50:49,474 - Logits shape: torch.Size([1, 66, 151936])
2025-02-11 07:50:49,474 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,474 - Next token logits device: cuda:0
2025-02-11 07:50:49,475 - Entered do_sample
2025-02-11 07:50:49,475 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,477 - Probs max: 0.65625
2025-02-11 07:50:49,478 - Pre-cat
2025-02-11 07:50:49,478 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438,   1632,     13,
           2055,     11,    279]], device='cuda:0')
2025-02-11 07:50:49,482 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:49,482 - Current_ids shape: torch.Size([1, 66])
2025-02-11 07:50:49,482 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,482 - Step 35: Generated next token
2025-02-11 07:50:49,482 - Step 35: Updated current_ids
2025-02-11 07:50:49,482 - Step 35: Decoded token text:  wall
2025-02-11 07:50:49,482 - Step 35: Updated current_phrase
2025-02-11 07:50:49,483 - Step 35: Created step_acts
2025-02-11 07:50:49,483 - Step 35: Added to generation_acts
2025-02-11 07:50:49,484 - Step 35: Updated generated_texts
2025-02-11 07:50:49,485 - Step 35: Updated recent_tokens
2025-02-11 07:50:49,485 - Step 35: Decoded current text
2025-02-11 07:50:49,485 - Step 35: Reset consecutive_fillers
2025-02-11 07:50:49,485 - Step 35: Calculated unique_ratio: 0.875
2025-02-11 07:50:49,485 - 
Starting step 36
2025-02-11 07:50:49,485 - Current_ids device: cuda:0
2025-02-11 07:50:49,485 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,520 - Model output complete
2025-02-11 07:50:49,520 - Logits shape: torch.Size([1, 67, 151936])
2025-02-11 07:50:49,520 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,520 - Next token logits device: cuda:0
2025-02-11 07:50:49,520 - Entered do_sample
2025-02-11 07:50:49,520 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,524 - Probs max: 0.78564453125
2025-02-11 07:50:49,525 - Pre-cat
2025-02-11 07:50:49,525 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438,   1632,     13,
           2055,     11,    279,   7002]], device='cuda:0')
2025-02-11 07:50:49,527 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:49,527 - Current_ids shape: torch.Size([1, 67])
2025-02-11 07:50:49,527 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,528 - Step 36: Generated next token
2025-02-11 07:50:49,528 - Step 36: Updated current_ids
2025-02-11 07:50:49,528 - Step 36: Decoded token text:  will
2025-02-11 07:50:49,528 - Step 36: Updated current_phrase
2025-02-11 07:50:49,528 - Step 36: Created step_acts
2025-02-11 07:50:49,528 - Step 36: Added to generation_acts
2025-02-11 07:50:49,528 - Step 36: Updated recent_tokens
2025-02-11 07:50:49,530 - Step 36: Decoded current text
2025-02-11 07:50:49,530 - Step 36: Reset consecutive_fillers
2025-02-11 07:50:49,530 - Step 36: Calculated unique_ratio: 0.8125
2025-02-11 07:50:49,530 - 
Starting step 37
2025-02-11 07:50:49,531 - Current_ids device: cuda:0
2025-02-11 07:50:49,531 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,556 - Model output complete
2025-02-11 07:50:49,556 - Logits shape: torch.Size([1, 68, 151936])
2025-02-11 07:50:49,557 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,557 - Next token logits device: cuda:0
2025-02-11 07:50:49,557 - Entered do_sample
2025-02-11 07:50:49,557 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,560 - Probs max: 0.391357421875
2025-02-11 07:50:49,561 - Pre-cat
2025-02-11 07:50:49,561 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438,   1632,     13,
           2055,     11,    279,   7002,    686]], device='cuda:0')
2025-02-11 07:50:49,563 - Next token: tensor([[42744]], device='cuda:0')
2025-02-11 07:50:49,564 - Current_ids shape: torch.Size([1, 68])
2025-02-11 07:50:49,564 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,564 - Step 37: Generated next token
2025-02-11 07:50:49,564 - Step 37: Updated current_ids
2025-02-11 07:50:49,564 - Step 37: Decoded token text:  exert
2025-02-11 07:50:49,564 - Step 37: Updated current_phrase
2025-02-11 07:50:49,565 - Step 37: Created step_acts
2025-02-11 07:50:49,565 - Step 37: Added to generation_acts
2025-02-11 07:50:49,565 - Step 37: Updated recent_tokens
2025-02-11 07:50:49,566 - Step 37: Decoded current text
2025-02-11 07:50:49,566 - Step 37: Reset consecutive_fillers
2025-02-11 07:50:49,566 - Step 37: Calculated unique_ratio: 0.875
2025-02-11 07:50:49,566 - 
Starting step 38
2025-02-11 07:50:49,567 - Current_ids device: cuda:0
2025-02-11 07:50:49,567 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,590 - Model output complete
2025-02-11 07:50:49,590 - Logits shape: torch.Size([1, 69, 151936])
2025-02-11 07:50:49,591 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,591 - Next token logits device: cuda:0
2025-02-11 07:50:49,591 - Entered do_sample
2025-02-11 07:50:49,591 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,595 - Probs max: 0.97705078125
2025-02-11 07:50:49,595 - Pre-cat
2025-02-11 07:50:49,595 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438,   1632,     13,
           2055,     11,    279,   7002,    686,  42744]], device='cuda:0')
2025-02-11 07:50:49,598 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:49,598 - Current_ids shape: torch.Size([1, 69])
2025-02-11 07:50:49,598 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,599 - Step 38: Generated next token
2025-02-11 07:50:49,599 - Step 38: Updated current_ids
2025-02-11 07:50:49,599 - Step 38: Decoded token text:  a
2025-02-11 07:50:49,599 - Step 38: Updated current_phrase
2025-02-11 07:50:49,599 - Step 38: Created step_acts
2025-02-11 07:50:49,599 - Step 38: Added to generation_acts
2025-02-11 07:50:49,599 - Step 38: Updated recent_tokens
2025-02-11 07:50:49,602 - Step 38: Decoded current text
2025-02-11 07:50:49,602 - Step 38: Reset consecutive_fillers
2025-02-11 07:50:49,602 - Step 38: Calculated unique_ratio: 0.875
2025-02-11 07:50:49,602 - 
Starting step 39
2025-02-11 07:50:49,602 - Current_ids device: cuda:0
2025-02-11 07:50:49,602 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,625 - Model output complete
2025-02-11 07:50:49,625 - Logits shape: torch.Size([1, 70, 151936])
2025-02-11 07:50:49,625 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,625 - Next token logits device: cuda:0
2025-02-11 07:50:49,625 - Entered do_sample
2025-02-11 07:50:49,625 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,630 - Probs max: 0.3720703125
2025-02-11 07:50:49,631 - Pre-cat
2025-02-11 07:50:49,631 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438,   1632,     13,
           2055,     11,    279,   7002,    686,  42744,    264]],
       device='cuda:0')
2025-02-11 07:50:49,634 - Next token: tensor([[5344]], device='cuda:0')
2025-02-11 07:50:49,635 - Current_ids shape: torch.Size([1, 70])
2025-02-11 07:50:49,635 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,635 - Step 39: Generated next token
2025-02-11 07:50:49,635 - Step 39: Updated current_ids
2025-02-11 07:50:49,635 - Step 39: Decoded token text:  force
2025-02-11 07:50:49,635 - Step 39: Updated current_phrase
2025-02-11 07:50:49,636 - Step 39: Created step_acts
2025-02-11 07:50:49,636 - Step 39: Added to generation_acts
2025-02-11 07:50:49,636 - Step 39: Updated recent_tokens
2025-02-11 07:50:49,637 - Step 39: Decoded current text
2025-02-11 07:50:49,637 - Step 39: Reset consecutive_fillers
2025-02-11 07:50:49,638 - Step 39: Calculated unique_ratio: 0.9375
2025-02-11 07:50:49,638 - 
Starting step 40
2025-02-11 07:50:49,638 - Current_ids device: cuda:0
2025-02-11 07:50:49,638 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,661 - Model output complete
2025-02-11 07:50:49,661 - Logits shape: torch.Size([1, 71, 151936])
2025-02-11 07:50:49,661 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,661 - Next token logits device: cuda:0
2025-02-11 07:50:49,661 - Entered do_sample
2025-02-11 07:50:49,661 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,666 - Probs max: 0.98779296875
2025-02-11 07:50:49,667 - Pre-cat
2025-02-11 07:50:49,667 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438,   1632,     13,
           2055,     11,    279,   7002,    686,  42744,    264,   5344]],
       device='cuda:0')
2025-02-11 07:50:49,669 - Next token: tensor([[389]], device='cuda:0')
2025-02-11 07:50:49,669 - Current_ids shape: torch.Size([1, 71])
2025-02-11 07:50:49,669 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,670 - Step 40: Generated next token
2025-02-11 07:50:49,670 - Step 40: Updated current_ids
2025-02-11 07:50:49,670 - Step 40: Decoded token text:  on
2025-02-11 07:50:49,670 - Step 40: Updated current_phrase
2025-02-11 07:50:49,670 - Step 40: Created step_acts
2025-02-11 07:50:49,670 - Step 40: Added to generation_acts
2025-02-11 07:50:49,672 - Step 40: Updated generated_texts
2025-02-11 07:50:49,672 - Step 40: Updated recent_tokens
2025-02-11 07:50:49,672 - Step 40: Decoded current text
2025-02-11 07:50:49,672 - Step 40: Reset consecutive_fillers
2025-02-11 07:50:49,672 - Step 40: Calculated unique_ratio: 0.9375
2025-02-11 07:50:49,672 - 
Starting step 41
2025-02-11 07:50:49,672 - Current_ids device: cuda:0
2025-02-11 07:50:49,673 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,696 - Model output complete
2025-02-11 07:50:49,697 - Logits shape: torch.Size([1, 72, 151936])
2025-02-11 07:50:49,697 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,697 - Next token logits device: cuda:0
2025-02-11 07:50:49,697 - Entered do_sample
2025-02-11 07:50:49,697 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,700 - Probs max: 0.99951171875
2025-02-11 07:50:49,701 - Pre-cat
2025-02-11 07:50:49,701 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438,   1632,     13,
           2055,     11,    279,   7002,    686,  42744,    264,   5344,    389]],
       device='cuda:0')
2025-02-11 07:50:49,704 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:49,704 - Current_ids shape: torch.Size([1, 72])
2025-02-11 07:50:49,704 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,704 - Step 41: Generated next token
2025-02-11 07:50:49,704 - Step 41: Updated current_ids
2025-02-11 07:50:49,705 - Step 41: Decoded token text:  the
2025-02-11 07:50:49,705 - Step 41: Updated current_phrase
2025-02-11 07:50:49,705 - Step 41: Created step_acts
2025-02-11 07:50:49,705 - Step 41: Added to generation_acts
2025-02-11 07:50:49,705 - Step 41: Updated recent_tokens
2025-02-11 07:50:49,707 - Step 41: Decoded current text
2025-02-11 07:50:49,707 - Step 41: Incremented consecutive_fillers to 1
2025-02-11 07:50:49,707 - Step 41: Calculated unique_ratio: 0.9375
2025-02-11 07:50:49,707 - 
Starting step 42
2025-02-11 07:50:49,707 - Current_ids device: cuda:0
2025-02-11 07:50:49,707 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,734 - Model output complete
2025-02-11 07:50:49,734 - Logits shape: torch.Size([1, 73, 151936])
2025-02-11 07:50:49,734 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,734 - Next token logits device: cuda:0
2025-02-11 07:50:49,734 - Entered do_sample
2025-02-11 07:50:49,734 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,736 - Probs max: 0.9970703125
2025-02-11 07:50:49,737 - Pre-cat
2025-02-11 07:50:49,737 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438,   1632,     13,
           2055,     11,    279,   7002,    686,  42744,    264,   5344,    389,
            279]], device='cuda:0')
2025-02-11 07:50:49,740 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:49,740 - Current_ids shape: torch.Size([1, 73])
2025-02-11 07:50:49,740 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,741 - Step 42: Generated next token
2025-02-11 07:50:49,741 - Step 42: Updated current_ids
2025-02-11 07:50:49,741 - Step 42: Decoded token text:  ball
2025-02-11 07:50:49,741 - Step 42: Updated current_phrase
2025-02-11 07:50:49,741 - Step 42: Created step_acts
2025-02-11 07:50:49,741 - Step 42: Added to generation_acts
2025-02-11 07:50:49,741 - Step 42: Updated recent_tokens
2025-02-11 07:50:49,743 - Step 42: Decoded current text
2025-02-11 07:50:49,743 - Step 42: Incremented consecutive_fillers to 2
2025-02-11 07:50:49,743 - Step 42: Calculated unique_ratio: 0.9375
2025-02-11 07:50:49,743 - 
Starting step 43
2025-02-11 07:50:49,743 - Current_ids device: cuda:0
2025-02-11 07:50:49,743 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,768 - Model output complete
2025-02-11 07:50:49,768 - Logits shape: torch.Size([1, 74, 151936])
2025-02-11 07:50:49,768 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,768 - Next token logits device: cuda:0
2025-02-11 07:50:49,768 - Entered do_sample
2025-02-11 07:50:49,768 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,772 - Probs max: 0.64892578125
2025-02-11 07:50:49,773 - Pre-cat
2025-02-11 07:50:49,773 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            614,    264,   1602,   1550,   4628,     11,    714,    279,   7002,
            686,    614,    264,   1602,   1550,   4628,    438,   1632,     13,
           2055,     11,    279,   7002,    686,  42744,    264,   5344,    389,
            279,   4935]], device='cuda:0')
2025-02-11 07:50:49,775 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:49,776 - Current_ids shape: torch.Size([1, 74])
2025-02-11 07:50:49,776 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,776 - Step 43: Generated next token
2025-02-11 07:50:49,776 - Step 43: Updated current_ids
2025-02-11 07:50:49,776 - Step 43: Decoded token text: ,
2025-02-11 07:50:49,776 - Step 43: Updated current_phrase
2025-02-11 07:50:49,777 - Step 43: Created step_acts
2025-02-11 07:50:49,777 - Step 43: Added to generation_acts
2025-02-11 07:50:49,777 - Step 43: Updated recent_tokens
2025-02-11 07:50:49,779 - Step 43: Found phrase end token
2025-02-11 07:50:49,779 - Step 43: Updated recent_phrases
2025-02-11 07:50:49,779 - Step 43: Calculated similarity: 0.0
2025-02-11 07:50:49,780 - Step 43: Decoded current text
2025-02-11 07:50:49,780 - Step 43: Incremented consecutive_fillers to 3
2025-02-11 07:50:49,892 - 
Starting step 0
2025-02-11 07:50:49,892 - Current_ids device: cuda:0
2025-02-11 07:50:49,893 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,921 - Model output complete
2025-02-11 07:50:49,922 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:49,922 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,922 - Next token logits device: cuda:0
2025-02-11 07:50:49,922 - Entered do_sample
2025-02-11 07:50:49,922 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,925 - Probs max: 0.50341796875
2025-02-11 07:50:49,926 - Pre-cat
2025-02-11 07:50:49,926 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:49,928 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:49,929 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:49,929 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,929 - Step 0: Generated next token
2025-02-11 07:50:49,929 - Step 0: Updated current_ids
2025-02-11 07:50:49,929 - Step 0: Decoded token text:  The
2025-02-11 07:50:49,929 - Step 0: Updated current_phrase
2025-02-11 07:50:49,930 - Step 0: Created step_acts
2025-02-11 07:50:49,930 - Step 0: Added to generation_acts
2025-02-11 07:50:49,931 - Step 0: Updated generated_texts
2025-02-11 07:50:49,931 - Step 0: Updated recent_tokens
2025-02-11 07:50:49,931 - Step 0: Decoded current text
2025-02-11 07:50:49,931 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:49,931 - 
Starting step 1
2025-02-11 07:50:49,932 - Current_ids device: cuda:0
2025-02-11 07:50:49,932 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,959 - Model output complete
2025-02-11 07:50:49,959 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:49,960 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,960 - Next token logits device: cuda:0
2025-02-11 07:50:49,960 - Entered do_sample
2025-02-11 07:50:49,960 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,962 - Probs max: 0.83837890625
2025-02-11 07:50:49,962 - Pre-cat
2025-02-11 07:50:49,963 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:50:49,964 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:49,964 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:49,964 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,964 - Step 1: Generated next token
2025-02-11 07:50:49,964 - Step 1: Updated current_ids
2025-02-11 07:50:49,965 - Step 1: Decoded token text:  ball
2025-02-11 07:50:49,965 - Step 1: Updated current_phrase
2025-02-11 07:50:49,965 - Step 1: Created step_acts
2025-02-11 07:50:49,965 - Step 1: Added to generation_acts
2025-02-11 07:50:49,965 - Step 1: Updated recent_tokens
2025-02-11 07:50:49,966 - Step 1: Decoded current text
2025-02-11 07:50:49,967 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:49,967 - 
Starting step 2
2025-02-11 07:50:49,967 - Current_ids device: cuda:0
2025-02-11 07:50:49,967 - Current_ids dtype: torch.int64
2025-02-11 07:50:49,992 - Model output complete
2025-02-11 07:50:49,992 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:49,992 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,992 - Next token logits device: cuda:0
2025-02-11 07:50:49,992 - Entered do_sample
2025-02-11 07:50:49,992 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:49,994 - Probs max: 0.583984375
2025-02-11 07:50:49,995 - Pre-cat
2025-02-11 07:50:49,995 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:50:49,996 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:49,997 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:49,997 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:49,997 - Step 2: Generated next token
2025-02-11 07:50:49,997 - Step 2: Updated current_ids
2025-02-11 07:50:49,997 - Step 2: Decoded token text:  is
2025-02-11 07:50:49,997 - Step 2: Updated current_phrase
2025-02-11 07:50:49,997 - Step 2: Created step_acts
2025-02-11 07:50:49,998 - Step 2: Added to generation_acts
2025-02-11 07:50:49,998 - Step 2: Updated recent_tokens
2025-02-11 07:50:49,999 - Step 2: Decoded current text
2025-02-11 07:50:49,999 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:49,999 - 
Starting step 3
2025-02-11 07:50:49,999 - Current_ids device: cuda:0
2025-02-11 07:50:49,999 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,061 - Model output complete
2025-02-11 07:50:50,061 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:50,061 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,061 - Next token logits device: cuda:0
2025-02-11 07:50:50,061 - Entered do_sample
2025-02-11 07:50:50,061 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,064 - Probs max: 0.59521484375
2025-02-11 07:50:50,064 - Pre-cat
2025-02-11 07:50:50,064 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374]],
       device='cuda:0')
2025-02-11 07:50:50,066 - Next token: tensor([[7218]], device='cuda:0')
2025-02-11 07:50:50,066 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:50,066 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,066 - Step 3: Generated next token
2025-02-11 07:50:50,066 - Step 3: Updated current_ids
2025-02-11 07:50:50,066 - Step 3: Decoded token text:  moving
2025-02-11 07:50:50,066 - Step 3: Updated current_phrase
2025-02-11 07:50:50,067 - Step 3: Created step_acts
2025-02-11 07:50:50,067 - Step 3: Added to generation_acts
2025-02-11 07:50:50,067 - Step 3: Updated recent_tokens
2025-02-11 07:50:50,068 - Step 3: Decoded current text
2025-02-11 07:50:50,068 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:50,068 - 
Starting step 4
2025-02-11 07:50:50,068 - Current_ids device: cuda:0
2025-02-11 07:50:50,068 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,092 - Model output complete
2025-02-11 07:50:50,092 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:50,092 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,092 - Next token logits device: cuda:0
2025-02-11 07:50:50,092 - Entered do_sample
2025-02-11 07:50:50,092 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,095 - Probs max: 0.677734375
2025-02-11 07:50:50,096 - Pre-cat
2025-02-11 07:50:50,096 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218]],
       device='cuda:0')
2025-02-11 07:50:50,097 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:50:50,098 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:50,098 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,098 - Step 4: Generated next token
2025-02-11 07:50:50,098 - Step 4: Updated current_ids
2025-02-11 07:50:50,098 - Step 4: Decoded token text:  at
2025-02-11 07:50:50,098 - Step 4: Updated current_phrase
2025-02-11 07:50:50,098 - Step 4: Created step_acts
2025-02-11 07:50:50,098 - Step 4: Added to generation_acts
2025-02-11 07:50:50,098 - Step 4: Updated recent_tokens
2025-02-11 07:50:50,100 - Step 4: Decoded current text
2025-02-11 07:50:50,100 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:50,100 - 
Starting step 5
2025-02-11 07:50:50,100 - Current_ids device: cuda:0
2025-02-11 07:50:50,100 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,124 - Model output complete
2025-02-11 07:50:50,125 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:50,125 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,125 - Next token logits device: cuda:0
2025-02-11 07:50:50,125 - Entered do_sample
2025-02-11 07:50:50,125 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,128 - Probs max: 0.50146484375
2025-02-11 07:50:50,128 - Pre-cat
2025-02-11 07:50:50,129 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518]],
       device='cuda:0')
2025-02-11 07:50:50,130 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:50,131 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:50,131 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,131 - Step 5: Generated next token
2025-02-11 07:50:50,131 - Step 5: Updated current_ids
2025-02-11 07:50:50,131 - Step 5: Decoded token text:  a
2025-02-11 07:50:50,131 - Step 5: Updated current_phrase
2025-02-11 07:50:50,132 - Step 5: Created step_acts
2025-02-11 07:50:50,132 - Step 5: Added to generation_acts
2025-02-11 07:50:50,133 - Step 5: Updated generated_texts
2025-02-11 07:50:50,134 - Step 5: Updated recent_tokens
2025-02-11 07:50:50,135 - Step 5: Decoded current text
2025-02-11 07:50:50,135 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:50,135 - 
Starting step 6
2025-02-11 07:50:50,135 - Current_ids device: cuda:0
2025-02-11 07:50:50,135 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,192 - Model output complete
2025-02-11 07:50:50,192 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:50,192 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,193 - Next token logits device: cuda:0
2025-02-11 07:50:50,193 - Entered do_sample
2025-02-11 07:50:50,193 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,196 - Probs max: 0.3330078125
2025-02-11 07:50:50,196 - Pre-cat
2025-02-11 07:50:50,196 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264]], device='cuda:0')
2025-02-11 07:50:50,198 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:50:50,199 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:50,199 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,199 - Step 6: Generated next token
2025-02-11 07:50:50,199 - Step 6: Updated current_ids
2025-02-11 07:50:50,199 - Step 6: Decoded token text:  speed
2025-02-11 07:50:50,199 - Step 6: Updated current_phrase
2025-02-11 07:50:50,200 - Step 6: Created step_acts
2025-02-11 07:50:50,200 - Step 6: Added to generation_acts
2025-02-11 07:50:50,200 - Step 6: Updated recent_tokens
2025-02-11 07:50:50,201 - Step 6: Decoded current text
2025-02-11 07:50:50,201 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:50,201 - 
Starting step 7
2025-02-11 07:50:50,201 - Current_ids device: cuda:0
2025-02-11 07:50:50,201 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,224 - Model output complete
2025-02-11 07:50:50,224 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:50,224 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,224 - Next token logits device: cuda:0
2025-02-11 07:50:50,224 - Entered do_sample
2025-02-11 07:50:50,224 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,227 - Probs max: 0.357666015625
2025-02-11 07:50:50,227 - Pre-cat
2025-02-11 07:50:50,227 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628]], device='cuda:0')
2025-02-11 07:50:50,229 - Next token: tensor([[892]], device='cuda:0')
2025-02-11 07:50:50,229 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:50,229 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,229 - Step 7: Generated next token
2025-02-11 07:50:50,229 - Step 7: Updated current_ids
2025-02-11 07:50:50,230 - Step 7: Decoded token text:  which
2025-02-11 07:50:50,230 - Step 7: Updated current_phrase
2025-02-11 07:50:50,230 - Step 7: Created step_acts
2025-02-11 07:50:50,231 - Step 7: Added to generation_acts
2025-02-11 07:50:50,231 - Step 7: Updated recent_tokens
2025-02-11 07:50:50,232 - Step 7: Decoded current text
2025-02-11 07:50:50,232 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:50,233 - 
Starting step 8
2025-02-11 07:50:50,233 - Current_ids device: cuda:0
2025-02-11 07:50:50,233 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,257 - Model output complete
2025-02-11 07:50:50,257 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:50,257 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,257 - Next token logits device: cuda:0
2025-02-11 07:50:50,257 - Entered do_sample
2025-02-11 07:50:50,258 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,260 - Probs max: 0.9873046875
2025-02-11 07:50:50,260 - Pre-cat
2025-02-11 07:50:50,261 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    892]], device='cuda:0')
2025-02-11 07:50:50,263 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:50,264 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:50,264 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,264 - Step 8: Generated next token
2025-02-11 07:50:50,264 - Step 8: Updated current_ids
2025-02-11 07:50:50,264 - Step 8: Decoded token text:  is
2025-02-11 07:50:50,264 - Step 8: Updated current_phrase
2025-02-11 07:50:50,265 - Step 8: Created step_acts
2025-02-11 07:50:50,265 - Step 8: Added to generation_acts
2025-02-11 07:50:50,265 - Step 8: Updated recent_tokens
2025-02-11 07:50:50,266 - Step 8: Decoded current text
2025-02-11 07:50:50,267 - Step 8: Incremented consecutive_fillers to 1
2025-02-11 07:50:50,267 - 
Starting step 9
2025-02-11 07:50:50,267 - Current_ids device: cuda:0
2025-02-11 07:50:50,267 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,294 - Model output complete
2025-02-11 07:50:50,294 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:50,294 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,294 - Next token logits device: cuda:0
2025-02-11 07:50:50,294 - Entered do_sample
2025-02-11 07:50:50,294 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,298 - Probs max: 0.226318359375
2025-02-11 07:50:50,299 - Pre-cat
2025-02-11 07:50:50,299 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    892,    374]], device='cuda:0')
2025-02-11 07:50:50,300 - Next token: tensor([[6144]], device='cuda:0')
2025-02-11 07:50:50,301 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:50,301 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,301 - Step 9: Generated next token
2025-02-11 07:50:50,301 - Step 9: Updated current_ids
2025-02-11 07:50:50,301 - Step 9: Decoded token text:  equal
2025-02-11 07:50:50,301 - Step 9: Updated current_phrase
2025-02-11 07:50:50,302 - Step 9: Created step_acts
2025-02-11 07:50:50,302 - Step 9: Added to generation_acts
2025-02-11 07:50:50,302 - Step 9: Updated recent_tokens
2025-02-11 07:50:50,303 - Step 9: Decoded current text
2025-02-11 07:50:50,303 - Step 9: Incremented consecutive_fillers to 2
2025-02-11 07:50:50,303 - 
Starting step 10
2025-02-11 07:50:50,303 - Current_ids device: cuda:0
2025-02-11 07:50:50,304 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,326 - Model output complete
2025-02-11 07:50:50,327 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:50,327 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,327 - Next token logits device: cuda:0
2025-02-11 07:50:50,327 - Entered do_sample
2025-02-11 07:50:50,327 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,331 - Probs max: 0.998046875
2025-02-11 07:50:50,332 - Pre-cat
2025-02-11 07:50:50,332 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    892,    374,   6144]], device='cuda:0')
2025-02-11 07:50:50,334 - Next token: tensor([[311]], device='cuda:0')
2025-02-11 07:50:50,334 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:50,334 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,334 - Step 10: Generated next token
2025-02-11 07:50:50,334 - Step 10: Updated current_ids
2025-02-11 07:50:50,334 - Step 10: Decoded token text:  to
2025-02-11 07:50:50,334 - Step 10: Updated current_phrase
2025-02-11 07:50:50,335 - Step 10: Created step_acts
2025-02-11 07:50:50,335 - Step 10: Added to generation_acts
2025-02-11 07:50:50,336 - Step 10: Updated generated_texts
2025-02-11 07:50:50,336 - Step 10: Updated recent_tokens
2025-02-11 07:50:50,336 - Step 10: Decoded current text
2025-02-11 07:50:50,336 - Step 10: Incremented consecutive_fillers to 3
2025-02-11 07:50:50,446 - 
Starting step 0
2025-02-11 07:50:50,446 - Current_ids device: cuda:0
2025-02-11 07:50:50,447 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,475 - Model output complete
2025-02-11 07:50:50,475 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:50,475 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,475 - Next token logits device: cuda:0
2025-02-11 07:50:50,476 - Entered do_sample
2025-02-11 07:50:50,476 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,478 - Probs max: 0.50341796875
2025-02-11 07:50:50,479 - Pre-cat
2025-02-11 07:50:50,479 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:50,481 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:50,481 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:50,481 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,481 - Step 0: Generated next token
2025-02-11 07:50:50,481 - Step 0: Updated current_ids
2025-02-11 07:50:50,481 - Step 0: Decoded token text:  The
2025-02-11 07:50:50,481 - Step 0: Updated current_phrase
2025-02-11 07:50:50,482 - Step 0: Created step_acts
2025-02-11 07:50:50,482 - Step 0: Added to generation_acts
2025-02-11 07:50:50,483 - Step 0: Updated generated_texts
2025-02-11 07:50:50,483 - Step 0: Updated recent_tokens
2025-02-11 07:50:50,483 - Step 0: Decoded current text
2025-02-11 07:50:50,484 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:50,484 - 
Starting step 1
2025-02-11 07:50:50,484 - Current_ids device: cuda:0
2025-02-11 07:50:50,484 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,510 - Model output complete
2025-02-11 07:50:50,510 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:50,511 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,511 - Next token logits device: cuda:0
2025-02-11 07:50:50,511 - Entered do_sample
2025-02-11 07:50:50,511 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,513 - Probs max: 0.83837890625
2025-02-11 07:50:50,514 - Pre-cat
2025-02-11 07:50:50,514 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:50:50,515 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:50,515 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:50,515 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,515 - Step 1: Generated next token
2025-02-11 07:50:50,515 - Step 1: Updated current_ids
2025-02-11 07:50:50,516 - Step 1: Decoded token text:  ball
2025-02-11 07:50:50,516 - Step 1: Updated current_phrase
2025-02-11 07:50:50,516 - Step 1: Created step_acts
2025-02-11 07:50:50,516 - Step 1: Added to generation_acts
2025-02-11 07:50:50,516 - Step 1: Updated recent_tokens
2025-02-11 07:50:50,517 - Step 1: Decoded current text
2025-02-11 07:50:50,517 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:50,517 - 
Starting step 2
2025-02-11 07:50:50,517 - Current_ids device: cuda:0
2025-02-11 07:50:50,517 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,547 - Model output complete
2025-02-11 07:50:50,547 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:50,547 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,548 - Next token logits device: cuda:0
2025-02-11 07:50:50,548 - Entered do_sample
2025-02-11 07:50:50,548 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,550 - Probs max: 0.583984375
2025-02-11 07:50:50,551 - Pre-cat
2025-02-11 07:50:50,551 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:50:50,552 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:50,553 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:50,553 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,553 - Step 2: Generated next token
2025-02-11 07:50:50,553 - Step 2: Updated current_ids
2025-02-11 07:50:50,553 - Step 2: Decoded token text:  is
2025-02-11 07:50:50,553 - Step 2: Updated current_phrase
2025-02-11 07:50:50,553 - Step 2: Created step_acts
2025-02-11 07:50:50,553 - Step 2: Added to generation_acts
2025-02-11 07:50:50,554 - Step 2: Updated recent_tokens
2025-02-11 07:50:50,556 - Step 2: Decoded current text
2025-02-11 07:50:50,556 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:50,556 - 
Starting step 3
2025-02-11 07:50:50,556 - Current_ids device: cuda:0
2025-02-11 07:50:50,556 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,579 - Model output complete
2025-02-11 07:50:50,579 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:50,579 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,580 - Next token logits device: cuda:0
2025-02-11 07:50:50,580 - Entered do_sample
2025-02-11 07:50:50,580 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,582 - Probs max: 0.59521484375
2025-02-11 07:50:50,583 - Pre-cat
2025-02-11 07:50:50,583 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374]],
       device='cuda:0')
2025-02-11 07:50:50,584 - Next token: tensor([[7218]], device='cuda:0')
2025-02-11 07:50:50,584 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:50,585 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,585 - Step 3: Generated next token
2025-02-11 07:50:50,585 - Step 3: Updated current_ids
2025-02-11 07:50:50,585 - Step 3: Decoded token text:  moving
2025-02-11 07:50:50,585 - Step 3: Updated current_phrase
2025-02-11 07:50:50,585 - Step 3: Created step_acts
2025-02-11 07:50:50,585 - Step 3: Added to generation_acts
2025-02-11 07:50:50,585 - Step 3: Updated recent_tokens
2025-02-11 07:50:50,587 - Step 3: Decoded current text
2025-02-11 07:50:50,587 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:50,587 - 
Starting step 4
2025-02-11 07:50:50,587 - Current_ids device: cuda:0
2025-02-11 07:50:50,587 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,611 - Model output complete
2025-02-11 07:50:50,612 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:50,612 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,612 - Next token logits device: cuda:0
2025-02-11 07:50:50,612 - Entered do_sample
2025-02-11 07:50:50,612 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,614 - Probs max: 0.677734375
2025-02-11 07:50:50,615 - Pre-cat
2025-02-11 07:50:50,615 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218]],
       device='cuda:0')
2025-02-11 07:50:50,616 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:50:50,616 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:50,617 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,617 - Step 4: Generated next token
2025-02-11 07:50:50,617 - Step 4: Updated current_ids
2025-02-11 07:50:50,617 - Step 4: Decoded token text:  at
2025-02-11 07:50:50,617 - Step 4: Updated current_phrase
2025-02-11 07:50:50,617 - Step 4: Created step_acts
2025-02-11 07:50:50,617 - Step 4: Added to generation_acts
2025-02-11 07:50:50,617 - Step 4: Updated recent_tokens
2025-02-11 07:50:50,619 - Step 4: Decoded current text
2025-02-11 07:50:50,619 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:50,619 - 
Starting step 5
2025-02-11 07:50:50,619 - Current_ids device: cuda:0
2025-02-11 07:50:50,619 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,642 - Model output complete
2025-02-11 07:50:50,642 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:50,642 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,643 - Next token logits device: cuda:0
2025-02-11 07:50:50,643 - Entered do_sample
2025-02-11 07:50:50,643 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,645 - Probs max: 0.50146484375
2025-02-11 07:50:50,647 - Pre-cat
2025-02-11 07:50:50,647 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518]],
       device='cuda:0')
2025-02-11 07:50:50,650 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:50,650 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:50,650 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,650 - Step 5: Generated next token
2025-02-11 07:50:50,651 - Step 5: Updated current_ids
2025-02-11 07:50:50,651 - Step 5: Decoded token text:  a
2025-02-11 07:50:50,651 - Step 5: Updated current_phrase
2025-02-11 07:50:50,652 - Step 5: Created step_acts
2025-02-11 07:50:50,652 - Step 5: Added to generation_acts
2025-02-11 07:50:50,654 - Step 5: Updated generated_texts
2025-02-11 07:50:50,654 - Step 5: Updated recent_tokens
2025-02-11 07:50:50,654 - Step 5: Decoded current text
2025-02-11 07:50:50,654 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:50,654 - 
Starting step 6
2025-02-11 07:50:50,654 - Current_ids device: cuda:0
2025-02-11 07:50:50,654 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,700 - Model output complete
2025-02-11 07:50:50,700 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:50,701 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,701 - Next token logits device: cuda:0
2025-02-11 07:50:50,701 - Entered do_sample
2025-02-11 07:50:50,701 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,703 - Probs max: 0.3330078125
2025-02-11 07:50:50,704 - Pre-cat
2025-02-11 07:50:50,704 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264]], device='cuda:0')
2025-02-11 07:50:50,705 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:50:50,706 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:50,706 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,706 - Step 6: Generated next token
2025-02-11 07:50:50,706 - Step 6: Updated current_ids
2025-02-11 07:50:50,706 - Step 6: Decoded token text:  speed
2025-02-11 07:50:50,706 - Step 6: Updated current_phrase
2025-02-11 07:50:50,706 - Step 6: Created step_acts
2025-02-11 07:50:50,707 - Step 6: Added to generation_acts
2025-02-11 07:50:50,707 - Step 6: Updated recent_tokens
2025-02-11 07:50:50,708 - Step 6: Decoded current text
2025-02-11 07:50:50,708 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:50,708 - 
Starting step 7
2025-02-11 07:50:50,708 - Current_ids device: cuda:0
2025-02-11 07:50:50,708 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,732 - Model output complete
2025-02-11 07:50:50,732 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:50,733 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,733 - Next token logits device: cuda:0
2025-02-11 07:50:50,733 - Entered do_sample
2025-02-11 07:50:50,733 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,736 - Probs max: 0.357666015625
2025-02-11 07:50:50,737 - Pre-cat
2025-02-11 07:50:50,737 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628]], device='cuda:0')
2025-02-11 07:50:50,738 - Next token: tensor([[315]], device='cuda:0')
2025-02-11 07:50:50,739 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:50,739 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,739 - Step 7: Generated next token
2025-02-11 07:50:50,739 - Step 7: Updated current_ids
2025-02-11 07:50:50,739 - Step 7: Decoded token text:  of
2025-02-11 07:50:50,739 - Step 7: Updated current_phrase
2025-02-11 07:50:50,739 - Step 7: Created step_acts
2025-02-11 07:50:50,739 - Step 7: Added to generation_acts
2025-02-11 07:50:50,740 - Step 7: Updated recent_tokens
2025-02-11 07:50:50,741 - Step 7: Decoded current text
2025-02-11 07:50:50,741 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:50,741 - 
Starting step 8
2025-02-11 07:50:50,741 - Current_ids device: cuda:0
2025-02-11 07:50:50,741 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,766 - Model output complete
2025-02-11 07:50:50,766 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:50,766 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,767 - Next token logits device: cuda:0
2025-02-11 07:50:50,767 - Entered do_sample
2025-02-11 07:50:50,767 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,769 - Probs max: 0.86669921875
2025-02-11 07:50:50,769 - Pre-cat
2025-02-11 07:50:50,770 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315]], device='cuda:0')
2025-02-11 07:50:50,771 - Next token: tensor([[348]], device='cuda:0')
2025-02-11 07:50:50,771 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:50,771 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,771 - Step 8: Generated next token
2025-02-11 07:50:50,771 - Step 8: Updated current_ids
2025-02-11 07:50:50,772 - Step 8: Decoded token text:  v
2025-02-11 07:50:50,772 - Step 8: Updated current_phrase
2025-02-11 07:50:50,772 - Step 8: Created step_acts
2025-02-11 07:50:50,772 - Step 8: Added to generation_acts
2025-02-11 07:50:50,772 - Step 8: Updated recent_tokens
2025-02-11 07:50:50,773 - Step 8: Decoded current text
2025-02-11 07:50:50,774 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:50,774 - 
Starting step 9
2025-02-11 07:50:50,774 - Current_ids device: cuda:0
2025-02-11 07:50:50,774 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,797 - Model output complete
2025-02-11 07:50:50,797 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:50,797 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,797 - Next token logits device: cuda:0
2025-02-11 07:50:50,797 - Entered do_sample
2025-02-11 07:50:50,797 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,800 - Probs max: 0.52001953125
2025-02-11 07:50:50,800 - Pre-cat
2025-02-11 07:50:50,800 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348]], device='cuda:0')
2025-02-11 07:50:50,802 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:50,802 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:50,802 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,802 - Step 9: Generated next token
2025-02-11 07:50:50,802 - Step 9: Updated current_ids
2025-02-11 07:50:50,802 - Step 9: Decoded token text: ,
2025-02-11 07:50:50,802 - Step 9: Updated current_phrase
2025-02-11 07:50:50,803 - Step 9: Created step_acts
2025-02-11 07:50:50,803 - Step 9: Added to generation_acts
2025-02-11 07:50:50,803 - Step 9: Updated recent_tokens
2025-02-11 07:50:50,804 - Step 9: Found phrase end token
2025-02-11 07:50:50,804 - Step 9: Updated recent_phrases
2025-02-11 07:50:50,804 - Step 9: Decoded current text
2025-02-11 07:50:50,804 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:50,804 - 
Starting step 10
2025-02-11 07:50:50,805 - Current_ids device: cuda:0
2025-02-11 07:50:50,805 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,829 - Model output complete
2025-02-11 07:50:50,829 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:50,829 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,829 - Next token logits device: cuda:0
2025-02-11 07:50:50,829 - Entered do_sample
2025-02-11 07:50:50,829 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,832 - Probs max: 0.71044921875
2025-02-11 07:50:50,832 - Pre-cat
2025-02-11 07:50:50,832 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,     11]], device='cuda:0')
2025-02-11 07:50:50,834 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:50,834 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:50,834 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,834 - Step 10: Generated next token
2025-02-11 07:50:50,834 - Step 10: Updated current_ids
2025-02-11 07:50:50,835 - Step 10: Decoded token text:  the
2025-02-11 07:50:50,835 - Step 10: Updated current_phrase
2025-02-11 07:50:50,835 - Step 10: Created step_acts
2025-02-11 07:50:50,835 - Step 10: Added to generation_acts
2025-02-11 07:50:50,836 - Step 10: Updated generated_texts
2025-02-11 07:50:50,836 - Step 10: Updated recent_tokens
2025-02-11 07:50:50,837 - Step 10: Decoded current text
2025-02-11 07:50:50,837 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:50,837 - 
Starting step 11
2025-02-11 07:50:50,837 - Current_ids device: cuda:0
2025-02-11 07:50:50,837 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,860 - Model output complete
2025-02-11 07:50:50,860 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:50,860 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,860 - Next token logits device: cuda:0
2025-02-11 07:50:50,860 - Entered do_sample
2025-02-11 07:50:50,860 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,862 - Probs max: 0.9833984375
2025-02-11 07:50:50,863 - Pre-cat
2025-02-11 07:50:50,863 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,     11,    279]], device='cuda:0')
2025-02-11 07:50:50,865 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:50,865 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:50,865 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,865 - Step 11: Generated next token
2025-02-11 07:50:50,865 - Step 11: Updated current_ids
2025-02-11 07:50:50,865 - Step 11: Decoded token text:  wall
2025-02-11 07:50:50,865 - Step 11: Updated current_phrase
2025-02-11 07:50:50,866 - Step 11: Created step_acts
2025-02-11 07:50:50,866 - Step 11: Added to generation_acts
2025-02-11 07:50:50,866 - Step 11: Updated recent_tokens
2025-02-11 07:50:50,867 - Step 11: Decoded current text
2025-02-11 07:50:50,867 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:50,867 - 
Starting step 12
2025-02-11 07:50:50,867 - Current_ids device: cuda:0
2025-02-11 07:50:50,867 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,890 - Model output complete
2025-02-11 07:50:50,890 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:50,890 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,890 - Next token logits device: cuda:0
2025-02-11 07:50:50,891 - Entered do_sample
2025-02-11 07:50:50,891 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,893 - Probs max: 0.984375
2025-02-11 07:50:50,894 - Pre-cat
2025-02-11 07:50:50,894 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,     11,    279,   7002]],
       device='cuda:0')
2025-02-11 07:50:50,896 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:50,896 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:50,896 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,896 - Step 12: Generated next token
2025-02-11 07:50:50,896 - Step 12: Updated current_ids
2025-02-11 07:50:50,897 - Step 12: Decoded token text:  is
2025-02-11 07:50:50,897 - Step 12: Updated current_phrase
2025-02-11 07:50:50,897 - Step 12: Created step_acts
2025-02-11 07:50:50,897 - Step 12: Added to generation_acts
2025-02-11 07:50:50,897 - Step 12: Updated recent_tokens
2025-02-11 07:50:50,899 - Step 12: Decoded current text
2025-02-11 07:50:50,899 - Step 12: Reset consecutive_fillers
2025-02-11 07:50:50,899 - 
Starting step 13
2025-02-11 07:50:50,899 - Current_ids device: cuda:0
2025-02-11 07:50:50,899 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,923 - Model output complete
2025-02-11 07:50:50,923 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:50,923 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,923 - Next token logits device: cuda:0
2025-02-11 07:50:50,923 - Entered do_sample
2025-02-11 07:50:50,923 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,925 - Probs max: 0.56201171875
2025-02-11 07:50:50,926 - Pre-cat
2025-02-11 07:50:50,926 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,     11,    279,   7002,    374]],
       device='cuda:0')
2025-02-11 07:50:50,928 - Next token: tensor([[7218]], device='cuda:0')
2025-02-11 07:50:50,929 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:50,929 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,929 - Step 13: Generated next token
2025-02-11 07:50:50,929 - Step 13: Updated current_ids
2025-02-11 07:50:50,929 - Step 13: Decoded token text:  moving
2025-02-11 07:50:50,929 - Step 13: Updated current_phrase
2025-02-11 07:50:50,929 - Step 13: Created step_acts
2025-02-11 07:50:50,930 - Step 13: Added to generation_acts
2025-02-11 07:50:50,930 - Step 13: Updated recent_tokens
2025-02-11 07:50:50,931 - Step 13: Decoded current text
2025-02-11 07:50:50,931 - Step 13: Reset consecutive_fillers
2025-02-11 07:50:50,931 - 
Starting step 14
2025-02-11 07:50:50,931 - Current_ids device: cuda:0
2025-02-11 07:50:50,931 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,953 - Model output complete
2025-02-11 07:50:50,954 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:50:50,954 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,954 - Next token logits device: cuda:0
2025-02-11 07:50:50,954 - Entered do_sample
2025-02-11 07:50:50,954 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,956 - Probs max: 0.98974609375
2025-02-11 07:50:50,957 - Pre-cat
2025-02-11 07:50:50,957 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,     11,    279,   7002,    374,   7218]],
       device='cuda:0')
2025-02-11 07:50:50,959 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:50:50,960 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:50:50,960 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,960 - Step 14: Generated next token
2025-02-11 07:50:50,960 - Step 14: Updated current_ids
2025-02-11 07:50:50,960 - Step 14: Decoded token text:  at
2025-02-11 07:50:50,960 - Step 14: Updated current_phrase
2025-02-11 07:50:50,961 - Step 14: Created step_acts
2025-02-11 07:50:50,961 - Step 14: Added to generation_acts
2025-02-11 07:50:50,961 - Step 14: Updated recent_tokens
2025-02-11 07:50:50,962 - Step 14: Decoded current text
2025-02-11 07:50:50,962 - Step 14: Reset consecutive_fillers
2025-02-11 07:50:50,962 - 
Starting step 15
2025-02-11 07:50:50,962 - Current_ids device: cuda:0
2025-02-11 07:50:50,962 - Current_ids dtype: torch.int64
2025-02-11 07:50:50,986 - Model output complete
2025-02-11 07:50:50,987 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:50:50,987 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,987 - Next token logits device: cuda:0
2025-02-11 07:50:50,987 - Entered do_sample
2025-02-11 07:50:50,987 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:50,989 - Probs max: 0.5498046875
2025-02-11 07:50:50,990 - Pre-cat
2025-02-11 07:50:50,990 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,     11,    279,   7002,    374,   7218,
            518]], device='cuda:0')
2025-02-11 07:50:50,992 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:50,992 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:50:50,992 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:50,993 - Step 15: Generated next token
2025-02-11 07:50:50,993 - Step 15: Updated current_ids
2025-02-11 07:50:50,993 - Step 15: Decoded token text:  a
2025-02-11 07:50:50,993 - Step 15: Updated current_phrase
2025-02-11 07:50:50,993 - Step 15: Created step_acts
2025-02-11 07:50:50,993 - Step 15: Added to generation_acts
2025-02-11 07:50:50,995 - Step 15: Updated generated_texts
2025-02-11 07:50:50,995 - Step 15: Updated recent_tokens
2025-02-11 07:50:50,995 - Step 15: Decoded current text
2025-02-11 07:50:50,995 - Step 15: Reset consecutive_fillers
2025-02-11 07:50:50,995 - 
Starting step 16
2025-02-11 07:50:50,995 - Current_ids device: cuda:0
2025-02-11 07:50:50,995 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,018 - Model output complete
2025-02-11 07:50:51,018 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:50:51,018 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,018 - Next token logits device: cuda:0
2025-02-11 07:50:51,018 - Entered do_sample
2025-02-11 07:50:51,019 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,021 - Probs max: 0.9990234375
2025-02-11 07:50:51,022 - Pre-cat
2025-02-11 07:50:51,022 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,     11,    279,   7002,    374,   7218,
            518,    264]], device='cuda:0')
2025-02-11 07:50:51,024 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:50:51,024 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:50:51,024 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,024 - Step 16: Generated next token
2025-02-11 07:50:51,024 - Step 16: Updated current_ids
2025-02-11 07:50:51,024 - Step 16: Decoded token text:  speed
2025-02-11 07:50:51,024 - Step 16: Updated current_phrase
2025-02-11 07:50:51,025 - Step 16: Created step_acts
2025-02-11 07:50:51,025 - Step 16: Added to generation_acts
2025-02-11 07:50:51,025 - Step 16: Updated recent_tokens
2025-02-11 07:50:51,026 - Step 16: Decoded current text
2025-02-11 07:50:51,026 - Step 16: Reset consecutive_fillers
2025-02-11 07:50:51,027 - Step 16: Calculated unique_ratio: 0.6875
2025-02-11 07:50:51,027 - 
Starting step 17
2025-02-11 07:50:51,027 - Current_ids device: cuda:0
2025-02-11 07:50:51,027 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,050 - Model output complete
2025-02-11 07:50:51,050 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:50:51,050 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,051 - Next token logits device: cuda:0
2025-02-11 07:50:51,051 - Entered do_sample
2025-02-11 07:50:51,051 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,053 - Probs max: 0.9423828125
2025-02-11 07:50:51,054 - Pre-cat
2025-02-11 07:50:51,054 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,     11,    279,   7002,    374,   7218,
            518,    264,   4628]], device='cuda:0')
2025-02-11 07:50:51,055 - Next token: tensor([[289]], device='cuda:0')
2025-02-11 07:50:51,056 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:50:51,056 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,056 - Step 17: Generated next token
2025-02-11 07:50:51,056 - Step 17: Updated current_ids
2025-02-11 07:50:51,056 - Step 17: Decoded token text:  w
2025-02-11 07:50:51,056 - Step 17: Updated current_phrase
2025-02-11 07:50:51,057 - Step 17: Created step_acts
2025-02-11 07:50:51,057 - Step 17: Added to generation_acts
2025-02-11 07:50:51,057 - Step 17: Updated recent_tokens
2025-02-11 07:50:51,058 - Step 17: Decoded current text
2025-02-11 07:50:51,058 - Step 17: Reset consecutive_fillers
2025-02-11 07:50:51,058 - Step 17: Calculated unique_ratio: 0.6875
2025-02-11 07:50:51,058 - 
Starting step 18
2025-02-11 07:50:51,058 - Current_ids device: cuda:0
2025-02-11 07:50:51,058 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,082 - Model output complete
2025-02-11 07:50:51,082 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:50:51,082 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,082 - Next token logits device: cuda:0
2025-02-11 07:50:51,082 - Entered do_sample
2025-02-11 07:50:51,082 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,085 - Probs max: 0.54052734375
2025-02-11 07:50:51,085 - Pre-cat
2025-02-11 07:50:51,085 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,     11,    279,   7002,    374,   7218,
            518,    264,   4628,    289]], device='cuda:0')
2025-02-11 07:50:51,087 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:51,087 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:50:51,088 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,088 - Step 18: Generated next token
2025-02-11 07:50:51,088 - Step 18: Updated current_ids
2025-02-11 07:50:51,088 - Step 18: Decoded token text: .
2025-02-11 07:50:51,088 - Step 18: Updated current_phrase
2025-02-11 07:50:51,088 - Step 18: Created step_acts
2025-02-11 07:50:51,088 - Step 18: Added to generation_acts
2025-02-11 07:50:51,089 - Step 18: Updated recent_tokens
2025-02-11 07:50:51,090 - Step 18: Found phrase end token
2025-02-11 07:50:51,090 - Step 18: Updated recent_phrases
2025-02-11 07:50:51,090 - Step 18: Calculated similarity: 0.75
2025-02-11 07:50:51,090 - Step 18: Decoded current text
2025-02-11 07:50:51,090 - Step 18: Reset consecutive_fillers
2025-02-11 07:50:51,090 - Step 18: Calculated unique_ratio: 0.75
2025-02-11 07:50:51,090 - 
Starting step 19
2025-02-11 07:50:51,090 - Current_ids device: cuda:0
2025-02-11 07:50:51,090 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,113 - Model output complete
2025-02-11 07:50:51,113 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:50:51,113 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,113 - Next token logits device: cuda:0
2025-02-11 07:50:51,113 - Entered do_sample
2025-02-11 07:50:51,113 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,116 - Probs max: 0.4931640625
2025-02-11 07:50:51,117 - Pre-cat
2025-02-11 07:50:51,117 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,     11,    279,   7002,    374,   7218,
            518,    264,   4628,    289,     13]], device='cuda:0')
2025-02-11 07:50:51,118 - Next token: tensor([[2055]], device='cuda:0')
2025-02-11 07:50:51,119 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:50:51,119 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,119 - Step 19: Generated next token
2025-02-11 07:50:51,119 - Step 19: Updated current_ids
2025-02-11 07:50:51,119 - Step 19: Decoded token text:  So
2025-02-11 07:50:51,119 - Step 19: Updated current_phrase
2025-02-11 07:50:51,119 - Step 19: Created step_acts
2025-02-11 07:50:51,119 - Step 19: Added to generation_acts
2025-02-11 07:50:51,120 - Step 19: Updated recent_tokens
2025-02-11 07:50:51,121 - Step 19: Decoded current text
2025-02-11 07:50:51,121 - Step 19: Reset consecutive_fillers
2025-02-11 07:50:51,121 - Step 19: Calculated unique_ratio: 0.8125
2025-02-11 07:50:51,121 - 
Starting step 20
2025-02-11 07:50:51,121 - Current_ids device: cuda:0
2025-02-11 07:50:51,121 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,144 - Model output complete
2025-02-11 07:50:51,144 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:50:51,144 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,144 - Next token logits device: cuda:0
2025-02-11 07:50:51,144 - Entered do_sample
2025-02-11 07:50:51,144 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,147 - Probs max: 0.68603515625
2025-02-11 07:50:51,148 - Pre-cat
2025-02-11 07:50:51,148 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,     11,    279,   7002,    374,   7218,
            518,    264,   4628,    289,     13,   2055]], device='cuda:0')
2025-02-11 07:50:51,150 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:51,150 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:50:51,150 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,150 - Step 20: Generated next token
2025-02-11 07:50:51,150 - Step 20: Updated current_ids
2025-02-11 07:50:51,150 - Step 20: Decoded token text:  the
2025-02-11 07:50:51,150 - Step 20: Updated current_phrase
2025-02-11 07:50:51,151 - Step 20: Created step_acts
2025-02-11 07:50:51,151 - Step 20: Added to generation_acts
2025-02-11 07:50:51,152 - Step 20: Updated generated_texts
2025-02-11 07:50:51,152 - Step 20: Updated recent_tokens
2025-02-11 07:50:51,152 - Step 20: Decoded current text
2025-02-11 07:50:51,152 - Step 20: Incremented consecutive_fillers to 1
2025-02-11 07:50:51,153 - Step 20: Calculated unique_ratio: 0.8125
2025-02-11 07:50:51,153 - 
Starting step 21
2025-02-11 07:50:51,153 - Current_ids device: cuda:0
2025-02-11 07:50:51,153 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,176 - Model output complete
2025-02-11 07:50:51,176 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:50:51,176 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,176 - Next token logits device: cuda:0
2025-02-11 07:50:51,176 - Entered do_sample
2025-02-11 07:50:51,176 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,179 - Probs max: 0.734375
2025-02-11 07:50:51,180 - Pre-cat
2025-02-11 07:50:51,180 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,     11,    279,   7002,    374,   7218,
            518,    264,   4628,    289,     13,   2055,    279]],
       device='cuda:0')
2025-02-11 07:50:51,182 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:51,182 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:50:51,182 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,182 - Step 21: Generated next token
2025-02-11 07:50:51,182 - Step 21: Updated current_ids
2025-02-11 07:50:51,182 - Step 21: Decoded token text:  ball
2025-02-11 07:50:51,182 - Step 21: Updated current_phrase
2025-02-11 07:50:51,183 - Step 21: Created step_acts
2025-02-11 07:50:51,183 - Step 21: Added to generation_acts
2025-02-11 07:50:51,183 - Step 21: Updated recent_tokens
2025-02-11 07:50:51,184 - Step 21: Decoded current text
2025-02-11 07:50:51,184 - Step 21: Incremented consecutive_fillers to 2
2025-02-11 07:50:51,184 - Step 21: Calculated unique_ratio: 0.875
2025-02-11 07:50:51,185 - 
Starting step 22
2025-02-11 07:50:51,185 - Current_ids device: cuda:0
2025-02-11 07:50:51,185 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,208 - Model output complete
2025-02-11 07:50:51,208 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:50:51,208 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,208 - Next token logits device: cuda:0
2025-02-11 07:50:51,208 - Entered do_sample
2025-02-11 07:50:51,208 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,212 - Probs max: 0.39453125
2025-02-11 07:50:51,212 - Pre-cat
2025-02-11 07:50:51,212 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,     11,    279,   7002,    374,   7218,
            518,    264,   4628,    289,     13,   2055,    279,   4935]],
       device='cuda:0')
2025-02-11 07:50:51,214 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:50:51,215 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:50:51,215 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,215 - Step 22: Generated next token
2025-02-11 07:50:51,215 - Step 22: Updated current_ids
2025-02-11 07:50:51,215 - Step 22: Decoded token text: 's
2025-02-11 07:50:51,215 - Step 22: Updated current_phrase
2025-02-11 07:50:51,215 - Step 22: Created step_acts
2025-02-11 07:50:51,215 - Step 22: Added to generation_acts
2025-02-11 07:50:51,216 - Step 22: Updated recent_tokens
2025-02-11 07:50:51,217 - Step 22: Decoded current text
2025-02-11 07:50:51,217 - Step 22: Incremented consecutive_fillers to 3
2025-02-11 07:50:51,325 - 
Starting step 0
2025-02-11 07:50:51,326 - Current_ids device: cuda:0
2025-02-11 07:50:51,326 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,356 - Model output complete
2025-02-11 07:50:51,356 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:51,356 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,356 - Next token logits device: cuda:0
2025-02-11 07:50:51,356 - Entered do_sample
2025-02-11 07:50:51,356 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,358 - Probs max: 0.50341796875
2025-02-11 07:50:51,360 - Pre-cat
2025-02-11 07:50:51,360 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:51,362 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:50:51,362 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:51,362 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,362 - Step 0: Generated next token
2025-02-11 07:50:51,362 - Step 0: Updated current_ids
2025-02-11 07:50:51,362 - Step 0: Decoded token text:  If
2025-02-11 07:50:51,362 - Step 0: Updated current_phrase
2025-02-11 07:50:51,363 - Step 0: Created step_acts
2025-02-11 07:50:51,363 - Step 0: Added to generation_acts
2025-02-11 07:50:51,364 - Step 0: Updated generated_texts
2025-02-11 07:50:51,364 - Step 0: Updated recent_tokens
2025-02-11 07:50:51,365 - Step 0: Decoded current text
2025-02-11 07:50:51,365 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:51,365 - 
Starting step 1
2025-02-11 07:50:51,365 - Current_ids device: cuda:0
2025-02-11 07:50:51,365 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,389 - Model output complete
2025-02-11 07:50:51,390 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:51,390 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,390 - Next token logits device: cuda:0
2025-02-11 07:50:51,390 - Entered do_sample
2025-02-11 07:50:51,390 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,392 - Probs max: 0.7099609375
2025-02-11 07:50:51,393 - Pre-cat
2025-02-11 07:50:51,393 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:50:51,394 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:51,394 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:51,394 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,394 - Step 1: Generated next token
2025-02-11 07:50:51,395 - Step 1: Updated current_ids
2025-02-11 07:50:51,395 - Step 1: Decoded token text:  the
2025-02-11 07:50:51,395 - Step 1: Updated current_phrase
2025-02-11 07:50:51,395 - Step 1: Created step_acts
2025-02-11 07:50:51,395 - Step 1: Added to generation_acts
2025-02-11 07:50:51,395 - Step 1: Updated recent_tokens
2025-02-11 07:50:51,397 - Step 1: Decoded current text
2025-02-11 07:50:51,397 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:51,397 - 
Starting step 2
2025-02-11 07:50:51,397 - Current_ids device: cuda:0
2025-02-11 07:50:51,397 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,427 - Model output complete
2025-02-11 07:50:51,427 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:51,427 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,427 - Next token logits device: cuda:0
2025-02-11 07:50:51,427 - Entered do_sample
2025-02-11 07:50:51,427 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,429 - Probs max: 0.76123046875
2025-02-11 07:50:51,430 - Pre-cat
2025-02-11 07:50:51,430 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279]], device='cuda:0')
2025-02-11 07:50:51,431 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:51,432 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:51,432 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,432 - Step 2: Generated next token
2025-02-11 07:50:51,432 - Step 2: Updated current_ids
2025-02-11 07:50:51,432 - Step 2: Decoded token text:  ball
2025-02-11 07:50:51,432 - Step 2: Updated current_phrase
2025-02-11 07:50:51,432 - Step 2: Created step_acts
2025-02-11 07:50:51,432 - Step 2: Added to generation_acts
2025-02-11 07:50:51,432 - Step 2: Updated recent_tokens
2025-02-11 07:50:51,434 - Step 2: Decoded current text
2025-02-11 07:50:51,434 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:51,434 - 
Starting step 3
2025-02-11 07:50:51,434 - Current_ids device: cuda:0
2025-02-11 07:50:51,434 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,455 - Model output complete
2025-02-11 07:50:51,455 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:51,455 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,456 - Next token logits device: cuda:0
2025-02-11 07:50:51,456 - Entered do_sample
2025-02-11 07:50:51,456 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,459 - Probs max: 0.9921875
2025-02-11 07:50:51,459 - Pre-cat
2025-02-11 07:50:51,459 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935]],
       device='cuda:0')
2025-02-11 07:50:51,461 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:51,462 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:51,462 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,462 - Step 3: Generated next token
2025-02-11 07:50:51,462 - Step 3: Updated current_ids
2025-02-11 07:50:51,462 - Step 3: Decoded token text:  is
2025-02-11 07:50:51,462 - Step 3: Updated current_phrase
2025-02-11 07:50:51,462 - Step 3: Created step_acts
2025-02-11 07:50:51,463 - Step 3: Added to generation_acts
2025-02-11 07:50:51,463 - Step 3: Updated recent_tokens
2025-02-11 07:50:51,464 - Step 3: Decoded current text
2025-02-11 07:50:51,464 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:51,464 - 
Starting step 4
2025-02-11 07:50:51,464 - Current_ids device: cuda:0
2025-02-11 07:50:51,464 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,487 - Model output complete
2025-02-11 07:50:51,487 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:51,487 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,487 - Next token logits device: cuda:0
2025-02-11 07:50:51,487 - Entered do_sample
2025-02-11 07:50:51,487 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,490 - Probs max: 0.94921875
2025-02-11 07:50:51,490 - Pre-cat
2025-02-11 07:50:51,490 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374]],
       device='cuda:0')
2025-02-11 07:50:51,492 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:50:51,492 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:51,492 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,492 - Step 4: Generated next token
2025-02-11 07:50:51,492 - Step 4: Updated current_ids
2025-02-11 07:50:51,493 - Step 4: Decoded token text:  thrown
2025-02-11 07:50:51,493 - Step 4: Updated current_phrase
2025-02-11 07:50:51,493 - Step 4: Created step_acts
2025-02-11 07:50:51,493 - Step 4: Added to generation_acts
2025-02-11 07:50:51,493 - Step 4: Updated recent_tokens
2025-02-11 07:50:51,494 - Step 4: Decoded current text
2025-02-11 07:50:51,494 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:51,495 - 
Starting step 5
2025-02-11 07:50:51,495 - Current_ids device: cuda:0
2025-02-11 07:50:51,495 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,517 - Model output complete
2025-02-11 07:50:51,517 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:51,517 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,517 - Next token logits device: cuda:0
2025-02-11 07:50:51,517 - Entered do_sample
2025-02-11 07:50:51,517 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,520 - Probs max: 0.7861328125
2025-02-11 07:50:51,520 - Pre-cat
2025-02-11 07:50:51,521 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:50:51,522 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:50:51,522 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:51,522 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,522 - Step 5: Generated next token
2025-02-11 07:50:51,522 - Step 5: Updated current_ids
2025-02-11 07:50:51,523 - Step 5: Decoded token text:  at
2025-02-11 07:50:51,523 - Step 5: Updated current_phrase
2025-02-11 07:50:51,523 - Step 5: Created step_acts
2025-02-11 07:50:51,523 - Step 5: Added to generation_acts
2025-02-11 07:50:51,524 - Step 5: Updated generated_texts
2025-02-11 07:50:51,524 - Step 5: Updated recent_tokens
2025-02-11 07:50:51,525 - Step 5: Decoded current text
2025-02-11 07:50:51,525 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:51,525 - 
Starting step 6
2025-02-11 07:50:51,525 - Current_ids device: cuda:0
2025-02-11 07:50:51,525 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,547 - Model output complete
2025-02-11 07:50:51,548 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:51,548 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,548 - Next token logits device: cuda:0
2025-02-11 07:50:51,548 - Entered do_sample
2025-02-11 07:50:51,548 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,550 - Probs max: 0.8837890625
2025-02-11 07:50:51,551 - Pre-cat
2025-02-11 07:50:51,551 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:50:51,553 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:51,553 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:51,553 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,553 - Step 6: Generated next token
2025-02-11 07:50:51,554 - Step 6: Updated current_ids
2025-02-11 07:50:51,554 - Step 6: Decoded token text:  a
2025-02-11 07:50:51,554 - Step 6: Updated current_phrase
2025-02-11 07:50:51,554 - Step 6: Created step_acts
2025-02-11 07:50:51,554 - Step 6: Added to generation_acts
2025-02-11 07:50:51,554 - Step 6: Updated recent_tokens
2025-02-11 07:50:51,556 - Step 6: Decoded current text
2025-02-11 07:50:51,556 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:51,556 - 
Starting step 7
2025-02-11 07:50:51,556 - Current_ids device: cuda:0
2025-02-11 07:50:51,556 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,579 - Model output complete
2025-02-11 07:50:51,579 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:51,579 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,579 - Next token logits device: cuda:0
2025-02-11 07:50:51,579 - Entered do_sample
2025-02-11 07:50:51,579 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,581 - Probs max: 0.98974609375
2025-02-11 07:50:51,582 - Pre-cat
2025-02-11 07:50:51,582 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:50:51,584 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:51,584 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:51,584 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,584 - Step 7: Generated next token
2025-02-11 07:50:51,584 - Step 7: Updated current_ids
2025-02-11 07:50:51,585 - Step 7: Decoded token text:  wall
2025-02-11 07:50:51,585 - Step 7: Updated current_phrase
2025-02-11 07:50:51,585 - Step 7: Created step_acts
2025-02-11 07:50:51,585 - Step 7: Added to generation_acts
2025-02-11 07:50:51,585 - Step 7: Updated recent_tokens
2025-02-11 07:50:51,587 - Step 7: Decoded current text
2025-02-11 07:50:51,587 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:51,587 - 
Starting step 8
2025-02-11 07:50:51,587 - Current_ids device: cuda:0
2025-02-11 07:50:51,587 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,610 - Model output complete
2025-02-11 07:50:51,610 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:51,610 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,610 - Next token logits device: cuda:0
2025-02-11 07:50:51,610 - Entered do_sample
2025-02-11 07:50:51,610 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,612 - Probs max: 0.99462890625
2025-02-11 07:50:51,613 - Pre-cat
2025-02-11 07:50:51,613 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:50:51,615 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:51,616 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:51,616 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,616 - Step 8: Generated next token
2025-02-11 07:50:51,616 - Step 8: Updated current_ids
2025-02-11 07:50:51,616 - Step 8: Decoded token text:  very
2025-02-11 07:50:51,616 - Step 8: Updated current_phrase
2025-02-11 07:50:51,617 - Step 8: Created step_acts
2025-02-11 07:50:51,617 - Step 8: Added to generation_acts
2025-02-11 07:50:51,617 - Step 8: Updated recent_tokens
2025-02-11 07:50:51,618 - Step 8: Decoded current text
2025-02-11 07:50:51,619 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:51,619 - 
Starting step 9
2025-02-11 07:50:51,619 - Current_ids device: cuda:0
2025-02-11 07:50:51,619 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,642 - Model output complete
2025-02-11 07:50:51,642 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:51,642 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,642 - Next token logits device: cuda:0
2025-02-11 07:50:51,642 - Entered do_sample
2025-02-11 07:50:51,642 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,645 - Probs max: 0.99755859375
2025-02-11 07:50:51,646 - Pre-cat
2025-02-11 07:50:51,646 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:50:51,647 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:51,647 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:51,647 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,647 - Step 9: Generated next token
2025-02-11 07:50:51,648 - Step 9: Updated current_ids
2025-02-11 07:50:51,648 - Step 9: Decoded token text:  fast
2025-02-11 07:50:51,648 - Step 9: Updated current_phrase
2025-02-11 07:50:51,648 - Step 9: Created step_acts
2025-02-11 07:50:51,648 - Step 9: Added to generation_acts
2025-02-11 07:50:51,648 - Step 9: Updated recent_tokens
2025-02-11 07:50:51,650 - Step 9: Decoded current text
2025-02-11 07:50:51,650 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:51,650 - 
Starting step 10
2025-02-11 07:50:51,650 - Current_ids device: cuda:0
2025-02-11 07:50:51,650 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,673 - Model output complete
2025-02-11 07:50:51,673 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:51,673 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,673 - Next token logits device: cuda:0
2025-02-11 07:50:51,673 - Entered do_sample
2025-02-11 07:50:51,673 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,676 - Probs max: 0.99853515625
2025-02-11 07:50:51,676 - Pre-cat
2025-02-11 07:50:51,676 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:50:51,678 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:51,678 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:51,678 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,678 - Step 10: Generated next token
2025-02-11 07:50:51,678 - Step 10: Updated current_ids
2025-02-11 07:50:51,678 - Step 10: Decoded token text: ,
2025-02-11 07:50:51,678 - Step 10: Updated current_phrase
2025-02-11 07:50:51,679 - Step 10: Created step_acts
2025-02-11 07:50:51,679 - Step 10: Added to generation_acts
2025-02-11 07:50:51,680 - Step 10: Updated generated_texts
2025-02-11 07:50:51,680 - Step 10: Updated recent_tokens
2025-02-11 07:50:51,680 - Step 10: Found phrase end token
2025-02-11 07:50:51,680 - Step 10: Updated recent_phrases
2025-02-11 07:50:51,681 - Step 10: Decoded current text
2025-02-11 07:50:51,681 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:51,681 - 
Starting step 11
2025-02-11 07:50:51,681 - Current_ids device: cuda:0
2025-02-11 07:50:51,681 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,705 - Model output complete
2025-02-11 07:50:51,705 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:51,705 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,705 - Next token logits device: cuda:0
2025-02-11 07:50:51,705 - Entered do_sample
2025-02-11 07:50:51,705 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,708 - Probs max: 0.66357421875
2025-02-11 07:50:51,708 - Pre-cat
2025-02-11 07:50:51,708 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:50:51,710 - Next token: tensor([[432]], device='cuda:0')
2025-02-11 07:50:51,710 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:51,710 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,710 - Step 11: Generated next token
2025-02-11 07:50:51,710 - Step 11: Updated current_ids
2025-02-11 07:50:51,710 - Step 11: Decoded token text:  it
2025-02-11 07:50:51,710 - Step 11: Updated current_phrase
2025-02-11 07:50:51,711 - Step 11: Created step_acts
2025-02-11 07:50:51,711 - Step 11: Added to generation_acts
2025-02-11 07:50:51,711 - Step 11: Updated recent_tokens
2025-02-11 07:50:51,712 - Step 11: Decoded current text
2025-02-11 07:50:51,712 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:51,712 - 
Starting step 12
2025-02-11 07:50:51,712 - Current_ids device: cuda:0
2025-02-11 07:50:51,713 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,736 - Model output complete
2025-02-11 07:50:51,736 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:51,737 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,737 - Next token logits device: cuda:0
2025-02-11 07:50:51,737 - Entered do_sample
2025-02-11 07:50:51,737 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,739 - Probs max: 0.91845703125
2025-02-11 07:50:51,740 - Pre-cat
2025-02-11 07:50:51,740 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    432]],
       device='cuda:0')
2025-02-11 07:50:51,742 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:51,742 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:51,742 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,742 - Step 12: Generated next token
2025-02-11 07:50:51,742 - Step 12: Updated current_ids
2025-02-11 07:50:51,742 - Step 12: Decoded token text:  will
2025-02-11 07:50:51,742 - Step 12: Updated current_phrase
2025-02-11 07:50:51,743 - Step 12: Created step_acts
2025-02-11 07:50:51,743 - Step 12: Added to generation_acts
2025-02-11 07:50:51,743 - Step 12: Updated recent_tokens
2025-02-11 07:50:51,744 - Step 12: Decoded current text
2025-02-11 07:50:51,744 - Step 12: Reset consecutive_fillers
2025-02-11 07:50:51,744 - 
Starting step 13
2025-02-11 07:50:51,744 - Current_ids device: cuda:0
2025-02-11 07:50:51,744 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,772 - Model output complete
2025-02-11 07:50:51,772 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:51,772 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,772 - Next token logits device: cuda:0
2025-02-11 07:50:51,772 - Entered do_sample
2025-02-11 07:50:51,772 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,774 - Probs max: 0.63037109375
2025-02-11 07:50:51,775 - Pre-cat
2025-02-11 07:50:51,775 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    432,    686]],
       device='cuda:0')
2025-02-11 07:50:51,778 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:50:51,778 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:51,778 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,778 - Step 13: Generated next token
2025-02-11 07:50:51,778 - Step 13: Updated current_ids
2025-02-11 07:50:51,779 - Step 13: Decoded token text:  hit
2025-02-11 07:50:51,779 - Step 13: Updated current_phrase
2025-02-11 07:50:51,779 - Step 13: Created step_acts
2025-02-11 07:50:51,779 - Step 13: Added to generation_acts
2025-02-11 07:50:51,779 - Step 13: Updated recent_tokens
2025-02-11 07:50:51,780 - Step 13: Decoded current text
2025-02-11 07:50:51,780 - Step 13: Reset consecutive_fillers
2025-02-11 07:50:51,780 - 
Starting step 14
2025-02-11 07:50:51,781 - Current_ids device: cuda:0
2025-02-11 07:50:51,781 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,813 - Model output complete
2025-02-11 07:50:51,813 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:50:51,813 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,813 - Next token logits device: cuda:0
2025-02-11 07:50:51,813 - Entered do_sample
2025-02-11 07:50:51,813 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,815 - Probs max: 0.9921875
2025-02-11 07:50:51,816 - Pre-cat
2025-02-11 07:50:51,816 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    432,    686,   4201]],
       device='cuda:0')
2025-02-11 07:50:51,817 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:51,818 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:50:51,818 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,818 - Step 14: Generated next token
2025-02-11 07:50:51,818 - Step 14: Updated current_ids
2025-02-11 07:50:51,818 - Step 14: Decoded token text:  the
2025-02-11 07:50:51,818 - Step 14: Updated current_phrase
2025-02-11 07:50:51,819 - Step 14: Created step_acts
2025-02-11 07:50:51,819 - Step 14: Added to generation_acts
2025-02-11 07:50:51,819 - Step 14: Updated recent_tokens
2025-02-11 07:50:51,820 - Step 14: Decoded current text
2025-02-11 07:50:51,820 - Step 14: Reset consecutive_fillers
2025-02-11 07:50:51,820 - 
Starting step 15
2025-02-11 07:50:51,820 - Current_ids device: cuda:0
2025-02-11 07:50:51,820 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,843 - Model output complete
2025-02-11 07:50:51,844 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:50:51,844 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,844 - Next token logits device: cuda:0
2025-02-11 07:50:51,844 - Entered do_sample
2025-02-11 07:50:51,844 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,847 - Probs max: 0.99853515625
2025-02-11 07:50:51,847 - Pre-cat
2025-02-11 07:50:51,847 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    432,    686,   4201,
            279]], device='cuda:0')
2025-02-11 07:50:51,850 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:51,850 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:50:51,850 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,850 - Step 15: Generated next token
2025-02-11 07:50:51,850 - Step 15: Updated current_ids
2025-02-11 07:50:51,851 - Step 15: Decoded token text:  wall
2025-02-11 07:50:51,851 - Step 15: Updated current_phrase
2025-02-11 07:50:51,851 - Step 15: Created step_acts
2025-02-11 07:50:51,851 - Step 15: Added to generation_acts
2025-02-11 07:50:51,852 - Step 15: Updated generated_texts
2025-02-11 07:50:51,853 - Step 15: Updated recent_tokens
2025-02-11 07:50:51,853 - Step 15: Decoded current text
2025-02-11 07:50:51,853 - Step 15: Reset consecutive_fillers
2025-02-11 07:50:51,853 - 
Starting step 16
2025-02-11 07:50:51,853 - Current_ids device: cuda:0
2025-02-11 07:50:51,853 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,882 - Model output complete
2025-02-11 07:50:51,883 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:50:51,883 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,883 - Next token logits device: cuda:0
2025-02-11 07:50:51,883 - Entered do_sample
2025-02-11 07:50:51,883 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,885 - Probs max: 0.326904296875
2025-02-11 07:50:51,886 - Pre-cat
2025-02-11 07:50:51,886 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    432,    686,   4201,
            279,   7002]], device='cuda:0')
2025-02-11 07:50:51,890 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:51,890 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:50:51,890 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,890 - Step 16: Generated next token
2025-02-11 07:50:51,890 - Step 16: Updated current_ids
2025-02-11 07:50:51,891 - Step 16: Decoded token text:  very
2025-02-11 07:50:51,891 - Step 16: Updated current_phrase
2025-02-11 07:50:51,891 - Step 16: Created step_acts
2025-02-11 07:50:51,891 - Step 16: Added to generation_acts
2025-02-11 07:50:51,891 - Step 16: Updated recent_tokens
2025-02-11 07:50:51,893 - Step 16: Decoded current text
2025-02-11 07:50:51,893 - Step 16: Reset consecutive_fillers
2025-02-11 07:50:51,893 - Step 16: Calculated unique_ratio: 0.8125
2025-02-11 07:50:51,893 - 
Starting step 17
2025-02-11 07:50:51,893 - Current_ids device: cuda:0
2025-02-11 07:50:51,893 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,917 - Model output complete
2025-02-11 07:50:51,917 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:50:51,917 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,917 - Next token logits device: cuda:0
2025-02-11 07:50:51,917 - Entered do_sample
2025-02-11 07:50:51,918 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,920 - Probs max: 0.56396484375
2025-02-11 07:50:51,921 - Pre-cat
2025-02-11 07:50:51,921 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    432,    686,   4201,
            279,   7002,   1602]], device='cuda:0')
2025-02-11 07:50:51,923 - Next token: tensor([[6157]], device='cuda:0')
2025-02-11 07:50:51,923 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:50:51,923 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,923 - Step 17: Generated next token
2025-02-11 07:50:51,923 - Step 17: Updated current_ids
2025-02-11 07:50:51,924 - Step 17: Decoded token text:  quickly
2025-02-11 07:50:51,924 - Step 17: Updated current_phrase
2025-02-11 07:50:51,924 - Step 17: Created step_acts
2025-02-11 07:50:51,924 - Step 17: Added to generation_acts
2025-02-11 07:50:51,924 - Step 17: Updated recent_tokens
2025-02-11 07:50:51,925 - Step 17: Decoded current text
2025-02-11 07:50:51,925 - Step 17: Reset consecutive_fillers
2025-02-11 07:50:51,926 - Step 17: Calculated unique_ratio: 0.875
2025-02-11 07:50:51,926 - 
Starting step 18
2025-02-11 07:50:51,926 - Current_ids device: cuda:0
2025-02-11 07:50:51,926 - Current_ids dtype: torch.int64
2025-02-11 07:50:51,948 - Model output complete
2025-02-11 07:50:51,948 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:50:51,948 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,948 - Next token logits device: cuda:0
2025-02-11 07:50:51,948 - Entered do_sample
2025-02-11 07:50:51,948 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:51,951 - Probs max: 0.6083984375
2025-02-11 07:50:51,952 - Pre-cat
2025-02-11 07:50:51,952 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    432,    686,   4201,
            279,   7002,   1602,   6157]], device='cuda:0')
2025-02-11 07:50:51,954 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:51,954 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:50:51,954 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:51,954 - Step 18: Generated next token
2025-02-11 07:50:51,955 - Step 18: Updated current_ids
2025-02-11 07:50:51,955 - Step 18: Decoded token text: ,
2025-02-11 07:50:51,955 - Step 18: Updated current_phrase
2025-02-11 07:50:51,955 - Step 18: Created step_acts
2025-02-11 07:50:51,955 - Step 18: Added to generation_acts
2025-02-11 07:50:51,955 - Step 18: Updated recent_tokens
2025-02-11 07:50:51,956 - Step 18: Found phrase end token
2025-02-11 07:50:51,957 - Step 18: Updated recent_phrases
2025-02-11 07:50:51,957 - Step 18: Calculated similarity: 0.42857142857142855
2025-02-11 07:50:51,957 - Step 18: Decoded current text
2025-02-11 07:50:51,957 - Step 18: Reset consecutive_fillers
2025-02-11 07:50:51,957 - Step 18: Calculated unique_ratio: 0.8125
2025-02-11 07:50:51,957 - 
Starting step 19
2025-02-11 07:50:51,957 - Current_ids device: cuda:0
2025-02-11 07:50:51,957 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,016 - Model output complete
2025-02-11 07:50:52,016 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:50:52,016 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,016 - Next token logits device: cuda:0
2025-02-11 07:50:52,016 - Entered do_sample
2025-02-11 07:50:52,016 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,018 - Probs max: 0.56689453125
2025-02-11 07:50:52,019 - Pre-cat
2025-02-11 07:50:52,019 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    432,    686,   4201,
            279,   7002,   1602,   6157,     11]], device='cuda:0')
2025-02-11 07:50:52,021 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:50:52,021 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:50:52,021 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,021 - Step 19: Generated next token
2025-02-11 07:50:52,021 - Step 19: Updated current_ids
2025-02-11 07:50:52,021 - Step 19: Decoded token text:  and
2025-02-11 07:50:52,021 - Step 19: Updated current_phrase
2025-02-11 07:50:52,022 - Step 19: Created step_acts
2025-02-11 07:50:52,022 - Step 19: Added to generation_acts
2025-02-11 07:50:52,022 - Step 19: Updated recent_tokens
2025-02-11 07:50:52,028 - Step 19: Decoded current text
2025-02-11 07:50:52,029 - Step 19: Reset consecutive_fillers
2025-02-11 07:50:52,030 - Step 19: Calculated unique_ratio: 0.8125
2025-02-11 07:50:52,030 - 
Starting step 20
2025-02-11 07:50:52,030 - Current_ids device: cuda:0
2025-02-11 07:50:52,031 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,068 - Model output complete
2025-02-11 07:50:52,068 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:50:52,068 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,069 - Next token logits device: cuda:0
2025-02-11 07:50:52,069 - Entered do_sample
2025-02-11 07:50:52,069 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,071 - Probs max: 0.61767578125
2025-02-11 07:50:52,072 - Pre-cat
2025-02-11 07:50:52,072 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    432,    686,   4201,
            279,   7002,   1602,   6157,     11,    323]], device='cuda:0')
2025-02-11 07:50:52,077 - Next token: tensor([[773]], device='cuda:0')
2025-02-11 07:50:52,078 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:50:52,078 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,078 - Step 20: Generated next token
2025-02-11 07:50:52,078 - Step 20: Updated current_ids
2025-02-11 07:50:52,078 - Step 20: Decoded token text:  so
2025-02-11 07:50:52,078 - Step 20: Updated current_phrase
2025-02-11 07:50:52,079 - Step 20: Created step_acts
2025-02-11 07:50:52,079 - Step 20: Added to generation_acts
2025-02-11 07:50:52,080 - Step 20: Updated generated_texts
2025-02-11 07:50:52,080 - Step 20: Updated recent_tokens
2025-02-11 07:50:52,081 - Step 20: Decoded current text
2025-02-11 07:50:52,081 - Step 20: Reset consecutive_fillers
2025-02-11 07:50:52,081 - Step 20: Calculated unique_ratio: 0.8125
2025-02-11 07:50:52,081 - 
Starting step 21
2025-02-11 07:50:52,081 - Current_ids device: cuda:0
2025-02-11 07:50:52,081 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,116 - Model output complete
2025-02-11 07:50:52,116 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:50:52,116 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,116 - Next token logits device: cuda:0
2025-02-11 07:50:52,116 - Entered do_sample
2025-02-11 07:50:52,117 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,119 - Probs max: 0.88671875
2025-02-11 07:50:52,120 - Pre-cat
2025-02-11 07:50:52,120 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    432,    686,   4201,
            279,   7002,   1602,   6157,     11,    323,    773]],
       device='cuda:0')
2025-02-11 07:50:52,124 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:52,124 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:50:52,124 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,125 - Step 21: Generated next token
2025-02-11 07:50:52,125 - Step 21: Updated current_ids
2025-02-11 07:50:52,125 - Step 21: Decoded token text:  the
2025-02-11 07:50:52,125 - Step 21: Updated current_phrase
2025-02-11 07:50:52,125 - Step 21: Created step_acts
2025-02-11 07:50:52,125 - Step 21: Added to generation_acts
2025-02-11 07:50:52,126 - Step 21: Updated recent_tokens
2025-02-11 07:50:52,127 - Step 21: Decoded current text
2025-02-11 07:50:52,127 - Step 21: Incremented consecutive_fillers to 1
2025-02-11 07:50:52,127 - Step 21: Calculated unique_ratio: 0.75
2025-02-11 07:50:52,127 - 
Starting step 22
2025-02-11 07:50:52,127 - Current_ids device: cuda:0
2025-02-11 07:50:52,128 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,154 - Model output complete
2025-02-11 07:50:52,155 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:50:52,155 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,155 - Next token logits device: cuda:0
2025-02-11 07:50:52,155 - Entered do_sample
2025-02-11 07:50:52,155 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,157 - Probs max: 0.446533203125
2025-02-11 07:50:52,158 - Pre-cat
2025-02-11 07:50:52,158 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    432,    686,   4201,
            279,   7002,   1602,   6157,     11,    323,    773,    279]],
       device='cuda:0')
2025-02-11 07:50:52,160 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:52,160 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:50:52,160 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,161 - Step 22: Generated next token
2025-02-11 07:50:52,161 - Step 22: Updated current_ids
2025-02-11 07:50:52,161 - Step 22: Decoded token text:  ball
2025-02-11 07:50:52,161 - Step 22: Updated current_phrase
2025-02-11 07:50:52,161 - Step 22: Created step_acts
2025-02-11 07:50:52,161 - Step 22: Added to generation_acts
2025-02-11 07:50:52,161 - Step 22: Updated recent_tokens
2025-02-11 07:50:52,163 - Step 22: Decoded current text
2025-02-11 07:50:52,163 - Step 22: Incremented consecutive_fillers to 2
2025-02-11 07:50:52,163 - Step 22: Calculated unique_ratio: 0.75
2025-02-11 07:50:52,163 - 
Starting step 23
2025-02-11 07:50:52,163 - Current_ids device: cuda:0
2025-02-11 07:50:52,163 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,186 - Model output complete
2025-02-11 07:50:52,186 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:50:52,186 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,187 - Next token logits device: cuda:0
2025-02-11 07:50:52,187 - Entered do_sample
2025-02-11 07:50:52,187 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,190 - Probs max: 0.64453125
2025-02-11 07:50:52,191 - Pre-cat
2025-02-11 07:50:52,191 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    432,    686,   4201,
            279,   7002,   1602,   6157,     11,    323,    773,    279,   4935]],
       device='cuda:0')
2025-02-11 07:50:52,193 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:52,194 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:50:52,194 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,194 - Step 23: Generated next token
2025-02-11 07:50:52,194 - Step 23: Updated current_ids
2025-02-11 07:50:52,194 - Step 23: Decoded token text:  will
2025-02-11 07:50:52,194 - Step 23: Updated current_phrase
2025-02-11 07:50:52,194 - Step 23: Created step_acts
2025-02-11 07:50:52,194 - Step 23: Added to generation_acts
2025-02-11 07:50:52,195 - Step 23: Updated recent_tokens
2025-02-11 07:50:52,196 - Step 23: Decoded current text
2025-02-11 07:50:52,196 - Step 23: Incremented consecutive_fillers to 3
2025-02-11 07:50:52,300 - 
Starting step 0
2025-02-11 07:50:52,300 - Current_ids device: cuda:0
2025-02-11 07:50:52,300 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,333 - Model output complete
2025-02-11 07:50:52,334 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:52,334 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,334 - Next token logits device: cuda:0
2025-02-11 07:50:52,334 - Entered do_sample
2025-02-11 07:50:52,334 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,336 - Probs max: 0.50341796875
2025-02-11 07:50:52,338 - Pre-cat
2025-02-11 07:50:52,338 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:52,339 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:50:52,340 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:52,340 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,340 - Step 0: Generated next token
2025-02-11 07:50:52,340 - Step 0: Updated current_ids
2025-02-11 07:50:52,340 - Step 0: Decoded token text:  If
2025-02-11 07:50:52,340 - Step 0: Updated current_phrase
2025-02-11 07:50:52,341 - Step 0: Created step_acts
2025-02-11 07:50:52,341 - Step 0: Added to generation_acts
2025-02-11 07:50:52,342 - Step 0: Updated generated_texts
2025-02-11 07:50:52,342 - Step 0: Updated recent_tokens
2025-02-11 07:50:52,342 - Step 0: Decoded current text
2025-02-11 07:50:52,342 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:52,342 - 
Starting step 1
2025-02-11 07:50:52,342 - Current_ids device: cuda:0
2025-02-11 07:50:52,343 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,369 - Model output complete
2025-02-11 07:50:52,370 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:52,370 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,370 - Next token logits device: cuda:0
2025-02-11 07:50:52,370 - Entered do_sample
2025-02-11 07:50:52,370 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,372 - Probs max: 0.7099609375
2025-02-11 07:50:52,373 - Pre-cat
2025-02-11 07:50:52,373 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:50:52,375 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:52,375 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:52,376 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,376 - Step 1: Generated next token
2025-02-11 07:50:52,376 - Step 1: Updated current_ids
2025-02-11 07:50:52,376 - Step 1: Decoded token text:  the
2025-02-11 07:50:52,376 - Step 1: Updated current_phrase
2025-02-11 07:50:52,376 - Step 1: Created step_acts
2025-02-11 07:50:52,377 - Step 1: Added to generation_acts
2025-02-11 07:50:52,377 - Step 1: Updated recent_tokens
2025-02-11 07:50:52,378 - Step 1: Decoded current text
2025-02-11 07:50:52,378 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:52,378 - 
Starting step 2
2025-02-11 07:50:52,378 - Current_ids device: cuda:0
2025-02-11 07:50:52,378 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,402 - Model output complete
2025-02-11 07:50:52,402 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:52,402 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,403 - Next token logits device: cuda:0
2025-02-11 07:50:52,403 - Entered do_sample
2025-02-11 07:50:52,403 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,405 - Probs max: 0.76123046875
2025-02-11 07:50:52,407 - Pre-cat
2025-02-11 07:50:52,407 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279]], device='cuda:0')
2025-02-11 07:50:52,410 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:52,411 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:52,411 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,411 - Step 2: Generated next token
2025-02-11 07:50:52,411 - Step 2: Updated current_ids
2025-02-11 07:50:52,411 - Step 2: Decoded token text:  ball
2025-02-11 07:50:52,412 - Step 2: Updated current_phrase
2025-02-11 07:50:52,412 - Step 2: Created step_acts
2025-02-11 07:50:52,412 - Step 2: Added to generation_acts
2025-02-11 07:50:52,412 - Step 2: Updated recent_tokens
2025-02-11 07:50:52,413 - Step 2: Decoded current text
2025-02-11 07:50:52,414 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:52,414 - 
Starting step 3
2025-02-11 07:50:52,414 - Current_ids device: cuda:0
2025-02-11 07:50:52,414 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,464 - Model output complete
2025-02-11 07:50:52,464 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:52,464 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,464 - Next token logits device: cuda:0
2025-02-11 07:50:52,465 - Entered do_sample
2025-02-11 07:50:52,465 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,467 - Probs max: 0.9921875
2025-02-11 07:50:52,467 - Pre-cat
2025-02-11 07:50:52,468 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935]],
       device='cuda:0')
2025-02-11 07:50:52,469 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:52,469 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:52,469 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,469 - Step 3: Generated next token
2025-02-11 07:50:52,470 - Step 3: Updated current_ids
2025-02-11 07:50:52,470 - Step 3: Decoded token text:  is
2025-02-11 07:50:52,470 - Step 3: Updated current_phrase
2025-02-11 07:50:52,470 - Step 3: Created step_acts
2025-02-11 07:50:52,470 - Step 3: Added to generation_acts
2025-02-11 07:50:52,470 - Step 3: Updated recent_tokens
2025-02-11 07:50:52,471 - Step 3: Decoded current text
2025-02-11 07:50:52,471 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:52,472 - 
Starting step 4
2025-02-11 07:50:52,472 - Current_ids device: cuda:0
2025-02-11 07:50:52,472 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,495 - Model output complete
2025-02-11 07:50:52,495 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:52,495 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,495 - Next token logits device: cuda:0
2025-02-11 07:50:52,495 - Entered do_sample
2025-02-11 07:50:52,496 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,498 - Probs max: 0.94921875
2025-02-11 07:50:52,499 - Pre-cat
2025-02-11 07:50:52,499 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374]],
       device='cuda:0')
2025-02-11 07:50:52,500 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:50:52,500 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:52,501 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,501 - Step 4: Generated next token
2025-02-11 07:50:52,501 - Step 4: Updated current_ids
2025-02-11 07:50:52,501 - Step 4: Decoded token text:  thrown
2025-02-11 07:50:52,501 - Step 4: Updated current_phrase
2025-02-11 07:50:52,501 - Step 4: Created step_acts
2025-02-11 07:50:52,501 - Step 4: Added to generation_acts
2025-02-11 07:50:52,501 - Step 4: Updated recent_tokens
2025-02-11 07:50:52,503 - Step 4: Decoded current text
2025-02-11 07:50:52,503 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:52,503 - 
Starting step 5
2025-02-11 07:50:52,503 - Current_ids device: cuda:0
2025-02-11 07:50:52,503 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,525 - Model output complete
2025-02-11 07:50:52,525 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:52,525 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,525 - Next token logits device: cuda:0
2025-02-11 07:50:52,525 - Entered do_sample
2025-02-11 07:50:52,525 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,528 - Probs max: 0.7861328125
2025-02-11 07:50:52,529 - Pre-cat
2025-02-11 07:50:52,530 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:50:52,531 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:50:52,531 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:52,532 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,532 - Step 5: Generated next token
2025-02-11 07:50:52,532 - Step 5: Updated current_ids
2025-02-11 07:50:52,532 - Step 5: Decoded token text:  at
2025-02-11 07:50:52,532 - Step 5: Updated current_phrase
2025-02-11 07:50:52,532 - Step 5: Created step_acts
2025-02-11 07:50:52,532 - Step 5: Added to generation_acts
2025-02-11 07:50:52,534 - Step 5: Updated generated_texts
2025-02-11 07:50:52,534 - Step 5: Updated recent_tokens
2025-02-11 07:50:52,534 - Step 5: Decoded current text
2025-02-11 07:50:52,534 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:52,534 - 
Starting step 6
2025-02-11 07:50:52,534 - Current_ids device: cuda:0
2025-02-11 07:50:52,534 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,560 - Model output complete
2025-02-11 07:50:52,560 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:52,560 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,560 - Next token logits device: cuda:0
2025-02-11 07:50:52,560 - Entered do_sample
2025-02-11 07:50:52,560 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,562 - Probs max: 0.8837890625
2025-02-11 07:50:52,563 - Pre-cat
2025-02-11 07:50:52,563 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:50:52,565 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:52,566 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:52,566 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,566 - Step 6: Generated next token
2025-02-11 07:50:52,566 - Step 6: Updated current_ids
2025-02-11 07:50:52,566 - Step 6: Decoded token text:  a
2025-02-11 07:50:52,566 - Step 6: Updated current_phrase
2025-02-11 07:50:52,566 - Step 6: Created step_acts
2025-02-11 07:50:52,566 - Step 6: Added to generation_acts
2025-02-11 07:50:52,567 - Step 6: Updated recent_tokens
2025-02-11 07:50:52,568 - Step 6: Decoded current text
2025-02-11 07:50:52,568 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:52,568 - 
Starting step 7
2025-02-11 07:50:52,568 - Current_ids device: cuda:0
2025-02-11 07:50:52,568 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,591 - Model output complete
2025-02-11 07:50:52,591 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:52,591 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,591 - Next token logits device: cuda:0
2025-02-11 07:50:52,591 - Entered do_sample
2025-02-11 07:50:52,591 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,594 - Probs max: 0.98974609375
2025-02-11 07:50:52,594 - Pre-cat
2025-02-11 07:50:52,595 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:50:52,596 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:52,597 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:52,597 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,597 - Step 7: Generated next token
2025-02-11 07:50:52,597 - Step 7: Updated current_ids
2025-02-11 07:50:52,597 - Step 7: Decoded token text:  wall
2025-02-11 07:50:52,597 - Step 7: Updated current_phrase
2025-02-11 07:50:52,598 - Step 7: Created step_acts
2025-02-11 07:50:52,598 - Step 7: Added to generation_acts
2025-02-11 07:50:52,598 - Step 7: Updated recent_tokens
2025-02-11 07:50:52,599 - Step 7: Decoded current text
2025-02-11 07:50:52,599 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:52,599 - 
Starting step 8
2025-02-11 07:50:52,599 - Current_ids device: cuda:0
2025-02-11 07:50:52,599 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,624 - Model output complete
2025-02-11 07:50:52,624 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:52,624 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,624 - Next token logits device: cuda:0
2025-02-11 07:50:52,624 - Entered do_sample
2025-02-11 07:50:52,624 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,627 - Probs max: 0.99462890625
2025-02-11 07:50:52,628 - Pre-cat
2025-02-11 07:50:52,628 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:50:52,629 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:52,630 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:52,630 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,630 - Step 8: Generated next token
2025-02-11 07:50:52,630 - Step 8: Updated current_ids
2025-02-11 07:50:52,630 - Step 8: Decoded token text:  very
2025-02-11 07:50:52,630 - Step 8: Updated current_phrase
2025-02-11 07:50:52,630 - Step 8: Created step_acts
2025-02-11 07:50:52,631 - Step 8: Added to generation_acts
2025-02-11 07:50:52,631 - Step 8: Updated recent_tokens
2025-02-11 07:50:52,632 - Step 8: Decoded current text
2025-02-11 07:50:52,632 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:52,632 - 
Starting step 9
2025-02-11 07:50:52,632 - Current_ids device: cuda:0
2025-02-11 07:50:52,632 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,654 - Model output complete
2025-02-11 07:50:52,655 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:52,655 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,655 - Next token logits device: cuda:0
2025-02-11 07:50:52,655 - Entered do_sample
2025-02-11 07:50:52,655 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,658 - Probs max: 0.99755859375
2025-02-11 07:50:52,658 - Pre-cat
2025-02-11 07:50:52,658 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:50:52,661 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:52,661 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:52,661 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,661 - Step 9: Generated next token
2025-02-11 07:50:52,661 - Step 9: Updated current_ids
2025-02-11 07:50:52,661 - Step 9: Decoded token text:  fast
2025-02-11 07:50:52,661 - Step 9: Updated current_phrase
2025-02-11 07:50:52,662 - Step 9: Created step_acts
2025-02-11 07:50:52,662 - Step 9: Added to generation_acts
2025-02-11 07:50:52,662 - Step 9: Updated recent_tokens
2025-02-11 07:50:52,663 - Step 9: Decoded current text
2025-02-11 07:50:52,663 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:52,663 - 
Starting step 10
2025-02-11 07:50:52,663 - Current_ids device: cuda:0
2025-02-11 07:50:52,664 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,685 - Model output complete
2025-02-11 07:50:52,685 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:52,685 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,685 - Next token logits device: cuda:0
2025-02-11 07:50:52,685 - Entered do_sample
2025-02-11 07:50:52,685 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,689 - Probs max: 0.99853515625
2025-02-11 07:50:52,689 - Pre-cat
2025-02-11 07:50:52,690 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:50:52,691 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:52,692 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:52,692 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,692 - Step 10: Generated next token
2025-02-11 07:50:52,692 - Step 10: Updated current_ids
2025-02-11 07:50:52,692 - Step 10: Decoded token text: ,
2025-02-11 07:50:52,692 - Step 10: Updated current_phrase
2025-02-11 07:50:52,692 - Step 10: Created step_acts
2025-02-11 07:50:52,692 - Step 10: Added to generation_acts
2025-02-11 07:50:52,694 - Step 10: Updated generated_texts
2025-02-11 07:50:52,694 - Step 10: Updated recent_tokens
2025-02-11 07:50:52,695 - Step 10: Found phrase end token
2025-02-11 07:50:52,695 - Step 10: Updated recent_phrases
2025-02-11 07:50:52,695 - Step 10: Decoded current text
2025-02-11 07:50:52,695 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:52,695 - 
Starting step 11
2025-02-11 07:50:52,695 - Current_ids device: cuda:0
2025-02-11 07:50:52,695 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,719 - Model output complete
2025-02-11 07:50:52,719 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:52,719 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,719 - Next token logits device: cuda:0
2025-02-11 07:50:52,719 - Entered do_sample
2025-02-11 07:50:52,720 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,722 - Probs max: 0.66357421875
2025-02-11 07:50:52,723 - Pre-cat
2025-02-11 07:50:52,723 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:50:52,725 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:52,726 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:52,726 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,726 - Step 11: Generated next token
2025-02-11 07:50:52,726 - Step 11: Updated current_ids
2025-02-11 07:50:52,726 - Step 11: Decoded token text:  the
2025-02-11 07:50:52,726 - Step 11: Updated current_phrase
2025-02-11 07:50:52,726 - Step 11: Created step_acts
2025-02-11 07:50:52,726 - Step 11: Added to generation_acts
2025-02-11 07:50:52,727 - Step 11: Updated recent_tokens
2025-02-11 07:50:52,728 - Step 11: Decoded current text
2025-02-11 07:50:52,728 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:52,728 - 
Starting step 12
2025-02-11 07:50:52,728 - Current_ids device: cuda:0
2025-02-11 07:50:52,728 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,751 - Model output complete
2025-02-11 07:50:52,751 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:52,751 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,751 - Next token logits device: cuda:0
2025-02-11 07:50:52,751 - Entered do_sample
2025-02-11 07:50:52,751 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,754 - Probs max: 0.556640625
2025-02-11 07:50:52,755 - Pre-cat
2025-02-11 07:50:52,755 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279]],
       device='cuda:0')
2025-02-11 07:50:52,757 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:52,758 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:52,758 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,758 - Step 12: Generated next token
2025-02-11 07:50:52,758 - Step 12: Updated current_ids
2025-02-11 07:50:52,758 - Step 12: Decoded token text:  ball
2025-02-11 07:50:52,758 - Step 12: Updated current_phrase
2025-02-11 07:50:52,758 - Step 12: Created step_acts
2025-02-11 07:50:52,758 - Step 12: Added to generation_acts
2025-02-11 07:50:52,759 - Step 12: Updated recent_tokens
2025-02-11 07:50:52,760 - Step 12: Decoded current text
2025-02-11 07:50:52,760 - Step 12: Reset consecutive_fillers
2025-02-11 07:50:52,760 - 
Starting step 13
2025-02-11 07:50:52,760 - Current_ids device: cuda:0
2025-02-11 07:50:52,760 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,783 - Model output complete
2025-02-11 07:50:52,783 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:52,783 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,783 - Next token logits device: cuda:0
2025-02-11 07:50:52,783 - Entered do_sample
2025-02-11 07:50:52,783 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,786 - Probs max: 0.71337890625
2025-02-11 07:50:52,787 - Pre-cat
2025-02-11 07:50:52,787 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935]],
       device='cuda:0')
2025-02-11 07:50:52,789 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:52,789 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:52,789 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,789 - Step 13: Generated next token
2025-02-11 07:50:52,789 - Step 13: Updated current_ids
2025-02-11 07:50:52,789 - Step 13: Decoded token text:  is
2025-02-11 07:50:52,789 - Step 13: Updated current_phrase
2025-02-11 07:50:52,790 - Step 13: Created step_acts
2025-02-11 07:50:52,790 - Step 13: Added to generation_acts
2025-02-11 07:50:52,790 - Step 13: Updated recent_tokens
2025-02-11 07:50:52,791 - Step 13: Decoded current text
2025-02-11 07:50:52,791 - Step 13: Reset consecutive_fillers
2025-02-11 07:50:52,791 - 
Starting step 14
2025-02-11 07:50:52,791 - Current_ids device: cuda:0
2025-02-11 07:50:52,791 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,813 - Model output complete
2025-02-11 07:50:52,813 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:50:52,813 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,813 - Next token logits device: cuda:0
2025-02-11 07:50:52,813 - Entered do_sample
2025-02-11 07:50:52,814 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,817 - Probs max: 0.64306640625
2025-02-11 07:50:52,817 - Pre-cat
2025-02-11 07:50:52,817 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374]],
       device='cuda:0')
2025-02-11 07:50:52,819 - Next token: tensor([[7218]], device='cuda:0')
2025-02-11 07:50:52,819 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:50:52,819 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,819 - Step 14: Generated next token
2025-02-11 07:50:52,819 - Step 14: Updated current_ids
2025-02-11 07:50:52,820 - Step 14: Decoded token text:  moving
2025-02-11 07:50:52,820 - Step 14: Updated current_phrase
2025-02-11 07:50:52,820 - Step 14: Created step_acts
2025-02-11 07:50:52,820 - Step 14: Added to generation_acts
2025-02-11 07:50:52,820 - Step 14: Updated recent_tokens
2025-02-11 07:50:52,822 - Step 14: Decoded current text
2025-02-11 07:50:52,822 - Step 14: Reset consecutive_fillers
2025-02-11 07:50:52,822 - 
Starting step 15
2025-02-11 07:50:52,822 - Current_ids device: cuda:0
2025-02-11 07:50:52,822 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,844 - Model output complete
2025-02-11 07:50:52,844 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:50:52,844 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,844 - Next token logits device: cuda:0
2025-02-11 07:50:52,844 - Entered do_sample
2025-02-11 07:50:52,844 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,847 - Probs max: 0.6376953125
2025-02-11 07:50:52,848 - Pre-cat
2025-02-11 07:50:52,848 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218]], device='cuda:0')
2025-02-11 07:50:52,850 - Next token: tensor([[504]], device='cuda:0')
2025-02-11 07:50:52,850 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:50:52,850 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,850 - Step 15: Generated next token
2025-02-11 07:50:52,850 - Step 15: Updated current_ids
2025-02-11 07:50:52,851 - Step 15: Decoded token text:  from
2025-02-11 07:50:52,851 - Step 15: Updated current_phrase
2025-02-11 07:50:52,851 - Step 15: Created step_acts
2025-02-11 07:50:52,851 - Step 15: Added to generation_acts
2025-02-11 07:50:52,852 - Step 15: Updated generated_texts
2025-02-11 07:50:52,853 - Step 15: Updated recent_tokens
2025-02-11 07:50:52,853 - Step 15: Decoded current text
2025-02-11 07:50:52,853 - Step 15: Reset consecutive_fillers
2025-02-11 07:50:52,853 - 
Starting step 16
2025-02-11 07:50:52,853 - Current_ids device: cuda:0
2025-02-11 07:50:52,853 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,875 - Model output complete
2025-02-11 07:50:52,875 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:50:52,875 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,875 - Next token logits device: cuda:0
2025-02-11 07:50:52,876 - Entered do_sample
2025-02-11 07:50:52,876 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,879 - Probs max: 0.59619140625
2025-02-11 07:50:52,880 - Pre-cat
2025-02-11 07:50:52,880 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504]], device='cuda:0')
2025-02-11 07:50:52,882 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:52,882 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:50:52,882 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,882 - Step 16: Generated next token
2025-02-11 07:50:52,882 - Step 16: Updated current_ids
2025-02-11 07:50:52,882 - Step 16: Decoded token text:  the
2025-02-11 07:50:52,883 - Step 16: Updated current_phrase
2025-02-11 07:50:52,883 - Step 16: Created step_acts
2025-02-11 07:50:52,883 - Step 16: Added to generation_acts
2025-02-11 07:50:52,883 - Step 16: Updated recent_tokens
2025-02-11 07:50:52,884 - Step 16: Decoded current text
2025-02-11 07:50:52,884 - Step 16: Reset consecutive_fillers
2025-02-11 07:50:52,885 - Step 16: Calculated unique_ratio: 0.75
2025-02-11 07:50:52,885 - 
Starting step 17
2025-02-11 07:50:52,885 - Current_ids device: cuda:0
2025-02-11 07:50:52,885 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,909 - Model output complete
2025-02-11 07:50:52,909 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:50:52,909 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,909 - Next token logits device: cuda:0
2025-02-11 07:50:52,909 - Entered do_sample
2025-02-11 07:50:52,909 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,911 - Probs max: 0.923828125
2025-02-11 07:50:52,912 - Pre-cat
2025-02-11 07:50:52,912 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279]], device='cuda:0')
2025-02-11 07:50:52,914 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:52,914 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:50:52,914 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:52,914 - Step 17: Generated next token
2025-02-11 07:50:52,914 - Step 17: Updated current_ids
2025-02-11 07:50:52,914 - Step 17: Decoded token text:  wall
2025-02-11 07:50:52,914 - Step 17: Updated current_phrase
2025-02-11 07:50:52,915 - Step 17: Created step_acts
2025-02-11 07:50:52,915 - Step 17: Added to generation_acts
2025-02-11 07:50:52,915 - Step 17: Updated recent_tokens
2025-02-11 07:50:52,916 - Step 17: Decoded current text
2025-02-11 07:50:52,916 - Step 17: Reset consecutive_fillers
2025-02-11 07:50:52,917 - Step 17: Calculated unique_ratio: 0.75
2025-02-11 07:50:52,917 - 
Starting step 18
2025-02-11 07:50:52,917 - Current_ids device: cuda:0
2025-02-11 07:50:52,917 - Current_ids dtype: torch.int64
2025-02-11 07:50:52,996 - Model output complete
2025-02-11 07:50:52,996 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:50:52,996 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,996 - Next token logits device: cuda:0
2025-02-11 07:50:52,997 - Entered do_sample
2025-02-11 07:50:52,997 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:52,999 - Probs max: 0.462890625
2025-02-11 07:50:53,001 - Pre-cat
2025-02-11 07:50:53,001 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002]], device='cuda:0')
2025-02-11 07:50:53,005 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:50:53,005 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:50:53,005 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,005 - Step 18: Generated next token
2025-02-11 07:50:53,005 - Step 18: Updated current_ids
2025-02-11 07:50:53,005 - Step 18: Decoded token text: 's
2025-02-11 07:50:53,006 - Step 18: Updated current_phrase
2025-02-11 07:50:53,006 - Step 18: Created step_acts
2025-02-11 07:50:53,006 - Step 18: Added to generation_acts
2025-02-11 07:50:53,006 - Step 18: Updated recent_tokens
2025-02-11 07:50:53,008 - Step 18: Decoded current text
2025-02-11 07:50:53,008 - Step 18: Reset consecutive_fillers
2025-02-11 07:50:53,008 - Step 18: Calculated unique_ratio: 0.8125
2025-02-11 07:50:53,008 - 
Starting step 19
2025-02-11 07:50:53,008 - Current_ids device: cuda:0
2025-02-11 07:50:53,008 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,045 - Model output complete
2025-02-11 07:50:53,046 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:50:53,046 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,046 - Next token logits device: cuda:0
2025-02-11 07:50:53,046 - Entered do_sample
2025-02-11 07:50:53,046 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,048 - Probs max: 0.81494140625
2025-02-11 07:50:53,049 - Pre-cat
2025-02-11 07:50:53,049 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594]], device='cuda:0')
2025-02-11 07:50:53,051 - Next token: tensor([[13057]], device='cuda:0')
2025-02-11 07:50:53,051 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:50:53,052 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,052 - Step 19: Generated next token
2025-02-11 07:50:53,052 - Step 19: Updated current_ids
2025-02-11 07:50:53,052 - Step 19: Decoded token text:  perspective
2025-02-11 07:50:53,052 - Step 19: Updated current_phrase
2025-02-11 07:50:53,053 - Step 19: Created step_acts
2025-02-11 07:50:53,053 - Step 19: Added to generation_acts
2025-02-11 07:50:53,053 - Step 19: Updated recent_tokens
2025-02-11 07:50:53,054 - Step 19: Decoded current text
2025-02-11 07:50:53,054 - Step 19: Reset consecutive_fillers
2025-02-11 07:50:53,055 - Step 19: Calculated unique_ratio: 0.875
2025-02-11 07:50:53,055 - 
Starting step 20
2025-02-11 07:50:53,055 - Current_ids device: cuda:0
2025-02-11 07:50:53,055 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,079 - Model output complete
2025-02-11 07:50:53,079 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:50:53,079 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,079 - Next token logits device: cuda:0
2025-02-11 07:50:53,080 - Entered do_sample
2025-02-11 07:50:53,080 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,082 - Probs max: 0.263916015625
2025-02-11 07:50:53,083 - Pre-cat
2025-02-11 07:50:53,083 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057]], device='cuda:0')
2025-02-11 07:50:53,086 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:53,086 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:50:53,086 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,086 - Step 20: Generated next token
2025-02-11 07:50:53,087 - Step 20: Updated current_ids
2025-02-11 07:50:53,087 - Step 20: Decoded token text:  very
2025-02-11 07:50:53,087 - Step 20: Updated current_phrase
2025-02-11 07:50:53,087 - Step 20: Created step_acts
2025-02-11 07:50:53,087 - Step 20: Added to generation_acts
2025-02-11 07:50:53,089 - Step 20: Updated generated_texts
2025-02-11 07:50:53,089 - Step 20: Updated recent_tokens
2025-02-11 07:50:53,089 - Step 20: Decoded current text
2025-02-11 07:50:53,089 - Step 20: Reset consecutive_fillers
2025-02-11 07:50:53,090 - Step 20: Calculated unique_ratio: 0.8125
2025-02-11 07:50:53,090 - 
Starting step 21
2025-02-11 07:50:53,090 - Current_ids device: cuda:0
2025-02-11 07:50:53,090 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,116 - Model output complete
2025-02-11 07:50:53,116 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:50:53,116 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,116 - Next token logits device: cuda:0
2025-02-11 07:50:53,116 - Entered do_sample
2025-02-11 07:50:53,117 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,120 - Probs max: 0.9697265625
2025-02-11 07:50:53,121 - Pre-cat
2025-02-11 07:50:53,121 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602]],
       device='cuda:0')
2025-02-11 07:50:53,123 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:53,123 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:50:53,123 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,123 - Step 21: Generated next token
2025-02-11 07:50:53,124 - Step 21: Updated current_ids
2025-02-11 07:50:53,124 - Step 21: Decoded token text:  fast
2025-02-11 07:50:53,124 - Step 21: Updated current_phrase
2025-02-11 07:50:53,124 - Step 21: Created step_acts
2025-02-11 07:50:53,124 - Step 21: Added to generation_acts
2025-02-11 07:50:53,124 - Step 21: Updated recent_tokens
2025-02-11 07:50:53,126 - Step 21: Decoded current text
2025-02-11 07:50:53,126 - Step 21: Reset consecutive_fillers
2025-02-11 07:50:53,126 - Step 21: Calculated unique_ratio: 0.75
2025-02-11 07:50:53,126 - 
Starting step 22
2025-02-11 07:50:53,126 - Current_ids device: cuda:0
2025-02-11 07:50:53,126 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,149 - Model output complete
2025-02-11 07:50:53,149 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:50:53,149 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,149 - Next token logits device: cuda:0
2025-02-11 07:50:53,149 - Entered do_sample
2025-02-11 07:50:53,149 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,153 - Probs max: 0.453125
2025-02-11 07:50:53,154 - Pre-cat
2025-02-11 07:50:53,154 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937]],
       device='cuda:0')
2025-02-11 07:50:53,156 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:53,156 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:50:53,156 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,156 - Step 22: Generated next token
2025-02-11 07:50:53,156 - Step 22: Updated current_ids
2025-02-11 07:50:53,156 - Step 22: Decoded token text: .
2025-02-11 07:50:53,156 - Step 22: Updated current_phrase
2025-02-11 07:50:53,157 - Step 22: Created step_acts
2025-02-11 07:50:53,157 - Step 22: Added to generation_acts
2025-02-11 07:50:53,157 - Step 22: Updated recent_tokens
2025-02-11 07:50:53,158 - Step 22: Found phrase end token
2025-02-11 07:50:53,158 - Step 22: Updated recent_phrases
2025-02-11 07:50:53,158 - Step 22: Calculated similarity: 0.4444444444444444
2025-02-11 07:50:53,158 - Step 22: Decoded current text
2025-02-11 07:50:53,158 - Step 22: Reset consecutive_fillers
2025-02-11 07:50:53,159 - Step 22: Calculated unique_ratio: 0.75
2025-02-11 07:50:53,159 - 
Starting step 23
2025-02-11 07:50:53,159 - Current_ids device: cuda:0
2025-02-11 07:50:53,159 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,197 - Model output complete
2025-02-11 07:50:53,197 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:50:53,197 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,197 - Next token logits device: cuda:0
2025-02-11 07:50:53,197 - Entered do_sample
2025-02-11 07:50:53,197 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,199 - Probs max: 0.623046875
2025-02-11 07:50:53,201 - Pre-cat
2025-02-11 07:50:53,201 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13]],
       device='cuda:0')
2025-02-11 07:50:53,205 - Next token: tensor([[2055]], device='cuda:0')
2025-02-11 07:50:53,206 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:50:53,206 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,206 - Step 23: Generated next token
2025-02-11 07:50:53,206 - Step 23: Updated current_ids
2025-02-11 07:50:53,206 - Step 23: Decoded token text:  So
2025-02-11 07:50:53,206 - Step 23: Updated current_phrase
2025-02-11 07:50:53,207 - Step 23: Created step_acts
2025-02-11 07:50:53,207 - Step 23: Added to generation_acts
2025-02-11 07:50:53,207 - Step 23: Updated recent_tokens
2025-02-11 07:50:53,208 - Step 23: Decoded current text
2025-02-11 07:50:53,209 - Step 23: Reset consecutive_fillers
2025-02-11 07:50:53,209 - Step 23: Calculated unique_ratio: 0.8125
2025-02-11 07:50:53,209 - 
Starting step 24
2025-02-11 07:50:53,209 - Current_ids device: cuda:0
2025-02-11 07:50:53,209 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,246 - Model output complete
2025-02-11 07:50:53,246 - Logits shape: torch.Size([1, 55, 151936])
2025-02-11 07:50:53,246 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,246 - Next token logits device: cuda:0
2025-02-11 07:50:53,246 - Entered do_sample
2025-02-11 07:50:53,246 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,248 - Probs max: 0.86474609375
2025-02-11 07:50:53,249 - Pre-cat
2025-02-11 07:50:53,249 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055]], device='cuda:0')
2025-02-11 07:50:53,253 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:53,253 - Current_ids shape: torch.Size([1, 55])
2025-02-11 07:50:53,253 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,253 - Step 24: Generated next token
2025-02-11 07:50:53,253 - Step 24: Updated current_ids
2025-02-11 07:50:53,253 - Step 24: Decoded token text: ,
2025-02-11 07:50:53,253 - Step 24: Updated current_phrase
2025-02-11 07:50:53,254 - Step 24: Created step_acts
2025-02-11 07:50:53,254 - Step 24: Added to generation_acts
2025-02-11 07:50:53,254 - Step 24: Updated recent_tokens
2025-02-11 07:50:53,255 - Step 24: Found phrase end token
2025-02-11 07:50:53,255 - Step 24: Updated recent_phrases
2025-02-11 07:50:53,255 - Step 24: Calculated similarity: 0.0
2025-02-11 07:50:53,255 - Step 24: Decoded current text
2025-02-11 07:50:53,256 - Step 24: Reset consecutive_fillers
2025-02-11 07:50:53,256 - Step 24: Calculated unique_ratio: 0.8125
2025-02-11 07:50:53,256 - 
Starting step 25
2025-02-11 07:50:53,256 - Current_ids device: cuda:0
2025-02-11 07:50:53,256 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,288 - Model output complete
2025-02-11 07:50:53,288 - Logits shape: torch.Size([1, 56, 151936])
2025-02-11 07:50:53,288 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,288 - Next token logits device: cuda:0
2025-02-11 07:50:53,288 - Entered do_sample
2025-02-11 07:50:53,288 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,292 - Probs max: 0.9521484375
2025-02-11 07:50:53,292 - Pre-cat
2025-02-11 07:50:53,293 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11]], device='cuda:0')
2025-02-11 07:50:53,295 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:53,295 - Current_ids shape: torch.Size([1, 56])
2025-02-11 07:50:53,295 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,295 - Step 25: Generated next token
2025-02-11 07:50:53,295 - Step 25: Updated current_ids
2025-02-11 07:50:53,295 - Step 25: Decoded token text:  the
2025-02-11 07:50:53,295 - Step 25: Updated current_phrase
2025-02-11 07:50:53,296 - Step 25: Created step_acts
2025-02-11 07:50:53,296 - Step 25: Added to generation_acts
2025-02-11 07:50:53,297 - Step 25: Updated generated_texts
2025-02-11 07:50:53,297 - Step 25: Updated recent_tokens
2025-02-11 07:50:53,298 - Step 25: Decoded current text
2025-02-11 07:50:53,298 - Step 25: Reset consecutive_fillers
2025-02-11 07:50:53,298 - Step 25: Calculated unique_ratio: 0.8125
2025-02-11 07:50:53,298 - 
Starting step 26
2025-02-11 07:50:53,298 - Current_ids device: cuda:0
2025-02-11 07:50:53,298 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,321 - Model output complete
2025-02-11 07:50:53,321 - Logits shape: torch.Size([1, 57, 151936])
2025-02-11 07:50:53,321 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,321 - Next token logits device: cuda:0
2025-02-11 07:50:53,321 - Entered do_sample
2025-02-11 07:50:53,321 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,325 - Probs max: 0.80908203125
2025-02-11 07:50:53,326 - Pre-cat
2025-02-11 07:50:53,326 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11,    279]], device='cuda:0')
2025-02-11 07:50:53,328 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:53,328 - Current_ids shape: torch.Size([1, 57])
2025-02-11 07:50:53,329 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,329 - Step 26: Generated next token
2025-02-11 07:50:53,329 - Step 26: Updated current_ids
2025-02-11 07:50:53,329 - Step 26: Decoded token text:  ball
2025-02-11 07:50:53,329 - Step 26: Updated current_phrase
2025-02-11 07:50:53,329 - Step 26: Created step_acts
2025-02-11 07:50:53,329 - Step 26: Added to generation_acts
2025-02-11 07:50:53,329 - Step 26: Updated recent_tokens
2025-02-11 07:50:53,331 - Step 26: Decoded current text
2025-02-11 07:50:53,331 - Step 26: Reset consecutive_fillers
2025-02-11 07:50:53,331 - Step 26: Calculated unique_ratio: 0.8125
2025-02-11 07:50:53,331 - 
Starting step 27
2025-02-11 07:50:53,331 - Current_ids device: cuda:0
2025-02-11 07:50:53,331 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,401 - Model output complete
2025-02-11 07:50:53,401 - Logits shape: torch.Size([1, 58, 151936])
2025-02-11 07:50:53,401 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,402 - Next token logits device: cuda:0
2025-02-11 07:50:53,402 - Entered do_sample
2025-02-11 07:50:53,402 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,404 - Probs max: 0.56494140625
2025-02-11 07:50:53,407 - Pre-cat
2025-02-11 07:50:53,407 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11,    279,   4935]], device='cuda:0')
2025-02-11 07:50:53,411 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:53,412 - Current_ids shape: torch.Size([1, 58])
2025-02-11 07:50:53,412 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,412 - Step 27: Generated next token
2025-02-11 07:50:53,412 - Step 27: Updated current_ids
2025-02-11 07:50:53,412 - Step 27: Decoded token text:  is
2025-02-11 07:50:53,412 - Step 27: Updated current_phrase
2025-02-11 07:50:53,413 - Step 27: Created step_acts
2025-02-11 07:50:53,413 - Step 27: Added to generation_acts
2025-02-11 07:50:53,413 - Step 27: Updated recent_tokens
2025-02-11 07:50:53,414 - Step 27: Decoded current text
2025-02-11 07:50:53,414 - Step 27: Reset consecutive_fillers
2025-02-11 07:50:53,415 - Step 27: Calculated unique_ratio: 0.8125
2025-02-11 07:50:53,415 - 
Starting step 28
2025-02-11 07:50:53,415 - Current_ids device: cuda:0
2025-02-11 07:50:53,415 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,452 - Model output complete
2025-02-11 07:50:53,452 - Logits shape: torch.Size([1, 59, 151936])
2025-02-11 07:50:53,452 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,452 - Next token logits device: cuda:0
2025-02-11 07:50:53,452 - Entered do_sample
2025-02-11 07:50:53,452 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,457 - Probs max: 0.95556640625
2025-02-11 07:50:53,457 - Pre-cat
2025-02-11 07:50:53,458 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11,    279,   4935,    374]], device='cuda:0')
2025-02-11 07:50:53,460 - Next token: tensor([[7218]], device='cuda:0')
2025-02-11 07:50:53,460 - Current_ids shape: torch.Size([1, 59])
2025-02-11 07:50:53,460 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,460 - Step 28: Generated next token
2025-02-11 07:50:53,460 - Step 28: Updated current_ids
2025-02-11 07:50:53,461 - Step 28: Decoded token text:  moving
2025-02-11 07:50:53,461 - Step 28: Updated current_phrase
2025-02-11 07:50:53,461 - Step 28: Created step_acts
2025-02-11 07:50:53,461 - Step 28: Added to generation_acts
2025-02-11 07:50:53,461 - Step 28: Updated recent_tokens
2025-02-11 07:50:53,463 - Step 28: Decoded current text
2025-02-11 07:50:53,463 - Step 28: Reset consecutive_fillers
2025-02-11 07:50:53,463 - Step 28: Calculated unique_ratio: 0.8125
2025-02-11 07:50:53,463 - 
Starting step 29
2025-02-11 07:50:53,463 - Current_ids device: cuda:0
2025-02-11 07:50:53,463 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,500 - Model output complete
2025-02-11 07:50:53,500 - Logits shape: torch.Size([1, 60, 151936])
2025-02-11 07:50:53,500 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,501 - Next token logits device: cuda:0
2025-02-11 07:50:53,501 - Entered do_sample
2025-02-11 07:50:53,501 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,503 - Probs max: 0.36767578125
2025-02-11 07:50:53,505 - Pre-cat
2025-02-11 07:50:53,505 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11,    279,   4935,    374,   7218]], device='cuda:0')
2025-02-11 07:50:53,508 - Next token: tensor([[6974]], device='cuda:0')
2025-02-11 07:50:53,509 - Current_ids shape: torch.Size([1, 60])
2025-02-11 07:50:53,509 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,510 - Step 29: Generated next token
2025-02-11 07:50:53,510 - Step 29: Updated current_ids
2025-02-11 07:50:53,510 - Step 29: Decoded token text:  towards
2025-02-11 07:50:53,510 - Step 29: Updated current_phrase
2025-02-11 07:50:53,511 - Step 29: Created step_acts
2025-02-11 07:50:53,511 - Step 29: Added to generation_acts
2025-02-11 07:50:53,511 - Step 29: Updated recent_tokens
2025-02-11 07:50:53,513 - Step 29: Decoded current text
2025-02-11 07:50:53,513 - Step 29: Reset consecutive_fillers
2025-02-11 07:50:53,513 - Step 29: Calculated unique_ratio: 0.875
2025-02-11 07:50:53,513 - 
Starting step 30
2025-02-11 07:50:53,514 - Current_ids device: cuda:0
2025-02-11 07:50:53,514 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,548 - Model output complete
2025-02-11 07:50:53,549 - Logits shape: torch.Size([1, 61, 151936])
2025-02-11 07:50:53,549 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,549 - Next token logits device: cuda:0
2025-02-11 07:50:53,549 - Entered do_sample
2025-02-11 07:50:53,550 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,551 - Probs max: 0.994140625
2025-02-11 07:50:53,552 - Pre-cat
2025-02-11 07:50:53,552 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11,    279,   4935,    374,   7218,   6974]],
       device='cuda:0')
2025-02-11 07:50:53,555 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:53,556 - Current_ids shape: torch.Size([1, 61])
2025-02-11 07:50:53,556 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,556 - Step 30: Generated next token
2025-02-11 07:50:53,556 - Step 30: Updated current_ids
2025-02-11 07:50:53,556 - Step 30: Decoded token text:  the
2025-02-11 07:50:53,556 - Step 30: Updated current_phrase
2025-02-11 07:50:53,556 - Step 30: Created step_acts
2025-02-11 07:50:53,556 - Step 30: Added to generation_acts
2025-02-11 07:50:53,558 - Step 30: Updated generated_texts
2025-02-11 07:50:53,558 - Step 30: Updated recent_tokens
2025-02-11 07:50:53,558 - Step 30: Decoded current text
2025-02-11 07:50:53,558 - Step 30: Reset consecutive_fillers
2025-02-11 07:50:53,558 - Step 30: Calculated unique_ratio: 0.875
2025-02-11 07:50:53,559 - 
Starting step 31
2025-02-11 07:50:53,559 - Current_ids device: cuda:0
2025-02-11 07:50:53,559 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,581 - Model output complete
2025-02-11 07:50:53,581 - Logits shape: torch.Size([1, 62, 151936])
2025-02-11 07:50:53,582 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,582 - Next token logits device: cuda:0
2025-02-11 07:50:53,582 - Entered do_sample
2025-02-11 07:50:53,582 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,586 - Probs max: 0.9990234375
2025-02-11 07:50:53,586 - Pre-cat
2025-02-11 07:50:53,586 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11,    279,   4935,    374,   7218,   6974,    279]],
       device='cuda:0')
2025-02-11 07:50:53,588 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:53,589 - Current_ids shape: torch.Size([1, 62])
2025-02-11 07:50:53,589 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,589 - Step 31: Generated next token
2025-02-11 07:50:53,589 - Step 31: Updated current_ids
2025-02-11 07:50:53,589 - Step 31: Decoded token text:  wall
2025-02-11 07:50:53,589 - Step 31: Updated current_phrase
2025-02-11 07:50:53,589 - Step 31: Created step_acts
2025-02-11 07:50:53,590 - Step 31: Added to generation_acts
2025-02-11 07:50:53,590 - Step 31: Updated recent_tokens
2025-02-11 07:50:53,591 - Step 31: Decoded current text
2025-02-11 07:50:53,591 - Step 31: Reset consecutive_fillers
2025-02-11 07:50:53,591 - Step 31: Calculated unique_ratio: 0.8125
2025-02-11 07:50:53,591 - 
Starting step 32
2025-02-11 07:50:53,591 - Current_ids device: cuda:0
2025-02-11 07:50:53,592 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,613 - Model output complete
2025-02-11 07:50:53,614 - Logits shape: torch.Size([1, 63, 151936])
2025-02-11 07:50:53,614 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,614 - Next token logits device: cuda:0
2025-02-11 07:50:53,614 - Entered do_sample
2025-02-11 07:50:53,614 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,619 - Probs max: 0.25439453125
2025-02-11 07:50:53,619 - Pre-cat
2025-02-11 07:50:53,619 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11,    279,   4935,    374,   7218,   6974,    279,   7002]],
       device='cuda:0')
2025-02-11 07:50:53,622 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:50:53,622 - Current_ids shape: torch.Size([1, 63])
2025-02-11 07:50:53,622 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,622 - Step 32: Generated next token
2025-02-11 07:50:53,622 - Step 32: Updated current_ids
2025-02-11 07:50:53,622 - Step 32: Decoded token text: 's
2025-02-11 07:50:53,622 - Step 32: Updated current_phrase
2025-02-11 07:50:53,623 - Step 32: Created step_acts
2025-02-11 07:50:53,623 - Step 32: Added to generation_acts
2025-02-11 07:50:53,623 - Step 32: Updated recent_tokens
2025-02-11 07:50:53,624 - Step 32: Decoded current text
2025-02-11 07:50:53,624 - Step 32: Reset consecutive_fillers
2025-02-11 07:50:53,625 - Step 32: Calculated unique_ratio: 0.8125
2025-02-11 07:50:53,625 - 
Starting step 33
2025-02-11 07:50:53,625 - Current_ids device: cuda:0
2025-02-11 07:50:53,625 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,648 - Model output complete
2025-02-11 07:50:53,649 - Logits shape: torch.Size([1, 64, 151936])
2025-02-11 07:50:53,649 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,649 - Next token logits device: cuda:0
2025-02-11 07:50:53,649 - Entered do_sample
2025-02-11 07:50:53,649 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,651 - Probs max: 0.50341796875
2025-02-11 07:50:53,652 - Pre-cat
2025-02-11 07:50:53,652 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11,    279,   4935,    374,   7218,   6974,    279,   7002,
            594]], device='cuda:0')
2025-02-11 07:50:53,654 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:50:53,655 - Current_ids shape: torch.Size([1, 64])
2025-02-11 07:50:53,655 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,655 - Step 33: Generated next token
2025-02-11 07:50:53,655 - Step 33: Updated current_ids
2025-02-11 07:50:53,655 - Step 33: Decoded token text:  speed
2025-02-11 07:50:53,655 - Step 33: Updated current_phrase
2025-02-11 07:50:53,655 - Step 33: Created step_acts
2025-02-11 07:50:53,655 - Step 33: Added to generation_acts
2025-02-11 07:50:53,655 - Step 33: Updated recent_tokens
2025-02-11 07:50:53,657 - Step 33: Decoded current text
2025-02-11 07:50:53,657 - Step 33: Reset consecutive_fillers
2025-02-11 07:50:53,657 - Step 33: Calculated unique_ratio: 0.875
2025-02-11 07:50:53,657 - 
Starting step 34
2025-02-11 07:50:53,657 - Current_ids device: cuda:0
2025-02-11 07:50:53,657 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,681 - Model output complete
2025-02-11 07:50:53,682 - Logits shape: torch.Size([1, 65, 151936])
2025-02-11 07:50:53,682 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,682 - Next token logits device: cuda:0
2025-02-11 07:50:53,682 - Entered do_sample
2025-02-11 07:50:53,682 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,685 - Probs max: 0.7197265625
2025-02-11 07:50:53,686 - Pre-cat
2025-02-11 07:50:53,686 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11,    279,   4935,    374,   7218,   6974,    279,   7002,
            594,   4628]], device='cuda:0')
2025-02-11 07:50:53,689 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:53,689 - Current_ids shape: torch.Size([1, 65])
2025-02-11 07:50:53,689 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,689 - Step 34: Generated next token
2025-02-11 07:50:53,690 - Step 34: Updated current_ids
2025-02-11 07:50:53,690 - Step 34: Decoded token text: ,
2025-02-11 07:50:53,690 - Step 34: Updated current_phrase
2025-02-11 07:50:53,690 - Step 34: Created step_acts
2025-02-11 07:50:53,690 - Step 34: Added to generation_acts
2025-02-11 07:50:53,691 - Step 34: Updated recent_tokens
2025-02-11 07:50:53,692 - Step 34: Found phrase end token
2025-02-11 07:50:53,692 - Step 34: Updated recent_phrases
2025-02-11 07:50:53,692 - Step 34: Calculated similarity: 0.0
2025-02-11 07:50:53,692 - Step 34: Decoded current text
2025-02-11 07:50:53,692 - Step 34: Reset consecutive_fillers
2025-02-11 07:50:53,693 - Step 34: Calculated unique_ratio: 0.875
2025-02-11 07:50:53,693 - 
Starting step 35
2025-02-11 07:50:53,693 - Current_ids device: cuda:0
2025-02-11 07:50:53,693 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,716 - Model output complete
2025-02-11 07:50:53,716 - Logits shape: torch.Size([1, 66, 151936])
2025-02-11 07:50:53,717 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,717 - Next token logits device: cuda:0
2025-02-11 07:50:53,717 - Entered do_sample
2025-02-11 07:50:53,717 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,721 - Probs max: 0.27587890625
2025-02-11 07:50:53,722 - Pre-cat
2025-02-11 07:50:53,722 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11,    279,   4935,    374,   7218,   6974,    279,   7002,
            594,   4628,     11]], device='cuda:0')
2025-02-11 07:50:53,724 - Next token: tensor([[892]], device='cuda:0')
2025-02-11 07:50:53,725 - Current_ids shape: torch.Size([1, 66])
2025-02-11 07:50:53,725 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,725 - Step 35: Generated next token
2025-02-11 07:50:53,725 - Step 35: Updated current_ids
2025-02-11 07:50:53,725 - Step 35: Decoded token text:  which
2025-02-11 07:50:53,725 - Step 35: Updated current_phrase
2025-02-11 07:50:53,725 - Step 35: Created step_acts
2025-02-11 07:50:53,726 - Step 35: Added to generation_acts
2025-02-11 07:50:53,727 - Step 35: Updated generated_texts
2025-02-11 07:50:53,727 - Step 35: Updated recent_tokens
2025-02-11 07:50:53,728 - Step 35: Decoded current text
2025-02-11 07:50:53,728 - Step 35: Reset consecutive_fillers
2025-02-11 07:50:53,729 - Step 35: Calculated unique_ratio: 0.875
2025-02-11 07:50:53,729 - 
Starting step 36
2025-02-11 07:50:53,729 - Current_ids device: cuda:0
2025-02-11 07:50:53,729 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,752 - Model output complete
2025-02-11 07:50:53,752 - Logits shape: torch.Size([1, 67, 151936])
2025-02-11 07:50:53,752 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,752 - Next token logits device: cuda:0
2025-02-11 07:50:53,752 - Entered do_sample
2025-02-11 07:50:53,752 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,756 - Probs max: 0.59228515625
2025-02-11 07:50:53,757 - Pre-cat
2025-02-11 07:50:53,757 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11,    279,   4935,    374,   7218,   6974,    279,   7002,
            594,   4628,     11,    892]], device='cuda:0')
2025-02-11 07:50:53,759 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:53,759 - Current_ids shape: torch.Size([1, 67])
2025-02-11 07:50:53,759 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,760 - Step 36: Generated next token
2025-02-11 07:50:53,760 - Step 36: Updated current_ids
2025-02-11 07:50:53,760 - Step 36: Decoded token text:  is
2025-02-11 07:50:53,760 - Step 36: Updated current_phrase
2025-02-11 07:50:53,761 - Step 36: Created step_acts
2025-02-11 07:50:53,761 - Step 36: Added to generation_acts
2025-02-11 07:50:53,761 - Step 36: Updated recent_tokens
2025-02-11 07:50:53,762 - Step 36: Decoded current text
2025-02-11 07:50:53,762 - Step 36: Incremented consecutive_fillers to 1
2025-02-11 07:50:53,763 - Step 36: Calculated unique_ratio: 0.8125
2025-02-11 07:50:53,763 - 
Starting step 37
2025-02-11 07:50:53,763 - Current_ids device: cuda:0
2025-02-11 07:50:53,763 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,786 - Model output complete
2025-02-11 07:50:53,787 - Logits shape: torch.Size([1, 68, 151936])
2025-02-11 07:50:53,787 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,787 - Next token logits device: cuda:0
2025-02-11 07:50:53,787 - Entered do_sample
2025-02-11 07:50:53,787 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,791 - Probs max: 0.40966796875
2025-02-11 07:50:53,792 - Pre-cat
2025-02-11 07:50:53,792 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11,    279,   4935,    374,   7218,   6974,    279,   7002,
            594,   4628,     11,    892,    374]], device='cuda:0')
2025-02-11 07:50:53,796 - Next token: tensor([[5080]], device='cuda:0')
2025-02-11 07:50:53,796 - Current_ids shape: torch.Size([1, 68])
2025-02-11 07:50:53,796 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,796 - Step 37: Generated next token
2025-02-11 07:50:53,796 - Step 37: Updated current_ids
2025-02-11 07:50:53,797 - Step 37: Decoded token text:  higher
2025-02-11 07:50:53,797 - Step 37: Updated current_phrase
2025-02-11 07:50:53,797 - Step 37: Created step_acts
2025-02-11 07:50:53,797 - Step 37: Added to generation_acts
2025-02-11 07:50:53,797 - Step 37: Updated recent_tokens
2025-02-11 07:50:53,799 - Step 37: Decoded current text
2025-02-11 07:50:53,799 - Step 37: Incremented consecutive_fillers to 2
2025-02-11 07:50:53,799 - Step 37: Calculated unique_ratio: 0.8125
2025-02-11 07:50:53,799 - 
Starting step 38
2025-02-11 07:50:53,799 - Current_ids device: cuda:0
2025-02-11 07:50:53,799 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,822 - Model output complete
2025-02-11 07:50:53,822 - Logits shape: torch.Size([1, 69, 151936])
2025-02-11 07:50:53,823 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,823 - Next token logits device: cuda:0
2025-02-11 07:50:53,823 - Entered do_sample
2025-02-11 07:50:53,823 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,827 - Probs max: 0.912109375
2025-02-11 07:50:53,828 - Pre-cat
2025-02-11 07:50:53,829 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           7218,    504,    279,   7002,    594,  13057,   1602,   4937,     13,
           2055,     11,    279,   4935,    374,   7218,   6974,    279,   7002,
            594,   4628,     11,    892,    374,   5080]], device='cuda:0')
2025-02-11 07:50:53,831 - Next token: tensor([[1091]], device='cuda:0')
2025-02-11 07:50:53,831 - Current_ids shape: torch.Size([1, 69])
2025-02-11 07:50:53,831 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,832 - Step 38: Generated next token
2025-02-11 07:50:53,832 - Step 38: Updated current_ids
2025-02-11 07:50:53,832 - Step 38: Decoded token text:  than
2025-02-11 07:50:53,832 - Step 38: Updated current_phrase
2025-02-11 07:50:53,832 - Step 38: Created step_acts
2025-02-11 07:50:53,832 - Step 38: Added to generation_acts
2025-02-11 07:50:53,832 - Step 38: Updated recent_tokens
2025-02-11 07:50:53,834 - Step 38: Decoded current text
2025-02-11 07:50:53,834 - Step 38: Incremented consecutive_fillers to 3
2025-02-11 07:50:53,937 - 
Starting step 0
2025-02-11 07:50:53,937 - Current_ids device: cuda:0
2025-02-11 07:50:53,937 - Current_ids dtype: torch.int64
2025-02-11 07:50:53,964 - Model output complete
2025-02-11 07:50:53,964 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:53,964 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,964 - Next token logits device: cuda:0
2025-02-11 07:50:53,964 - Entered do_sample
2025-02-11 07:50:53,964 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:53,968 - Probs max: 0.50341796875
2025-02-11 07:50:53,968 - Pre-cat
2025-02-11 07:50:53,969 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:53,971 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:50:53,972 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:53,972 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:53,972 - Step 0: Generated next token
2025-02-11 07:50:53,972 - Step 0: Updated current_ids
2025-02-11 07:50:53,972 - Step 0: Decoded token text:  It
2025-02-11 07:50:53,973 - Step 0: Updated current_phrase
2025-02-11 07:50:53,973 - Step 0: Created step_acts
2025-02-11 07:50:53,973 - Step 0: Added to generation_acts
2025-02-11 07:50:53,975 - Step 0: Updated generated_texts
2025-02-11 07:50:53,975 - Step 0: Updated recent_tokens
2025-02-11 07:50:53,975 - Step 0: Decoded current text
2025-02-11 07:50:53,975 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:53,975 - 
Starting step 1
2025-02-11 07:50:53,975 - Current_ids device: cuda:0
2025-02-11 07:50:53,976 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,004 - Model output complete
2025-02-11 07:50:54,004 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:54,004 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,004 - Next token logits device: cuda:0
2025-02-11 07:50:54,004 - Entered do_sample
2025-02-11 07:50:54,005 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,007 - Probs max: 0.9384765625
2025-02-11 07:50:54,007 - Pre-cat
2025-02-11 07:50:54,007 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084]], device='cuda:0')
2025-02-11 07:50:54,009 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:54,009 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:54,009 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,009 - Step 1: Generated next token
2025-02-11 07:50:54,009 - Step 1: Updated current_ids
2025-02-11 07:50:54,009 - Step 1: Decoded token text:  will
2025-02-11 07:50:54,009 - Step 1: Updated current_phrase
2025-02-11 07:50:54,010 - Step 1: Created step_acts
2025-02-11 07:50:54,010 - Step 1: Added to generation_acts
2025-02-11 07:50:54,010 - Step 1: Updated recent_tokens
2025-02-11 07:50:54,011 - Step 1: Decoded current text
2025-02-11 07:50:54,011 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:54,011 - 
Starting step 2
2025-02-11 07:50:54,011 - Current_ids device: cuda:0
2025-02-11 07:50:54,011 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,035 - Model output complete
2025-02-11 07:50:54,035 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:54,035 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,035 - Next token logits device: cuda:0
2025-02-11 07:50:54,035 - Entered do_sample
2025-02-11 07:50:54,035 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,038 - Probs max: 0.2442626953125
2025-02-11 07:50:54,038 - Pre-cat
2025-02-11 07:50:54,038 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686]], device='cuda:0')
2025-02-11 07:50:54,040 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:50:54,040 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:54,040 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,040 - Step 2: Generated next token
2025-02-11 07:50:54,040 - Step 2: Updated current_ids
2025-02-11 07:50:54,040 - Step 2: Decoded token text:  hit
2025-02-11 07:50:54,040 - Step 2: Updated current_phrase
2025-02-11 07:50:54,041 - Step 2: Created step_acts
2025-02-11 07:50:54,041 - Step 2: Added to generation_acts
2025-02-11 07:50:54,041 - Step 2: Updated recent_tokens
2025-02-11 07:50:54,042 - Step 2: Decoded current text
2025-02-11 07:50:54,042 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:54,042 - 
Starting step 3
2025-02-11 07:50:54,042 - Current_ids device: cuda:0
2025-02-11 07:50:54,043 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,111 - Model output complete
2025-02-11 07:50:54,111 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:54,111 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,111 - Next token logits device: cuda:0
2025-02-11 07:50:54,111 - Entered do_sample
2025-02-11 07:50:54,111 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,114 - Probs max: 0.99169921875
2025-02-11 07:50:54,114 - Pre-cat
2025-02-11 07:50:54,114 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201]],
       device='cuda:0')
2025-02-11 07:50:54,116 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:54,116 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:54,116 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,116 - Step 3: Generated next token
2025-02-11 07:50:54,116 - Step 3: Updated current_ids
2025-02-11 07:50:54,116 - Step 3: Decoded token text:  the
2025-02-11 07:50:54,116 - Step 3: Updated current_phrase
2025-02-11 07:50:54,117 - Step 3: Created step_acts
2025-02-11 07:50:54,117 - Step 3: Added to generation_acts
2025-02-11 07:50:54,117 - Step 3: Updated recent_tokens
2025-02-11 07:50:54,118 - Step 3: Decoded current text
2025-02-11 07:50:54,118 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:54,118 - 
Starting step 4
2025-02-11 07:50:54,118 - Current_ids device: cuda:0
2025-02-11 07:50:54,118 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,142 - Model output complete
2025-02-11 07:50:54,142 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:54,142 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,142 - Next token logits device: cuda:0
2025-02-11 07:50:54,142 - Entered do_sample
2025-02-11 07:50:54,142 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,144 - Probs max: 0.998046875
2025-02-11 07:50:54,145 - Pre-cat
2025-02-11 07:50:54,145 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279]],
       device='cuda:0')
2025-02-11 07:50:54,146 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:54,147 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:54,147 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,147 - Step 4: Generated next token
2025-02-11 07:50:54,147 - Step 4: Updated current_ids
2025-02-11 07:50:54,147 - Step 4: Decoded token text:  wall
2025-02-11 07:50:54,147 - Step 4: Updated current_phrase
2025-02-11 07:50:54,147 - Step 4: Created step_acts
2025-02-11 07:50:54,147 - Step 4: Added to generation_acts
2025-02-11 07:50:54,147 - Step 4: Updated recent_tokens
2025-02-11 07:50:54,149 - Step 4: Decoded current text
2025-02-11 07:50:54,149 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:54,149 - 
Starting step 5
2025-02-11 07:50:54,149 - Current_ids device: cuda:0
2025-02-11 07:50:54,149 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,171 - Model output complete
2025-02-11 07:50:54,172 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:54,172 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,172 - Next token logits device: cuda:0
2025-02-11 07:50:54,172 - Entered do_sample
2025-02-11 07:50:54,172 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,174 - Probs max: 0.41455078125
2025-02-11 07:50:54,175 - Pre-cat
2025-02-11 07:50:54,175 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002]],
       device='cuda:0')
2025-02-11 07:50:54,177 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:54,178 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:54,178 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,178 - Step 5: Generated next token
2025-02-11 07:50:54,178 - Step 5: Updated current_ids
2025-02-11 07:50:54,178 - Step 5: Decoded token text: ,
2025-02-11 07:50:54,178 - Step 5: Updated current_phrase
2025-02-11 07:50:54,179 - Step 5: Created step_acts
2025-02-11 07:50:54,179 - Step 5: Added to generation_acts
2025-02-11 07:50:54,180 - Step 5: Updated generated_texts
2025-02-11 07:50:54,180 - Step 5: Updated recent_tokens
2025-02-11 07:50:54,180 - Step 5: Found phrase end token
2025-02-11 07:50:54,180 - Step 5: Updated recent_phrases
2025-02-11 07:50:54,181 - Step 5: Decoded current text
2025-02-11 07:50:54,181 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:54,181 - 
Starting step 6
2025-02-11 07:50:54,181 - Current_ids device: cuda:0
2025-02-11 07:50:54,181 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,205 - Model output complete
2025-02-11 07:50:54,205 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:54,205 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,205 - Next token logits device: cuda:0
2025-02-11 07:50:54,205 - Entered do_sample
2025-02-11 07:50:54,206 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,208 - Probs max: 0.377197265625
2025-02-11 07:50:54,208 - Pre-cat
2025-02-11 07:50:54,209 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11]], device='cuda:0')
2025-02-11 07:50:54,210 - Next token: tensor([[714]], device='cuda:0')
2025-02-11 07:50:54,210 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:54,210 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,210 - Step 6: Generated next token
2025-02-11 07:50:54,211 - Step 6: Updated current_ids
2025-02-11 07:50:54,211 - Step 6: Decoded token text:  but
2025-02-11 07:50:54,211 - Step 6: Updated current_phrase
2025-02-11 07:50:54,211 - Step 6: Created step_acts
2025-02-11 07:50:54,211 - Step 6: Added to generation_acts
2025-02-11 07:50:54,211 - Step 6: Updated recent_tokens
2025-02-11 07:50:54,212 - Step 6: Decoded current text
2025-02-11 07:50:54,213 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:54,213 - 
Starting step 7
2025-02-11 07:50:54,213 - Current_ids device: cuda:0
2025-02-11 07:50:54,213 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,236 - Model output complete
2025-02-11 07:50:54,236 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:54,236 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,236 - Next token logits device: cuda:0
2025-02-11 07:50:54,236 - Entered do_sample
2025-02-11 07:50:54,237 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,241 - Probs max: 0.377685546875
2025-02-11 07:50:54,241 - Pre-cat
2025-02-11 07:50:54,241 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714]], device='cuda:0')
2025-02-11 07:50:54,243 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:54,243 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:54,243 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,243 - Step 7: Generated next token
2025-02-11 07:50:54,243 - Step 7: Updated current_ids
2025-02-11 07:50:54,243 - Step 7: Decoded token text:  the
2025-02-11 07:50:54,243 - Step 7: Updated current_phrase
2025-02-11 07:50:54,244 - Step 7: Created step_acts
2025-02-11 07:50:54,244 - Step 7: Added to generation_acts
2025-02-11 07:50:54,244 - Step 7: Updated recent_tokens
2025-02-11 07:50:54,245 - Step 7: Decoded current text
2025-02-11 07:50:54,245 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:54,246 - 
Starting step 8
2025-02-11 07:50:54,246 - Current_ids device: cuda:0
2025-02-11 07:50:54,246 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,268 - Model output complete
2025-02-11 07:50:54,269 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:54,269 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,269 - Next token logits device: cuda:0
2025-02-11 07:50:54,269 - Entered do_sample
2025-02-11 07:50:54,269 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,272 - Probs max: 0.7275390625
2025-02-11 07:50:54,272 - Pre-cat
2025-02-11 07:50:54,272 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279]], device='cuda:0')
2025-02-11 07:50:54,274 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:54,274 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:54,274 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,274 - Step 8: Generated next token
2025-02-11 07:50:54,274 - Step 8: Updated current_ids
2025-02-11 07:50:54,274 - Step 8: Decoded token text:  wall
2025-02-11 07:50:54,274 - Step 8: Updated current_phrase
2025-02-11 07:50:54,275 - Step 8: Created step_acts
2025-02-11 07:50:54,275 - Step 8: Added to generation_acts
2025-02-11 07:50:54,275 - Step 8: Updated recent_tokens
2025-02-11 07:50:54,276 - Step 8: Decoded current text
2025-02-11 07:50:54,277 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:54,277 - 
Starting step 9
2025-02-11 07:50:54,277 - Current_ids device: cuda:0
2025-02-11 07:50:54,277 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,322 - Model output complete
2025-02-11 07:50:54,322 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:54,322 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,322 - Next token logits device: cuda:0
2025-02-11 07:50:54,322 - Entered do_sample
2025-02-11 07:50:54,322 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,325 - Probs max: 0.9453125
2025-02-11 07:50:54,325 - Pre-cat
2025-02-11 07:50:54,325 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002]], device='cuda:0')
2025-02-11 07:50:54,327 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:54,327 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:54,327 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,328 - Step 9: Generated next token
2025-02-11 07:50:54,328 - Step 9: Updated current_ids
2025-02-11 07:50:54,328 - Step 9: Decoded token text:  will
2025-02-11 07:50:54,328 - Step 9: Updated current_phrase
2025-02-11 07:50:54,328 - Step 9: Created step_acts
2025-02-11 07:50:54,328 - Step 9: Added to generation_acts
2025-02-11 07:50:54,328 - Step 9: Updated recent_tokens
2025-02-11 07:50:54,330 - Step 9: Decoded current text
2025-02-11 07:50:54,330 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:54,330 - 
Starting step 10
2025-02-11 07:50:54,330 - Current_ids device: cuda:0
2025-02-11 07:50:54,330 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,352 - Model output complete
2025-02-11 07:50:54,352 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:54,352 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,352 - Next token logits device: cuda:0
2025-02-11 07:50:54,352 - Entered do_sample
2025-02-11 07:50:54,353 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,355 - Probs max: 0.59228515625
2025-02-11 07:50:54,356 - Pre-cat
2025-02-11 07:50:54,356 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686]], device='cuda:0')
2025-02-11 07:50:54,358 - Next token: tensor([[537]], device='cuda:0')
2025-02-11 07:50:54,358 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:54,358 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,358 - Step 10: Generated next token
2025-02-11 07:50:54,358 - Step 10: Updated current_ids
2025-02-11 07:50:54,358 - Step 10: Decoded token text:  not
2025-02-11 07:50:54,358 - Step 10: Updated current_phrase
2025-02-11 07:50:54,359 - Step 10: Created step_acts
2025-02-11 07:50:54,359 - Step 10: Added to generation_acts
2025-02-11 07:50:54,360 - Step 10: Updated generated_texts
2025-02-11 07:50:54,360 - Step 10: Updated recent_tokens
2025-02-11 07:50:54,360 - Step 10: Decoded current text
2025-02-11 07:50:54,360 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:54,360 - 
Starting step 11
2025-02-11 07:50:54,361 - Current_ids device: cuda:0
2025-02-11 07:50:54,361 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,383 - Model output complete
2025-02-11 07:50:54,383 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:54,383 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,383 - Next token logits device: cuda:0
2025-02-11 07:50:54,383 - Entered do_sample
2025-02-11 07:50:54,383 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,386 - Probs max: 0.70068359375
2025-02-11 07:50:54,387 - Pre-cat
2025-02-11 07:50:54,387 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537]], device='cuda:0')
2025-02-11 07:50:54,389 - Next token: tensor([[387]], device='cuda:0')
2025-02-11 07:50:54,389 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:54,389 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,390 - Step 11: Generated next token
2025-02-11 07:50:54,390 - Step 11: Updated current_ids
2025-02-11 07:50:54,390 - Step 11: Decoded token text:  be
2025-02-11 07:50:54,390 - Step 11: Updated current_phrase
2025-02-11 07:50:54,390 - Step 11: Created step_acts
2025-02-11 07:50:54,390 - Step 11: Added to generation_acts
2025-02-11 07:50:54,390 - Step 11: Updated recent_tokens
2025-02-11 07:50:54,392 - Step 11: Decoded current text
2025-02-11 07:50:54,392 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:54,392 - 
Starting step 12
2025-02-11 07:50:54,392 - Current_ids device: cuda:0
2025-02-11 07:50:54,392 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,415 - Model output complete
2025-02-11 07:50:54,415 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:54,415 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,415 - Next token logits device: cuda:0
2025-02-11 07:50:54,415 - Entered do_sample
2025-02-11 07:50:54,415 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,418 - Probs max: 0.42724609375
2025-02-11 07:50:54,419 - Pre-cat
2025-02-11 07:50:54,419 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537,    387]],
       device='cuda:0')
2025-02-11 07:50:54,420 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:50:54,420 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:54,420 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,421 - Step 12: Generated next token
2025-02-11 07:50:54,421 - Step 12: Updated current_ids
2025-02-11 07:50:54,421 - Step 12: Decoded token text:  hit
2025-02-11 07:50:54,421 - Step 12: Updated current_phrase
2025-02-11 07:50:54,421 - Step 12: Created step_acts
2025-02-11 07:50:54,421 - Step 12: Added to generation_acts
2025-02-11 07:50:54,421 - Step 12: Updated recent_tokens
2025-02-11 07:50:54,423 - Step 12: Decoded current text
2025-02-11 07:50:54,423 - Step 12: Reset consecutive_fillers
2025-02-11 07:50:54,423 - 
Starting step 13
2025-02-11 07:50:54,423 - Current_ids device: cuda:0
2025-02-11 07:50:54,423 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,446 - Model output complete
2025-02-11 07:50:54,446 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:54,446 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,446 - Next token logits device: cuda:0
2025-02-11 07:50:54,446 - Entered do_sample
2025-02-11 07:50:54,446 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,449 - Probs max: 0.38525390625
2025-02-11 07:50:54,449 - Pre-cat
2025-02-11 07:50:54,449 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537,    387,   4201]],
       device='cuda:0')
2025-02-11 07:50:54,451 - Next token: tensor([[553]], device='cuda:0')
2025-02-11 07:50:54,451 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:54,451 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,451 - Step 13: Generated next token
2025-02-11 07:50:54,451 - Step 13: Updated current_ids
2025-02-11 07:50:54,451 - Step 13: Decoded token text:  by
2025-02-11 07:50:54,452 - Step 13: Updated current_phrase
2025-02-11 07:50:54,452 - Step 13: Created step_acts
2025-02-11 07:50:54,452 - Step 13: Added to generation_acts
2025-02-11 07:50:54,452 - Step 13: Updated recent_tokens
2025-02-11 07:50:54,453 - Step 13: Decoded current text
2025-02-11 07:50:54,453 - Step 13: Reset consecutive_fillers
2025-02-11 07:50:54,453 - 
Starting step 14
2025-02-11 07:50:54,454 - Current_ids device: cuda:0
2025-02-11 07:50:54,454 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,476 - Model output complete
2025-02-11 07:50:54,477 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:50:54,477 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,477 - Next token logits device: cuda:0
2025-02-11 07:50:54,477 - Entered do_sample
2025-02-11 07:50:54,477 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,479 - Probs max: 0.94140625
2025-02-11 07:50:54,480 - Pre-cat
2025-02-11 07:50:54,480 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537,    387,   4201,    553]],
       device='cuda:0')
2025-02-11 07:50:54,482 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:54,482 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:50:54,482 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,482 - Step 14: Generated next token
2025-02-11 07:50:54,482 - Step 14: Updated current_ids
2025-02-11 07:50:54,482 - Step 14: Decoded token text:  the
2025-02-11 07:50:54,482 - Step 14: Updated current_phrase
2025-02-11 07:50:54,483 - Step 14: Created step_acts
2025-02-11 07:50:54,483 - Step 14: Added to generation_acts
2025-02-11 07:50:54,483 - Step 14: Updated recent_tokens
2025-02-11 07:50:54,484 - Step 14: Decoded current text
2025-02-11 07:50:54,484 - Step 14: Reset consecutive_fillers
2025-02-11 07:50:54,484 - 
Starting step 15
2025-02-11 07:50:54,484 - Current_ids device: cuda:0
2025-02-11 07:50:54,485 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,511 - Model output complete
2025-02-11 07:50:54,511 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:50:54,511 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,511 - Next token logits device: cuda:0
2025-02-11 07:50:54,511 - Entered do_sample
2025-02-11 07:50:54,511 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,513 - Probs max: 0.96044921875
2025-02-11 07:50:54,514 - Pre-cat
2025-02-11 07:50:54,514 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537,    387,   4201,    553,
            279]], device='cuda:0')
2025-02-11 07:50:54,516 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:54,516 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:50:54,516 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,516 - Step 15: Generated next token
2025-02-11 07:50:54,516 - Step 15: Updated current_ids
2025-02-11 07:50:54,517 - Step 15: Decoded token text:  ball
2025-02-11 07:50:54,517 - Step 15: Updated current_phrase
2025-02-11 07:50:54,517 - Step 15: Created step_acts
2025-02-11 07:50:54,517 - Step 15: Added to generation_acts
2025-02-11 07:50:54,518 - Step 15: Updated generated_texts
2025-02-11 07:50:54,519 - Step 15: Updated recent_tokens
2025-02-11 07:50:54,519 - Step 15: Decoded current text
2025-02-11 07:50:54,519 - Step 15: Reset consecutive_fillers
2025-02-11 07:50:54,519 - 
Starting step 16
2025-02-11 07:50:54,519 - Current_ids device: cuda:0
2025-02-11 07:50:54,519 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,542 - Model output complete
2025-02-11 07:50:54,542 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:50:54,543 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,543 - Next token logits device: cuda:0
2025-02-11 07:50:54,543 - Entered do_sample
2025-02-11 07:50:54,543 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,545 - Probs max: 0.69677734375
2025-02-11 07:50:54,546 - Pre-cat
2025-02-11 07:50:54,546 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537,    387,   4201,    553,
            279,   4935]], device='cuda:0')
2025-02-11 07:50:54,548 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:54,548 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:50:54,548 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,548 - Step 16: Generated next token
2025-02-11 07:50:54,548 - Step 16: Updated current_ids
2025-02-11 07:50:54,548 - Step 16: Decoded token text: .
2025-02-11 07:50:54,548 - Step 16: Updated current_phrase
2025-02-11 07:50:54,549 - Step 16: Created step_acts
2025-02-11 07:50:54,549 - Step 16: Added to generation_acts
2025-02-11 07:50:54,549 - Step 16: Updated recent_tokens
2025-02-11 07:50:54,550 - Step 16: Found phrase end token
2025-02-11 07:50:54,550 - Step 16: Updated recent_phrases
2025-02-11 07:50:54,550 - Step 16: Calculated similarity: 0.6
2025-02-11 07:50:54,550 - Step 16: Decoded current text
2025-02-11 07:50:54,550 - Step 16: Reset consecutive_fillers
2025-02-11 07:50:54,551 - Step 16: Calculated unique_ratio: 0.6875
2025-02-11 07:50:54,551 - 
Starting step 17
2025-02-11 07:50:54,551 - Current_ids device: cuda:0
2025-02-11 07:50:54,551 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,574 - Model output complete
2025-02-11 07:50:54,574 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:50:54,574 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,575 - Next token logits device: cuda:0
2025-02-11 07:50:54,575 - Entered do_sample
2025-02-11 07:50:54,575 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,577 - Probs max: 0.385986328125
2025-02-11 07:50:54,578 - Pre-cat
2025-02-11 07:50:54,578 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537,    387,   4201,    553,
            279,   4935,     13]], device='cuda:0')
2025-02-11 07:50:54,580 - Next token: tensor([[2055]], device='cuda:0')
2025-02-11 07:50:54,580 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:50:54,580 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,580 - Step 17: Generated next token
2025-02-11 07:50:54,580 - Step 17: Updated current_ids
2025-02-11 07:50:54,580 - Step 17: Decoded token text:  So
2025-02-11 07:50:54,580 - Step 17: Updated current_phrase
2025-02-11 07:50:54,581 - Step 17: Created step_acts
2025-02-11 07:50:54,581 - Step 17: Added to generation_acts
2025-02-11 07:50:54,581 - Step 17: Updated recent_tokens
2025-02-11 07:50:54,582 - Step 17: Decoded current text
2025-02-11 07:50:54,582 - Step 17: Reset consecutive_fillers
2025-02-11 07:50:54,582 - Step 17: Calculated unique_ratio: 0.75
2025-02-11 07:50:54,582 - 
Starting step 18
2025-02-11 07:50:54,583 - Current_ids device: cuda:0
2025-02-11 07:50:54,583 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,604 - Model output complete
2025-02-11 07:50:54,604 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:50:54,604 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,605 - Next token logits device: cuda:0
2025-02-11 07:50:54,605 - Entered do_sample
2025-02-11 07:50:54,605 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,608 - Probs max: 0.7119140625
2025-02-11 07:50:54,609 - Pre-cat
2025-02-11 07:50:54,609 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537,    387,   4201,    553,
            279,   4935,     13,   2055]], device='cuda:0')
2025-02-11 07:50:54,611 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:54,611 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:50:54,611 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,611 - Step 18: Generated next token
2025-02-11 07:50:54,612 - Step 18: Updated current_ids
2025-02-11 07:50:54,612 - Step 18: Decoded token text: ,
2025-02-11 07:50:54,612 - Step 18: Updated current_phrase
2025-02-11 07:50:54,612 - Step 18: Created step_acts
2025-02-11 07:50:54,612 - Step 18: Added to generation_acts
2025-02-11 07:50:54,612 - Step 18: Updated recent_tokens
2025-02-11 07:50:54,613 - Step 18: Found phrase end token
2025-02-11 07:50:54,613 - Step 18: Updated recent_phrases
2025-02-11 07:50:54,614 - Step 18: Calculated similarity: 0.0
2025-02-11 07:50:54,614 - Step 18: Decoded current text
2025-02-11 07:50:54,614 - Step 18: Reset consecutive_fillers
2025-02-11 07:50:54,614 - Step 18: Calculated unique_ratio: 0.75
2025-02-11 07:50:54,614 - 
Starting step 19
2025-02-11 07:50:54,614 - Current_ids device: cuda:0
2025-02-11 07:50:54,614 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,636 - Model output complete
2025-02-11 07:50:54,636 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:50:54,636 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,636 - Next token logits device: cuda:0
2025-02-11 07:50:54,636 - Entered do_sample
2025-02-11 07:50:54,636 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,640 - Probs max: 0.93505859375
2025-02-11 07:50:54,640 - Pre-cat
2025-02-11 07:50:54,640 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537,    387,   4201,    553,
            279,   4935,     13,   2055,     11]], device='cuda:0')
2025-02-11 07:50:54,643 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:54,644 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:50:54,644 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,644 - Step 19: Generated next token
2025-02-11 07:50:54,644 - Step 19: Updated current_ids
2025-02-11 07:50:54,644 - Step 19: Decoded token text:  the
2025-02-11 07:50:54,644 - Step 19: Updated current_phrase
2025-02-11 07:50:54,645 - Step 19: Created step_acts
2025-02-11 07:50:54,645 - Step 19: Added to generation_acts
2025-02-11 07:50:54,645 - Step 19: Updated recent_tokens
2025-02-11 07:50:54,646 - Step 19: Decoded current text
2025-02-11 07:50:54,646 - Step 19: Reset consecutive_fillers
2025-02-11 07:50:54,646 - Step 19: Calculated unique_ratio: 0.75
2025-02-11 07:50:54,646 - 
Starting step 20
2025-02-11 07:50:54,646 - Current_ids device: cuda:0
2025-02-11 07:50:54,646 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,669 - Model output complete
2025-02-11 07:50:54,669 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:50:54,670 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,670 - Next token logits device: cuda:0
2025-02-11 07:50:54,670 - Entered do_sample
2025-02-11 07:50:54,670 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,673 - Probs max: 0.59716796875
2025-02-11 07:50:54,673 - Pre-cat
2025-02-11 07:50:54,673 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537,    387,   4201,    553,
            279,   4935,     13,   2055,     11,    279]], device='cuda:0')
2025-02-11 07:50:54,675 - Next token: tensor([[4226]], device='cuda:0')
2025-02-11 07:50:54,675 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:50:54,675 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,675 - Step 20: Generated next token
2025-02-11 07:50:54,676 - Step 20: Updated current_ids
2025-02-11 07:50:54,676 - Step 20: Decoded token text:  answer
2025-02-11 07:50:54,676 - Step 20: Updated current_phrase
2025-02-11 07:50:54,676 - Step 20: Created step_acts
2025-02-11 07:50:54,676 - Step 20: Added to generation_acts
2025-02-11 07:50:54,678 - Step 20: Updated generated_texts
2025-02-11 07:50:54,678 - Step 20: Updated recent_tokens
2025-02-11 07:50:54,678 - Step 20: Decoded current text
2025-02-11 07:50:54,678 - Step 20: Reset consecutive_fillers
2025-02-11 07:50:54,678 - Step 20: Calculated unique_ratio: 0.8125
2025-02-11 07:50:54,678 - 
Starting step 21
2025-02-11 07:50:54,678 - Current_ids device: cuda:0
2025-02-11 07:50:54,678 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,703 - Model output complete
2025-02-11 07:50:54,704 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:50:54,704 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,704 - Next token logits device: cuda:0
2025-02-11 07:50:54,704 - Entered do_sample
2025-02-11 07:50:54,704 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,706 - Probs max: 0.916015625
2025-02-11 07:50:54,707 - Pre-cat
2025-02-11 07:50:54,707 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537,    387,   4201,    553,
            279,   4935,     13,   2055,     11,    279,   4226]],
       device='cuda:0')
2025-02-11 07:50:54,709 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:54,709 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:50:54,709 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,709 - Step 21: Generated next token
2025-02-11 07:50:54,709 - Step 21: Updated current_ids
2025-02-11 07:50:54,710 - Step 21: Decoded token text:  is
2025-02-11 07:50:54,710 - Step 21: Updated current_phrase
2025-02-11 07:50:54,710 - Step 21: Created step_acts
2025-02-11 07:50:54,710 - Step 21: Added to generation_acts
2025-02-11 07:50:54,710 - Step 21: Updated recent_tokens
2025-02-11 07:50:54,712 - Step 21: Decoded current text
2025-02-11 07:50:54,712 - Step 21: Reset consecutive_fillers
2025-02-11 07:50:54,712 - Step 21: Calculated unique_ratio: 0.875
2025-02-11 07:50:54,712 - 
Starting step 22
2025-02-11 07:50:54,712 - Current_ids device: cuda:0
2025-02-11 07:50:54,712 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,740 - Model output complete
2025-02-11 07:50:54,740 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:50:54,740 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,740 - Next token logits device: cuda:0
2025-02-11 07:50:54,740 - Entered do_sample
2025-02-11 07:50:54,740 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,744 - Probs max: 0.302978515625
2025-02-11 07:50:54,744 - Pre-cat
2025-02-11 07:50:54,745 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537,    387,   4201,    553,
            279,   4935,     13,   2055,     11,    279,   4226,    374]],
       device='cuda:0')
2025-02-11 07:50:54,747 - Next token: tensor([[362]], device='cuda:0')
2025-02-11 07:50:54,747 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:50:54,747 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,747 - Step 22: Generated next token
2025-02-11 07:50:54,747 - Step 22: Updated current_ids
2025-02-11 07:50:54,747 - Step 22: Decoded token text:  A
2025-02-11 07:50:54,748 - Step 22: Updated current_phrase
2025-02-11 07:50:54,748 - Step 22: Created step_acts
2025-02-11 07:50:54,748 - Step 22: Added to generation_acts
2025-02-11 07:50:54,748 - Step 22: Updated recent_tokens
2025-02-11 07:50:54,749 - Step 22: Decoded current text
2025-02-11 07:50:54,750 - Step 22: Incremented consecutive_fillers to 1
2025-02-11 07:50:54,750 - Step 22: Calculated unique_ratio: 0.875
2025-02-11 07:50:54,750 - 
Starting step 23
2025-02-11 07:50:54,750 - Current_ids device: cuda:0
2025-02-11 07:50:54,750 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,773 - Model output complete
2025-02-11 07:50:54,773 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:50:54,773 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,773 - Next token logits device: cuda:0
2025-02-11 07:50:54,774 - Entered do_sample
2025-02-11 07:50:54,774 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,777 - Probs max: 0.5068359375
2025-02-11 07:50:54,777 - Pre-cat
2025-02-11 07:50:54,777 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537,    387,   4201,    553,
            279,   4935,     13,   2055,     11,    279,   4226,    374,    362]],
       device='cuda:0')
2025-02-11 07:50:54,780 - Next token: tensor([[382]], device='cuda:0')
2025-02-11 07:50:54,780 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:50:54,780 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,780 - Step 23: Generated next token
2025-02-11 07:50:54,780 - Step 23: Updated current_ids
2025-02-11 07:50:54,781 - Step 23: Decoded token text: .


2025-02-11 07:50:54,781 - Step 23: Updated current_phrase
2025-02-11 07:50:54,781 - Step 23: Created step_acts
2025-02-11 07:50:54,781 - Step 23: Added to generation_acts
2025-02-11 07:50:54,781 - Step 23: Updated recent_tokens
2025-02-11 07:50:54,783 - Step 23: Decoded current text
2025-02-11 07:50:54,783 - Step 23: Incremented consecutive_fillers to 2
2025-02-11 07:50:54,783 - Step 23: Calculated unique_ratio: 0.9375
2025-02-11 07:50:54,783 - 
Starting step 24
2025-02-11 07:50:54,783 - Current_ids device: cuda:0
2025-02-11 07:50:54,783 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,806 - Model output complete
2025-02-11 07:50:54,806 - Logits shape: torch.Size([1, 55, 151936])
2025-02-11 07:50:54,806 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,806 - Next token logits device: cuda:0
2025-02-11 07:50:54,806 - Entered do_sample
2025-02-11 07:50:54,806 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,810 - Probs max: 0.5556640625
2025-02-11 07:50:54,810 - Pre-cat
2025-02-11 07:50:54,810 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
             11,    714,    279,   7002,    686,    537,    387,   4201,    553,
            279,   4935,     13,   2055,     11,    279,   4226,    374,    362,
            382]], device='cuda:0')
2025-02-11 07:50:54,813 - Next token: tensor([[3983]], device='cuda:0')
2025-02-11 07:50:54,814 - Current_ids shape: torch.Size([1, 55])
2025-02-11 07:50:54,814 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,814 - Step 24: Generated next token
2025-02-11 07:50:54,814 - Step 24: Updated current_ids
2025-02-11 07:50:54,814 - Step 24: Decoded token text: But
2025-02-11 07:50:54,814 - Step 24: Updated current_phrase
2025-02-11 07:50:54,814 - Step 24: Created step_acts
2025-02-11 07:50:54,814 - Step 24: Added to generation_acts
2025-02-11 07:50:54,815 - Step 24: Updated recent_tokens
2025-02-11 07:50:54,816 - Step 24: Decoded current text
2025-02-11 07:50:54,816 - Step 24: Incremented consecutive_fillers to 3
2025-02-11 07:50:54,924 - 
Starting step 0
2025-02-11 07:50:54,924 - Current_ids device: cuda:0
2025-02-11 07:50:54,925 - Current_ids dtype: torch.int64
2025-02-11 07:50:54,960 - Model output complete
2025-02-11 07:50:54,960 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:54,960 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,960 - Next token logits device: cuda:0
2025-02-11 07:50:54,960 - Entered do_sample
2025-02-11 07:50:54,960 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:54,963 - Probs max: 0.50341796875
2025-02-11 07:50:54,964 - Pre-cat
2025-02-11 07:50:54,964 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:54,966 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:54,967 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:54,967 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:54,967 - Step 0: Generated next token
2025-02-11 07:50:54,967 - Step 0: Updated current_ids
2025-02-11 07:50:54,967 - Step 0: Decoded token text:  a
2025-02-11 07:50:54,967 - Step 0: Updated current_phrase
2025-02-11 07:50:54,967 - Step 0: Created step_acts
2025-02-11 07:50:54,967 - Step 0: Added to generation_acts
2025-02-11 07:50:54,969 - Step 0: Updated generated_texts
2025-02-11 07:50:54,969 - Step 0: Updated recent_tokens
2025-02-11 07:50:54,970 - Step 0: Decoded current text
2025-02-11 07:50:54,970 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:54,970 - 
Starting step 1
2025-02-11 07:50:54,970 - Current_ids device: cuda:0
2025-02-11 07:50:54,971 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,049 - Model output complete
2025-02-11 07:50:55,049 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:55,049 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,049 - Next token logits device: cuda:0
2025-02-11 07:50:55,049 - Entered do_sample
2025-02-11 07:50:55,050 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,052 - Probs max: 0.82080078125
2025-02-11 07:50:55,052 - Pre-cat
2025-02-11 07:50:55,052 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    264]], device='cuda:0')
2025-02-11 07:50:55,054 - Next token: tensor([[8]], device='cuda:0')
2025-02-11 07:50:55,054 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:55,054 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,054 - Step 1: Generated next token
2025-02-11 07:50:55,054 - Step 1: Updated current_ids
2025-02-11 07:50:55,054 - Step 1: Decoded token text: )
2025-02-11 07:50:55,054 - Step 1: Updated current_phrase
2025-02-11 07:50:55,055 - Step 1: Created step_acts
2025-02-11 07:50:55,055 - Step 1: Added to generation_acts
2025-02-11 07:50:55,055 - Step 1: Updated recent_tokens
2025-02-11 07:50:55,056 - Step 1: Decoded current text
2025-02-11 07:50:55,056 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:55,056 - 
Starting step 2
2025-02-11 07:50:55,056 - Current_ids device: cuda:0
2025-02-11 07:50:55,056 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,079 - Model output complete
2025-02-11 07:50:55,079 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:55,079 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,079 - Next token logits device: cuda:0
2025-02-11 07:50:55,079 - Entered do_sample
2025-02-11 07:50:55,079 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,082 - Probs max: 0.28955078125
2025-02-11 07:50:55,082 - Pre-cat
2025-02-11 07:50:55,082 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    264,      8]], device='cuda:0')
2025-02-11 07:50:55,084 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:55,084 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:55,084 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,084 - Step 2: Generated next token
2025-02-11 07:50:55,084 - Step 2: Updated current_ids
2025-02-11 07:50:55,084 - Step 2: Decoded token text:  the
2025-02-11 07:50:55,084 - Step 2: Updated current_phrase
2025-02-11 07:50:55,085 - Step 2: Created step_acts
2025-02-11 07:50:55,085 - Step 2: Added to generation_acts
2025-02-11 07:50:55,085 - Step 2: Updated recent_tokens
2025-02-11 07:50:55,086 - Step 2: Decoded current text
2025-02-11 07:50:55,086 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:55,086 - 
Starting step 3
2025-02-11 07:50:55,086 - Current_ids device: cuda:0
2025-02-11 07:50:55,086 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,110 - Model output complete
2025-02-11 07:50:55,110 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:55,110 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,110 - Next token logits device: cuda:0
2025-02-11 07:50:55,110 - Entered do_sample
2025-02-11 07:50:55,111 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,113 - Probs max: 0.771484375
2025-02-11 07:50:55,114 - Pre-cat
2025-02-11 07:50:55,114 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    264,      8,    279]],
       device='cuda:0')
2025-02-11 07:50:55,115 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:55,115 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:55,115 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,115 - Step 3: Generated next token
2025-02-11 07:50:55,115 - Step 3: Updated current_ids
2025-02-11 07:50:55,116 - Step 3: Decoded token text:  ball
2025-02-11 07:50:55,116 - Step 3: Updated current_phrase
2025-02-11 07:50:55,116 - Step 3: Created step_acts
2025-02-11 07:50:55,116 - Step 3: Added to generation_acts
2025-02-11 07:50:55,116 - Step 3: Updated recent_tokens
2025-02-11 07:50:55,117 - Step 3: Decoded current text
2025-02-11 07:50:55,117 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:55,117 - 
Starting step 4
2025-02-11 07:50:55,118 - Current_ids device: cuda:0
2025-02-11 07:50:55,118 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,159 - Model output complete
2025-02-11 07:50:55,159 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:55,159 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,159 - Next token logits device: cuda:0
2025-02-11 07:50:55,159 - Entered do_sample
2025-02-11 07:50:55,159 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,161 - Probs max: 0.39306640625
2025-02-11 07:50:55,162 - Pre-cat
2025-02-11 07:50:55,162 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    264,      8,    279,   4935]],
       device='cuda:0')
2025-02-11 07:50:55,164 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:50:55,165 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:55,165 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,165 - Step 4: Generated next token
2025-02-11 07:50:55,165 - Step 4: Updated current_ids
2025-02-11 07:50:55,165 - Step 4: Decoded token text:  and
2025-02-11 07:50:55,166 - Step 4: Updated current_phrase
2025-02-11 07:50:55,166 - Step 4: Created step_acts
2025-02-11 07:50:55,166 - Step 4: Added to generation_acts
2025-02-11 07:50:55,166 - Step 4: Updated recent_tokens
2025-02-11 07:50:55,168 - Step 4: Decoded current text
2025-02-11 07:50:55,168 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:55,168 - 
Starting step 5
2025-02-11 07:50:55,168 - Current_ids device: cuda:0
2025-02-11 07:50:55,168 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,228 - Model output complete
2025-02-11 07:50:55,228 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:55,228 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,228 - Next token logits device: cuda:0
2025-02-11 07:50:55,228 - Entered do_sample
2025-02-11 07:50:55,228 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,230 - Probs max: 0.76318359375
2025-02-11 07:50:55,231 - Pre-cat
2025-02-11 07:50:55,231 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    264,      8,    279,   4935,    323]],
       device='cuda:0')
2025-02-11 07:50:55,234 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:55,234 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:55,234 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,234 - Step 5: Generated next token
2025-02-11 07:50:55,234 - Step 5: Updated current_ids
2025-02-11 07:50:55,234 - Step 5: Decoded token text:  the
2025-02-11 07:50:55,235 - Step 5: Updated current_phrase
2025-02-11 07:50:55,235 - Step 5: Created step_acts
2025-02-11 07:50:55,235 - Step 5: Added to generation_acts
2025-02-11 07:50:55,236 - Step 5: Updated generated_texts
2025-02-11 07:50:55,236 - Step 5: Updated recent_tokens
2025-02-11 07:50:55,237 - Step 5: Decoded current text
2025-02-11 07:50:55,237 - Step 5: Incremented consecutive_fillers to 1
2025-02-11 07:50:55,237 - 
Starting step 6
2025-02-11 07:50:55,237 - Current_ids device: cuda:0
2025-02-11 07:50:55,237 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,262 - Model output complete
2025-02-11 07:50:55,262 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:55,262 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,262 - Next token logits device: cuda:0
2025-02-11 07:50:55,262 - Entered do_sample
2025-02-11 07:50:55,262 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,265 - Probs max: 0.9990234375
2025-02-11 07:50:55,265 - Pre-cat
2025-02-11 07:50:55,265 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    264,      8,    279,   4935,    323,
            279]], device='cuda:0')
2025-02-11 07:50:55,267 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:55,267 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:55,267 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,267 - Step 6: Generated next token
2025-02-11 07:50:55,267 - Step 6: Updated current_ids
2025-02-11 07:50:55,267 - Step 6: Decoded token text:  wall
2025-02-11 07:50:55,267 - Step 6: Updated current_phrase
2025-02-11 07:50:55,268 - Step 6: Created step_acts
2025-02-11 07:50:55,268 - Step 6: Added to generation_acts
2025-02-11 07:50:55,268 - Step 6: Updated recent_tokens
2025-02-11 07:50:55,269 - Step 6: Decoded current text
2025-02-11 07:50:55,269 - Step 6: Incremented consecutive_fillers to 2
2025-02-11 07:50:55,269 - 
Starting step 7
2025-02-11 07:50:55,269 - Current_ids device: cuda:0
2025-02-11 07:50:55,269 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,292 - Model output complete
2025-02-11 07:50:55,292 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:55,292 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,292 - Next token logits device: cuda:0
2025-02-11 07:50:55,292 - Entered do_sample
2025-02-11 07:50:55,293 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,295 - Probs max: 0.382568359375
2025-02-11 07:50:55,296 - Pre-cat
2025-02-11 07:50:55,296 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    264,      8,    279,   4935,    323,
            279,   7002]], device='cuda:0')
2025-02-11 07:50:55,298 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:55,298 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:55,298 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,298 - Step 7: Generated next token
2025-02-11 07:50:55,298 - Step 7: Updated current_ids
2025-02-11 07:50:55,299 - Step 7: Decoded token text:  will
2025-02-11 07:50:55,299 - Step 7: Updated current_phrase
2025-02-11 07:50:55,299 - Step 7: Created step_acts
2025-02-11 07:50:55,299 - Step 7: Added to generation_acts
2025-02-11 07:50:55,299 - Step 7: Updated recent_tokens
2025-02-11 07:50:55,300 - Step 7: Decoded current text
2025-02-11 07:50:55,301 - Step 7: Incremented consecutive_fillers to 3
2025-02-11 07:50:55,405 - 
Starting step 0
2025-02-11 07:50:55,406 - Current_ids device: cuda:0
2025-02-11 07:50:55,406 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,449 - Model output complete
2025-02-11 07:50:55,449 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:55,449 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,449 - Next token logits device: cuda:0
2025-02-11 07:50:55,449 - Entered do_sample
2025-02-11 07:50:55,449 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,451 - Probs max: 0.50341796875
2025-02-11 07:50:55,452 - Pre-cat
2025-02-11 07:50:55,452 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:55,454 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:50:55,455 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:55,455 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,455 - Step 0: Generated next token
2025-02-11 07:50:55,455 - Step 0: Updated current_ids
2025-02-11 07:50:55,455 - Step 0: Decoded token text:  It
2025-02-11 07:50:55,455 - Step 0: Updated current_phrase
2025-02-11 07:50:55,455 - Step 0: Created step_acts
2025-02-11 07:50:55,455 - Step 0: Added to generation_acts
2025-02-11 07:50:55,457 - Step 0: Updated generated_texts
2025-02-11 07:50:55,457 - Step 0: Updated recent_tokens
2025-02-11 07:50:55,457 - Step 0: Decoded current text
2025-02-11 07:50:55,457 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:55,457 - 
Starting step 1
2025-02-11 07:50:55,457 - Current_ids device: cuda:0
2025-02-11 07:50:55,457 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,481 - Model output complete
2025-02-11 07:50:55,481 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:55,481 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,481 - Next token logits device: cuda:0
2025-02-11 07:50:55,481 - Entered do_sample
2025-02-11 07:50:55,481 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,484 - Probs max: 0.9384765625
2025-02-11 07:50:55,484 - Pre-cat
2025-02-11 07:50:55,484 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084]], device='cuda:0')
2025-02-11 07:50:55,486 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:55,486 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:55,486 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,486 - Step 1: Generated next token
2025-02-11 07:50:55,486 - Step 1: Updated current_ids
2025-02-11 07:50:55,487 - Step 1: Decoded token text:  will
2025-02-11 07:50:55,487 - Step 1: Updated current_phrase
2025-02-11 07:50:55,487 - Step 1: Created step_acts
2025-02-11 07:50:55,487 - Step 1: Added to generation_acts
2025-02-11 07:50:55,487 - Step 1: Updated recent_tokens
2025-02-11 07:50:55,488 - Step 1: Decoded current text
2025-02-11 07:50:55,488 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:55,488 - 
Starting step 2
2025-02-11 07:50:55,488 - Current_ids device: cuda:0
2025-02-11 07:50:55,489 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,511 - Model output complete
2025-02-11 07:50:55,511 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:55,511 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,511 - Next token logits device: cuda:0
2025-02-11 07:50:55,512 - Entered do_sample
2025-02-11 07:50:55,512 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,514 - Probs max: 0.2442626953125
2025-02-11 07:50:55,515 - Pre-cat
2025-02-11 07:50:55,515 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686]], device='cuda:0')
2025-02-11 07:50:55,516 - Next token: tensor([[537]], device='cuda:0')
2025-02-11 07:50:55,516 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:55,516 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,516 - Step 2: Generated next token
2025-02-11 07:50:55,517 - Step 2: Updated current_ids
2025-02-11 07:50:55,517 - Step 2: Decoded token text:  not
2025-02-11 07:50:55,517 - Step 2: Updated current_phrase
2025-02-11 07:50:55,517 - Step 2: Created step_acts
2025-02-11 07:50:55,517 - Step 2: Added to generation_acts
2025-02-11 07:50:55,517 - Step 2: Updated recent_tokens
2025-02-11 07:50:55,518 - Step 2: Decoded current text
2025-02-11 07:50:55,519 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:55,519 - 
Starting step 3
2025-02-11 07:50:55,519 - Current_ids device: cuda:0
2025-02-11 07:50:55,519 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,541 - Model output complete
2025-02-11 07:50:55,541 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:55,541 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,541 - Next token logits device: cuda:0
2025-02-11 07:50:55,541 - Entered do_sample
2025-02-11 07:50:55,541 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,544 - Probs max: 0.1690673828125
2025-02-11 07:50:55,642 - 
Starting step 0
2025-02-11 07:50:55,642 - Current_ids device: cuda:0
2025-02-11 07:50:55,642 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,670 - Model output complete
2025-02-11 07:50:55,670 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:55,670 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,670 - Next token logits device: cuda:0
2025-02-11 07:50:55,670 - Entered do_sample
2025-02-11 07:50:55,670 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,673 - Probs max: 0.50341796875
2025-02-11 07:50:55,674 - Pre-cat
2025-02-11 07:50:55,674 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:55,676 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:50:55,676 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:55,676 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,676 - Step 0: Generated next token
2025-02-11 07:50:55,676 - Step 0: Updated current_ids
2025-02-11 07:50:55,677 - Step 0: Decoded token text:  If
2025-02-11 07:50:55,677 - Step 0: Updated current_phrase
2025-02-11 07:50:55,677 - Step 0: Created step_acts
2025-02-11 07:50:55,677 - Step 0: Added to generation_acts
2025-02-11 07:50:55,678 - Step 0: Updated generated_texts
2025-02-11 07:50:55,678 - Step 0: Updated recent_tokens
2025-02-11 07:50:55,679 - Step 0: Decoded current text
2025-02-11 07:50:55,679 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:55,679 - 
Starting step 1
2025-02-11 07:50:55,679 - Current_ids device: cuda:0
2025-02-11 07:50:55,679 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,705 - Model output complete
2025-02-11 07:50:55,705 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:55,705 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,705 - Next token logits device: cuda:0
2025-02-11 07:50:55,705 - Entered do_sample
2025-02-11 07:50:55,706 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,708 - Probs max: 0.7099609375
2025-02-11 07:50:55,708 - Pre-cat
2025-02-11 07:50:55,708 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:50:55,710 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:55,710 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:55,710 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,710 - Step 1: Generated next token
2025-02-11 07:50:55,710 - Step 1: Updated current_ids
2025-02-11 07:50:55,711 - Step 1: Decoded token text:  a
2025-02-11 07:50:55,711 - Step 1: Updated current_phrase
2025-02-11 07:50:55,711 - Step 1: Created step_acts
2025-02-11 07:50:55,711 - Step 1: Added to generation_acts
2025-02-11 07:50:55,711 - Step 1: Updated recent_tokens
2025-02-11 07:50:55,713 - Step 1: Decoded current text
2025-02-11 07:50:55,713 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:55,713 - 
Starting step 2
2025-02-11 07:50:55,713 - Current_ids device: cuda:0
2025-02-11 07:50:55,713 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,736 - Model output complete
2025-02-11 07:50:55,736 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:55,736 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,736 - Next token logits device: cuda:0
2025-02-11 07:50:55,736 - Entered do_sample
2025-02-11 07:50:55,737 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,739 - Probs max: 0.9970703125
2025-02-11 07:50:55,739 - Pre-cat
2025-02-11 07:50:55,740 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264]], device='cuda:0')
2025-02-11 07:50:55,741 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:55,741 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:55,741 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,741 - Step 2: Generated next token
2025-02-11 07:50:55,741 - Step 2: Updated current_ids
2025-02-11 07:50:55,742 - Step 2: Decoded token text:  ball
2025-02-11 07:50:55,742 - Step 2: Updated current_phrase
2025-02-11 07:50:55,742 - Step 2: Created step_acts
2025-02-11 07:50:55,742 - Step 2: Added to generation_acts
2025-02-11 07:50:55,742 - Step 2: Updated recent_tokens
2025-02-11 07:50:55,743 - Step 2: Decoded current text
2025-02-11 07:50:55,743 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:55,743 - 
Starting step 3
2025-02-11 07:50:55,743 - Current_ids device: cuda:0
2025-02-11 07:50:55,744 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,766 - Model output complete
2025-02-11 07:50:55,766 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:55,766 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,766 - Next token logits device: cuda:0
2025-02-11 07:50:55,766 - Entered do_sample
2025-02-11 07:50:55,767 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,769 - Probs max: 0.99951171875
2025-02-11 07:50:55,770 - Pre-cat
2025-02-11 07:50:55,770 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935]],
       device='cuda:0')
2025-02-11 07:50:55,771 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:55,771 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:55,772 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,772 - Step 3: Generated next token
2025-02-11 07:50:55,772 - Step 3: Updated current_ids
2025-02-11 07:50:55,772 - Step 3: Decoded token text:  is
2025-02-11 07:50:55,772 - Step 3: Updated current_phrase
2025-02-11 07:50:55,772 - Step 3: Created step_acts
2025-02-11 07:50:55,772 - Step 3: Added to generation_acts
2025-02-11 07:50:55,772 - Step 3: Updated recent_tokens
2025-02-11 07:50:55,774 - Step 3: Decoded current text
2025-02-11 07:50:55,774 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:55,774 - 
Starting step 4
2025-02-11 07:50:55,774 - Current_ids device: cuda:0
2025-02-11 07:50:55,774 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,828 - Model output complete
2025-02-11 07:50:55,828 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:55,828 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,828 - Next token logits device: cuda:0
2025-02-11 07:50:55,828 - Entered do_sample
2025-02-11 07:50:55,828 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,830 - Probs max: 0.998046875
2025-02-11 07:50:55,832 - Pre-cat
2025-02-11 07:50:55,832 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374]],
       device='cuda:0')
2025-02-11 07:50:55,834 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:50:55,835 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:55,835 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,835 - Step 4: Generated next token
2025-02-11 07:50:55,835 - Step 4: Updated current_ids
2025-02-11 07:50:55,835 - Step 4: Decoded token text:  thrown
2025-02-11 07:50:55,835 - Step 4: Updated current_phrase
2025-02-11 07:50:55,835 - Step 4: Created step_acts
2025-02-11 07:50:55,836 - Step 4: Added to generation_acts
2025-02-11 07:50:55,836 - Step 4: Updated recent_tokens
2025-02-11 07:50:55,837 - Step 4: Decoded current text
2025-02-11 07:50:55,837 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:55,837 - 
Starting step 5
2025-02-11 07:50:55,837 - Current_ids device: cuda:0
2025-02-11 07:50:55,837 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,871 - Model output complete
2025-02-11 07:50:55,871 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:55,872 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,872 - Next token logits device: cuda:0
2025-02-11 07:50:55,872 - Entered do_sample
2025-02-11 07:50:55,872 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,874 - Probs max: 0.9697265625
2025-02-11 07:50:55,875 - Pre-cat
2025-02-11 07:50:55,875 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:50:55,876 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:50:55,877 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:55,877 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,877 - Step 5: Generated next token
2025-02-11 07:50:55,877 - Step 5: Updated current_ids
2025-02-11 07:50:55,877 - Step 5: Decoded token text:  at
2025-02-11 07:50:55,877 - Step 5: Updated current_phrase
2025-02-11 07:50:55,877 - Step 5: Created step_acts
2025-02-11 07:50:55,877 - Step 5: Added to generation_acts
2025-02-11 07:50:55,879 - Step 5: Updated generated_texts
2025-02-11 07:50:55,879 - Step 5: Updated recent_tokens
2025-02-11 07:50:55,879 - Step 5: Decoded current text
2025-02-11 07:50:55,879 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:55,879 - 
Starting step 6
2025-02-11 07:50:55,879 - Current_ids device: cuda:0
2025-02-11 07:50:55,879 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,902 - Model output complete
2025-02-11 07:50:55,902 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:55,902 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,902 - Next token logits device: cuda:0
2025-02-11 07:50:55,902 - Entered do_sample
2025-02-11 07:50:55,902 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,905 - Probs max: 0.99951171875
2025-02-11 07:50:55,906 - Pre-cat
2025-02-11 07:50:55,906 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:50:55,908 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:55,909 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:55,909 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,909 - Step 6: Generated next token
2025-02-11 07:50:55,909 - Step 6: Updated current_ids
2025-02-11 07:50:55,909 - Step 6: Decoded token text:  a
2025-02-11 07:50:55,909 - Step 6: Updated current_phrase
2025-02-11 07:50:55,910 - Step 6: Created step_acts
2025-02-11 07:50:55,910 - Step 6: Added to generation_acts
2025-02-11 07:50:55,910 - Step 6: Updated recent_tokens
2025-02-11 07:50:55,912 - Step 6: Decoded current text
2025-02-11 07:50:55,912 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:55,912 - 
Starting step 7
2025-02-11 07:50:55,912 - Current_ids device: cuda:0
2025-02-11 07:50:55,912 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,943 - Model output complete
2025-02-11 07:50:55,943 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:55,943 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,943 - Next token logits device: cuda:0
2025-02-11 07:50:55,943 - Entered do_sample
2025-02-11 07:50:55,943 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,946 - Probs max: 0.9990234375
2025-02-11 07:50:55,947 - Pre-cat
2025-02-11 07:50:55,947 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:50:55,948 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:55,949 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:55,949 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,949 - Step 7: Generated next token
2025-02-11 07:50:55,949 - Step 7: Updated current_ids
2025-02-11 07:50:55,949 - Step 7: Decoded token text:  wall
2025-02-11 07:50:55,949 - Step 7: Updated current_phrase
2025-02-11 07:50:55,950 - Step 7: Created step_acts
2025-02-11 07:50:55,950 - Step 7: Added to generation_acts
2025-02-11 07:50:55,950 - Step 7: Updated recent_tokens
2025-02-11 07:50:55,951 - Step 7: Decoded current text
2025-02-11 07:50:55,951 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:55,951 - 
Starting step 8
2025-02-11 07:50:55,951 - Current_ids device: cuda:0
2025-02-11 07:50:55,951 - Current_ids dtype: torch.int64
2025-02-11 07:50:55,975 - Model output complete
2025-02-11 07:50:55,975 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:55,975 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,975 - Next token logits device: cuda:0
2025-02-11 07:50:55,976 - Entered do_sample
2025-02-11 07:50:55,976 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:55,978 - Probs max: 0.9912109375
2025-02-11 07:50:55,979 - Pre-cat
2025-02-11 07:50:55,979 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:50:55,980 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:55,980 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:55,980 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:55,980 - Step 8: Generated next token
2025-02-11 07:50:55,981 - Step 8: Updated current_ids
2025-02-11 07:50:55,981 - Step 8: Decoded token text:  very
2025-02-11 07:50:55,981 - Step 8: Updated current_phrase
2025-02-11 07:50:55,981 - Step 8: Created step_acts
2025-02-11 07:50:55,981 - Step 8: Added to generation_acts
2025-02-11 07:50:55,981 - Step 8: Updated recent_tokens
2025-02-11 07:50:55,983 - Step 8: Decoded current text
2025-02-11 07:50:55,983 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:55,983 - 
Starting step 9
2025-02-11 07:50:55,983 - Current_ids device: cuda:0
2025-02-11 07:50:55,983 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,005 - Model output complete
2025-02-11 07:50:56,005 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:56,005 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,005 - Next token logits device: cuda:0
2025-02-11 07:50:56,005 - Entered do_sample
2025-02-11 07:50:56,005 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,008 - Probs max: 0.998046875
2025-02-11 07:50:56,009 - Pre-cat
2025-02-11 07:50:56,009 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:50:56,011 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:56,011 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:56,011 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,011 - Step 9: Generated next token
2025-02-11 07:50:56,011 - Step 9: Updated current_ids
2025-02-11 07:50:56,011 - Step 9: Decoded token text:  fast
2025-02-11 07:50:56,011 - Step 9: Updated current_phrase
2025-02-11 07:50:56,012 - Step 9: Created step_acts
2025-02-11 07:50:56,012 - Step 9: Added to generation_acts
2025-02-11 07:50:56,012 - Step 9: Updated recent_tokens
2025-02-11 07:50:56,013 - Step 9: Decoded current text
2025-02-11 07:50:56,013 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:56,013 - 
Starting step 10
2025-02-11 07:50:56,013 - Current_ids device: cuda:0
2025-02-11 07:50:56,013 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,036 - Model output complete
2025-02-11 07:50:56,036 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:56,036 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,036 - Next token logits device: cuda:0
2025-02-11 07:50:56,036 - Entered do_sample
2025-02-11 07:50:56,036 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,039 - Probs max: 0.99853515625
2025-02-11 07:50:56,039 - Pre-cat
2025-02-11 07:50:56,039 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:50:56,041 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:56,041 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:56,041 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,041 - Step 10: Generated next token
2025-02-11 07:50:56,041 - Step 10: Updated current_ids
2025-02-11 07:50:56,042 - Step 10: Decoded token text: ,
2025-02-11 07:50:56,042 - Step 10: Updated current_phrase
2025-02-11 07:50:56,042 - Step 10: Created step_acts
2025-02-11 07:50:56,042 - Step 10: Added to generation_acts
2025-02-11 07:50:56,043 - Step 10: Updated generated_texts
2025-02-11 07:50:56,044 - Step 10: Updated recent_tokens
2025-02-11 07:50:56,044 - Step 10: Found phrase end token
2025-02-11 07:50:56,044 - Step 10: Updated recent_phrases
2025-02-11 07:50:56,044 - Step 10: Decoded current text
2025-02-11 07:50:56,044 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:56,044 - 
Starting step 11
2025-02-11 07:50:56,044 - Current_ids device: cuda:0
2025-02-11 07:50:56,044 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,068 - Model output complete
2025-02-11 07:50:56,068 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:56,068 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,068 - Next token logits device: cuda:0
2025-02-11 07:50:56,068 - Entered do_sample
2025-02-11 07:50:56,068 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,070 - Probs max: 0.58154296875
2025-02-11 07:50:56,071 - Pre-cat
2025-02-11 07:50:56,071 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:50:56,073 - Next token: tensor([[1221]], device='cuda:0')
2025-02-11 07:50:56,074 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:56,074 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,074 - Step 11: Generated next token
2025-02-11 07:50:56,074 - Step 11: Updated current_ids
2025-02-11 07:50:56,074 - Step 11: Decoded token text:  then
2025-02-11 07:50:56,074 - Step 11: Updated current_phrase
2025-02-11 07:50:56,075 - Step 11: Created step_acts
2025-02-11 07:50:56,075 - Step 11: Added to generation_acts
2025-02-11 07:50:56,075 - Step 11: Updated recent_tokens
2025-02-11 07:50:56,076 - Step 11: Decoded current text
2025-02-11 07:50:56,076 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:56,076 - 
Starting step 12
2025-02-11 07:50:56,076 - Current_ids device: cuda:0
2025-02-11 07:50:56,076 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,099 - Model output complete
2025-02-11 07:50:56,100 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:56,100 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,100 - Next token logits device: cuda:0
2025-02-11 07:50:56,100 - Entered do_sample
2025-02-11 07:50:56,100 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,102 - Probs max: 0.87744140625
2025-02-11 07:50:56,103 - Pre-cat
2025-02-11 07:50:56,103 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,   1221]],
       device='cuda:0')
2025-02-11 07:50:56,105 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:56,105 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:56,105 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,105 - Step 12: Generated next token
2025-02-11 07:50:56,105 - Step 12: Updated current_ids
2025-02-11 07:50:56,105 - Step 12: Decoded token text:  the
2025-02-11 07:50:56,105 - Step 12: Updated current_phrase
2025-02-11 07:50:56,106 - Step 12: Created step_acts
2025-02-11 07:50:56,106 - Step 12: Added to generation_acts
2025-02-11 07:50:56,106 - Step 12: Updated recent_tokens
2025-02-11 07:50:56,107 - Step 12: Decoded current text
2025-02-11 07:50:56,107 - Step 12: Incremented consecutive_fillers to 1
2025-02-11 07:50:56,107 - 
Starting step 13
2025-02-11 07:50:56,108 - Current_ids device: cuda:0
2025-02-11 07:50:56,108 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,130 - Model output complete
2025-02-11 07:50:56,130 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:56,130 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,130 - Next token logits device: cuda:0
2025-02-11 07:50:56,130 - Entered do_sample
2025-02-11 07:50:56,131 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,133 - Probs max: 0.5595703125
2025-02-11 07:50:56,134 - Pre-cat
2025-02-11 07:50:56,134 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,   1221,    279]],
       device='cuda:0')
2025-02-11 07:50:56,136 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:56,136 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:56,136 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,136 - Step 13: Generated next token
2025-02-11 07:50:56,136 - Step 13: Updated current_ids
2025-02-11 07:50:56,136 - Step 13: Decoded token text:  wall
2025-02-11 07:50:56,136 - Step 13: Updated current_phrase
2025-02-11 07:50:56,137 - Step 13: Created step_acts
2025-02-11 07:50:56,137 - Step 13: Added to generation_acts
2025-02-11 07:50:56,137 - Step 13: Updated recent_tokens
2025-02-11 07:50:56,138 - Step 13: Decoded current text
2025-02-11 07:50:56,138 - Step 13: Incremented consecutive_fillers to 2
2025-02-11 07:50:56,138 - 
Starting step 14
2025-02-11 07:50:56,138 - Current_ids device: cuda:0
2025-02-11 07:50:56,139 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,161 - Model output complete
2025-02-11 07:50:56,162 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:50:56,162 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,162 - Next token logits device: cuda:0
2025-02-11 07:50:56,162 - Entered do_sample
2025-02-11 07:50:56,162 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,164 - Probs max: 0.7060546875
2025-02-11 07:50:56,165 - Pre-cat
2025-02-11 07:50:56,165 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,   1221,    279,   7002]],
       device='cuda:0')
2025-02-11 07:50:56,167 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:56,167 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:50:56,167 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,167 - Step 14: Generated next token
2025-02-11 07:50:56,167 - Step 14: Updated current_ids
2025-02-11 07:50:56,167 - Step 14: Decoded token text:  will
2025-02-11 07:50:56,167 - Step 14: Updated current_phrase
2025-02-11 07:50:56,168 - Step 14: Created step_acts
2025-02-11 07:50:56,168 - Step 14: Added to generation_acts
2025-02-11 07:50:56,168 - Step 14: Updated recent_tokens
2025-02-11 07:50:56,169 - Step 14: Decoded current text
2025-02-11 07:50:56,169 - Step 14: Incremented consecutive_fillers to 3
2025-02-11 07:50:56,269 - 
Starting step 0
2025-02-11 07:50:56,270 - Current_ids device: cuda:0
2025-02-11 07:50:56,270 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,301 - Model output complete
2025-02-11 07:50:56,301 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:56,301 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,301 - Next token logits device: cuda:0
2025-02-11 07:50:56,301 - Entered do_sample
2025-02-11 07:50:56,301 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,304 - Probs max: 0.50341796875
2025-02-11 07:50:56,305 - Pre-cat
2025-02-11 07:50:56,305 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:56,307 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:50:56,307 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:56,307 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,307 - Step 0: Generated next token
2025-02-11 07:50:56,307 - Step 0: Updated current_ids
2025-02-11 07:50:56,308 - Step 0: Decoded token text:  It
2025-02-11 07:50:56,308 - Step 0: Updated current_phrase
2025-02-11 07:50:56,308 - Step 0: Created step_acts
2025-02-11 07:50:56,308 - Step 0: Added to generation_acts
2025-02-11 07:50:56,309 - Step 0: Updated generated_texts
2025-02-11 07:50:56,309 - Step 0: Updated recent_tokens
2025-02-11 07:50:56,310 - Step 0: Decoded current text
2025-02-11 07:50:56,310 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:56,310 - 
Starting step 1
2025-02-11 07:50:56,310 - Current_ids device: cuda:0
2025-02-11 07:50:56,310 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,335 - Model output complete
2025-02-11 07:50:56,335 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:56,335 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,335 - Next token logits device: cuda:0
2025-02-11 07:50:56,335 - Entered do_sample
2025-02-11 07:50:56,335 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,338 - Probs max: 0.9384765625
2025-02-11 07:50:56,339 - Pre-cat
2025-02-11 07:50:56,339 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084]], device='cuda:0')
2025-02-11 07:50:56,340 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:56,340 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:56,341 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,341 - Step 1: Generated next token
2025-02-11 07:50:56,341 - Step 1: Updated current_ids
2025-02-11 07:50:56,341 - Step 1: Decoded token text:  will
2025-02-11 07:50:56,341 - Step 1: Updated current_phrase
2025-02-11 07:50:56,341 - Step 1: Created step_acts
2025-02-11 07:50:56,341 - Step 1: Added to generation_acts
2025-02-11 07:50:56,341 - Step 1: Updated recent_tokens
2025-02-11 07:50:56,343 - Step 1: Decoded current text
2025-02-11 07:50:56,343 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:56,343 - 
Starting step 2
2025-02-11 07:50:56,343 - Current_ids device: cuda:0
2025-02-11 07:50:56,343 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,366 - Model output complete
2025-02-11 07:50:56,366 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:56,366 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,366 - Next token logits device: cuda:0
2025-02-11 07:50:56,366 - Entered do_sample
2025-02-11 07:50:56,366 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,370 - Probs max: 0.2442626953125
2025-02-11 07:50:56,370 - Pre-cat
2025-02-11 07:50:56,371 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686]], device='cuda:0')
2025-02-11 07:50:56,372 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:50:56,372 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:56,372 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,372 - Step 2: Generated next token
2025-02-11 07:50:56,372 - Step 2: Updated current_ids
2025-02-11 07:50:56,373 - Step 2: Decoded token text:  hit
2025-02-11 07:50:56,373 - Step 2: Updated current_phrase
2025-02-11 07:50:56,373 - Step 2: Created step_acts
2025-02-11 07:50:56,373 - Step 2: Added to generation_acts
2025-02-11 07:50:56,373 - Step 2: Updated recent_tokens
2025-02-11 07:50:56,374 - Step 2: Decoded current text
2025-02-11 07:50:56,374 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:56,375 - 
Starting step 3
2025-02-11 07:50:56,375 - Current_ids device: cuda:0
2025-02-11 07:50:56,375 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,398 - Model output complete
2025-02-11 07:50:56,398 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:56,398 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,398 - Next token logits device: cuda:0
2025-02-11 07:50:56,398 - Entered do_sample
2025-02-11 07:50:56,399 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,402 - Probs max: 0.99169921875
2025-02-11 07:50:56,402 - Pre-cat
2025-02-11 07:50:56,403 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201]],
       device='cuda:0')
2025-02-11 07:50:56,404 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:56,404 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:56,404 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,404 - Step 3: Generated next token
2025-02-11 07:50:56,404 - Step 3: Updated current_ids
2025-02-11 07:50:56,405 - Step 3: Decoded token text:  the
2025-02-11 07:50:56,405 - Step 3: Updated current_phrase
2025-02-11 07:50:56,405 - Step 3: Created step_acts
2025-02-11 07:50:56,405 - Step 3: Added to generation_acts
2025-02-11 07:50:56,405 - Step 3: Updated recent_tokens
2025-02-11 07:50:56,406 - Step 3: Decoded current text
2025-02-11 07:50:56,407 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:56,407 - 
Starting step 4
2025-02-11 07:50:56,407 - Current_ids device: cuda:0
2025-02-11 07:50:56,407 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,430 - Model output complete
2025-02-11 07:50:56,430 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:56,430 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,430 - Next token logits device: cuda:0
2025-02-11 07:50:56,430 - Entered do_sample
2025-02-11 07:50:56,430 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,433 - Probs max: 0.998046875
2025-02-11 07:50:56,434 - Pre-cat
2025-02-11 07:50:56,434 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279]],
       device='cuda:0')
2025-02-11 07:50:56,436 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:56,437 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:56,437 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,437 - Step 4: Generated next token
2025-02-11 07:50:56,437 - Step 4: Updated current_ids
2025-02-11 07:50:56,437 - Step 4: Decoded token text:  wall
2025-02-11 07:50:56,437 - Step 4: Updated current_phrase
2025-02-11 07:50:56,437 - Step 4: Created step_acts
2025-02-11 07:50:56,437 - Step 4: Added to generation_acts
2025-02-11 07:50:56,438 - Step 4: Updated recent_tokens
2025-02-11 07:50:56,439 - Step 4: Decoded current text
2025-02-11 07:50:56,439 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:56,439 - 
Starting step 5
2025-02-11 07:50:56,439 - Current_ids device: cuda:0
2025-02-11 07:50:56,439 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,462 - Model output complete
2025-02-11 07:50:56,462 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:56,462 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,462 - Next token logits device: cuda:0
2025-02-11 07:50:56,462 - Entered do_sample
2025-02-11 07:50:56,462 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,466 - Probs max: 0.41455078125
2025-02-11 07:50:56,466 - Pre-cat
2025-02-11 07:50:56,467 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002]],
       device='cuda:0')
2025-02-11 07:50:56,468 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:56,468 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:56,468 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,468 - Step 5: Generated next token
2025-02-11 07:50:56,469 - Step 5: Updated current_ids
2025-02-11 07:50:56,469 - Step 5: Decoded token text:  very
2025-02-11 07:50:56,469 - Step 5: Updated current_phrase
2025-02-11 07:50:56,469 - Step 5: Created step_acts
2025-02-11 07:50:56,469 - Step 5: Added to generation_acts
2025-02-11 07:50:56,470 - Step 5: Updated generated_texts
2025-02-11 07:50:56,471 - Step 5: Updated recent_tokens
2025-02-11 07:50:56,471 - Step 5: Decoded current text
2025-02-11 07:50:56,471 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:56,471 - 
Starting step 6
2025-02-11 07:50:56,471 - Current_ids device: cuda:0
2025-02-11 07:50:56,471 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,493 - Model output complete
2025-02-11 07:50:56,493 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:56,493 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,493 - Next token logits device: cuda:0
2025-02-11 07:50:56,494 - Entered do_sample
2025-02-11 07:50:56,494 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,498 - Probs max: 0.7509765625
2025-02-11 07:50:56,498 - Pre-cat
2025-02-11 07:50:56,498 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602]], device='cuda:0')
2025-02-11 07:50:56,500 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:56,500 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:56,500 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,500 - Step 6: Generated next token
2025-02-11 07:50:56,500 - Step 6: Updated current_ids
2025-02-11 07:50:56,500 - Step 6: Decoded token text:  fast
2025-02-11 07:50:56,501 - Step 6: Updated current_phrase
2025-02-11 07:50:56,501 - Step 6: Created step_acts
2025-02-11 07:50:56,501 - Step 6: Added to generation_acts
2025-02-11 07:50:56,501 - Step 6: Updated recent_tokens
2025-02-11 07:50:56,502 - Step 6: Decoded current text
2025-02-11 07:50:56,502 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:56,502 - 
Starting step 7
2025-02-11 07:50:56,502 - Current_ids device: cuda:0
2025-02-11 07:50:56,502 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,527 - Model output complete
2025-02-11 07:50:56,528 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:56,528 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,528 - Next token logits device: cuda:0
2025-02-11 07:50:56,528 - Entered do_sample
2025-02-11 07:50:56,528 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,530 - Probs max: 0.378173828125
2025-02-11 07:50:56,531 - Pre-cat
2025-02-11 07:50:56,531 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937]], device='cuda:0')
2025-02-11 07:50:56,533 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:56,533 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:56,533 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,533 - Step 7: Generated next token
2025-02-11 07:50:56,534 - Step 7: Updated current_ids
2025-02-11 07:50:56,534 - Step 7: Decoded token text: .
2025-02-11 07:50:56,534 - Step 7: Updated current_phrase
2025-02-11 07:50:56,534 - Step 7: Created step_acts
2025-02-11 07:50:56,534 - Step 7: Added to generation_acts
2025-02-11 07:50:56,534 - Step 7: Updated recent_tokens
2025-02-11 07:50:56,536 - Step 7: Found phrase end token
2025-02-11 07:50:56,536 - Step 7: Updated recent_phrases
2025-02-11 07:50:56,536 - Step 7: Decoded current text
2025-02-11 07:50:56,536 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:56,536 - 
Starting step 8
2025-02-11 07:50:56,536 - Current_ids device: cuda:0
2025-02-11 07:50:56,536 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,559 - Model output complete
2025-02-11 07:50:56,559 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:56,559 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,559 - Next token logits device: cuda:0
2025-02-11 07:50:56,559 - Entered do_sample
2025-02-11 07:50:56,559 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,563 - Probs max: 0.8935546875
2025-02-11 07:50:56,564 - Pre-cat
2025-02-11 07:50:56,564 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937,     13]], device='cuda:0')
2025-02-11 07:50:56,566 - Next token: tensor([[425]], device='cuda:0')
2025-02-11 07:50:56,566 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:56,566 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,566 - Step 8: Generated next token
2025-02-11 07:50:56,566 - Step 8: Updated current_ids
2025-02-11 07:50:56,566 - Step 8: Decoded token text:  B
2025-02-11 07:50:56,566 - Step 8: Updated current_phrase
2025-02-11 07:50:56,567 - Step 8: Created step_acts
2025-02-11 07:50:56,567 - Step 8: Added to generation_acts
2025-02-11 07:50:56,567 - Step 8: Updated recent_tokens
2025-02-11 07:50:56,568 - Step 8: Decoded current text
2025-02-11 07:50:56,568 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:56,568 - 
Starting step 9
2025-02-11 07:50:56,568 - Current_ids device: cuda:0
2025-02-11 07:50:56,568 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,597 - Model output complete
2025-02-11 07:50:56,597 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:56,597 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,597 - Next token logits device: cuda:0
2025-02-11 07:50:56,597 - Entered do_sample
2025-02-11 07:50:56,598 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,600 - Probs max: 1.0
2025-02-11 07:50:56,601 - Pre-cat
2025-02-11 07:50:56,601 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937,     13,    425]], device='cuda:0')
2025-02-11 07:50:56,603 - Next token: tensor([[25]], device='cuda:0')
2025-02-11 07:50:56,604 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:56,604 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,604 - Step 9: Generated next token
2025-02-11 07:50:56,604 - Step 9: Updated current_ids
2025-02-11 07:50:56,604 - Step 9: Decoded token text: :
2025-02-11 07:50:56,604 - Step 9: Updated current_phrase
2025-02-11 07:50:56,604 - Step 9: Created step_acts
2025-02-11 07:50:56,605 - Step 9: Added to generation_acts
2025-02-11 07:50:56,605 - Step 9: Updated recent_tokens
2025-02-11 07:50:56,606 - Step 9: Found phrase end token
2025-02-11 07:50:56,606 - Step 9: Updated recent_phrases
2025-02-11 07:50:56,606 - Step 9: Calculated similarity: 0.0
2025-02-11 07:50:56,606 - Step 9: Decoded current text
2025-02-11 07:50:56,606 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:56,606 - 
Starting step 10
2025-02-11 07:50:56,606 - Current_ids device: cuda:0
2025-02-11 07:50:56,606 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,638 - Model output complete
2025-02-11 07:50:56,639 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:56,639 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,639 - Next token logits device: cuda:0
2025-02-11 07:50:56,639 - Entered do_sample
2025-02-11 07:50:56,639 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,641 - Probs max: 0.9814453125
2025-02-11 07:50:56,643 - Pre-cat
2025-02-11 07:50:56,643 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937,     13,    425,     25]], device='cuda:0')
2025-02-11 07:50:56,645 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:50:56,645 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:56,645 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,645 - Step 10: Generated next token
2025-02-11 07:50:56,646 - Step 10: Updated current_ids
2025-02-11 07:50:56,646 - Step 10: Decoded token text:  It
2025-02-11 07:50:56,646 - Step 10: Updated current_phrase
2025-02-11 07:50:56,646 - Step 10: Created step_acts
2025-02-11 07:50:56,646 - Step 10: Added to generation_acts
2025-02-11 07:50:56,648 - Step 10: Updated generated_texts
2025-02-11 07:50:56,648 - Step 10: Updated recent_tokens
2025-02-11 07:50:56,648 - Step 10: Decoded current text
2025-02-11 07:50:56,648 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:56,648 - 
Starting step 11
2025-02-11 07:50:56,648 - Current_ids device: cuda:0
2025-02-11 07:50:56,648 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,674 - Model output complete
2025-02-11 07:50:56,674 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:56,674 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,675 - Next token logits device: cuda:0
2025-02-11 07:50:56,675 - Entered do_sample
2025-02-11 07:50:56,675 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,677 - Probs max: 0.9931640625
2025-02-11 07:50:56,678 - Pre-cat
2025-02-11 07:50:56,678 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937,     13,    425,     25,   1084]], device='cuda:0')
2025-02-11 07:50:56,680 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:56,680 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:56,680 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,680 - Step 11: Generated next token
2025-02-11 07:50:56,680 - Step 11: Updated current_ids
2025-02-11 07:50:56,681 - Step 11: Decoded token text:  will
2025-02-11 07:50:56,681 - Step 11: Updated current_phrase
2025-02-11 07:50:56,681 - Step 11: Created step_acts
2025-02-11 07:50:56,681 - Step 11: Added to generation_acts
2025-02-11 07:50:56,681 - Step 11: Updated recent_tokens
2025-02-11 07:50:56,682 - Step 11: Decoded current text
2025-02-11 07:50:56,683 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:56,683 - 
Starting step 12
2025-02-11 07:50:56,683 - Current_ids device: cuda:0
2025-02-11 07:50:56,683 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,706 - Model output complete
2025-02-11 07:50:56,706 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:56,706 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,706 - Next token logits device: cuda:0
2025-02-11 07:50:56,706 - Entered do_sample
2025-02-11 07:50:56,707 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,710 - Probs max: 0.85595703125
2025-02-11 07:50:56,710 - Pre-cat
2025-02-11 07:50:56,711 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937,     13,    425,     25,   1084,    686]],
       device='cuda:0')
2025-02-11 07:50:56,712 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:50:56,713 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:56,713 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,713 - Step 12: Generated next token
2025-02-11 07:50:56,713 - Step 12: Updated current_ids
2025-02-11 07:50:56,713 - Step 12: Decoded token text:  hit
2025-02-11 07:50:56,713 - Step 12: Updated current_phrase
2025-02-11 07:50:56,713 - Step 12: Created step_acts
2025-02-11 07:50:56,713 - Step 12: Added to generation_acts
2025-02-11 07:50:56,714 - Step 12: Updated recent_tokens
2025-02-11 07:50:56,715 - Step 12: Decoded current text
2025-02-11 07:50:56,715 - Step 12: Reset consecutive_fillers
2025-02-11 07:50:56,715 - 
Starting step 13
2025-02-11 07:50:56,715 - Current_ids device: cuda:0
2025-02-11 07:50:56,715 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,738 - Model output complete
2025-02-11 07:50:56,738 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:56,739 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,739 - Next token logits device: cuda:0
2025-02-11 07:50:56,739 - Entered do_sample
2025-02-11 07:50:56,739 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,742 - Probs max: 0.994140625
2025-02-11 07:50:56,743 - Pre-cat
2025-02-11 07:50:56,743 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937,     13,    425,     25,   1084,    686,   4201]],
       device='cuda:0')
2025-02-11 07:50:56,745 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:56,745 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:56,745 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,745 - Step 13: Generated next token
2025-02-11 07:50:56,745 - Step 13: Updated current_ids
2025-02-11 07:50:56,745 - Step 13: Decoded token text:  the
2025-02-11 07:50:56,745 - Step 13: Updated current_phrase
2025-02-11 07:50:56,746 - Step 13: Created step_acts
2025-02-11 07:50:56,746 - Step 13: Added to generation_acts
2025-02-11 07:50:56,746 - Step 13: Updated recent_tokens
2025-02-11 07:50:56,747 - Step 13: Decoded current text
2025-02-11 07:50:56,747 - Step 13: Reset consecutive_fillers
2025-02-11 07:50:56,747 - 
Starting step 14
2025-02-11 07:50:56,747 - Current_ids device: cuda:0
2025-02-11 07:50:56,747 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,770 - Model output complete
2025-02-11 07:50:56,770 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:50:56,770 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,770 - Next token logits device: cuda:0
2025-02-11 07:50:56,770 - Entered do_sample
2025-02-11 07:50:56,770 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,774 - Probs max: 1.0
2025-02-11 07:50:56,775 - Pre-cat
2025-02-11 07:50:56,775 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937,     13,    425,     25,   1084,    686,   4201,    279]],
       device='cuda:0')
2025-02-11 07:50:56,777 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:56,777 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:50:56,777 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,777 - Step 14: Generated next token
2025-02-11 07:50:56,777 - Step 14: Updated current_ids
2025-02-11 07:50:56,777 - Step 14: Decoded token text:  wall
2025-02-11 07:50:56,777 - Step 14: Updated current_phrase
2025-02-11 07:50:56,778 - Step 14: Created step_acts
2025-02-11 07:50:56,778 - Step 14: Added to generation_acts
2025-02-11 07:50:56,778 - Step 14: Updated recent_tokens
2025-02-11 07:50:56,779 - Step 14: Decoded current text
2025-02-11 07:50:56,779 - Step 14: Reset consecutive_fillers
2025-02-11 07:50:56,779 - 
Starting step 15
2025-02-11 07:50:56,779 - Current_ids device: cuda:0
2025-02-11 07:50:56,779 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,803 - Model output complete
2025-02-11 07:50:56,804 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:50:56,804 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,804 - Next token logits device: cuda:0
2025-02-11 07:50:56,804 - Entered do_sample
2025-02-11 07:50:56,804 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,807 - Probs max: 0.626953125
2025-02-11 07:50:56,808 - Pre-cat
2025-02-11 07:50:56,808 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937,     13,    425,     25,   1084,    686,   4201,    279,
           7002]], device='cuda:0')
2025-02-11 07:50:56,809 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:56,810 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:50:56,810 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,810 - Step 15: Generated next token
2025-02-11 07:50:56,810 - Step 15: Updated current_ids
2025-02-11 07:50:56,810 - Step 15: Decoded token text:  very
2025-02-11 07:50:56,810 - Step 15: Updated current_phrase
2025-02-11 07:50:56,811 - Step 15: Created step_acts
2025-02-11 07:50:56,811 - Step 15: Added to generation_acts
2025-02-11 07:50:56,812 - Step 15: Updated generated_texts
2025-02-11 07:50:56,812 - Step 15: Updated recent_tokens
2025-02-11 07:50:56,812 - Step 15: Decoded current text
2025-02-11 07:50:56,812 - Step 15: Reset consecutive_fillers
2025-02-11 07:50:56,812 - 
Starting step 16
2025-02-11 07:50:56,812 - Current_ids device: cuda:0
2025-02-11 07:50:56,813 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,854 - Model output complete
2025-02-11 07:50:56,854 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:50:56,854 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,854 - Next token logits device: cuda:0
2025-02-11 07:50:56,854 - Entered do_sample
2025-02-11 07:50:56,855 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,856 - Probs max: 0.253662109375
2025-02-11 07:50:56,858 - Pre-cat
2025-02-11 07:50:56,858 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937,     13,    425,     25,   1084,    686,   4201,    279,
           7002,   1602]], device='cuda:0')
2025-02-11 07:50:56,863 - Next token: tensor([[3265]], device='cuda:0')
2025-02-11 07:50:56,864 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:50:56,864 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,864 - Step 16: Generated next token
2025-02-11 07:50:56,864 - Step 16: Updated current_ids
2025-02-11 07:50:56,864 - Step 16: Decoded token text:  close
2025-02-11 07:50:56,865 - Step 16: Updated current_phrase
2025-02-11 07:50:56,865 - Step 16: Created step_acts
2025-02-11 07:50:56,866 - Step 16: Added to generation_acts
2025-02-11 07:50:56,866 - Step 16: Updated recent_tokens
2025-02-11 07:50:56,867 - Step 16: Decoded current text
2025-02-11 07:50:56,867 - Step 16: Reset consecutive_fillers
2025-02-11 07:50:56,868 - Step 16: Calculated unique_ratio: 0.6875
2025-02-11 07:50:56,868 - 
Starting step 17
2025-02-11 07:50:56,868 - Current_ids device: cuda:0
2025-02-11 07:50:56,868 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,907 - Model output complete
2025-02-11 07:50:56,907 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:50:56,907 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,907 - Next token logits device: cuda:0
2025-02-11 07:50:56,907 - Entered do_sample
2025-02-11 07:50:56,908 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,909 - Probs max: 0.9736328125
2025-02-11 07:50:56,910 - Pre-cat
2025-02-11 07:50:56,910 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937,     13,    425,     25,   1084,    686,   4201,    279,
           7002,   1602,   3265]], device='cuda:0')
2025-02-11 07:50:56,914 - Next token: tensor([[311]], device='cuda:0')
2025-02-11 07:50:56,914 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:50:56,914 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,914 - Step 17: Generated next token
2025-02-11 07:50:56,914 - Step 17: Updated current_ids
2025-02-11 07:50:56,915 - Step 17: Decoded token text:  to
2025-02-11 07:50:56,915 - Step 17: Updated current_phrase
2025-02-11 07:50:56,915 - Step 17: Created step_acts
2025-02-11 07:50:56,915 - Step 17: Added to generation_acts
2025-02-11 07:50:56,915 - Step 17: Updated recent_tokens
2025-02-11 07:50:56,917 - Step 17: Decoded current text
2025-02-11 07:50:56,917 - Step 17: Reset consecutive_fillers
2025-02-11 07:50:56,917 - Step 17: Calculated unique_ratio: 0.75
2025-02-11 07:50:56,917 - 
Starting step 18
2025-02-11 07:50:56,917 - Current_ids device: cuda:0
2025-02-11 07:50:56,917 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,940 - Model output complete
2025-02-11 07:50:56,940 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:50:56,940 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,940 - Next token logits device: cuda:0
2025-02-11 07:50:56,940 - Entered do_sample
2025-02-11 07:50:56,940 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,944 - Probs max: 0.95751953125
2025-02-11 07:50:56,945 - Pre-cat
2025-02-11 07:50:56,945 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937,     13,    425,     25,   1084,    686,   4201,    279,
           7002,   1602,   3265,    311]], device='cuda:0')
2025-02-11 07:50:56,947 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:56,947 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:50:56,947 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,947 - Step 18: Generated next token
2025-02-11 07:50:56,947 - Step 18: Updated current_ids
2025-02-11 07:50:56,947 - Step 18: Decoded token text:  the
2025-02-11 07:50:56,947 - Step 18: Updated current_phrase
2025-02-11 07:50:56,948 - Step 18: Created step_acts
2025-02-11 07:50:56,948 - Step 18: Added to generation_acts
2025-02-11 07:50:56,948 - Step 18: Updated recent_tokens
2025-02-11 07:50:56,950 - Step 18: Decoded current text
2025-02-11 07:50:56,950 - Step 18: Incremented consecutive_fillers to 1
2025-02-11 07:50:56,950 - Step 18: Calculated unique_ratio: 0.75
2025-02-11 07:50:56,950 - 
Starting step 19
2025-02-11 07:50:56,950 - Current_ids device: cuda:0
2025-02-11 07:50:56,950 - Current_ids dtype: torch.int64
2025-02-11 07:50:56,973 - Model output complete
2025-02-11 07:50:56,974 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:50:56,974 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,974 - Next token logits device: cuda:0
2025-02-11 07:50:56,974 - Entered do_sample
2025-02-11 07:50:56,974 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:56,977 - Probs max: 0.9521484375
2025-02-11 07:50:56,977 - Pre-cat
2025-02-11 07:50:56,977 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937,     13,    425,     25,   1084,    686,   4201,    279,
           7002,   1602,   3265,    311,    279]], device='cuda:0')
2025-02-11 07:50:56,979 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:56,979 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:50:56,979 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:56,980 - Step 19: Generated next token
2025-02-11 07:50:56,980 - Step 19: Updated current_ids
2025-02-11 07:50:56,980 - Step 19: Decoded token text:  wall
2025-02-11 07:50:56,980 - Step 19: Updated current_phrase
2025-02-11 07:50:56,980 - Step 19: Created step_acts
2025-02-11 07:50:56,980 - Step 19: Added to generation_acts
2025-02-11 07:50:56,980 - Step 19: Updated recent_tokens
2025-02-11 07:50:56,982 - Step 19: Decoded current text
2025-02-11 07:50:56,982 - Step 19: Incremented consecutive_fillers to 2
2025-02-11 07:50:56,982 - Step 19: Calculated unique_ratio: 0.75
2025-02-11 07:50:56,982 - 
Starting step 20
2025-02-11 07:50:56,982 - Current_ids device: cuda:0
2025-02-11 07:50:56,982 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,005 - Model output complete
2025-02-11 07:50:57,005 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:50:57,005 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,005 - Next token logits device: cuda:0
2025-02-11 07:50:57,005 - Entered do_sample
2025-02-11 07:50:57,005 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,008 - Probs max: 0.94482421875
2025-02-11 07:50:57,008 - Pre-cat
2025-02-11 07:50:57,009 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
           1602,   4937,     13,    425,     25,   1084,    686,   4201,    279,
           7002,   1602,   3265,    311,    279,   7002]], device='cuda:0')
2025-02-11 07:50:57,010 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:57,011 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:50:57,011 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,011 - Step 20: Generated next token
2025-02-11 07:50:57,011 - Step 20: Updated current_ids
2025-02-11 07:50:57,011 - Step 20: Decoded token text: .
2025-02-11 07:50:57,011 - Step 20: Updated current_phrase
2025-02-11 07:50:57,011 - Step 20: Created step_acts
2025-02-11 07:50:57,011 - Step 20: Added to generation_acts
2025-02-11 07:50:57,013 - Step 20: Updated generated_texts
2025-02-11 07:50:57,013 - Step 20: Updated recent_tokens
2025-02-11 07:50:57,013 - Step 20: Found phrase end token
2025-02-11 07:50:57,013 - Step 20: Updated recent_phrases
2025-02-11 07:50:57,013 - Step 20: Calculated similarity: 0.0
2025-02-11 07:50:57,013 - Step 20: Decoded current text
2025-02-11 07:50:57,013 - Step 20: Incremented consecutive_fillers to 3
2025-02-11 07:50:57,109 - 
Starting step 0
2025-02-11 07:50:57,109 - Current_ids device: cuda:0
2025-02-11 07:50:57,109 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,135 - Model output complete
2025-02-11 07:50:57,135 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:57,135 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,135 - Next token logits device: cuda:0
2025-02-11 07:50:57,135 - Entered do_sample
2025-02-11 07:50:57,135 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,138 - Probs max: 0.50341796875
2025-02-11 07:50:57,138 - Pre-cat
2025-02-11 07:50:57,138 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:57,140 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:50:57,140 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:57,140 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,140 - Step 0: Generated next token
2025-02-11 07:50:57,140 - Step 0: Updated current_ids
2025-02-11 07:50:57,140 - Step 0: Decoded token text:  It
2025-02-11 07:50:57,140 - Step 0: Updated current_phrase
2025-02-11 07:50:57,141 - Step 0: Created step_acts
2025-02-11 07:50:57,141 - Step 0: Added to generation_acts
2025-02-11 07:50:57,142 - Step 0: Updated generated_texts
2025-02-11 07:50:57,142 - Step 0: Updated recent_tokens
2025-02-11 07:50:57,143 - Step 0: Decoded current text
2025-02-11 07:50:57,143 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:57,143 - 
Starting step 1
2025-02-11 07:50:57,143 - Current_ids device: cuda:0
2025-02-11 07:50:57,143 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,169 - Model output complete
2025-02-11 07:50:57,169 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:57,169 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,169 - Next token logits device: cuda:0
2025-02-11 07:50:57,169 - Entered do_sample
2025-02-11 07:50:57,169 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,171 - Probs max: 0.9384765625
2025-02-11 07:50:57,172 - Pre-cat
2025-02-11 07:50:57,172 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084]], device='cuda:0')
2025-02-11 07:50:57,174 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:57,174 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:57,174 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,174 - Step 1: Generated next token
2025-02-11 07:50:57,174 - Step 1: Updated current_ids
2025-02-11 07:50:57,174 - Step 1: Decoded token text:  will
2025-02-11 07:50:57,174 - Step 1: Updated current_phrase
2025-02-11 07:50:57,175 - Step 1: Created step_acts
2025-02-11 07:50:57,175 - Step 1: Added to generation_acts
2025-02-11 07:50:57,175 - Step 1: Updated recent_tokens
2025-02-11 07:50:57,176 - Step 1: Decoded current text
2025-02-11 07:50:57,176 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:57,176 - 
Starting step 2
2025-02-11 07:50:57,176 - Current_ids device: cuda:0
2025-02-11 07:50:57,177 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,199 - Model output complete
2025-02-11 07:50:57,199 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:57,199 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,199 - Next token logits device: cuda:0
2025-02-11 07:50:57,199 - Entered do_sample
2025-02-11 07:50:57,199 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,202 - Probs max: 0.2442626953125
2025-02-11 07:50:57,203 - Pre-cat
2025-02-11 07:50:57,203 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686]], device='cuda:0')
2025-02-11 07:50:57,204 - Next token: tensor([[1438]], device='cuda:0')
2025-02-11 07:50:57,205 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:57,205 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,205 - Step 2: Generated next token
2025-02-11 07:50:57,205 - Step 2: Updated current_ids
2025-02-11 07:50:57,205 - Step 2: Decoded token text:  break
2025-02-11 07:50:57,205 - Step 2: Updated current_phrase
2025-02-11 07:50:57,205 - Step 2: Created step_acts
2025-02-11 07:50:57,205 - Step 2: Added to generation_acts
2025-02-11 07:50:57,205 - Step 2: Updated recent_tokens
2025-02-11 07:50:57,207 - Step 2: Decoded current text
2025-02-11 07:50:57,207 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:57,207 - 
Starting step 3
2025-02-11 07:50:57,207 - Current_ids device: cuda:0
2025-02-11 07:50:57,207 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,230 - Model output complete
2025-02-11 07:50:57,230 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:57,230 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,230 - Next token logits device: cuda:0
2025-02-11 07:50:57,230 - Entered do_sample
2025-02-11 07:50:57,230 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,232 - Probs max: 0.5283203125
2025-02-11 07:50:57,233 - Pre-cat
2025-02-11 07:50:57,233 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1438]],
       device='cuda:0')
2025-02-11 07:50:57,234 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:57,235 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:57,235 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,235 - Step 3: Generated next token
2025-02-11 07:50:57,235 - Step 3: Updated current_ids
2025-02-11 07:50:57,235 - Step 3: Decoded token text: .
2025-02-11 07:50:57,235 - Step 3: Updated current_phrase
2025-02-11 07:50:57,236 - Step 3: Created step_acts
2025-02-11 07:50:57,236 - Step 3: Added to generation_acts
2025-02-11 07:50:57,236 - Step 3: Updated recent_tokens
2025-02-11 07:50:57,237 - Step 3: Found phrase end token
2025-02-11 07:50:57,237 - Step 3: Updated recent_phrases
2025-02-11 07:50:57,237 - Step 3: Decoded current text
2025-02-11 07:50:57,237 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:57,237 - 
Starting step 4
2025-02-11 07:50:57,237 - Current_ids device: cuda:0
2025-02-11 07:50:57,237 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,260 - Model output complete
2025-02-11 07:50:57,260 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:57,260 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,260 - Next token logits device: cuda:0
2025-02-11 07:50:57,260 - Entered do_sample
2025-02-11 07:50:57,260 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,262 - Probs max: 0.6591796875
2025-02-11 07:50:57,263 - Pre-cat
2025-02-11 07:50:57,263 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1438,     13]],
       device='cuda:0')
2025-02-11 07:50:57,264 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:50:57,265 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:57,265 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,265 - Step 4: Generated next token
2025-02-11 07:50:57,265 - Step 4: Updated current_ids
2025-02-11 07:50:57,265 - Step 4: Decoded token text:  It
2025-02-11 07:50:57,265 - Step 4: Updated current_phrase
2025-02-11 07:50:57,265 - Step 4: Created step_acts
2025-02-11 07:50:57,266 - Step 4: Added to generation_acts
2025-02-11 07:50:57,266 - Step 4: Updated recent_tokens
2025-02-11 07:50:57,267 - Step 4: Decoded current text
2025-02-11 07:50:57,267 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:57,267 - 
Starting step 5
2025-02-11 07:50:57,267 - Current_ids device: cuda:0
2025-02-11 07:50:57,267 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,290 - Model output complete
2025-02-11 07:50:57,290 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:57,290 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,290 - Next token logits device: cuda:0
2025-02-11 07:50:57,290 - Entered do_sample
2025-02-11 07:50:57,291 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,293 - Probs max: 0.95703125
2025-02-11 07:50:57,293 - Pre-cat
2025-02-11 07:50:57,293 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1438,     13,   1084]],
       device='cuda:0')
2025-02-11 07:50:57,295 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:57,295 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:57,295 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,295 - Step 5: Generated next token
2025-02-11 07:50:57,295 - Step 5: Updated current_ids
2025-02-11 07:50:57,295 - Step 5: Decoded token text:  will
2025-02-11 07:50:57,295 - Step 5: Updated current_phrase
2025-02-11 07:50:57,296 - Step 5: Created step_acts
2025-02-11 07:50:57,296 - Step 5: Added to generation_acts
2025-02-11 07:50:57,297 - Step 5: Updated generated_texts
2025-02-11 07:50:57,297 - Step 5: Updated recent_tokens
2025-02-11 07:50:57,297 - Step 5: Decoded current text
2025-02-11 07:50:57,298 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:57,298 - 
Starting step 6
2025-02-11 07:50:57,298 - Current_ids device: cuda:0
2025-02-11 07:50:57,298 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,320 - Model output complete
2025-02-11 07:50:57,321 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:57,321 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,321 - Next token logits device: cuda:0
2025-02-11 07:50:57,321 - Entered do_sample
2025-02-11 07:50:57,321 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,323 - Probs max: 0.689453125
2025-02-11 07:50:57,324 - Pre-cat
2025-02-11 07:50:57,324 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1438,     13,   1084,
            686]], device='cuda:0')
2025-02-11 07:50:57,326 - Next token: tensor([[1438]], device='cuda:0')
2025-02-11 07:50:57,326 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:57,326 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,326 - Step 6: Generated next token
2025-02-11 07:50:57,326 - Step 6: Updated current_ids
2025-02-11 07:50:57,326 - Step 6: Decoded token text:  break
2025-02-11 07:50:57,326 - Step 6: Updated current_phrase
2025-02-11 07:50:57,327 - Step 6: Created step_acts
2025-02-11 07:50:57,327 - Step 6: Added to generation_acts
2025-02-11 07:50:57,327 - Step 6: Updated recent_tokens
2025-02-11 07:50:57,328 - Step 6: Decoded current text
2025-02-11 07:50:57,328 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:57,328 - 
Starting step 7
2025-02-11 07:50:57,328 - Current_ids device: cuda:0
2025-02-11 07:50:57,328 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,351 - Model output complete
2025-02-11 07:50:57,351 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:57,351 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,351 - Next token logits device: cuda:0
2025-02-11 07:50:57,351 - Entered do_sample
2025-02-11 07:50:57,351 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,354 - Probs max: 0.68310546875
2025-02-11 07:50:57,355 - Pre-cat
2025-02-11 07:50:57,355 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1438,     13,   1084,
            686,   1438]], device='cuda:0')
2025-02-11 07:50:57,357 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:57,357 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:57,357 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,357 - Step 7: Generated next token
2025-02-11 07:50:57,358 - Step 7: Updated current_ids
2025-02-11 07:50:57,358 - Step 7: Decoded token text: .
2025-02-11 07:50:57,358 - Step 7: Updated current_phrase
2025-02-11 07:50:57,358 - Step 7: Created step_acts
2025-02-11 07:50:57,358 - Step 7: Added to generation_acts
2025-02-11 07:50:57,358 - Step 7: Updated recent_tokens
2025-02-11 07:50:57,360 - Step 7: Found phrase end token
2025-02-11 07:50:57,360 - Step 7: Updated recent_phrases
2025-02-11 07:50:57,360 - Step 7: Calculated similarity: 1.0
2025-02-11 07:50:57,456 - 
Starting step 0
2025-02-11 07:50:57,456 - Current_ids device: cuda:0
2025-02-11 07:50:57,457 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,482 - Model output complete
2025-02-11 07:50:57,483 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:57,483 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,483 - Next token logits device: cuda:0
2025-02-11 07:50:57,483 - Entered do_sample
2025-02-11 07:50:57,483 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,485 - Probs max: 0.50341796875
2025-02-11 07:50:57,486 - Pre-cat
2025-02-11 07:50:57,486 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:57,487 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:57,488 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:57,488 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,488 - Step 0: Generated next token
2025-02-11 07:50:57,488 - Step 0: Updated current_ids
2025-02-11 07:50:57,488 - Step 0: Decoded token text:  The
2025-02-11 07:50:57,488 - Step 0: Updated current_phrase
2025-02-11 07:50:57,488 - Step 0: Created step_acts
2025-02-11 07:50:57,488 - Step 0: Added to generation_acts
2025-02-11 07:50:57,490 - Step 0: Updated generated_texts
2025-02-11 07:50:57,490 - Step 0: Updated recent_tokens
2025-02-11 07:50:57,490 - Step 0: Decoded current text
2025-02-11 07:50:57,490 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:57,490 - 
Starting step 1
2025-02-11 07:50:57,490 - Current_ids device: cuda:0
2025-02-11 07:50:57,490 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,516 - Model output complete
2025-02-11 07:50:57,516 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:57,516 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,516 - Next token logits device: cuda:0
2025-02-11 07:50:57,516 - Entered do_sample
2025-02-11 07:50:57,516 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,519 - Probs max: 0.83837890625
2025-02-11 07:50:57,519 - Pre-cat
2025-02-11 07:50:57,519 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:50:57,521 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:57,521 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:57,521 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,521 - Step 1: Generated next token
2025-02-11 07:50:57,521 - Step 1: Updated current_ids
2025-02-11 07:50:57,521 - Step 1: Decoded token text:  ball
2025-02-11 07:50:57,522 - Step 1: Updated current_phrase
2025-02-11 07:50:57,522 - Step 1: Created step_acts
2025-02-11 07:50:57,522 - Step 1: Added to generation_acts
2025-02-11 07:50:57,522 - Step 1: Updated recent_tokens
2025-02-11 07:50:57,523 - Step 1: Decoded current text
2025-02-11 07:50:57,523 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:57,523 - 
Starting step 2
2025-02-11 07:50:57,524 - Current_ids device: cuda:0
2025-02-11 07:50:57,524 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,546 - Model output complete
2025-02-11 07:50:57,547 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:57,547 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,547 - Next token logits device: cuda:0
2025-02-11 07:50:57,547 - Entered do_sample
2025-02-11 07:50:57,547 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,549 - Probs max: 0.583984375
2025-02-11 07:50:57,550 - Pre-cat
2025-02-11 07:50:57,550 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:50:57,552 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:57,552 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:57,552 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,552 - Step 2: Generated next token
2025-02-11 07:50:57,553 - Step 2: Updated current_ids
2025-02-11 07:50:57,553 - Step 2: Decoded token text:  is
2025-02-11 07:50:57,553 - Step 2: Updated current_phrase
2025-02-11 07:50:57,553 - Step 2: Created step_acts
2025-02-11 07:50:57,553 - Step 2: Added to generation_acts
2025-02-11 07:50:57,553 - Step 2: Updated recent_tokens
2025-02-11 07:50:57,555 - Step 2: Decoded current text
2025-02-11 07:50:57,555 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:57,555 - 
Starting step 3
2025-02-11 07:50:57,555 - Current_ids device: cuda:0
2025-02-11 07:50:57,555 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,586 - Model output complete
2025-02-11 07:50:57,587 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:57,587 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,587 - Next token logits device: cuda:0
2025-02-11 07:50:57,587 - Entered do_sample
2025-02-11 07:50:57,587 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,589 - Probs max: 0.59521484375
2025-02-11 07:50:57,591 - Pre-cat
2025-02-11 07:50:57,591 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374]],
       device='cuda:0')
2025-02-11 07:50:57,593 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:50:57,594 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:57,594 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,594 - Step 3: Generated next token
2025-02-11 07:50:57,594 - Step 3: Updated current_ids
2025-02-11 07:50:57,594 - Step 3: Decoded token text:  thrown
2025-02-11 07:50:57,594 - Step 3: Updated current_phrase
2025-02-11 07:50:57,595 - Step 3: Created step_acts
2025-02-11 07:50:57,595 - Step 3: Added to generation_acts
2025-02-11 07:50:57,595 - Step 3: Updated recent_tokens
2025-02-11 07:50:57,596 - Step 3: Decoded current text
2025-02-11 07:50:57,596 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:57,596 - 
Starting step 4
2025-02-11 07:50:57,596 - Current_ids device: cuda:0
2025-02-11 07:50:57,596 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,624 - Model output complete
2025-02-11 07:50:57,624 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:57,624 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,624 - Next token logits device: cuda:0
2025-02-11 07:50:57,624 - Entered do_sample
2025-02-11 07:50:57,624 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,626 - Probs max: 0.67431640625
2025-02-11 07:50:57,627 - Pre-cat
2025-02-11 07:50:57,627 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:50:57,630 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:50:57,630 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:57,630 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,631 - Step 4: Generated next token
2025-02-11 07:50:57,631 - Step 4: Updated current_ids
2025-02-11 07:50:57,631 - Step 4: Decoded token text:  at
2025-02-11 07:50:57,631 - Step 4: Updated current_phrase
2025-02-11 07:50:57,631 - Step 4: Created step_acts
2025-02-11 07:50:57,631 - Step 4: Added to generation_acts
2025-02-11 07:50:57,631 - Step 4: Updated recent_tokens
2025-02-11 07:50:57,633 - Step 4: Decoded current text
2025-02-11 07:50:57,633 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:57,633 - 
Starting step 5
2025-02-11 07:50:57,633 - Current_ids device: cuda:0
2025-02-11 07:50:57,633 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,663 - Model output complete
2025-02-11 07:50:57,664 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:57,664 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,664 - Next token logits device: cuda:0
2025-02-11 07:50:57,664 - Entered do_sample
2025-02-11 07:50:57,664 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,666 - Probs max: 0.58251953125
2025-02-11 07:50:57,667 - Pre-cat
2025-02-11 07:50:57,667 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518]],
       device='cuda:0')
2025-02-11 07:50:57,668 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:57,668 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:57,668 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,669 - Step 5: Generated next token
2025-02-11 07:50:57,669 - Step 5: Updated current_ids
2025-02-11 07:50:57,669 - Step 5: Decoded token text:  a
2025-02-11 07:50:57,669 - Step 5: Updated current_phrase
2025-02-11 07:50:57,669 - Step 5: Created step_acts
2025-02-11 07:50:57,669 - Step 5: Added to generation_acts
2025-02-11 07:50:57,670 - Step 5: Updated generated_texts
2025-02-11 07:50:57,671 - Step 5: Updated recent_tokens
2025-02-11 07:50:57,671 - Step 5: Decoded current text
2025-02-11 07:50:57,671 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:57,671 - 
Starting step 6
2025-02-11 07:50:57,671 - Current_ids device: cuda:0
2025-02-11 07:50:57,671 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,694 - Model output complete
2025-02-11 07:50:57,694 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:57,695 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,695 - Next token logits device: cuda:0
2025-02-11 07:50:57,695 - Entered do_sample
2025-02-11 07:50:57,695 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,697 - Probs max: 0.9208984375
2025-02-11 07:50:57,698 - Pre-cat
2025-02-11 07:50:57,698 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264]], device='cuda:0')
2025-02-11 07:50:57,699 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:57,699 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:57,699 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,699 - Step 6: Generated next token
2025-02-11 07:50:57,699 - Step 6: Updated current_ids
2025-02-11 07:50:57,700 - Step 6: Decoded token text:  wall
2025-02-11 07:50:57,700 - Step 6: Updated current_phrase
2025-02-11 07:50:57,700 - Step 6: Created step_acts
2025-02-11 07:50:57,700 - Step 6: Added to generation_acts
2025-02-11 07:50:57,700 - Step 6: Updated recent_tokens
2025-02-11 07:50:57,701 - Step 6: Decoded current text
2025-02-11 07:50:57,701 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:57,701 - 
Starting step 7
2025-02-11 07:50:57,702 - Current_ids device: cuda:0
2025-02-11 07:50:57,702 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,726 - Model output complete
2025-02-11 07:50:57,726 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:57,726 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,726 - Next token logits device: cuda:0
2025-02-11 07:50:57,726 - Entered do_sample
2025-02-11 07:50:57,726 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,728 - Probs max: 0.96826171875
2025-02-11 07:50:57,729 - Pre-cat
2025-02-11 07:50:57,729 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002]], device='cuda:0')
2025-02-11 07:50:57,730 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:57,731 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:57,731 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,731 - Step 7: Generated next token
2025-02-11 07:50:57,731 - Step 7: Updated current_ids
2025-02-11 07:50:57,731 - Step 7: Decoded token text:  very
2025-02-11 07:50:57,731 - Step 7: Updated current_phrase
2025-02-11 07:50:57,731 - Step 7: Created step_acts
2025-02-11 07:50:57,731 - Step 7: Added to generation_acts
2025-02-11 07:50:57,732 - Step 7: Updated recent_tokens
2025-02-11 07:50:57,733 - Step 7: Decoded current text
2025-02-11 07:50:57,733 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:57,733 - 
Starting step 8
2025-02-11 07:50:57,733 - Current_ids device: cuda:0
2025-02-11 07:50:57,733 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,756 - Model output complete
2025-02-11 07:50:57,756 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:57,756 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,757 - Next token logits device: cuda:0
2025-02-11 07:50:57,757 - Entered do_sample
2025-02-11 07:50:57,757 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,759 - Probs max: 0.99609375
2025-02-11 07:50:57,760 - Pre-cat
2025-02-11 07:50:57,760 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602]], device='cuda:0')
2025-02-11 07:50:57,761 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:57,761 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:57,761 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,762 - Step 8: Generated next token
2025-02-11 07:50:57,762 - Step 8: Updated current_ids
2025-02-11 07:50:57,762 - Step 8: Decoded token text:  fast
2025-02-11 07:50:57,762 - Step 8: Updated current_phrase
2025-02-11 07:50:57,762 - Step 8: Created step_acts
2025-02-11 07:50:57,762 - Step 8: Added to generation_acts
2025-02-11 07:50:57,762 - Step 8: Updated recent_tokens
2025-02-11 07:50:57,764 - Step 8: Decoded current text
2025-02-11 07:50:57,764 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:57,764 - 
Starting step 9
2025-02-11 07:50:57,764 - Current_ids device: cuda:0
2025-02-11 07:50:57,764 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,787 - Model output complete
2025-02-11 07:50:57,787 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:57,787 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,787 - Next token logits device: cuda:0
2025-02-11 07:50:57,788 - Entered do_sample
2025-02-11 07:50:57,788 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,790 - Probs max: 0.544921875
2025-02-11 07:50:57,791 - Pre-cat
2025-02-11 07:50:57,791 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:50:57,792 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:57,793 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:57,793 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,793 - Step 9: Generated next token
2025-02-11 07:50:57,793 - Step 9: Updated current_ids
2025-02-11 07:50:57,793 - Step 9: Decoded token text: .
2025-02-11 07:50:57,793 - Step 9: Updated current_phrase
2025-02-11 07:50:57,793 - Step 9: Created step_acts
2025-02-11 07:50:57,793 - Step 9: Added to generation_acts
2025-02-11 07:50:57,794 - Step 9: Updated recent_tokens
2025-02-11 07:50:57,795 - Step 9: Found phrase end token
2025-02-11 07:50:57,795 - Step 9: Updated recent_phrases
2025-02-11 07:50:57,795 - Step 9: Decoded current text
2025-02-11 07:50:57,795 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:57,795 - 
Starting step 10
2025-02-11 07:50:57,795 - Current_ids device: cuda:0
2025-02-11 07:50:57,795 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,817 - Model output complete
2025-02-11 07:50:57,818 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:57,818 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,818 - Next token logits device: cuda:0
2025-02-11 07:50:57,818 - Entered do_sample
2025-02-11 07:50:57,818 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,821 - Probs max: 0.8466796875
2025-02-11 07:50:57,821 - Pre-cat
2025-02-11 07:50:57,821 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13]], device='cuda:0')
2025-02-11 07:50:57,823 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:57,823 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:57,823 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,823 - Step 10: Generated next token
2025-02-11 07:50:57,824 - Step 10: Updated current_ids
2025-02-11 07:50:57,824 - Step 10: Decoded token text:  The
2025-02-11 07:50:57,824 - Step 10: Updated current_phrase
2025-02-11 07:50:57,824 - Step 10: Created step_acts
2025-02-11 07:50:57,824 - Step 10: Added to generation_acts
2025-02-11 07:50:57,826 - Step 10: Updated generated_texts
2025-02-11 07:50:57,826 - Step 10: Updated recent_tokens
2025-02-11 07:50:57,826 - Step 10: Decoded current text
2025-02-11 07:50:57,826 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:57,826 - 
Starting step 11
2025-02-11 07:50:57,826 - Current_ids device: cuda:0
2025-02-11 07:50:57,826 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,850 - Model output complete
2025-02-11 07:50:57,850 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:57,850 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,850 - Next token logits device: cuda:0
2025-02-11 07:50:57,850 - Entered do_sample
2025-02-11 07:50:57,850 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,853 - Probs max: 0.68896484375
2025-02-11 07:50:57,853 - Pre-cat
2025-02-11 07:50:57,853 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576]], device='cuda:0')
2025-02-11 07:50:57,855 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:57,856 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:57,856 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,856 - Step 11: Generated next token
2025-02-11 07:50:57,856 - Step 11: Updated current_ids
2025-02-11 07:50:57,856 - Step 11: Decoded token text:  ball
2025-02-11 07:50:57,856 - Step 11: Updated current_phrase
2025-02-11 07:50:57,856 - Step 11: Created step_acts
2025-02-11 07:50:57,856 - Step 11: Added to generation_acts
2025-02-11 07:50:57,857 - Step 11: Updated recent_tokens
2025-02-11 07:50:57,858 - Step 11: Decoded current text
2025-02-11 07:50:57,858 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:57,858 - 
Starting step 12
2025-02-11 07:50:57,858 - Current_ids device: cuda:0
2025-02-11 07:50:57,858 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,881 - Model output complete
2025-02-11 07:50:57,881 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:57,881 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,881 - Next token logits device: cuda:0
2025-02-11 07:50:57,881 - Entered do_sample
2025-02-11 07:50:57,881 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,884 - Probs max: 0.85888671875
2025-02-11 07:50:57,885 - Pre-cat
2025-02-11 07:50:57,885 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576,   4935]],
       device='cuda:0')
2025-02-11 07:50:57,887 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:57,888 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:57,888 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,888 - Step 12: Generated next token
2025-02-11 07:50:57,888 - Step 12: Updated current_ids
2025-02-11 07:50:57,888 - Step 12: Decoded token text:  is
2025-02-11 07:50:57,888 - Step 12: Updated current_phrase
2025-02-11 07:50:57,888 - Step 12: Created step_acts
2025-02-11 07:50:57,888 - Step 12: Added to generation_acts
2025-02-11 07:50:57,889 - Step 12: Updated recent_tokens
2025-02-11 07:50:57,890 - Step 12: Decoded current text
2025-02-11 07:50:57,890 - Step 12: Reset consecutive_fillers
2025-02-11 07:50:57,890 - 
Starting step 13
2025-02-11 07:50:57,890 - Current_ids device: cuda:0
2025-02-11 07:50:57,890 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,912 - Model output complete
2025-02-11 07:50:57,913 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:57,913 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,913 - Next token logits device: cuda:0
2025-02-11 07:50:57,913 - Entered do_sample
2025-02-11 07:50:57,913 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,916 - Probs max: 0.654296875
2025-02-11 07:50:57,916 - Pre-cat
2025-02-11 07:50:57,916 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576,   4935,    374]],
       device='cuda:0')
2025-02-11 07:50:57,918 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:50:57,918 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:57,918 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,918 - Step 13: Generated next token
2025-02-11 07:50:57,918 - Step 13: Updated current_ids
2025-02-11 07:50:57,918 - Step 13: Decoded token text:  thrown
2025-02-11 07:50:57,918 - Step 13: Updated current_phrase
2025-02-11 07:50:57,919 - Step 13: Created step_acts
2025-02-11 07:50:57,919 - Step 13: Added to generation_acts
2025-02-11 07:50:57,919 - Step 13: Updated recent_tokens
2025-02-11 07:50:57,920 - Step 13: Decoded current text
2025-02-11 07:50:57,920 - Step 13: Reset consecutive_fillers
2025-02-11 07:50:57,920 - 
Starting step 14
2025-02-11 07:50:57,920 - Current_ids device: cuda:0
2025-02-11 07:50:57,920 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,943 - Model output complete
2025-02-11 07:50:57,943 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:50:57,944 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,944 - Next token logits device: cuda:0
2025-02-11 07:50:57,944 - Entered do_sample
2025-02-11 07:50:57,944 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,946 - Probs max: 0.875
2025-02-11 07:50:57,946 - Pre-cat
2025-02-11 07:50:57,947 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:50:57,948 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:50:57,948 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:50:57,948 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,949 - Step 14: Generated next token
2025-02-11 07:50:57,949 - Step 14: Updated current_ids
2025-02-11 07:50:57,949 - Step 14: Decoded token text:  at
2025-02-11 07:50:57,949 - Step 14: Updated current_phrase
2025-02-11 07:50:57,949 - Step 14: Created step_acts
2025-02-11 07:50:57,949 - Step 14: Added to generation_acts
2025-02-11 07:50:57,949 - Step 14: Updated recent_tokens
2025-02-11 07:50:57,951 - Step 14: Decoded current text
2025-02-11 07:50:57,951 - Step 14: Reset consecutive_fillers
2025-02-11 07:50:57,951 - 
Starting step 15
2025-02-11 07:50:57,951 - Current_ids device: cuda:0
2025-02-11 07:50:57,951 - Current_ids dtype: torch.int64
2025-02-11 07:50:57,974 - Model output complete
2025-02-11 07:50:57,974 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:50:57,974 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,974 - Next token logits device: cuda:0
2025-02-11 07:50:57,974 - Entered do_sample
2025-02-11 07:50:57,974 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:57,977 - Probs max: 0.94482421875
2025-02-11 07:50:57,977 - Pre-cat
2025-02-11 07:50:57,977 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:50:57,979 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:57,979 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:50:57,979 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:57,979 - Step 15: Generated next token
2025-02-11 07:50:57,979 - Step 15: Updated current_ids
2025-02-11 07:50:57,979 - Step 15: Decoded token text:  a
2025-02-11 07:50:57,980 - Step 15: Updated current_phrase
2025-02-11 07:50:57,980 - Step 15: Created step_acts
2025-02-11 07:50:57,980 - Step 15: Added to generation_acts
2025-02-11 07:50:57,981 - Step 15: Updated generated_texts
2025-02-11 07:50:57,981 - Step 15: Updated recent_tokens
2025-02-11 07:50:57,981 - Step 15: Decoded current text
2025-02-11 07:50:57,982 - Step 15: Reset consecutive_fillers
2025-02-11 07:50:57,982 - 
Starting step 16
2025-02-11 07:50:57,982 - Current_ids device: cuda:0
2025-02-11 07:50:57,982 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,006 - Model output complete
2025-02-11 07:50:58,007 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:50:58,007 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,007 - Next token logits device: cuda:0
2025-02-11 07:50:58,007 - Entered do_sample
2025-02-11 07:50:58,007 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,009 - Probs max: 0.99658203125
2025-02-11 07:50:58,010 - Pre-cat
2025-02-11 07:50:58,010 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:50:58,012 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:58,013 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:50:58,013 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,013 - Step 16: Generated next token
2025-02-11 07:50:58,013 - Step 16: Updated current_ids
2025-02-11 07:50:58,013 - Step 16: Decoded token text:  wall
2025-02-11 07:50:58,013 - Step 16: Updated current_phrase
2025-02-11 07:50:58,013 - Step 16: Created step_acts
2025-02-11 07:50:58,013 - Step 16: Added to generation_acts
2025-02-11 07:50:58,014 - Step 16: Updated recent_tokens
2025-02-11 07:50:58,015 - Step 16: Decoded current text
2025-02-11 07:50:58,015 - Step 16: Reset consecutive_fillers
2025-02-11 07:50:58,015 - Step 16: Calculated unique_ratio: 0.625
2025-02-11 07:50:58,015 - 
Starting step 17
2025-02-11 07:50:58,015 - Current_ids device: cuda:0
2025-02-11 07:50:58,015 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,045 - Model output complete
2025-02-11 07:50:58,045 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:50:58,045 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,046 - Next token logits device: cuda:0
2025-02-11 07:50:58,046 - Entered do_sample
2025-02-11 07:50:58,046 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,048 - Probs max: 0.98828125
2025-02-11 07:50:58,048 - Pre-cat
2025-02-11 07:50:58,048 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:50:58,050 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:58,051 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:50:58,051 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,051 - Step 17: Generated next token
2025-02-11 07:50:58,051 - Step 17: Updated current_ids
2025-02-11 07:50:58,051 - Step 17: Decoded token text:  very
2025-02-11 07:50:58,051 - Step 17: Updated current_phrase
2025-02-11 07:50:58,051 - Step 17: Created step_acts
2025-02-11 07:50:58,051 - Step 17: Added to generation_acts
2025-02-11 07:50:58,052 - Step 17: Updated recent_tokens
2025-02-11 07:50:58,053 - Step 17: Decoded current text
2025-02-11 07:50:58,053 - Step 17: Reset consecutive_fillers
2025-02-11 07:50:58,053 - Step 17: Calculated unique_ratio: 0.625
2025-02-11 07:50:58,053 - 
Starting step 18
2025-02-11 07:50:58,053 - Current_ids device: cuda:0
2025-02-11 07:50:58,054 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,076 - Model output complete
2025-02-11 07:50:58,076 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:50:58,076 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,077 - Next token logits device: cuda:0
2025-02-11 07:50:58,077 - Entered do_sample
2025-02-11 07:50:58,077 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,079 - Probs max: 0.99560546875
2025-02-11 07:50:58,080 - Pre-cat
2025-02-11 07:50:58,080 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:50:58,083 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:58,083 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:50:58,083 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,083 - Step 18: Generated next token
2025-02-11 07:50:58,083 - Step 18: Updated current_ids
2025-02-11 07:50:58,084 - Step 18: Decoded token text:  fast
2025-02-11 07:50:58,084 - Step 18: Updated current_phrase
2025-02-11 07:50:58,084 - Step 18: Created step_acts
2025-02-11 07:50:58,084 - Step 18: Added to generation_acts
2025-02-11 07:50:58,084 - Step 18: Updated recent_tokens
2025-02-11 07:50:58,086 - Step 18: Decoded current text
2025-02-11 07:50:58,086 - Step 18: Reset consecutive_fillers
2025-02-11 07:50:58,086 - Step 18: Calculated unique_ratio: 0.625
2025-02-11 07:50:58,086 - 
Starting step 19
2025-02-11 07:50:58,086 - Current_ids device: cuda:0
2025-02-11 07:50:58,086 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,109 - Model output complete
2025-02-11 07:50:58,109 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:50:58,110 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,110 - Next token logits device: cuda:0
2025-02-11 07:50:58,110 - Entered do_sample
2025-02-11 07:50:58,110 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,112 - Probs max: 0.84619140625
2025-02-11 07:50:58,113 - Pre-cat
2025-02-11 07:50:58,113 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:50:58,115 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:58,115 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:50:58,115 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,115 - Step 19: Generated next token
2025-02-11 07:50:58,115 - Step 19: Updated current_ids
2025-02-11 07:50:58,115 - Step 19: Decoded token text: .
2025-02-11 07:50:58,116 - Step 19: Updated current_phrase
2025-02-11 07:50:58,116 - Step 19: Created step_acts
2025-02-11 07:50:58,116 - Step 19: Added to generation_acts
2025-02-11 07:50:58,116 - Step 19: Updated recent_tokens
2025-02-11 07:50:58,117 - Step 19: Found phrase end token
2025-02-11 07:50:58,117 - Step 19: Updated recent_phrases
2025-02-11 07:50:58,117 - Step 19: Calculated similarity: 1.0
2025-02-11 07:50:58,213 - 
Starting step 0
2025-02-11 07:50:58,213 - Current_ids device: cuda:0
2025-02-11 07:50:58,213 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,253 - Model output complete
2025-02-11 07:50:58,253 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:58,253 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,254 - Next token logits device: cuda:0
2025-02-11 07:50:58,254 - Entered do_sample
2025-02-11 07:50:58,254 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,256 - Probs max: 0.50341796875
2025-02-11 07:50:58,257 - Pre-cat
2025-02-11 07:50:58,257 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:58,258 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:58,259 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:58,259 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,259 - Step 0: Generated next token
2025-02-11 07:50:58,259 - Step 0: Updated current_ids
2025-02-11 07:50:58,259 - Step 0: Decoded token text:  The
2025-02-11 07:50:58,259 - Step 0: Updated current_phrase
2025-02-11 07:50:58,259 - Step 0: Created step_acts
2025-02-11 07:50:58,260 - Step 0: Added to generation_acts
2025-02-11 07:50:58,261 - Step 0: Updated generated_texts
2025-02-11 07:50:58,261 - Step 0: Updated recent_tokens
2025-02-11 07:50:58,261 - Step 0: Decoded current text
2025-02-11 07:50:58,261 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:58,261 - 
Starting step 1
2025-02-11 07:50:58,261 - Current_ids device: cuda:0
2025-02-11 07:50:58,261 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,285 - Model output complete
2025-02-11 07:50:58,286 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:58,286 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,286 - Next token logits device: cuda:0
2025-02-11 07:50:58,286 - Entered do_sample
2025-02-11 07:50:58,286 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,288 - Probs max: 0.83837890625
2025-02-11 07:50:58,289 - Pre-cat
2025-02-11 07:50:58,289 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:50:58,290 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:58,290 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:58,290 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,290 - Step 1: Generated next token
2025-02-11 07:50:58,290 - Step 1: Updated current_ids
2025-02-11 07:50:58,291 - Step 1: Decoded token text:  ball
2025-02-11 07:50:58,291 - Step 1: Updated current_phrase
2025-02-11 07:50:58,291 - Step 1: Created step_acts
2025-02-11 07:50:58,291 - Step 1: Added to generation_acts
2025-02-11 07:50:58,291 - Step 1: Updated recent_tokens
2025-02-11 07:50:58,292 - Step 1: Decoded current text
2025-02-11 07:50:58,292 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:58,292 - 
Starting step 2
2025-02-11 07:50:58,292 - Current_ids device: cuda:0
2025-02-11 07:50:58,292 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,316 - Model output complete
2025-02-11 07:50:58,316 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:58,316 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,316 - Next token logits device: cuda:0
2025-02-11 07:50:58,316 - Entered do_sample
2025-02-11 07:50:58,316 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,318 - Probs max: 0.583984375
2025-02-11 07:50:58,319 - Pre-cat
2025-02-11 07:50:58,319 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:50:58,321 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:58,321 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:58,321 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,321 - Step 2: Generated next token
2025-02-11 07:50:58,321 - Step 2: Updated current_ids
2025-02-11 07:50:58,322 - Step 2: Decoded token text:  will
2025-02-11 07:50:58,322 - Step 2: Updated current_phrase
2025-02-11 07:50:58,322 - Step 2: Created step_acts
2025-02-11 07:50:58,322 - Step 2: Added to generation_acts
2025-02-11 07:50:58,322 - Step 2: Updated recent_tokens
2025-02-11 07:50:58,323 - Step 2: Decoded current text
2025-02-11 07:50:58,323 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:58,323 - 
Starting step 3
2025-02-11 07:50:58,323 - Current_ids device: cuda:0
2025-02-11 07:50:58,323 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,347 - Model output complete
2025-02-11 07:50:58,347 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:58,347 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,347 - Next token logits device: cuda:0
2025-02-11 07:50:58,347 - Entered do_sample
2025-02-11 07:50:58,347 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,350 - Probs max: 0.56884765625
2025-02-11 07:50:58,350 - Pre-cat
2025-02-11 07:50:58,350 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686]],
       device='cuda:0')
2025-02-11 07:50:58,352 - Next token: tensor([[387]], device='cuda:0')
2025-02-11 07:50:58,352 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:58,352 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,352 - Step 3: Generated next token
2025-02-11 07:50:58,352 - Step 3: Updated current_ids
2025-02-11 07:50:58,352 - Step 3: Decoded token text:  be
2025-02-11 07:50:58,352 - Step 3: Updated current_phrase
2025-02-11 07:50:58,353 - Step 3: Created step_acts
2025-02-11 07:50:58,353 - Step 3: Added to generation_acts
2025-02-11 07:50:58,353 - Step 3: Updated recent_tokens
2025-02-11 07:50:58,354 - Step 3: Decoded current text
2025-02-11 07:50:58,354 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:58,354 - 
Starting step 4
2025-02-11 07:50:58,354 - Current_ids device: cuda:0
2025-02-11 07:50:58,354 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,377 - Model output complete
2025-02-11 07:50:58,377 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:58,377 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,377 - Next token logits device: cuda:0
2025-02-11 07:50:58,377 - Entered do_sample
2025-02-11 07:50:58,377 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,380 - Probs max: 0.689453125
2025-02-11 07:50:58,380 - Pre-cat
2025-02-11 07:50:58,380 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    387]],
       device='cuda:0')
2025-02-11 07:50:58,382 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:58,382 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:58,382 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,382 - Step 4: Generated next token
2025-02-11 07:50:58,382 - Step 4: Updated current_ids
2025-02-11 07:50:58,382 - Step 4: Decoded token text:  very
2025-02-11 07:50:58,383 - Step 4: Updated current_phrase
2025-02-11 07:50:58,383 - Step 4: Created step_acts
2025-02-11 07:50:58,383 - Step 4: Added to generation_acts
2025-02-11 07:50:58,383 - Step 4: Updated recent_tokens
2025-02-11 07:50:58,384 - Step 4: Decoded current text
2025-02-11 07:50:58,384 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:58,384 - 
Starting step 5
2025-02-11 07:50:58,384 - Current_ids device: cuda:0
2025-02-11 07:50:58,384 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,408 - Model output complete
2025-02-11 07:50:58,408 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:58,408 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,408 - Next token logits device: cuda:0
2025-02-11 07:50:58,408 - Entered do_sample
2025-02-11 07:50:58,408 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,410 - Probs max: 0.70556640625
2025-02-11 07:50:58,411 - Pre-cat
2025-02-11 07:50:58,411 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    387,   1602]],
       device='cuda:0')
2025-02-11 07:50:58,412 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:58,413 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:58,413 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,413 - Step 5: Generated next token
2025-02-11 07:50:58,413 - Step 5: Updated current_ids
2025-02-11 07:50:58,413 - Step 5: Decoded token text:  fast
2025-02-11 07:50:58,413 - Step 5: Updated current_phrase
2025-02-11 07:50:58,413 - Step 5: Created step_acts
2025-02-11 07:50:58,413 - Step 5: Added to generation_acts
2025-02-11 07:50:58,415 - Step 5: Updated generated_texts
2025-02-11 07:50:58,415 - Step 5: Updated recent_tokens
2025-02-11 07:50:58,415 - Step 5: Decoded current text
2025-02-11 07:50:58,415 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:58,415 - 
Starting step 6
2025-02-11 07:50:58,415 - Current_ids device: cuda:0
2025-02-11 07:50:58,415 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,439 - Model output complete
2025-02-11 07:50:58,439 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:58,439 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,439 - Next token logits device: cuda:0
2025-02-11 07:50:58,439 - Entered do_sample
2025-02-11 07:50:58,439 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,441 - Probs max: 0.60986328125
2025-02-11 07:50:58,442 - Pre-cat
2025-02-11 07:50:58,442 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    387,   1602,
           4937]], device='cuda:0')
2025-02-11 07:50:58,444 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:58,444 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:58,444 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,444 - Step 6: Generated next token
2025-02-11 07:50:58,444 - Step 6: Updated current_ids
2025-02-11 07:50:58,444 - Step 6: Decoded token text: ,
2025-02-11 07:50:58,444 - Step 6: Updated current_phrase
2025-02-11 07:50:58,445 - Step 6: Created step_acts
2025-02-11 07:50:58,445 - Step 6: Added to generation_acts
2025-02-11 07:50:58,445 - Step 6: Updated recent_tokens
2025-02-11 07:50:58,446 - Step 6: Found phrase end token
2025-02-11 07:50:58,446 - Step 6: Updated recent_phrases
2025-02-11 07:50:58,446 - Step 6: Decoded current text
2025-02-11 07:50:58,446 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:58,446 - 
Starting step 7
2025-02-11 07:50:58,446 - Current_ids device: cuda:0
2025-02-11 07:50:58,446 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,469 - Model output complete
2025-02-11 07:50:58,469 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:58,469 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,469 - Next token logits device: cuda:0
2025-02-11 07:50:58,469 - Entered do_sample
2025-02-11 07:50:58,469 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,472 - Probs max: 0.449462890625
2025-02-11 07:50:58,472 - Pre-cat
2025-02-11 07:50:58,472 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    387,   1602,
           4937,     11]], device='cuda:0')
2025-02-11 07:50:58,474 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:58,474 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:58,474 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,474 - Step 7: Generated next token
2025-02-11 07:50:58,474 - Step 7: Updated current_ids
2025-02-11 07:50:58,475 - Step 7: Decoded token text:  the
2025-02-11 07:50:58,475 - Step 7: Updated current_phrase
2025-02-11 07:50:58,475 - Step 7: Created step_acts
2025-02-11 07:50:58,475 - Step 7: Added to generation_acts
2025-02-11 07:50:58,475 - Step 7: Updated recent_tokens
2025-02-11 07:50:58,476 - Step 7: Decoded current text
2025-02-11 07:50:58,476 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:58,477 - 
Starting step 8
2025-02-11 07:50:58,477 - Current_ids device: cuda:0
2025-02-11 07:50:58,477 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,500 - Model output complete
2025-02-11 07:50:58,500 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:58,500 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,500 - Next token logits device: cuda:0
2025-02-11 07:50:58,500 - Entered do_sample
2025-02-11 07:50:58,500 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,502 - Probs max: 0.99560546875
2025-02-11 07:50:58,503 - Pre-cat
2025-02-11 07:50:58,503 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    387,   1602,
           4937,     11,    279]], device='cuda:0')
2025-02-11 07:50:58,505 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:58,505 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:58,505 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,505 - Step 8: Generated next token
2025-02-11 07:50:58,505 - Step 8: Updated current_ids
2025-02-11 07:50:58,506 - Step 8: Decoded token text:  wall
2025-02-11 07:50:58,506 - Step 8: Updated current_phrase
2025-02-11 07:50:58,506 - Step 8: Created step_acts
2025-02-11 07:50:58,506 - Step 8: Added to generation_acts
2025-02-11 07:50:58,506 - Step 8: Updated recent_tokens
2025-02-11 07:50:58,507 - Step 8: Decoded current text
2025-02-11 07:50:58,507 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:58,507 - 
Starting step 9
2025-02-11 07:50:58,508 - Current_ids device: cuda:0
2025-02-11 07:50:58,508 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,533 - Model output complete
2025-02-11 07:50:58,533 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:58,533 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,533 - Next token logits device: cuda:0
2025-02-11 07:50:58,533 - Entered do_sample
2025-02-11 07:50:58,533 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,536 - Probs max: 0.7529296875
2025-02-11 07:50:58,536 - Pre-cat
2025-02-11 07:50:58,536 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    387,   1602,
           4937,     11,    279,   7002]], device='cuda:0')
2025-02-11 07:50:58,539 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:50:58,539 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:58,539 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,539 - Step 9: Generated next token
2025-02-11 07:50:58,539 - Step 9: Updated current_ids
2025-02-11 07:50:58,540 - Step 9: Decoded token text:  will
2025-02-11 07:50:58,540 - Step 9: Updated current_phrase
2025-02-11 07:50:58,540 - Step 9: Created step_acts
2025-02-11 07:50:58,540 - Step 9: Added to generation_acts
2025-02-11 07:50:58,540 - Step 9: Updated recent_tokens
2025-02-11 07:50:58,541 - Step 9: Decoded current text
2025-02-11 07:50:58,541 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:58,542 - 
Starting step 10
2025-02-11 07:50:58,542 - Current_ids device: cuda:0
2025-02-11 07:50:58,542 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,564 - Model output complete
2025-02-11 07:50:58,564 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:58,564 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,564 - Next token logits device: cuda:0
2025-02-11 07:50:58,564 - Entered do_sample
2025-02-11 07:50:58,564 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,567 - Probs max: 0.9619140625
2025-02-11 07:50:58,568 - Pre-cat
2025-02-11 07:50:58,568 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    387,   1602,
           4937,     11,    279,   7002,    686]], device='cuda:0')
2025-02-11 07:50:58,569 - Next token: tensor([[387]], device='cuda:0')
2025-02-11 07:50:58,570 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:58,570 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,570 - Step 10: Generated next token
2025-02-11 07:50:58,570 - Step 10: Updated current_ids
2025-02-11 07:50:58,570 - Step 10: Decoded token text:  be
2025-02-11 07:50:58,570 - Step 10: Updated current_phrase
2025-02-11 07:50:58,571 - Step 10: Created step_acts
2025-02-11 07:50:58,571 - Step 10: Added to generation_acts
2025-02-11 07:50:58,572 - Step 10: Updated generated_texts
2025-02-11 07:50:58,572 - Step 10: Updated recent_tokens
2025-02-11 07:50:58,572 - Step 10: Decoded current text
2025-02-11 07:50:58,572 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:58,572 - 
Starting step 11
2025-02-11 07:50:58,572 - Current_ids device: cuda:0
2025-02-11 07:50:58,572 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,594 - Model output complete
2025-02-11 07:50:58,594 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:58,595 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,595 - Next token logits device: cuda:0
2025-02-11 07:50:58,595 - Entered do_sample
2025-02-11 07:50:58,595 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,598 - Probs max: 0.97265625
2025-02-11 07:50:58,599 - Pre-cat
2025-02-11 07:50:58,599 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    387,   1602,
           4937,     11,    279,   7002,    686,    387]], device='cuda:0')
2025-02-11 07:50:58,601 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:58,601 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:58,601 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,601 - Step 11: Generated next token
2025-02-11 07:50:58,601 - Step 11: Updated current_ids
2025-02-11 07:50:58,601 - Step 11: Decoded token text:  very
2025-02-11 07:50:58,601 - Step 11: Updated current_phrase
2025-02-11 07:50:58,602 - Step 11: Created step_acts
2025-02-11 07:50:58,602 - Step 11: Added to generation_acts
2025-02-11 07:50:58,602 - Step 11: Updated recent_tokens
2025-02-11 07:50:58,603 - Step 11: Decoded current text
2025-02-11 07:50:58,603 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:58,603 - 
Starting step 12
2025-02-11 07:50:58,604 - Current_ids device: cuda:0
2025-02-11 07:50:58,604 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,627 - Model output complete
2025-02-11 07:50:58,627 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:58,627 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,627 - Next token logits device: cuda:0
2025-02-11 07:50:58,627 - Entered do_sample
2025-02-11 07:50:58,628 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,630 - Probs max: 0.9765625
2025-02-11 07:50:58,630 - Pre-cat
2025-02-11 07:50:58,631 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    387,   1602,
           4937,     11,    279,   7002,    686,    387,   1602]],
       device='cuda:0')
2025-02-11 07:50:58,633 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:58,633 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:58,633 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,633 - Step 12: Generated next token
2025-02-11 07:50:58,633 - Step 12: Updated current_ids
2025-02-11 07:50:58,633 - Step 12: Decoded token text:  fast
2025-02-11 07:50:58,633 - Step 12: Updated current_phrase
2025-02-11 07:50:58,634 - Step 12: Created step_acts
2025-02-11 07:50:58,634 - Step 12: Added to generation_acts
2025-02-11 07:50:58,634 - Step 12: Updated recent_tokens
2025-02-11 07:50:58,635 - Step 12: Decoded current text
2025-02-11 07:50:58,635 - Step 12: Reset consecutive_fillers
2025-02-11 07:50:58,635 - 
Starting step 13
2025-02-11 07:50:58,635 - Current_ids device: cuda:0
2025-02-11 07:50:58,635 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,659 - Model output complete
2025-02-11 07:50:58,659 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:58,659 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,659 - Next token logits device: cuda:0
2025-02-11 07:50:58,659 - Entered do_sample
2025-02-11 07:50:58,659 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,662 - Probs max: 0.98291015625
2025-02-11 07:50:58,662 - Pre-cat
2025-02-11 07:50:58,662 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,    387,   1602,
           4937,     11,    279,   7002,    686,    387,   1602,   4937]],
       device='cuda:0')
2025-02-11 07:50:58,664 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:58,664 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:58,664 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,664 - Step 13: Generated next token
2025-02-11 07:50:58,664 - Step 13: Updated current_ids
2025-02-11 07:50:58,664 - Step 13: Decoded token text: ,
2025-02-11 07:50:58,664 - Step 13: Updated current_phrase
2025-02-11 07:50:58,665 - Step 13: Created step_acts
2025-02-11 07:50:58,665 - Step 13: Added to generation_acts
2025-02-11 07:50:58,665 - Step 13: Updated recent_tokens
2025-02-11 07:50:58,666 - Step 13: Found phrase end token
2025-02-11 07:50:58,666 - Step 13: Updated recent_phrases
2025-02-11 07:50:58,666 - Step 13: Calculated similarity: 0.8333333333333334
2025-02-11 07:50:58,771 - 
Starting step 0
2025-02-11 07:50:58,771 - Current_ids device: cuda:0
2025-02-11 07:50:58,772 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,801 - Model output complete
2025-02-11 07:50:58,802 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:58,802 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,802 - Next token logits device: cuda:0
2025-02-11 07:50:58,802 - Entered do_sample
2025-02-11 07:50:58,802 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,804 - Probs max: 0.50341796875
2025-02-11 07:50:58,807 - Pre-cat
2025-02-11 07:50:58,807 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:58,810 - Next token: tensor([[362]], device='cuda:0')
2025-02-11 07:50:58,811 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:58,811 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,811 - Step 0: Generated next token
2025-02-11 07:50:58,811 - Step 0: Updated current_ids
2025-02-11 07:50:58,812 - Step 0: Decoded token text:  A
2025-02-11 07:50:58,812 - Step 0: Updated current_phrase
2025-02-11 07:50:58,812 - Step 0: Created step_acts
2025-02-11 07:50:58,812 - Step 0: Added to generation_acts
2025-02-11 07:50:58,813 - Step 0: Updated generated_texts
2025-02-11 07:50:58,814 - Step 0: Updated recent_tokens
2025-02-11 07:50:58,814 - Step 0: Decoded current text
2025-02-11 07:50:58,814 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:58,814 - 
Starting step 1
2025-02-11 07:50:58,814 - Current_ids device: cuda:0
2025-02-11 07:50:58,814 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,843 - Model output complete
2025-02-11 07:50:58,844 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:58,844 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,844 - Next token logits device: cuda:0
2025-02-11 07:50:58,844 - Entered do_sample
2025-02-11 07:50:58,844 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,846 - Probs max: 0.95654296875
2025-02-11 07:50:58,847 - Pre-cat
2025-02-11 07:50:58,847 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362]], device='cuda:0')
2025-02-11 07:50:58,848 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:58,849 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:58,849 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,849 - Step 1: Generated next token
2025-02-11 07:50:58,849 - Step 1: Updated current_ids
2025-02-11 07:50:58,849 - Step 1: Decoded token text:  ball
2025-02-11 07:50:58,849 - Step 1: Updated current_phrase
2025-02-11 07:50:58,849 - Step 1: Created step_acts
2025-02-11 07:50:58,849 - Step 1: Added to generation_acts
2025-02-11 07:50:58,849 - Step 1: Updated recent_tokens
2025-02-11 07:50:58,851 - Step 1: Decoded current text
2025-02-11 07:50:58,851 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:58,851 - 
Starting step 2
2025-02-11 07:50:58,851 - Current_ids device: cuda:0
2025-02-11 07:50:58,851 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,874 - Model output complete
2025-02-11 07:50:58,875 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:58,875 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,875 - Next token logits device: cuda:0
2025-02-11 07:50:58,875 - Entered do_sample
2025-02-11 07:50:58,875 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,877 - Probs max: 0.9453125
2025-02-11 07:50:58,878 - Pre-cat
2025-02-11 07:50:58,878 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935]], device='cuda:0')
2025-02-11 07:50:58,879 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:58,879 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:58,879 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,880 - Step 2: Generated next token
2025-02-11 07:50:58,880 - Step 2: Updated current_ids
2025-02-11 07:50:58,880 - Step 2: Decoded token text:  is
2025-02-11 07:50:58,880 - Step 2: Updated current_phrase
2025-02-11 07:50:58,880 - Step 2: Created step_acts
2025-02-11 07:50:58,880 - Step 2: Added to generation_acts
2025-02-11 07:50:58,880 - Step 2: Updated recent_tokens
2025-02-11 07:50:58,882 - Step 2: Decoded current text
2025-02-11 07:50:58,882 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:58,882 - 
Starting step 3
2025-02-11 07:50:58,882 - Current_ids device: cuda:0
2025-02-11 07:50:58,882 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,905 - Model output complete
2025-02-11 07:50:58,905 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:58,905 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,905 - Next token logits device: cuda:0
2025-02-11 07:50:58,905 - Entered do_sample
2025-02-11 07:50:58,905 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,908 - Probs max: 0.9931640625
2025-02-11 07:50:58,908 - Pre-cat
2025-02-11 07:50:58,908 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935,    374]],
       device='cuda:0')
2025-02-11 07:50:58,910 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:50:58,910 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:58,910 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,910 - Step 3: Generated next token
2025-02-11 07:50:58,910 - Step 3: Updated current_ids
2025-02-11 07:50:58,910 - Step 3: Decoded token text:  thrown
2025-02-11 07:50:58,910 - Step 3: Updated current_phrase
2025-02-11 07:50:58,911 - Step 3: Created step_acts
2025-02-11 07:50:58,911 - Step 3: Added to generation_acts
2025-02-11 07:50:58,911 - Step 3: Updated recent_tokens
2025-02-11 07:50:58,912 - Step 3: Decoded current text
2025-02-11 07:50:58,912 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:58,912 - 
Starting step 4
2025-02-11 07:50:58,912 - Current_ids device: cuda:0
2025-02-11 07:50:58,912 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,942 - Model output complete
2025-02-11 07:50:58,942 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:58,942 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,942 - Next token logits device: cuda:0
2025-02-11 07:50:58,942 - Entered do_sample
2025-02-11 07:50:58,942 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,945 - Probs max: 0.9560546875
2025-02-11 07:50:58,945 - Pre-cat
2025-02-11 07:50:58,945 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:50:58,947 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:50:58,947 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:58,947 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,947 - Step 4: Generated next token
2025-02-11 07:50:58,947 - Step 4: Updated current_ids
2025-02-11 07:50:58,948 - Step 4: Decoded token text:  at
2025-02-11 07:50:58,948 - Step 4: Updated current_phrase
2025-02-11 07:50:58,948 - Step 4: Created step_acts
2025-02-11 07:50:58,948 - Step 4: Added to generation_acts
2025-02-11 07:50:58,948 - Step 4: Updated recent_tokens
2025-02-11 07:50:58,949 - Step 4: Decoded current text
2025-02-11 07:50:58,949 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:58,949 - 
Starting step 5
2025-02-11 07:50:58,949 - Current_ids device: cuda:0
2025-02-11 07:50:58,949 - Current_ids dtype: torch.int64
2025-02-11 07:50:58,972 - Model output complete
2025-02-11 07:50:58,973 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:58,973 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,973 - Next token logits device: cuda:0
2025-02-11 07:50:58,973 - Entered do_sample
2025-02-11 07:50:58,973 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:58,975 - Probs max: 0.9990234375
2025-02-11 07:50:58,976 - Pre-cat
2025-02-11 07:50:58,976 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935,    374,  14989,    518]],
       device='cuda:0')
2025-02-11 07:50:58,977 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:58,978 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:58,978 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:58,978 - Step 5: Generated next token
2025-02-11 07:50:58,978 - Step 5: Updated current_ids
2025-02-11 07:50:58,978 - Step 5: Decoded token text:  a
2025-02-11 07:50:58,978 - Step 5: Updated current_phrase
2025-02-11 07:50:58,979 - Step 5: Created step_acts
2025-02-11 07:50:58,979 - Step 5: Added to generation_acts
2025-02-11 07:50:58,980 - Step 5: Updated generated_texts
2025-02-11 07:50:58,980 - Step 5: Updated recent_tokens
2025-02-11 07:50:58,980 - Step 5: Decoded current text
2025-02-11 07:50:58,980 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:58,980 - 
Starting step 6
2025-02-11 07:50:58,980 - Current_ids device: cuda:0
2025-02-11 07:50:58,980 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,004 - Model output complete
2025-02-11 07:50:59,004 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:59,004 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,004 - Next token logits device: cuda:0
2025-02-11 07:50:59,004 - Entered do_sample
2025-02-11 07:50:59,005 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,007 - Probs max: 0.9970703125
2025-02-11 07:50:59,007 - Pre-cat
2025-02-11 07:50:59,007 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935,    374,  14989,    518,
            264]], device='cuda:0')
2025-02-11 07:50:59,009 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:59,009 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:59,009 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,009 - Step 6: Generated next token
2025-02-11 07:50:59,009 - Step 6: Updated current_ids
2025-02-11 07:50:59,010 - Step 6: Decoded token text:  wall
2025-02-11 07:50:59,010 - Step 6: Updated current_phrase
2025-02-11 07:50:59,010 - Step 6: Created step_acts
2025-02-11 07:50:59,010 - Step 6: Added to generation_acts
2025-02-11 07:50:59,010 - Step 6: Updated recent_tokens
2025-02-11 07:50:59,011 - Step 6: Decoded current text
2025-02-11 07:50:59,011 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:59,012 - 
Starting step 7
2025-02-11 07:50:59,012 - Current_ids device: cuda:0
2025-02-11 07:50:59,012 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,035 - Model output complete
2025-02-11 07:50:59,035 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:59,035 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,035 - Next token logits device: cuda:0
2025-02-11 07:50:59,035 - Entered do_sample
2025-02-11 07:50:59,035 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,038 - Probs max: 0.98974609375
2025-02-11 07:50:59,038 - Pre-cat
2025-02-11 07:50:59,038 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935,    374,  14989,    518,
            264,   7002]], device='cuda:0')
2025-02-11 07:50:59,040 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:50:59,040 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:59,040 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,040 - Step 7: Generated next token
2025-02-11 07:50:59,040 - Step 7: Updated current_ids
2025-02-11 07:50:59,040 - Step 7: Decoded token text:  very
2025-02-11 07:50:59,040 - Step 7: Updated current_phrase
2025-02-11 07:50:59,041 - Step 7: Created step_acts
2025-02-11 07:50:59,041 - Step 7: Added to generation_acts
2025-02-11 07:50:59,041 - Step 7: Updated recent_tokens
2025-02-11 07:50:59,042 - Step 7: Decoded current text
2025-02-11 07:50:59,042 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:59,042 - 
Starting step 8
2025-02-11 07:50:59,042 - Current_ids device: cuda:0
2025-02-11 07:50:59,042 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,065 - Model output complete
2025-02-11 07:50:59,065 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:59,065 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,065 - Next token logits device: cuda:0
2025-02-11 07:50:59,065 - Entered do_sample
2025-02-11 07:50:59,066 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,068 - Probs max: 0.99609375
2025-02-11 07:50:59,068 - Pre-cat
2025-02-11 07:50:59,068 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935,    374,  14989,    518,
            264,   7002,   1602]], device='cuda:0')
2025-02-11 07:50:59,070 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:50:59,070 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:59,070 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,070 - Step 8: Generated next token
2025-02-11 07:50:59,071 - Step 8: Updated current_ids
2025-02-11 07:50:59,071 - Step 8: Decoded token text:  fast
2025-02-11 07:50:59,071 - Step 8: Updated current_phrase
2025-02-11 07:50:59,071 - Step 8: Created step_acts
2025-02-11 07:50:59,071 - Step 8: Added to generation_acts
2025-02-11 07:50:59,071 - Step 8: Updated recent_tokens
2025-02-11 07:50:59,073 - Step 8: Decoded current text
2025-02-11 07:50:59,073 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:59,073 - 
Starting step 9
2025-02-11 07:50:59,073 - Current_ids device: cuda:0
2025-02-11 07:50:59,073 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,096 - Model output complete
2025-02-11 07:50:59,096 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:59,096 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,096 - Next token logits device: cuda:0
2025-02-11 07:50:59,096 - Entered do_sample
2025-02-11 07:50:59,096 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,098 - Probs max: 0.73486328125
2025-02-11 07:50:59,099 - Pre-cat
2025-02-11 07:50:59,099 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:50:59,101 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:50:59,101 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:59,101 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,101 - Step 9: Generated next token
2025-02-11 07:50:59,102 - Step 9: Updated current_ids
2025-02-11 07:50:59,102 - Step 9: Decoded token text: .
2025-02-11 07:50:59,102 - Step 9: Updated current_phrase
2025-02-11 07:50:59,102 - Step 9: Created step_acts
2025-02-11 07:50:59,102 - Step 9: Added to generation_acts
2025-02-11 07:50:59,102 - Step 9: Updated recent_tokens
2025-02-11 07:50:59,103 - Step 9: Found phrase end token
2025-02-11 07:50:59,103 - Step 9: Updated recent_phrases
2025-02-11 07:50:59,104 - Step 9: Decoded current text
2025-02-11 07:50:59,104 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:59,104 - 
Starting step 10
2025-02-11 07:50:59,104 - Current_ids device: cuda:0
2025-02-11 07:50:59,104 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,127 - Model output complete
2025-02-11 07:50:59,127 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:59,127 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,127 - Next token logits device: cuda:0
2025-02-11 07:50:59,128 - Entered do_sample
2025-02-11 07:50:59,128 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,130 - Probs max: 0.348388671875
2025-02-11 07:50:59,131 - Pre-cat
2025-02-11 07:50:59,131 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13]], device='cuda:0')
2025-02-11 07:50:59,134 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:59,134 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:59,134 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,134 - Step 10: Generated next token
2025-02-11 07:50:59,134 - Step 10: Updated current_ids
2025-02-11 07:50:59,134 - Step 10: Decoded token text:  The
2025-02-11 07:50:59,134 - Step 10: Updated current_phrase
2025-02-11 07:50:59,135 - Step 10: Created step_acts
2025-02-11 07:50:59,135 - Step 10: Added to generation_acts
2025-02-11 07:50:59,136 - Step 10: Updated generated_texts
2025-02-11 07:50:59,136 - Step 10: Updated recent_tokens
2025-02-11 07:50:59,137 - Step 10: Decoded current text
2025-02-11 07:50:59,137 - Step 10: Reset consecutive_fillers
2025-02-11 07:50:59,137 - 
Starting step 11
2025-02-11 07:50:59,137 - Current_ids device: cuda:0
2025-02-11 07:50:59,137 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,181 - Model output complete
2025-02-11 07:50:59,181 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:59,181 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,181 - Next token logits device: cuda:0
2025-02-11 07:50:59,181 - Entered do_sample
2025-02-11 07:50:59,182 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,183 - Probs max: 0.71142578125
2025-02-11 07:50:59,185 - Pre-cat
2025-02-11 07:50:59,185 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576]], device='cuda:0')
2025-02-11 07:50:59,188 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:59,189 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:59,189 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,189 - Step 11: Generated next token
2025-02-11 07:50:59,189 - Step 11: Updated current_ids
2025-02-11 07:50:59,189 - Step 11: Decoded token text:  wall
2025-02-11 07:50:59,189 - Step 11: Updated current_phrase
2025-02-11 07:50:59,190 - Step 11: Created step_acts
2025-02-11 07:50:59,190 - Step 11: Added to generation_acts
2025-02-11 07:50:59,190 - Step 11: Updated recent_tokens
2025-02-11 07:50:59,191 - Step 11: Decoded current text
2025-02-11 07:50:59,191 - Step 11: Reset consecutive_fillers
2025-02-11 07:50:59,191 - 
Starting step 12
2025-02-11 07:50:59,192 - Current_ids device: cuda:0
2025-02-11 07:50:59,192 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,230 - Model output complete
2025-02-11 07:50:59,230 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:59,230 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,230 - Next token logits device: cuda:0
2025-02-11 07:50:59,231 - Entered do_sample
2025-02-11 07:50:59,231 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,232 - Probs max: 0.8896484375
2025-02-11 07:50:59,233 - Pre-cat
2025-02-11 07:50:59,233 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576,   7002]],
       device='cuda:0')
2025-02-11 07:50:59,237 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:59,238 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:59,238 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,238 - Step 12: Generated next token
2025-02-11 07:50:59,238 - Step 12: Updated current_ids
2025-02-11 07:50:59,238 - Step 12: Decoded token text:  is
2025-02-11 07:50:59,238 - Step 12: Updated current_phrase
2025-02-11 07:50:59,239 - Step 12: Created step_acts
2025-02-11 07:50:59,239 - Step 12: Added to generation_acts
2025-02-11 07:50:59,239 - Step 12: Updated recent_tokens
2025-02-11 07:50:59,240 - Step 12: Decoded current text
2025-02-11 07:50:59,240 - Step 12: Reset consecutive_fillers
2025-02-11 07:50:59,240 - 
Starting step 13
2025-02-11 07:50:59,240 - Current_ids device: cuda:0
2025-02-11 07:50:59,240 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,264 - Model output complete
2025-02-11 07:50:59,264 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:50:59,264 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,264 - Next token logits device: cuda:0
2025-02-11 07:50:59,264 - Entered do_sample
2025-02-11 07:50:59,264 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,266 - Probs max: 0.402099609375
2025-02-11 07:50:59,267 - Pre-cat
2025-02-11 07:50:59,267 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576,   7002,    374]],
       device='cuda:0')
2025-02-11 07:50:59,269 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:50:59,269 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:50:59,269 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,269 - Step 13: Generated next token
2025-02-11 07:50:59,270 - Step 13: Updated current_ids
2025-02-11 07:50:59,270 - Step 13: Decoded token text:  at
2025-02-11 07:50:59,270 - Step 13: Updated current_phrase
2025-02-11 07:50:59,270 - Step 13: Created step_acts
2025-02-11 07:50:59,270 - Step 13: Added to generation_acts
2025-02-11 07:50:59,270 - Step 13: Updated recent_tokens
2025-02-11 07:50:59,272 - Step 13: Decoded current text
2025-02-11 07:50:59,272 - Step 13: Incremented consecutive_fillers to 1
2025-02-11 07:50:59,272 - 
Starting step 14
2025-02-11 07:50:59,272 - Current_ids device: cuda:0
2025-02-11 07:50:59,272 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,295 - Model output complete
2025-02-11 07:50:59,295 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:50:59,295 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,295 - Next token logits device: cuda:0
2025-02-11 07:50:59,295 - Entered do_sample
2025-02-11 07:50:59,295 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,298 - Probs max: 0.494384765625
2025-02-11 07:50:59,298 - Pre-cat
2025-02-11 07:50:59,298 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576,   7002,    374,    518]],
       device='cuda:0')
2025-02-11 07:50:59,300 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:50:59,301 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:50:59,301 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,301 - Step 14: Generated next token
2025-02-11 07:50:59,301 - Step 14: Updated current_ids
2025-02-11 07:50:59,301 - Step 14: Decoded token text:  a
2025-02-11 07:50:59,301 - Step 14: Updated current_phrase
2025-02-11 07:50:59,301 - Step 14: Created step_acts
2025-02-11 07:50:59,301 - Step 14: Added to generation_acts
2025-02-11 07:50:59,301 - Step 14: Updated recent_tokens
2025-02-11 07:50:59,303 - Step 14: Decoded current text
2025-02-11 07:50:59,303 - Step 14: Incremented consecutive_fillers to 2
2025-02-11 07:50:59,303 - 
Starting step 15
2025-02-11 07:50:59,303 - Current_ids device: cuda:0
2025-02-11 07:50:59,303 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,327 - Model output complete
2025-02-11 07:50:59,328 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:50:59,328 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,328 - Next token logits device: cuda:0
2025-02-11 07:50:59,328 - Entered do_sample
2025-02-11 07:50:59,328 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,330 - Probs max: 0.86181640625
2025-02-11 07:50:59,331 - Pre-cat
2025-02-11 07:50:59,331 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    362,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     13,    576,   7002,    374,    518,
            264]], device='cuda:0')
2025-02-11 07:50:59,335 - Next token: tensor([[2608]], device='cuda:0')
2025-02-11 07:50:59,335 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:50:59,335 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,335 - Step 15: Generated next token
2025-02-11 07:50:59,335 - Step 15: Updated current_ids
2025-02-11 07:50:59,336 - Step 15: Decoded token text:  height
2025-02-11 07:50:59,336 - Step 15: Updated current_phrase
2025-02-11 07:50:59,336 - Step 15: Created step_acts
2025-02-11 07:50:59,336 - Step 15: Added to generation_acts
2025-02-11 07:50:59,338 - Step 15: Updated generated_texts
2025-02-11 07:50:59,338 - Step 15: Updated recent_tokens
2025-02-11 07:50:59,338 - Step 15: Decoded current text
2025-02-11 07:50:59,338 - Step 15: Incremented consecutive_fillers to 3
2025-02-11 07:50:59,435 - 
Starting step 0
2025-02-11 07:50:59,435 - Current_ids device: cuda:0
2025-02-11 07:50:59,435 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,461 - Model output complete
2025-02-11 07:50:59,462 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:59,462 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,462 - Next token logits device: cuda:0
2025-02-11 07:50:59,462 - Entered do_sample
2025-02-11 07:50:59,462 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,464 - Probs max: 0.50341796875
2025-02-11 07:50:59,465 - Pre-cat
2025-02-11 07:50:59,465 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:50:59,467 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:50:59,467 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:50:59,467 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,467 - Step 0: Generated next token
2025-02-11 07:50:59,467 - Step 0: Updated current_ids
2025-02-11 07:50:59,467 - Step 0: Decoded token text:  The
2025-02-11 07:50:59,467 - Step 0: Updated current_phrase
2025-02-11 07:50:59,468 - Step 0: Created step_acts
2025-02-11 07:50:59,468 - Step 0: Added to generation_acts
2025-02-11 07:50:59,469 - Step 0: Updated generated_texts
2025-02-11 07:50:59,469 - Step 0: Updated recent_tokens
2025-02-11 07:50:59,469 - Step 0: Decoded current text
2025-02-11 07:50:59,469 - Step 0: Reset consecutive_fillers
2025-02-11 07:50:59,470 - 
Starting step 1
2025-02-11 07:50:59,470 - Current_ids device: cuda:0
2025-02-11 07:50:59,470 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,494 - Model output complete
2025-02-11 07:50:59,495 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:50:59,495 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,495 - Next token logits device: cuda:0
2025-02-11 07:50:59,495 - Entered do_sample
2025-02-11 07:50:59,495 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,497 - Probs max: 0.83837890625
2025-02-11 07:50:59,498 - Pre-cat
2025-02-11 07:50:59,499 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:50:59,501 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:59,501 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:50:59,501 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,501 - Step 1: Generated next token
2025-02-11 07:50:59,501 - Step 1: Updated current_ids
2025-02-11 07:50:59,501 - Step 1: Decoded token text:  ball
2025-02-11 07:50:59,502 - Step 1: Updated current_phrase
2025-02-11 07:50:59,502 - Step 1: Created step_acts
2025-02-11 07:50:59,502 - Step 1: Added to generation_acts
2025-02-11 07:50:59,502 - Step 1: Updated recent_tokens
2025-02-11 07:50:59,503 - Step 1: Decoded current text
2025-02-11 07:50:59,503 - Step 1: Reset consecutive_fillers
2025-02-11 07:50:59,503 - 
Starting step 2
2025-02-11 07:50:59,504 - Current_ids device: cuda:0
2025-02-11 07:50:59,504 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,527 - Model output complete
2025-02-11 07:50:59,527 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:50:59,527 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,527 - Next token logits device: cuda:0
2025-02-11 07:50:59,527 - Entered do_sample
2025-02-11 07:50:59,528 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,530 - Probs max: 0.583984375
2025-02-11 07:50:59,531 - Pre-cat
2025-02-11 07:50:59,531 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:50:59,532 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:59,533 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:50:59,533 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,533 - Step 2: Generated next token
2025-02-11 07:50:59,533 - Step 2: Updated current_ids
2025-02-11 07:50:59,533 - Step 2: Decoded token text:  is
2025-02-11 07:50:59,533 - Step 2: Updated current_phrase
2025-02-11 07:50:59,533 - Step 2: Created step_acts
2025-02-11 07:50:59,533 - Step 2: Added to generation_acts
2025-02-11 07:50:59,533 - Step 2: Updated recent_tokens
2025-02-11 07:50:59,535 - Step 2: Decoded current text
2025-02-11 07:50:59,535 - Step 2: Reset consecutive_fillers
2025-02-11 07:50:59,535 - 
Starting step 3
2025-02-11 07:50:59,535 - Current_ids device: cuda:0
2025-02-11 07:50:59,535 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,558 - Model output complete
2025-02-11 07:50:59,558 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:50:59,558 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,558 - Next token logits device: cuda:0
2025-02-11 07:50:59,558 - Entered do_sample
2025-02-11 07:50:59,558 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,560 - Probs max: 0.59521484375
2025-02-11 07:50:59,561 - Pre-cat
2025-02-11 07:50:59,561 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374]],
       device='cuda:0')
2025-02-11 07:50:59,563 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:50:59,563 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:50:59,563 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,563 - Step 3: Generated next token
2025-02-11 07:50:59,563 - Step 3: Updated current_ids
2025-02-11 07:50:59,563 - Step 3: Decoded token text:  thrown
2025-02-11 07:50:59,563 - Step 3: Updated current_phrase
2025-02-11 07:50:59,564 - Step 3: Created step_acts
2025-02-11 07:50:59,564 - Step 3: Added to generation_acts
2025-02-11 07:50:59,564 - Step 3: Updated recent_tokens
2025-02-11 07:50:59,565 - Step 3: Decoded current text
2025-02-11 07:50:59,565 - Step 3: Reset consecutive_fillers
2025-02-11 07:50:59,565 - 
Starting step 4
2025-02-11 07:50:59,565 - Current_ids device: cuda:0
2025-02-11 07:50:59,565 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,593 - Model output complete
2025-02-11 07:50:59,594 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:50:59,594 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,594 - Next token logits device: cuda:0
2025-02-11 07:50:59,594 - Entered do_sample
2025-02-11 07:50:59,594 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,596 - Probs max: 0.67431640625
2025-02-11 07:50:59,597 - Pre-cat
2025-02-11 07:50:59,597 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:50:59,599 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:50:59,600 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:50:59,600 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,600 - Step 4: Generated next token
2025-02-11 07:50:59,600 - Step 4: Updated current_ids
2025-02-11 07:50:59,600 - Step 4: Decoded token text:  and
2025-02-11 07:50:59,600 - Step 4: Updated current_phrase
2025-02-11 07:50:59,601 - Step 4: Created step_acts
2025-02-11 07:50:59,601 - Step 4: Added to generation_acts
2025-02-11 07:50:59,601 - Step 4: Updated recent_tokens
2025-02-11 07:50:59,602 - Step 4: Decoded current text
2025-02-11 07:50:59,602 - Step 4: Reset consecutive_fillers
2025-02-11 07:50:59,602 - 
Starting step 5
2025-02-11 07:50:59,602 - Current_ids device: cuda:0
2025-02-11 07:50:59,602 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,637 - Model output complete
2025-02-11 07:50:59,637 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:50:59,637 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,637 - Next token logits device: cuda:0
2025-02-11 07:50:59,637 - Entered do_sample
2025-02-11 07:50:59,637 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,640 - Probs max: 0.8154296875
2025-02-11 07:50:59,640 - Pre-cat
2025-02-11 07:50:59,640 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    323]],
       device='cuda:0')
2025-02-11 07:50:59,642 - Next token: tensor([[12983]], device='cuda:0')
2025-02-11 07:50:59,642 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:50:59,642 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,643 - Step 5: Generated next token
2025-02-11 07:50:59,643 - Step 5: Updated current_ids
2025-02-11 07:50:59,643 - Step 5: Decoded token text:  hits
2025-02-11 07:50:59,643 - Step 5: Updated current_phrase
2025-02-11 07:50:59,643 - Step 5: Created step_acts
2025-02-11 07:50:59,643 - Step 5: Added to generation_acts
2025-02-11 07:50:59,645 - Step 5: Updated generated_texts
2025-02-11 07:50:59,645 - Step 5: Updated recent_tokens
2025-02-11 07:50:59,645 - Step 5: Decoded current text
2025-02-11 07:50:59,645 - Step 5: Reset consecutive_fillers
2025-02-11 07:50:59,645 - 
Starting step 6
2025-02-11 07:50:59,645 - Current_ids device: cuda:0
2025-02-11 07:50:59,645 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,670 - Model output complete
2025-02-11 07:50:59,670 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:50:59,670 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,670 - Next token logits device: cuda:0
2025-02-11 07:50:59,670 - Entered do_sample
2025-02-11 07:50:59,670 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,673 - Probs max: 0.99609375
2025-02-11 07:50:59,673 - Pre-cat
2025-02-11 07:50:59,673 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    323,
          12983]], device='cuda:0')
2025-02-11 07:50:59,675 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:59,675 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:50:59,675 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,675 - Step 6: Generated next token
2025-02-11 07:50:59,675 - Step 6: Updated current_ids
2025-02-11 07:50:59,676 - Step 6: Decoded token text:  the
2025-02-11 07:50:59,676 - Step 6: Updated current_phrase
2025-02-11 07:50:59,676 - Step 6: Created step_acts
2025-02-11 07:50:59,676 - Step 6: Added to generation_acts
2025-02-11 07:50:59,676 - Step 6: Updated recent_tokens
2025-02-11 07:50:59,677 - Step 6: Decoded current text
2025-02-11 07:50:59,677 - Step 6: Reset consecutive_fillers
2025-02-11 07:50:59,677 - 
Starting step 7
2025-02-11 07:50:59,677 - Current_ids device: cuda:0
2025-02-11 07:50:59,677 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,701 - Model output complete
2025-02-11 07:50:59,701 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:50:59,701 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,701 - Next token logits device: cuda:0
2025-02-11 07:50:59,701 - Entered do_sample
2025-02-11 07:50:59,701 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,704 - Probs max: 0.9990234375
2025-02-11 07:50:59,704 - Pre-cat
2025-02-11 07:50:59,704 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    323,
          12983,    279]], device='cuda:0')
2025-02-11 07:50:59,706 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:50:59,706 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:50:59,706 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,706 - Step 7: Generated next token
2025-02-11 07:50:59,706 - Step 7: Updated current_ids
2025-02-11 07:50:59,707 - Step 7: Decoded token text:  wall
2025-02-11 07:50:59,707 - Step 7: Updated current_phrase
2025-02-11 07:50:59,707 - Step 7: Created step_acts
2025-02-11 07:50:59,707 - Step 7: Added to generation_acts
2025-02-11 07:50:59,707 - Step 7: Updated recent_tokens
2025-02-11 07:50:59,708 - Step 7: Decoded current text
2025-02-11 07:50:59,708 - Step 7: Reset consecutive_fillers
2025-02-11 07:50:59,708 - 
Starting step 8
2025-02-11 07:50:59,709 - Current_ids device: cuda:0
2025-02-11 07:50:59,709 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,731 - Model output complete
2025-02-11 07:50:59,731 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:50:59,731 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,732 - Next token logits device: cuda:0
2025-02-11 07:50:59,732 - Entered do_sample
2025-02-11 07:50:59,732 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,734 - Probs max: 0.380859375
2025-02-11 07:50:59,735 - Pre-cat
2025-02-11 07:50:59,735 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    323,
          12983,    279,   7002]], device='cuda:0')
2025-02-11 07:50:59,736 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:50:59,736 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:50:59,736 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,737 - Step 8: Generated next token
2025-02-11 07:50:59,737 - Step 8: Updated current_ids
2025-02-11 07:50:59,737 - Step 8: Decoded token text: ,
2025-02-11 07:50:59,737 - Step 8: Updated current_phrase
2025-02-11 07:50:59,737 - Step 8: Created step_acts
2025-02-11 07:50:59,737 - Step 8: Added to generation_acts
2025-02-11 07:50:59,737 - Step 8: Updated recent_tokens
2025-02-11 07:50:59,738 - Step 8: Found phrase end token
2025-02-11 07:50:59,739 - Step 8: Updated recent_phrases
2025-02-11 07:50:59,739 - Step 8: Decoded current text
2025-02-11 07:50:59,739 - Step 8: Reset consecutive_fillers
2025-02-11 07:50:59,739 - 
Starting step 9
2025-02-11 07:50:59,739 - Current_ids device: cuda:0
2025-02-11 07:50:59,739 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,763 - Model output complete
2025-02-11 07:50:59,763 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:50:59,763 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,763 - Next token logits device: cuda:0
2025-02-11 07:50:59,764 - Entered do_sample
2025-02-11 07:50:59,764 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,766 - Probs max: 0.2271728515625
2025-02-11 07:50:59,767 - Pre-cat
2025-02-11 07:50:59,767 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    323,
          12983,    279,   7002,     11]], device='cuda:0')
2025-02-11 07:50:59,768 - Next token: tensor([[1221]], device='cuda:0')
2025-02-11 07:50:59,769 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:50:59,769 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,769 - Step 9: Generated next token
2025-02-11 07:50:59,769 - Step 9: Updated current_ids
2025-02-11 07:50:59,769 - Step 9: Decoded token text:  then
2025-02-11 07:50:59,769 - Step 9: Updated current_phrase
2025-02-11 07:50:59,769 - Step 9: Created step_acts
2025-02-11 07:50:59,769 - Step 9: Added to generation_acts
2025-02-11 07:50:59,770 - Step 9: Updated recent_tokens
2025-02-11 07:50:59,771 - Step 9: Decoded current text
2025-02-11 07:50:59,771 - Step 9: Reset consecutive_fillers
2025-02-11 07:50:59,771 - 
Starting step 10
2025-02-11 07:50:59,771 - Current_ids device: cuda:0
2025-02-11 07:50:59,771 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,794 - Model output complete
2025-02-11 07:50:59,794 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:50:59,794 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,794 - Next token logits device: cuda:0
2025-02-11 07:50:59,794 - Entered do_sample
2025-02-11 07:50:59,794 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,797 - Probs max: 0.83837890625
2025-02-11 07:50:59,798 - Pre-cat
2025-02-11 07:50:59,798 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    323,
          12983,    279,   7002,     11,   1221]], device='cuda:0')
2025-02-11 07:50:59,799 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:50:59,799 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:50:59,800 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,800 - Step 10: Generated next token
2025-02-11 07:50:59,800 - Step 10: Updated current_ids
2025-02-11 07:50:59,800 - Step 10: Decoded token text:  the
2025-02-11 07:50:59,800 - Step 10: Updated current_phrase
2025-02-11 07:50:59,800 - Step 10: Created step_acts
2025-02-11 07:50:59,800 - Step 10: Added to generation_acts
2025-02-11 07:50:59,802 - Step 10: Updated generated_texts
2025-02-11 07:50:59,802 - Step 10: Updated recent_tokens
2025-02-11 07:50:59,802 - Step 10: Decoded current text
2025-02-11 07:50:59,802 - Step 10: Incremented consecutive_fillers to 1
2025-02-11 07:50:59,802 - 
Starting step 11
2025-02-11 07:50:59,802 - Current_ids device: cuda:0
2025-02-11 07:50:59,802 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,824 - Model output complete
2025-02-11 07:50:59,825 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:50:59,825 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,825 - Next token logits device: cuda:0
2025-02-11 07:50:59,825 - Entered do_sample
2025-02-11 07:50:59,825 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,828 - Probs max: 0.5087890625
2025-02-11 07:50:59,828 - Pre-cat
2025-02-11 07:50:59,828 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    323,
          12983,    279,   7002,     11,   1221,    279]], device='cuda:0')
2025-02-11 07:50:59,830 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:50:59,830 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:50:59,830 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,830 - Step 11: Generated next token
2025-02-11 07:50:59,830 - Step 11: Updated current_ids
2025-02-11 07:50:59,830 - Step 11: Decoded token text:  ball
2025-02-11 07:50:59,831 - Step 11: Updated current_phrase
2025-02-11 07:50:59,831 - Step 11: Created step_acts
2025-02-11 07:50:59,831 - Step 11: Added to generation_acts
2025-02-11 07:50:59,831 - Step 11: Updated recent_tokens
2025-02-11 07:50:59,832 - Step 11: Decoded current text
2025-02-11 07:50:59,832 - Step 11: Incremented consecutive_fillers to 2
2025-02-11 07:50:59,832 - 
Starting step 12
2025-02-11 07:50:59,832 - Current_ids device: cuda:0
2025-02-11 07:50:59,832 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,868 - Model output complete
2025-02-11 07:50:59,868 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:50:59,869 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,869 - Next token logits device: cuda:0
2025-02-11 07:50:59,869 - Entered do_sample
2025-02-11 07:50:59,869 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,871 - Probs max: 0.72900390625
2025-02-11 07:50:59,872 - Pre-cat
2025-02-11 07:50:59,872 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    323,
          12983,    279,   7002,     11,   1221,    279,   4935]],
       device='cuda:0')
2025-02-11 07:50:59,875 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:50:59,875 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:50:59,875 - Next token shape: torch.Size([1, 1])
2025-02-11 07:50:59,875 - Step 12: Generated next token
2025-02-11 07:50:59,875 - Step 12: Updated current_ids
2025-02-11 07:50:59,876 - Step 12: Decoded token text:  is
2025-02-11 07:50:59,876 - Step 12: Updated current_phrase
2025-02-11 07:50:59,876 - Step 12: Created step_acts
2025-02-11 07:50:59,876 - Step 12: Added to generation_acts
2025-02-11 07:50:59,876 - Step 12: Updated recent_tokens
2025-02-11 07:50:59,878 - Step 12: Decoded current text
2025-02-11 07:50:59,878 - Step 12: Incremented consecutive_fillers to 3
2025-02-11 07:50:59,973 - 
Starting step 0
2025-02-11 07:50:59,973 - Current_ids device: cuda:0
2025-02-11 07:50:59,973 - Current_ids dtype: torch.int64
2025-02-11 07:50:59,998 - Model output complete
2025-02-11 07:50:59,998 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:50:59,998 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:50:59,998 - Next token logits device: cuda:0
2025-02-11 07:50:59,998 - Entered do_sample
2025-02-11 07:50:59,998 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,001 - Probs max: 0.50341796875
2025-02-11 07:51:00,002 - Pre-cat
2025-02-11 07:51:00,002 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:51:00,003 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:51:00,003 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:51:00,003 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,004 - Step 0: Generated next token
2025-02-11 07:51:00,004 - Step 0: Updated current_ids
2025-02-11 07:51:00,004 - Step 0: Decoded token text:  If
2025-02-11 07:51:00,004 - Step 0: Updated current_phrase
2025-02-11 07:51:00,004 - Step 0: Created step_acts
2025-02-11 07:51:00,004 - Step 0: Added to generation_acts
2025-02-11 07:51:00,005 - Step 0: Updated generated_texts
2025-02-11 07:51:00,006 - Step 0: Updated recent_tokens
2025-02-11 07:51:00,006 - Step 0: Decoded current text
2025-02-11 07:51:00,006 - Step 0: Reset consecutive_fillers
2025-02-11 07:51:00,006 - 
Starting step 1
2025-02-11 07:51:00,006 - Current_ids device: cuda:0
2025-02-11 07:51:00,006 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,032 - Model output complete
2025-02-11 07:51:00,032 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:51:00,032 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,032 - Next token logits device: cuda:0
2025-02-11 07:51:00,032 - Entered do_sample
2025-02-11 07:51:00,032 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,035 - Probs max: 0.7099609375
2025-02-11 07:51:00,035 - Pre-cat
2025-02-11 07:51:00,036 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:51:00,037 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:51:00,037 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:51:00,037 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,037 - Step 1: Generated next token
2025-02-11 07:51:00,038 - Step 1: Updated current_ids
2025-02-11 07:51:00,038 - Step 1: Decoded token text:  a
2025-02-11 07:51:00,038 - Step 1: Updated current_phrase
2025-02-11 07:51:00,038 - Step 1: Created step_acts
2025-02-11 07:51:00,038 - Step 1: Added to generation_acts
2025-02-11 07:51:00,038 - Step 1: Updated recent_tokens
2025-02-11 07:51:00,039 - Step 1: Decoded current text
2025-02-11 07:51:00,040 - Step 1: Reset consecutive_fillers
2025-02-11 07:51:00,040 - 
Starting step 2
2025-02-11 07:51:00,040 - Current_ids device: cuda:0
2025-02-11 07:51:00,040 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,063 - Model output complete
2025-02-11 07:51:00,063 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:51:00,063 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,064 - Next token logits device: cuda:0
2025-02-11 07:51:00,064 - Entered do_sample
2025-02-11 07:51:00,064 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,066 - Probs max: 0.9970703125
2025-02-11 07:51:00,067 - Pre-cat
2025-02-11 07:51:00,067 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264]], device='cuda:0')
2025-02-11 07:51:00,068 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:51:00,068 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:51:00,068 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,068 - Step 2: Generated next token
2025-02-11 07:51:00,068 - Step 2: Updated current_ids
2025-02-11 07:51:00,069 - Step 2: Decoded token text:  ball
2025-02-11 07:51:00,069 - Step 2: Updated current_phrase
2025-02-11 07:51:00,069 - Step 2: Created step_acts
2025-02-11 07:51:00,069 - Step 2: Added to generation_acts
2025-02-11 07:51:00,069 - Step 2: Updated recent_tokens
2025-02-11 07:51:00,070 - Step 2: Decoded current text
2025-02-11 07:51:00,070 - Step 2: Reset consecutive_fillers
2025-02-11 07:51:00,070 - 
Starting step 3
2025-02-11 07:51:00,071 - Current_ids device: cuda:0
2025-02-11 07:51:00,071 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,093 - Model output complete
2025-02-11 07:51:00,093 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:51:00,093 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,093 - Next token logits device: cuda:0
2025-02-11 07:51:00,093 - Entered do_sample
2025-02-11 07:51:00,094 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,096 - Probs max: 0.99951171875
2025-02-11 07:51:00,096 - Pre-cat
2025-02-11 07:51:00,096 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935]],
       device='cuda:0')
2025-02-11 07:51:00,098 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:51:00,098 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:51:00,098 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,098 - Step 3: Generated next token
2025-02-11 07:51:00,098 - Step 3: Updated current_ids
2025-02-11 07:51:00,098 - Step 3: Decoded token text:  is
2025-02-11 07:51:00,098 - Step 3: Updated current_phrase
2025-02-11 07:51:00,099 - Step 3: Created step_acts
2025-02-11 07:51:00,099 - Step 3: Added to generation_acts
2025-02-11 07:51:00,099 - Step 3: Updated recent_tokens
2025-02-11 07:51:00,100 - Step 3: Decoded current text
2025-02-11 07:51:00,100 - Step 3: Reset consecutive_fillers
2025-02-11 07:51:00,100 - 
Starting step 4
2025-02-11 07:51:00,100 - Current_ids device: cuda:0
2025-02-11 07:51:00,100 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,123 - Model output complete
2025-02-11 07:51:00,124 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:51:00,124 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,124 - Next token logits device: cuda:0
2025-02-11 07:51:00,124 - Entered do_sample
2025-02-11 07:51:00,124 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,126 - Probs max: 0.998046875
2025-02-11 07:51:00,127 - Pre-cat
2025-02-11 07:51:00,127 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374]],
       device='cuda:0')
2025-02-11 07:51:00,128 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:51:00,129 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:51:00,129 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,129 - Step 4: Generated next token
2025-02-11 07:51:00,129 - Step 4: Updated current_ids
2025-02-11 07:51:00,129 - Step 4: Decoded token text:  thrown
2025-02-11 07:51:00,129 - Step 4: Updated current_phrase
2025-02-11 07:51:00,129 - Step 4: Created step_acts
2025-02-11 07:51:00,130 - Step 4: Added to generation_acts
2025-02-11 07:51:00,130 - Step 4: Updated recent_tokens
2025-02-11 07:51:00,131 - Step 4: Decoded current text
2025-02-11 07:51:00,131 - Step 4: Reset consecutive_fillers
2025-02-11 07:51:00,131 - 
Starting step 5
2025-02-11 07:51:00,131 - Current_ids device: cuda:0
2025-02-11 07:51:00,131 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,154 - Model output complete
2025-02-11 07:51:00,154 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:51:00,154 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,155 - Next token logits device: cuda:0
2025-02-11 07:51:00,155 - Entered do_sample
2025-02-11 07:51:00,155 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,157 - Probs max: 0.9697265625
2025-02-11 07:51:00,157 - Pre-cat
2025-02-11 07:51:00,157 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:51:00,159 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:51:00,159 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:51:00,159 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,159 - Step 5: Generated next token
2025-02-11 07:51:00,159 - Step 5: Updated current_ids
2025-02-11 07:51:00,159 - Step 5: Decoded token text:  at
2025-02-11 07:51:00,160 - Step 5: Updated current_phrase
2025-02-11 07:51:00,160 - Step 5: Created step_acts
2025-02-11 07:51:00,160 - Step 5: Added to generation_acts
2025-02-11 07:51:00,161 - Step 5: Updated generated_texts
2025-02-11 07:51:00,161 - Step 5: Updated recent_tokens
2025-02-11 07:51:00,162 - Step 5: Decoded current text
2025-02-11 07:51:00,162 - Step 5: Reset consecutive_fillers
2025-02-11 07:51:00,162 - 
Starting step 6
2025-02-11 07:51:00,162 - Current_ids device: cuda:0
2025-02-11 07:51:00,162 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,186 - Model output complete
2025-02-11 07:51:00,186 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:51:00,186 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,186 - Next token logits device: cuda:0
2025-02-11 07:51:00,186 - Entered do_sample
2025-02-11 07:51:00,186 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,188 - Probs max: 0.99951171875
2025-02-11 07:51:00,189 - Pre-cat
2025-02-11 07:51:00,189 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:51:00,190 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:51:00,191 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:51:00,191 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,191 - Step 6: Generated next token
2025-02-11 07:51:00,191 - Step 6: Updated current_ids
2025-02-11 07:51:00,191 - Step 6: Decoded token text:  a
2025-02-11 07:51:00,191 - Step 6: Updated current_phrase
2025-02-11 07:51:00,191 - Step 6: Created step_acts
2025-02-11 07:51:00,191 - Step 6: Added to generation_acts
2025-02-11 07:51:00,191 - Step 6: Updated recent_tokens
2025-02-11 07:51:00,193 - Step 6: Decoded current text
2025-02-11 07:51:00,193 - Step 6: Reset consecutive_fillers
2025-02-11 07:51:00,193 - 
Starting step 7
2025-02-11 07:51:00,193 - Current_ids device: cuda:0
2025-02-11 07:51:00,193 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,217 - Model output complete
2025-02-11 07:51:00,217 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:51:00,217 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,217 - Next token logits device: cuda:0
2025-02-11 07:51:00,217 - Entered do_sample
2025-02-11 07:51:00,217 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,220 - Probs max: 0.9990234375
2025-02-11 07:51:00,220 - Pre-cat
2025-02-11 07:51:00,220 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:51:00,222 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:51:00,222 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:51:00,222 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,223 - Step 7: Generated next token
2025-02-11 07:51:00,223 - Step 7: Updated current_ids
2025-02-11 07:51:00,223 - Step 7: Decoded token text:  wall
2025-02-11 07:51:00,223 - Step 7: Updated current_phrase
2025-02-11 07:51:00,223 - Step 7: Created step_acts
2025-02-11 07:51:00,223 - Step 7: Added to generation_acts
2025-02-11 07:51:00,223 - Step 7: Updated recent_tokens
2025-02-11 07:51:00,224 - Step 7: Decoded current text
2025-02-11 07:51:00,225 - Step 7: Reset consecutive_fillers
2025-02-11 07:51:00,225 - 
Starting step 8
2025-02-11 07:51:00,225 - Current_ids device: cuda:0
2025-02-11 07:51:00,225 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,248 - Model output complete
2025-02-11 07:51:00,248 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:51:00,248 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,248 - Next token logits device: cuda:0
2025-02-11 07:51:00,248 - Entered do_sample
2025-02-11 07:51:00,248 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,250 - Probs max: 0.9912109375
2025-02-11 07:51:00,251 - Pre-cat
2025-02-11 07:51:00,251 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:51:00,253 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:51:00,253 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:51:00,253 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,253 - Step 8: Generated next token
2025-02-11 07:51:00,253 - Step 8: Updated current_ids
2025-02-11 07:51:00,253 - Step 8: Decoded token text:  very
2025-02-11 07:51:00,253 - Step 8: Updated current_phrase
2025-02-11 07:51:00,253 - Step 8: Created step_acts
2025-02-11 07:51:00,253 - Step 8: Added to generation_acts
2025-02-11 07:51:00,254 - Step 8: Updated recent_tokens
2025-02-11 07:51:00,255 - Step 8: Decoded current text
2025-02-11 07:51:00,255 - Step 8: Reset consecutive_fillers
2025-02-11 07:51:00,255 - 
Starting step 9
2025-02-11 07:51:00,255 - Current_ids device: cuda:0
2025-02-11 07:51:00,255 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,278 - Model output complete
2025-02-11 07:51:00,279 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:51:00,279 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,279 - Next token logits device: cuda:0
2025-02-11 07:51:00,279 - Entered do_sample
2025-02-11 07:51:00,279 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,281 - Probs max: 0.998046875
2025-02-11 07:51:00,282 - Pre-cat
2025-02-11 07:51:00,282 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:51:00,284 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:51:00,284 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:51:00,284 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,284 - Step 9: Generated next token
2025-02-11 07:51:00,284 - Step 9: Updated current_ids
2025-02-11 07:51:00,285 - Step 9: Decoded token text:  fast
2025-02-11 07:51:00,285 - Step 9: Updated current_phrase
2025-02-11 07:51:00,285 - Step 9: Created step_acts
2025-02-11 07:51:00,285 - Step 9: Added to generation_acts
2025-02-11 07:51:00,285 - Step 9: Updated recent_tokens
2025-02-11 07:51:00,286 - Step 9: Decoded current text
2025-02-11 07:51:00,286 - Step 9: Reset consecutive_fillers
2025-02-11 07:51:00,286 - 
Starting step 10
2025-02-11 07:51:00,287 - Current_ids device: cuda:0
2025-02-11 07:51:00,287 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,309 - Model output complete
2025-02-11 07:51:00,309 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:51:00,309 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,309 - Next token logits device: cuda:0
2025-02-11 07:51:00,309 - Entered do_sample
2025-02-11 07:51:00,310 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,312 - Probs max: 0.99853515625
2025-02-11 07:51:00,313 - Pre-cat
2025-02-11 07:51:00,313 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:51:00,314 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:51:00,314 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:51:00,314 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,315 - Step 10: Generated next token
2025-02-11 07:51:00,315 - Step 10: Updated current_ids
2025-02-11 07:51:00,315 - Step 10: Decoded token text: ,
2025-02-11 07:51:00,315 - Step 10: Updated current_phrase
2025-02-11 07:51:00,315 - Step 10: Created step_acts
2025-02-11 07:51:00,315 - Step 10: Added to generation_acts
2025-02-11 07:51:00,317 - Step 10: Updated generated_texts
2025-02-11 07:51:00,317 - Step 10: Updated recent_tokens
2025-02-11 07:51:00,317 - Step 10: Found phrase end token
2025-02-11 07:51:00,317 - Step 10: Updated recent_phrases
2025-02-11 07:51:00,317 - Step 10: Decoded current text
2025-02-11 07:51:00,317 - Step 10: Reset consecutive_fillers
2025-02-11 07:51:00,317 - 
Starting step 11
2025-02-11 07:51:00,317 - Current_ids device: cuda:0
2025-02-11 07:51:00,317 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,340 - Model output complete
2025-02-11 07:51:00,340 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:51:00,340 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,340 - Next token logits device: cuda:0
2025-02-11 07:51:00,340 - Entered do_sample
2025-02-11 07:51:00,340 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,343 - Probs max: 0.58154296875
2025-02-11 07:51:00,343 - Pre-cat
2025-02-11 07:51:00,343 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:51:00,345 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:51:00,345 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:51:00,345 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,345 - Step 11: Generated next token
2025-02-11 07:51:00,345 - Step 11: Updated current_ids
2025-02-11 07:51:00,345 - Step 11: Decoded token text:  the
2025-02-11 07:51:00,345 - Step 11: Updated current_phrase
2025-02-11 07:51:00,346 - Step 11: Created step_acts
2025-02-11 07:51:00,346 - Step 11: Added to generation_acts
2025-02-11 07:51:00,346 - Step 11: Updated recent_tokens
2025-02-11 07:51:00,347 - Step 11: Decoded current text
2025-02-11 07:51:00,347 - Step 11: Reset consecutive_fillers
2025-02-11 07:51:00,347 - 
Starting step 12
2025-02-11 07:51:00,347 - Current_ids device: cuda:0
2025-02-11 07:51:00,347 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,370 - Model output complete
2025-02-11 07:51:00,370 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:51:00,370 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,371 - Next token logits device: cuda:0
2025-02-11 07:51:00,371 - Entered do_sample
2025-02-11 07:51:00,371 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,373 - Probs max: 0.6064453125
2025-02-11 07:51:00,373 - Pre-cat
2025-02-11 07:51:00,374 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279]],
       device='cuda:0')
2025-02-11 07:51:00,375 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:51:00,375 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:51:00,375 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,376 - Step 12: Generated next token
2025-02-11 07:51:00,376 - Step 12: Updated current_ids
2025-02-11 07:51:00,376 - Step 12: Decoded token text:  ball
2025-02-11 07:51:00,376 - Step 12: Updated current_phrase
2025-02-11 07:51:00,376 - Step 12: Created step_acts
2025-02-11 07:51:00,376 - Step 12: Added to generation_acts
2025-02-11 07:51:00,376 - Step 12: Updated recent_tokens
2025-02-11 07:51:00,378 - Step 12: Decoded current text
2025-02-11 07:51:00,378 - Step 12: Reset consecutive_fillers
2025-02-11 07:51:00,378 - 
Starting step 13
2025-02-11 07:51:00,378 - Current_ids device: cuda:0
2025-02-11 07:51:00,378 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,403 - Model output complete
2025-02-11 07:51:00,404 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:51:00,404 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,404 - Next token logits device: cuda:0
2025-02-11 07:51:00,404 - Entered do_sample
2025-02-11 07:51:00,404 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,406 - Probs max: 0.76953125
2025-02-11 07:51:00,407 - Pre-cat
2025-02-11 07:51:00,407 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935]],
       device='cuda:0')
2025-02-11 07:51:00,410 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:51:00,410 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:51:00,410 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,410 - Step 13: Generated next token
2025-02-11 07:51:00,410 - Step 13: Updated current_ids
2025-02-11 07:51:00,411 - Step 13: Decoded token text:  will
2025-02-11 07:51:00,411 - Step 13: Updated current_phrase
2025-02-11 07:51:00,411 - Step 13: Created step_acts
2025-02-11 07:51:00,411 - Step 13: Added to generation_acts
2025-02-11 07:51:00,411 - Step 13: Updated recent_tokens
2025-02-11 07:51:00,413 - Step 13: Decoded current text
2025-02-11 07:51:00,413 - Step 13: Reset consecutive_fillers
2025-02-11 07:51:00,413 - 
Starting step 14
2025-02-11 07:51:00,413 - Current_ids device: cuda:0
2025-02-11 07:51:00,413 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,457 - Model output complete
2025-02-11 07:51:00,457 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:51:00,457 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,457 - Next token logits device: cuda:0
2025-02-11 07:51:00,458 - Entered do_sample
2025-02-11 07:51:00,458 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,460 - Probs max: 0.43408203125
2025-02-11 07:51:00,461 - Pre-cat
2025-02-11 07:51:00,461 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686]],
       device='cuda:0')
2025-02-11 07:51:00,462 - Next token: tensor([[387]], device='cuda:0')
2025-02-11 07:51:00,463 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:51:00,463 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,463 - Step 14: Generated next token
2025-02-11 07:51:00,463 - Step 14: Updated current_ids
2025-02-11 07:51:00,463 - Step 14: Decoded token text:  be
2025-02-11 07:51:00,463 - Step 14: Updated current_phrase
2025-02-11 07:51:00,463 - Step 14: Created step_acts
2025-02-11 07:51:00,463 - Step 14: Added to generation_acts
2025-02-11 07:51:00,463 - Step 14: Updated recent_tokens
2025-02-11 07:51:00,465 - Step 14: Decoded current text
2025-02-11 07:51:00,465 - Step 14: Reset consecutive_fillers
2025-02-11 07:51:00,465 - 
Starting step 15
2025-02-11 07:51:00,465 - Current_ids device: cuda:0
2025-02-11 07:51:00,465 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,490 - Model output complete
2025-02-11 07:51:00,490 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:51:00,490 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,490 - Next token logits device: cuda:0
2025-02-11 07:51:00,490 - Entered do_sample
2025-02-11 07:51:00,490 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,492 - Probs max: 0.533203125
2025-02-11 07:51:00,493 - Pre-cat
2025-02-11 07:51:00,493 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387]], device='cuda:0')
2025-02-11 07:51:00,496 - Next token: tensor([[7218]], device='cuda:0')
2025-02-11 07:51:00,497 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:51:00,497 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,497 - Step 15: Generated next token
2025-02-11 07:51:00,497 - Step 15: Updated current_ids
2025-02-11 07:51:00,497 - Step 15: Decoded token text:  moving
2025-02-11 07:51:00,497 - Step 15: Updated current_phrase
2025-02-11 07:51:00,498 - Step 15: Created step_acts
2025-02-11 07:51:00,498 - Step 15: Added to generation_acts
2025-02-11 07:51:00,499 - Step 15: Updated generated_texts
2025-02-11 07:51:00,499 - Step 15: Updated recent_tokens
2025-02-11 07:51:00,499 - Step 15: Decoded current text
2025-02-11 07:51:00,499 - Step 15: Reset consecutive_fillers
2025-02-11 07:51:00,500 - 
Starting step 16
2025-02-11 07:51:00,500 - Current_ids device: cuda:0
2025-02-11 07:51:00,500 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,535 - Model output complete
2025-02-11 07:51:00,536 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:51:00,536 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,536 - Next token logits device: cuda:0
2025-02-11 07:51:00,536 - Entered do_sample
2025-02-11 07:51:00,536 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,538 - Probs max: 0.42822265625
2025-02-11 07:51:00,538 - Pre-cat
2025-02-11 07:51:00,538 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218]], device='cuda:0')
2025-02-11 07:51:00,540 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:51:00,541 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:51:00,541 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,541 - Step 16: Generated next token
2025-02-11 07:51:00,541 - Step 16: Updated current_ids
2025-02-11 07:51:00,541 - Step 16: Decoded token text:  at
2025-02-11 07:51:00,541 - Step 16: Updated current_phrase
2025-02-11 07:51:00,542 - Step 16: Created step_acts
2025-02-11 07:51:00,542 - Step 16: Added to generation_acts
2025-02-11 07:51:00,542 - Step 16: Updated recent_tokens
2025-02-11 07:51:00,543 - Step 16: Decoded current text
2025-02-11 07:51:00,543 - Step 16: Reset consecutive_fillers
2025-02-11 07:51:00,543 - Step 16: Calculated unique_ratio: 0.8125
2025-02-11 07:51:00,544 - 
Starting step 17
2025-02-11 07:51:00,544 - Current_ids device: cuda:0
2025-02-11 07:51:00,544 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,571 - Model output complete
2025-02-11 07:51:00,571 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:51:00,571 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,571 - Next token logits device: cuda:0
2025-02-11 07:51:00,571 - Entered do_sample
2025-02-11 07:51:00,571 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,574 - Probs max: 0.7265625
2025-02-11 07:51:00,574 - Pre-cat
2025-02-11 07:51:00,574 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,    518]], device='cuda:0')
2025-02-11 07:51:00,577 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:51:00,577 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:51:00,577 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,578 - Step 17: Generated next token
2025-02-11 07:51:00,578 - Step 17: Updated current_ids
2025-02-11 07:51:00,578 - Step 17: Decoded token text:  a
2025-02-11 07:51:00,578 - Step 17: Updated current_phrase
2025-02-11 07:51:00,578 - Step 17: Created step_acts
2025-02-11 07:51:00,578 - Step 17: Added to generation_acts
2025-02-11 07:51:00,579 - Step 17: Updated recent_tokens
2025-02-11 07:51:00,580 - Step 17: Decoded current text
2025-02-11 07:51:00,580 - Step 17: Reset consecutive_fillers
2025-02-11 07:51:00,580 - Step 17: Calculated unique_ratio: 0.8125
2025-02-11 07:51:00,580 - 
Starting step 18
2025-02-11 07:51:00,580 - Current_ids device: cuda:0
2025-02-11 07:51:00,580 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,604 - Model output complete
2025-02-11 07:51:00,605 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:51:00,605 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,605 - Next token logits device: cuda:0
2025-02-11 07:51:00,605 - Entered do_sample
2025-02-11 07:51:00,605 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,607 - Probs max: 0.4521484375
2025-02-11 07:51:00,608 - Pre-cat
2025-02-11 07:51:00,608 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,    518,    264]], device='cuda:0')
2025-02-11 07:51:00,612 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:51:00,612 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:51:00,612 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,612 - Step 18: Generated next token
2025-02-11 07:51:00,612 - Step 18: Updated current_ids
2025-02-11 07:51:00,612 - Step 18: Decoded token text:  very
2025-02-11 07:51:00,612 - Step 18: Updated current_phrase
2025-02-11 07:51:00,613 - Step 18: Created step_acts
2025-02-11 07:51:00,613 - Step 18: Added to generation_acts
2025-02-11 07:51:00,613 - Step 18: Updated recent_tokens
2025-02-11 07:51:00,614 - Step 18: Decoded current text
2025-02-11 07:51:00,615 - Step 18: Reset consecutive_fillers
2025-02-11 07:51:00,615 - Step 18: Calculated unique_ratio: 0.8125
2025-02-11 07:51:00,615 - 
Starting step 19
2025-02-11 07:51:00,615 - Current_ids device: cuda:0
2025-02-11 07:51:00,615 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,643 - Model output complete
2025-02-11 07:51:00,643 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:51:00,644 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,644 - Next token logits device: cuda:0
2025-02-11 07:51:00,644 - Entered do_sample
2025-02-11 07:51:00,644 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,646 - Probs max: 0.94091796875
2025-02-11 07:51:00,646 - Pre-cat
2025-02-11 07:51:00,647 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,    518,    264,   1602]], device='cuda:0')
2025-02-11 07:51:00,649 - Next token: tensor([[1550]], device='cuda:0')
2025-02-11 07:51:00,649 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:51:00,649 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,649 - Step 19: Generated next token
2025-02-11 07:51:00,650 - Step 19: Updated current_ids
2025-02-11 07:51:00,650 - Step 19: Decoded token text:  high
2025-02-11 07:51:00,650 - Step 19: Updated current_phrase
2025-02-11 07:51:00,650 - Step 19: Created step_acts
2025-02-11 07:51:00,650 - Step 19: Added to generation_acts
2025-02-11 07:51:00,650 - Step 19: Updated recent_tokens
2025-02-11 07:51:00,652 - Step 19: Decoded current text
2025-02-11 07:51:00,652 - Step 19: Reset consecutive_fillers
2025-02-11 07:51:00,652 - Step 19: Calculated unique_ratio: 0.8125
2025-02-11 07:51:00,652 - 
Starting step 20
2025-02-11 07:51:00,652 - Current_ids device: cuda:0
2025-02-11 07:51:00,652 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,676 - Model output complete
2025-02-11 07:51:00,677 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:51:00,677 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,677 - Next token logits device: cuda:0
2025-02-11 07:51:00,677 - Entered do_sample
2025-02-11 07:51:00,677 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,679 - Probs max: 0.91015625
2025-02-11 07:51:00,680 - Pre-cat
2025-02-11 07:51:00,680 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,    518,    264,   1602,   1550]], device='cuda:0')
2025-02-11 07:51:00,683 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:51:00,683 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:51:00,683 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,684 - Step 20: Generated next token
2025-02-11 07:51:00,684 - Step 20: Updated current_ids
2025-02-11 07:51:00,684 - Step 20: Decoded token text:  speed
2025-02-11 07:51:00,684 - Step 20: Updated current_phrase
2025-02-11 07:51:00,684 - Step 20: Created step_acts
2025-02-11 07:51:00,684 - Step 20: Added to generation_acts
2025-02-11 07:51:00,686 - Step 20: Updated generated_texts
2025-02-11 07:51:00,686 - Step 20: Updated recent_tokens
2025-02-11 07:51:00,686 - Step 20: Decoded current text
2025-02-11 07:51:00,686 - Step 20: Reset consecutive_fillers
2025-02-11 07:51:00,686 - Step 20: Calculated unique_ratio: 0.8125
2025-02-11 07:51:00,687 - 
Starting step 21
2025-02-11 07:51:00,687 - Current_ids device: cuda:0
2025-02-11 07:51:00,687 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,711 - Model output complete
2025-02-11 07:51:00,711 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:51:00,711 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,711 - Next token logits device: cuda:0
2025-02-11 07:51:00,711 - Entered do_sample
2025-02-11 07:51:00,711 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,714 - Probs max: 0.61474609375
2025-02-11 07:51:00,715 - Pre-cat
2025-02-11 07:51:00,715 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,    518,    264,   1602,   1550,   4628]],
       device='cuda:0')
2025-02-11 07:51:00,719 - Next token: tensor([[6974]], device='cuda:0')
2025-02-11 07:51:00,719 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:51:00,719 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,719 - Step 21: Generated next token
2025-02-11 07:51:00,719 - Step 21: Updated current_ids
2025-02-11 07:51:00,720 - Step 21: Decoded token text:  towards
2025-02-11 07:51:00,720 - Step 21: Updated current_phrase
2025-02-11 07:51:00,720 - Step 21: Created step_acts
2025-02-11 07:51:00,720 - Step 21: Added to generation_acts
2025-02-11 07:51:00,720 - Step 21: Updated recent_tokens
2025-02-11 07:51:00,722 - Step 21: Decoded current text
2025-02-11 07:51:00,722 - Step 21: Reset consecutive_fillers
2025-02-11 07:51:00,722 - Step 21: Calculated unique_ratio: 0.875
2025-02-11 07:51:00,722 - 
Starting step 22
2025-02-11 07:51:00,722 - Current_ids device: cuda:0
2025-02-11 07:51:00,722 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,749 - Model output complete
2025-02-11 07:51:00,750 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:51:00,750 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,750 - Next token logits device: cuda:0
2025-02-11 07:51:00,750 - Entered do_sample
2025-02-11 07:51:00,750 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,752 - Probs max: 0.99169921875
2025-02-11 07:51:00,753 - Pre-cat
2025-02-11 07:51:00,753 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,    518,    264,   1602,   1550,   4628,   6974]],
       device='cuda:0')
2025-02-11 07:51:00,755 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:51:00,756 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:51:00,756 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,756 - Step 22: Generated next token
2025-02-11 07:51:00,756 - Step 22: Updated current_ids
2025-02-11 07:51:00,756 - Step 22: Decoded token text:  the
2025-02-11 07:51:00,756 - Step 22: Updated current_phrase
2025-02-11 07:51:00,757 - Step 22: Created step_acts
2025-02-11 07:51:00,757 - Step 22: Added to generation_acts
2025-02-11 07:51:00,757 - Step 22: Updated recent_tokens
2025-02-11 07:51:00,758 - Step 22: Decoded current text
2025-02-11 07:51:00,758 - Step 22: Reset consecutive_fillers
2025-02-11 07:51:00,759 - Step 22: Calculated unique_ratio: 0.875
2025-02-11 07:51:00,759 - 
Starting step 23
2025-02-11 07:51:00,759 - Current_ids device: cuda:0
2025-02-11 07:51:00,759 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,785 - Model output complete
2025-02-11 07:51:00,785 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:51:00,785 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,785 - Next token logits device: cuda:0
2025-02-11 07:51:00,785 - Entered do_sample
2025-02-11 07:51:00,785 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,788 - Probs max: 0.99951171875
2025-02-11 07:51:00,788 - Pre-cat
2025-02-11 07:51:00,788 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,    518,    264,   1602,   1550,   4628,   6974,    279]],
       device='cuda:0')
2025-02-11 07:51:00,791 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:51:00,791 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:51:00,791 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,791 - Step 23: Generated next token
2025-02-11 07:51:00,791 - Step 23: Updated current_ids
2025-02-11 07:51:00,791 - Step 23: Decoded token text:  wall
2025-02-11 07:51:00,791 - Step 23: Updated current_phrase
2025-02-11 07:51:00,792 - Step 23: Created step_acts
2025-02-11 07:51:00,792 - Step 23: Added to generation_acts
2025-02-11 07:51:00,792 - Step 23: Updated recent_tokens
2025-02-11 07:51:00,793 - Step 23: Decoded current text
2025-02-11 07:51:00,794 - Step 23: Reset consecutive_fillers
2025-02-11 07:51:00,794 - Step 23: Calculated unique_ratio: 0.875
2025-02-11 07:51:00,794 - 
Starting step 24
2025-02-11 07:51:00,794 - Current_ids device: cuda:0
2025-02-11 07:51:00,794 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,817 - Model output complete
2025-02-11 07:51:00,818 - Logits shape: torch.Size([1, 55, 151936])
2025-02-11 07:51:00,818 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,818 - Next token logits device: cuda:0
2025-02-11 07:51:00,818 - Entered do_sample
2025-02-11 07:51:00,818 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,820 - Probs max: 0.796875
2025-02-11 07:51:00,821 - Pre-cat
2025-02-11 07:51:00,821 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,    518,    264,   1602,   1550,   4628,   6974,    279,
           7002]], device='cuda:0')
2025-02-11 07:51:00,823 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:51:00,824 - Current_ids shape: torch.Size([1, 55])
2025-02-11 07:51:00,824 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,824 - Step 24: Generated next token
2025-02-11 07:51:00,824 - Step 24: Updated current_ids
2025-02-11 07:51:00,824 - Step 24: Decoded token text: ,
2025-02-11 07:51:00,824 - Step 24: Updated current_phrase
2025-02-11 07:51:00,824 - Step 24: Created step_acts
2025-02-11 07:51:00,825 - Step 24: Added to generation_acts
2025-02-11 07:51:00,825 - Step 24: Updated recent_tokens
2025-02-11 07:51:00,826 - Step 24: Found phrase end token
2025-02-11 07:51:00,826 - Step 24: Updated recent_phrases
2025-02-11 07:51:00,826 - Step 24: Calculated similarity: 0.4444444444444444
2025-02-11 07:51:00,826 - Step 24: Decoded current text
2025-02-11 07:51:00,826 - Step 24: Reset consecutive_fillers
2025-02-11 07:51:00,827 - Step 24: Calculated unique_ratio: 0.875
2025-02-11 07:51:00,827 - 
Starting step 25
2025-02-11 07:51:00,827 - Current_ids device: cuda:0
2025-02-11 07:51:00,827 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,852 - Model output complete
2025-02-11 07:51:00,852 - Logits shape: torch.Size([1, 56, 151936])
2025-02-11 07:51:00,852 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,852 - Next token logits device: cuda:0
2025-02-11 07:51:00,852 - Entered do_sample
2025-02-11 07:51:00,852 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,854 - Probs max: 0.537109375
2025-02-11 07:51:00,855 - Pre-cat
2025-02-11 07:51:00,855 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,    518,    264,   1602,   1550,   4628,   6974,    279,
           7002,     11]], device='cuda:0')
2025-02-11 07:51:00,858 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:51:00,858 - Current_ids shape: torch.Size([1, 56])
2025-02-11 07:51:00,858 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,858 - Step 25: Generated next token
2025-02-11 07:51:00,858 - Step 25: Updated current_ids
2025-02-11 07:51:00,858 - Step 25: Decoded token text:  and
2025-02-11 07:51:00,859 - Step 25: Updated current_phrase
2025-02-11 07:51:00,859 - Step 25: Created step_acts
2025-02-11 07:51:00,859 - Step 25: Added to generation_acts
2025-02-11 07:51:00,860 - Step 25: Updated generated_texts
2025-02-11 07:51:00,861 - Step 25: Updated recent_tokens
2025-02-11 07:51:00,861 - Step 25: Decoded current text
2025-02-11 07:51:00,861 - Step 25: Reset consecutive_fillers
2025-02-11 07:51:00,861 - Step 25: Calculated unique_ratio: 0.875
2025-02-11 07:51:00,861 - 
Starting step 26
2025-02-11 07:51:00,861 - Current_ids device: cuda:0
2025-02-11 07:51:00,861 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,888 - Model output complete
2025-02-11 07:51:00,888 - Logits shape: torch.Size([1, 57, 151936])
2025-02-11 07:51:00,888 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,888 - Next token logits device: cuda:0
2025-02-11 07:51:00,889 - Entered do_sample
2025-02-11 07:51:00,889 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,891 - Probs max: 0.8642578125
2025-02-11 07:51:00,895 - Pre-cat
2025-02-11 07:51:00,895 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,    518,    264,   1602,   1550,   4628,   6974,    279,
           7002,     11,    323]], device='cuda:0')
2025-02-11 07:51:00,899 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:51:00,899 - Current_ids shape: torch.Size([1, 57])
2025-02-11 07:51:00,900 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,900 - Step 26: Generated next token
2025-02-11 07:51:00,900 - Step 26: Updated current_ids
2025-02-11 07:51:00,900 - Step 26: Decoded token text:  the
2025-02-11 07:51:00,900 - Step 26: Updated current_phrase
2025-02-11 07:51:00,901 - Step 26: Created step_acts
2025-02-11 07:51:00,901 - Step 26: Added to generation_acts
2025-02-11 07:51:00,901 - Step 26: Updated recent_tokens
2025-02-11 07:51:00,902 - Step 26: Decoded current text
2025-02-11 07:51:00,903 - Step 26: Incremented consecutive_fillers to 1
2025-02-11 07:51:00,903 - Step 26: Calculated unique_ratio: 0.875
2025-02-11 07:51:00,903 - 
Starting step 27
2025-02-11 07:51:00,903 - Current_ids device: cuda:0
2025-02-11 07:51:00,903 - Current_ids dtype: torch.int64
2025-02-11 07:51:00,962 - Model output complete
2025-02-11 07:51:00,962 - Logits shape: torch.Size([1, 58, 151936])
2025-02-11 07:51:00,962 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,962 - Next token logits device: cuda:0
2025-02-11 07:51:00,962 - Entered do_sample
2025-02-11 07:51:00,963 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:00,965 - Probs max: 0.9384765625
2025-02-11 07:51:00,966 - Pre-cat
2025-02-11 07:51:00,966 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,    518,    264,   1602,   1550,   4628,   6974,    279,
           7002,     11,    323,    279]], device='cuda:0')
2025-02-11 07:51:00,970 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:51:00,971 - Current_ids shape: torch.Size([1, 58])
2025-02-11 07:51:00,971 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:00,971 - Step 27: Generated next token
2025-02-11 07:51:00,971 - Step 27: Updated current_ids
2025-02-11 07:51:00,972 - Step 27: Decoded token text:  wall
2025-02-11 07:51:00,972 - Step 27: Updated current_phrase
2025-02-11 07:51:00,973 - Step 27: Created step_acts
2025-02-11 07:51:00,973 - Step 27: Added to generation_acts
2025-02-11 07:51:00,973 - Step 27: Updated recent_tokens
2025-02-11 07:51:00,975 - Step 27: Decoded current text
2025-02-11 07:51:00,975 - Step 27: Incremented consecutive_fillers to 2
2025-02-11 07:51:00,975 - Step 27: Calculated unique_ratio: 0.875
2025-02-11 07:51:00,976 - 
Starting step 28
2025-02-11 07:51:00,976 - Current_ids device: cuda:0
2025-02-11 07:51:00,976 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,015 - Model output complete
2025-02-11 07:51:01,016 - Logits shape: torch.Size([1, 59, 151936])
2025-02-11 07:51:01,016 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,016 - Next token logits device: cuda:0
2025-02-11 07:51:01,016 - Entered do_sample
2025-02-11 07:51:01,016 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,018 - Probs max: 0.95556640625
2025-02-11 07:51:01,019 - Pre-cat
2025-02-11 07:51:01,019 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,    518,    264,   1602,   1550,   4628,   6974,    279,
           7002,     11,    323,    279,   7002]], device='cuda:0')
2025-02-11 07:51:01,023 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:51:01,024 - Current_ids shape: torch.Size([1, 59])
2025-02-11 07:51:01,024 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,024 - Step 28: Generated next token
2025-02-11 07:51:01,024 - Step 28: Updated current_ids
2025-02-11 07:51:01,024 - Step 28: Decoded token text:  will
2025-02-11 07:51:01,024 - Step 28: Updated current_phrase
2025-02-11 07:51:01,025 - Step 28: Created step_acts
2025-02-11 07:51:01,025 - Step 28: Added to generation_acts
2025-02-11 07:51:01,025 - Step 28: Updated recent_tokens
2025-02-11 07:51:01,026 - Step 28: Decoded current text
2025-02-11 07:51:01,026 - Step 28: Incremented consecutive_fillers to 3
2025-02-11 07:51:01,162 - 
Starting step 0
2025-02-11 07:51:01,162 - Current_ids device: cuda:0
2025-02-11 07:51:01,162 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,193 - Model output complete
2025-02-11 07:51:01,193 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:51:01,193 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,193 - Next token logits device: cuda:0
2025-02-11 07:51:01,193 - Entered do_sample
2025-02-11 07:51:01,193 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,197 - Probs max: 0.50341796875
2025-02-11 07:51:01,197 - Pre-cat
2025-02-11 07:51:01,197 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:51:01,199 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:51:01,199 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:51:01,199 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,199 - Step 0: Generated next token
2025-02-11 07:51:01,200 - Step 0: Updated current_ids
2025-02-11 07:51:01,200 - Step 0: Decoded token text:  The
2025-02-11 07:51:01,200 - Step 0: Updated current_phrase
2025-02-11 07:51:01,200 - Step 0: Created step_acts
2025-02-11 07:51:01,201 - Step 0: Added to generation_acts
2025-02-11 07:51:01,202 - Step 0: Updated generated_texts
2025-02-11 07:51:01,202 - Step 0: Updated recent_tokens
2025-02-11 07:51:01,202 - Step 0: Decoded current text
2025-02-11 07:51:01,202 - Step 0: Reset consecutive_fillers
2025-02-11 07:51:01,202 - 
Starting step 1
2025-02-11 07:51:01,202 - Current_ids device: cuda:0
2025-02-11 07:51:01,202 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,227 - Model output complete
2025-02-11 07:51:01,227 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:51:01,227 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,227 - Next token logits device: cuda:0
2025-02-11 07:51:01,228 - Entered do_sample
2025-02-11 07:51:01,228 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,230 - Probs max: 0.83837890625
2025-02-11 07:51:01,230 - Pre-cat
2025-02-11 07:51:01,230 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:51:01,233 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:51:01,233 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:51:01,233 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,233 - Step 1: Generated next token
2025-02-11 07:51:01,233 - Step 1: Updated current_ids
2025-02-11 07:51:01,234 - Step 1: Decoded token text:  ball
2025-02-11 07:51:01,234 - Step 1: Updated current_phrase
2025-02-11 07:51:01,234 - Step 1: Created step_acts
2025-02-11 07:51:01,234 - Step 1: Added to generation_acts
2025-02-11 07:51:01,234 - Step 1: Updated recent_tokens
2025-02-11 07:51:01,236 - Step 1: Decoded current text
2025-02-11 07:51:01,236 - Step 1: Reset consecutive_fillers
2025-02-11 07:51:01,236 - 
Starting step 2
2025-02-11 07:51:01,236 - Current_ids device: cuda:0
2025-02-11 07:51:01,236 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,261 - Model output complete
2025-02-11 07:51:01,262 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:51:01,262 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,262 - Next token logits device: cuda:0
2025-02-11 07:51:01,262 - Entered do_sample
2025-02-11 07:51:01,262 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,264 - Probs max: 0.583984375
2025-02-11 07:51:01,265 - Pre-cat
2025-02-11 07:51:01,266 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:51:01,267 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:51:01,267 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:51:01,267 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,267 - Step 2: Generated next token
2025-02-11 07:51:01,267 - Step 2: Updated current_ids
2025-02-11 07:51:01,268 - Step 2: Decoded token text:  is
2025-02-11 07:51:01,268 - Step 2: Updated current_phrase
2025-02-11 07:51:01,268 - Step 2: Created step_acts
2025-02-11 07:51:01,268 - Step 2: Added to generation_acts
2025-02-11 07:51:01,268 - Step 2: Updated recent_tokens
2025-02-11 07:51:01,270 - Step 2: Decoded current text
2025-02-11 07:51:01,270 - Step 2: Reset consecutive_fillers
2025-02-11 07:51:01,270 - 
Starting step 3
2025-02-11 07:51:01,270 - Current_ids device: cuda:0
2025-02-11 07:51:01,270 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,294 - Model output complete
2025-02-11 07:51:01,294 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:51:01,294 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,295 - Next token logits device: cuda:0
2025-02-11 07:51:01,295 - Entered do_sample
2025-02-11 07:51:01,295 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,297 - Probs max: 0.59521484375
2025-02-11 07:51:01,298 - Pre-cat
2025-02-11 07:51:01,298 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374]],
       device='cuda:0')
2025-02-11 07:51:01,300 - Next token: tensor([[7218]], device='cuda:0')
2025-02-11 07:51:01,300 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:51:01,300 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,300 - Step 3: Generated next token
2025-02-11 07:51:01,300 - Step 3: Updated current_ids
2025-02-11 07:51:01,300 - Step 3: Decoded token text:  moving
2025-02-11 07:51:01,300 - Step 3: Updated current_phrase
2025-02-11 07:51:01,301 - Step 3: Created step_acts
2025-02-11 07:51:01,301 - Step 3: Added to generation_acts
2025-02-11 07:51:01,301 - Step 3: Updated recent_tokens
2025-02-11 07:51:01,302 - Step 3: Decoded current text
2025-02-11 07:51:01,302 - Step 3: Reset consecutive_fillers
2025-02-11 07:51:01,302 - 
Starting step 4
2025-02-11 07:51:01,302 - Current_ids device: cuda:0
2025-02-11 07:51:01,302 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,325 - Model output complete
2025-02-11 07:51:01,326 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:51:01,326 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,326 - Next token logits device: cuda:0
2025-02-11 07:51:01,326 - Entered do_sample
2025-02-11 07:51:01,326 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,329 - Probs max: 0.677734375
2025-02-11 07:51:01,330 - Pre-cat
2025-02-11 07:51:01,330 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218]],
       device='cuda:0')
2025-02-11 07:51:01,332 - Next token: tensor([[4637]], device='cuda:0')
2025-02-11 07:51:01,332 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:51:01,332 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,332 - Step 4: Generated next token
2025-02-11 07:51:01,333 - Step 4: Updated current_ids
2025-02-11 07:51:01,333 - Step 4: Decoded token text:  forward
2025-02-11 07:51:01,333 - Step 4: Updated current_phrase
2025-02-11 07:51:01,333 - Step 4: Created step_acts
2025-02-11 07:51:01,333 - Step 4: Added to generation_acts
2025-02-11 07:51:01,333 - Step 4: Updated recent_tokens
2025-02-11 07:51:01,334 - Step 4: Decoded current text
2025-02-11 07:51:01,335 - Step 4: Reset consecutive_fillers
2025-02-11 07:51:01,335 - 
Starting step 5
2025-02-11 07:51:01,335 - Current_ids device: cuda:0
2025-02-11 07:51:01,335 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,359 - Model output complete
2025-02-11 07:51:01,359 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:51:01,359 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,359 - Next token logits device: cuda:0
2025-02-11 07:51:01,359 - Entered do_sample
2025-02-11 07:51:01,359 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,362 - Probs max: 0.41015625
2025-02-11 07:51:01,363 - Pre-cat
2025-02-11 07:51:01,363 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637]],
       device='cuda:0')
2025-02-11 07:51:01,364 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:51:01,365 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:51:01,365 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,365 - Step 5: Generated next token
2025-02-11 07:51:01,365 - Step 5: Updated current_ids
2025-02-11 07:51:01,365 - Step 5: Decoded token text:  at
2025-02-11 07:51:01,365 - Step 5: Updated current_phrase
2025-02-11 07:51:01,366 - Step 5: Created step_acts
2025-02-11 07:51:01,366 - Step 5: Added to generation_acts
2025-02-11 07:51:01,367 - Step 5: Updated generated_texts
2025-02-11 07:51:01,367 - Step 5: Updated recent_tokens
2025-02-11 07:51:01,367 - Step 5: Decoded current text
2025-02-11 07:51:01,367 - Step 5: Reset consecutive_fillers
2025-02-11 07:51:01,368 - 
Starting step 6
2025-02-11 07:51:01,368 - Current_ids device: cuda:0
2025-02-11 07:51:01,368 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,390 - Model output complete
2025-02-11 07:51:01,390 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:51:01,390 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,390 - Next token logits device: cuda:0
2025-02-11 07:51:01,390 - Entered do_sample
2025-02-11 07:51:01,390 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,394 - Probs max: 0.52490234375
2025-02-11 07:51:01,395 - Pre-cat
2025-02-11 07:51:01,395 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518]], device='cuda:0')
2025-02-11 07:51:01,398 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:51:01,398 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:51:01,398 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,398 - Step 6: Generated next token
2025-02-11 07:51:01,399 - Step 6: Updated current_ids
2025-02-11 07:51:01,399 - Step 6: Decoded token text:  a
2025-02-11 07:51:01,399 - Step 6: Updated current_phrase
2025-02-11 07:51:01,399 - Step 6: Created step_acts
2025-02-11 07:51:01,399 - Step 6: Added to generation_acts
2025-02-11 07:51:01,399 - Step 6: Updated recent_tokens
2025-02-11 07:51:01,401 - Step 6: Decoded current text
2025-02-11 07:51:01,401 - Step 6: Reset consecutive_fillers
2025-02-11 07:51:01,401 - 
Starting step 7
2025-02-11 07:51:01,401 - Current_ids device: cuda:0
2025-02-11 07:51:01,401 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,428 - Model output complete
2025-02-11 07:51:01,428 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:51:01,428 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,428 - Next token logits device: cuda:0
2025-02-11 07:51:01,428 - Entered do_sample
2025-02-11 07:51:01,429 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,431 - Probs max: 0.357666015625
2025-02-11 07:51:01,432 - Pre-cat
2025-02-11 07:51:01,432 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264]], device='cuda:0')
2025-02-11 07:51:01,434 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:51:01,434 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:51:01,434 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,434 - Step 7: Generated next token
2025-02-11 07:51:01,434 - Step 7: Updated current_ids
2025-02-11 07:51:01,435 - Step 7: Decoded token text:  speed
2025-02-11 07:51:01,435 - Step 7: Updated current_phrase
2025-02-11 07:51:01,435 - Step 7: Created step_acts
2025-02-11 07:51:01,435 - Step 7: Added to generation_acts
2025-02-11 07:51:01,435 - Step 7: Updated recent_tokens
2025-02-11 07:51:01,436 - Step 7: Decoded current text
2025-02-11 07:51:01,436 - Step 7: Reset consecutive_fillers
2025-02-11 07:51:01,437 - 
Starting step 8
2025-02-11 07:51:01,437 - Current_ids device: cuda:0
2025-02-11 07:51:01,437 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,462 - Model output complete
2025-02-11 07:51:01,463 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:51:01,463 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,463 - Next token logits device: cuda:0
2025-02-11 07:51:01,463 - Entered do_sample
2025-02-11 07:51:01,463 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,465 - Probs max: 0.5498046875
2025-02-11 07:51:01,466 - Pre-cat
2025-02-11 07:51:01,466 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628]], device='cuda:0')
2025-02-11 07:51:01,468 - Next token: tensor([[315]], device='cuda:0')
2025-02-11 07:51:01,468 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:51:01,468 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,468 - Step 8: Generated next token
2025-02-11 07:51:01,468 - Step 8: Updated current_ids
2025-02-11 07:51:01,468 - Step 8: Decoded token text:  of
2025-02-11 07:51:01,468 - Step 8: Updated current_phrase
2025-02-11 07:51:01,469 - Step 8: Created step_acts
2025-02-11 07:51:01,469 - Step 8: Added to generation_acts
2025-02-11 07:51:01,469 - Step 8: Updated recent_tokens
2025-02-11 07:51:01,470 - Step 8: Decoded current text
2025-02-11 07:51:01,470 - Step 8: Reset consecutive_fillers
2025-02-11 07:51:01,470 - 
Starting step 9
2025-02-11 07:51:01,470 - Current_ids device: cuda:0
2025-02-11 07:51:01,470 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,494 - Model output complete
2025-02-11 07:51:01,494 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:51:01,494 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,494 - Next token logits device: cuda:0
2025-02-11 07:51:01,494 - Entered do_sample
2025-02-11 07:51:01,494 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,498 - Probs max: 0.87646484375
2025-02-11 07:51:01,498 - Pre-cat
2025-02-11 07:51:01,498 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315]], device='cuda:0')
2025-02-11 07:51:01,500 - Next token: tensor([[348]], device='cuda:0')
2025-02-11 07:51:01,500 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:51:01,500 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,500 - Step 9: Generated next token
2025-02-11 07:51:01,500 - Step 9: Updated current_ids
2025-02-11 07:51:01,501 - Step 9: Decoded token text:  v
2025-02-11 07:51:01,501 - Step 9: Updated current_phrase
2025-02-11 07:51:01,501 - Step 9: Created step_acts
2025-02-11 07:51:01,501 - Step 9: Added to generation_acts
2025-02-11 07:51:01,501 - Step 9: Updated recent_tokens
2025-02-11 07:51:01,502 - Step 9: Decoded current text
2025-02-11 07:51:01,502 - Step 9: Reset consecutive_fillers
2025-02-11 07:51:01,502 - 
Starting step 10
2025-02-11 07:51:01,503 - Current_ids device: cuda:0
2025-02-11 07:51:01,503 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,526 - Model output complete
2025-02-11 07:51:01,526 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:51:01,526 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,526 - Next token logits device: cuda:0
2025-02-11 07:51:01,526 - Entered do_sample
2025-02-11 07:51:01,526 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,529 - Probs max: 0.7919921875
2025-02-11 07:51:01,529 - Pre-cat
2025-02-11 07:51:01,530 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348]], device='cuda:0')
2025-02-11 07:51:01,532 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:51:01,533 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:51:01,533 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,533 - Step 10: Generated next token
2025-02-11 07:51:01,533 - Step 10: Updated current_ids
2025-02-11 07:51:01,533 - Step 10: Decoded token text: ,
2025-02-11 07:51:01,533 - Step 10: Updated current_phrase
2025-02-11 07:51:01,533 - Step 10: Created step_acts
2025-02-11 07:51:01,534 - Step 10: Added to generation_acts
2025-02-11 07:51:01,535 - Step 10: Updated generated_texts
2025-02-11 07:51:01,535 - Step 10: Updated recent_tokens
2025-02-11 07:51:01,535 - Step 10: Found phrase end token
2025-02-11 07:51:01,535 - Step 10: Updated recent_phrases
2025-02-11 07:51:01,535 - Step 10: Decoded current text
2025-02-11 07:51:01,535 - Step 10: Reset consecutive_fillers
2025-02-11 07:51:01,535 - 
Starting step 11
2025-02-11 07:51:01,536 - Current_ids device: cuda:0
2025-02-11 07:51:01,536 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,560 - Model output complete
2025-02-11 07:51:01,560 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:51:01,560 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,560 - Next token logits device: cuda:0
2025-02-11 07:51:01,560 - Entered do_sample
2025-02-11 07:51:01,561 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,563 - Probs max: 0.8125
2025-02-11 07:51:01,564 - Pre-cat
2025-02-11 07:51:01,564 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11]], device='cuda:0')
2025-02-11 07:51:01,566 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:51:01,566 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:51:01,566 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,566 - Step 11: Generated next token
2025-02-11 07:51:01,566 - Step 11: Updated current_ids
2025-02-11 07:51:01,566 - Step 11: Decoded token text:  the
2025-02-11 07:51:01,566 - Step 11: Updated current_phrase
2025-02-11 07:51:01,567 - Step 11: Created step_acts
2025-02-11 07:51:01,567 - Step 11: Added to generation_acts
2025-02-11 07:51:01,567 - Step 11: Updated recent_tokens
2025-02-11 07:51:01,568 - Step 11: Decoded current text
2025-02-11 07:51:01,568 - Step 11: Reset consecutive_fillers
2025-02-11 07:51:01,568 - 
Starting step 12
2025-02-11 07:51:01,568 - Current_ids device: cuda:0
2025-02-11 07:51:01,569 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,592 - Model output complete
2025-02-11 07:51:01,592 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:51:01,592 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,592 - Next token logits device: cuda:0
2025-02-11 07:51:01,592 - Entered do_sample
2025-02-11 07:51:01,592 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,596 - Probs max: 0.99658203125
2025-02-11 07:51:01,597 - Pre-cat
2025-02-11 07:51:01,597 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279]],
       device='cuda:0')
2025-02-11 07:51:01,598 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:51:01,598 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:51:01,599 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,599 - Step 12: Generated next token
2025-02-11 07:51:01,599 - Step 12: Updated current_ids
2025-02-11 07:51:01,599 - Step 12: Decoded token text:  wall
2025-02-11 07:51:01,599 - Step 12: Updated current_phrase
2025-02-11 07:51:01,599 - Step 12: Created step_acts
2025-02-11 07:51:01,599 - Step 12: Added to generation_acts
2025-02-11 07:51:01,599 - Step 12: Updated recent_tokens
2025-02-11 07:51:01,601 - Step 12: Decoded current text
2025-02-11 07:51:01,601 - Step 12: Reset consecutive_fillers
2025-02-11 07:51:01,601 - 
Starting step 13
2025-02-11 07:51:01,601 - Current_ids device: cuda:0
2025-02-11 07:51:01,601 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,630 - Model output complete
2025-02-11 07:51:01,631 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:51:01,631 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,631 - Next token logits device: cuda:0
2025-02-11 07:51:01,631 - Entered do_sample
2025-02-11 07:51:01,631 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,633 - Probs max: 0.9921875
2025-02-11 07:51:01,634 - Pre-cat
2025-02-11 07:51:01,634 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002]],
       device='cuda:0')
2025-02-11 07:51:01,637 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:51:01,638 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:51:01,638 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,638 - Step 13: Generated next token
2025-02-11 07:51:01,638 - Step 13: Updated current_ids
2025-02-11 07:51:01,639 - Step 13: Decoded token text:  is
2025-02-11 07:51:01,639 - Step 13: Updated current_phrase
2025-02-11 07:51:01,640 - Step 13: Created step_acts
2025-02-11 07:51:01,640 - Step 13: Added to generation_acts
2025-02-11 07:51:01,640 - Step 13: Updated recent_tokens
2025-02-11 07:51:01,641 - Step 13: Decoded current text
2025-02-11 07:51:01,642 - Step 13: Reset consecutive_fillers
2025-02-11 07:51:01,642 - 
Starting step 14
2025-02-11 07:51:01,642 - Current_ids device: cuda:0
2025-02-11 07:51:01,642 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,666 - Model output complete
2025-02-11 07:51:01,666 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:51:01,666 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,666 - Next token logits device: cuda:0
2025-02-11 07:51:01,666 - Entered do_sample
2025-02-11 07:51:01,666 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,669 - Probs max: 0.483154296875
2025-02-11 07:51:01,670 - Pre-cat
2025-02-11 07:51:01,670 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002,    374]],
       device='cuda:0')
2025-02-11 07:51:01,672 - Next token: tensor([[52635]], device='cuda:0')
2025-02-11 07:51:01,673 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:51:01,673 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,673 - Step 14: Generated next token
2025-02-11 07:51:01,673 - Step 14: Updated current_ids
2025-02-11 07:51:01,673 - Step 14: Decoded token text:  stationary
2025-02-11 07:51:01,673 - Step 14: Updated current_phrase
2025-02-11 07:51:01,674 - Step 14: Created step_acts
2025-02-11 07:51:01,674 - Step 14: Added to generation_acts
2025-02-11 07:51:01,674 - Step 14: Updated recent_tokens
2025-02-11 07:51:01,675 - Step 14: Decoded current text
2025-02-11 07:51:01,675 - Step 14: Reset consecutive_fillers
2025-02-11 07:51:01,675 - 
Starting step 15
2025-02-11 07:51:01,675 - Current_ids device: cuda:0
2025-02-11 07:51:01,675 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,703 - Model output complete
2025-02-11 07:51:01,703 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:51:01,703 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,703 - Next token logits device: cuda:0
2025-02-11 07:51:01,703 - Entered do_sample
2025-02-11 07:51:01,703 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,706 - Probs max: 0.69873046875
2025-02-11 07:51:01,707 - Pre-cat
2025-02-11 07:51:01,707 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002,    374,
          52635]], device='cuda:0')
2025-02-11 07:51:01,709 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:51:01,709 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:51:01,709 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,709 - Step 15: Generated next token
2025-02-11 07:51:01,709 - Step 15: Updated current_ids
2025-02-11 07:51:01,709 - Step 15: Decoded token text: .
2025-02-11 07:51:01,709 - Step 15: Updated current_phrase
2025-02-11 07:51:01,710 - Step 15: Created step_acts
2025-02-11 07:51:01,710 - Step 15: Added to generation_acts
2025-02-11 07:51:01,711 - Step 15: Updated generated_texts
2025-02-11 07:51:01,711 - Step 15: Updated recent_tokens
2025-02-11 07:51:01,711 - Step 15: Found phrase end token
2025-02-11 07:51:01,711 - Step 15: Updated recent_phrases
2025-02-11 07:51:01,712 - Step 15: Calculated similarity: 0.5
2025-02-11 07:51:01,712 - Step 15: Decoded current text
2025-02-11 07:51:01,712 - Step 15: Reset consecutive_fillers
2025-02-11 07:51:01,712 - 
Starting step 16
2025-02-11 07:51:01,712 - Current_ids device: cuda:0
2025-02-11 07:51:01,712 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,736 - Model output complete
2025-02-11 07:51:01,736 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:51:01,736 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,736 - Next token logits device: cuda:0
2025-02-11 07:51:01,736 - Entered do_sample
2025-02-11 07:51:01,737 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,741 - Probs max: 0.650390625
2025-02-11 07:51:01,741 - Pre-cat
2025-02-11 07:51:01,741 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002,    374,
          52635,     13]], device='cuda:0')
2025-02-11 07:51:01,744 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:51:01,744 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:51:01,744 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,744 - Step 16: Generated next token
2025-02-11 07:51:01,744 - Step 16: Updated current_ids
2025-02-11 07:51:01,744 - Step 16: Decoded token text:  The
2025-02-11 07:51:01,744 - Step 16: Updated current_phrase
2025-02-11 07:51:01,745 - Step 16: Created step_acts
2025-02-11 07:51:01,745 - Step 16: Added to generation_acts
2025-02-11 07:51:01,745 - Step 16: Updated recent_tokens
2025-02-11 07:51:01,746 - Step 16: Decoded current text
2025-02-11 07:51:01,746 - Step 16: Reset consecutive_fillers
2025-02-11 07:51:01,746 - Step 16: Calculated unique_ratio: 0.9375
2025-02-11 07:51:01,746 - 
Starting step 17
2025-02-11 07:51:01,746 - Current_ids device: cuda:0
2025-02-11 07:51:01,746 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,770 - Model output complete
2025-02-11 07:51:01,770 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:51:01,770 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,770 - Next token logits device: cuda:0
2025-02-11 07:51:01,770 - Entered do_sample
2025-02-11 07:51:01,770 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,773 - Probs max: 0.8740234375
2025-02-11 07:51:01,774 - Pre-cat
2025-02-11 07:51:01,774 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002,    374,
          52635,     13,    576]], device='cuda:0')
2025-02-11 07:51:01,777 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:51:01,777 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:51:01,777 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,777 - Step 17: Generated next token
2025-02-11 07:51:01,777 - Step 17: Updated current_ids
2025-02-11 07:51:01,777 - Step 17: Decoded token text:  ball
2025-02-11 07:51:01,777 - Step 17: Updated current_phrase
2025-02-11 07:51:01,778 - Step 17: Created step_acts
2025-02-11 07:51:01,778 - Step 17: Added to generation_acts
2025-02-11 07:51:01,778 - Step 17: Updated recent_tokens
2025-02-11 07:51:01,779 - Step 17: Decoded current text
2025-02-11 07:51:01,779 - Step 17: Reset consecutive_fillers
2025-02-11 07:51:01,780 - Step 17: Calculated unique_ratio: 0.9375
2025-02-11 07:51:01,780 - 
Starting step 18
2025-02-11 07:51:01,780 - Current_ids device: cuda:0
2025-02-11 07:51:01,780 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,803 - Model output complete
2025-02-11 07:51:01,803 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:51:01,803 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,803 - Next token logits device: cuda:0
2025-02-11 07:51:01,803 - Entered do_sample
2025-02-11 07:51:01,803 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,807 - Probs max: 0.521484375
2025-02-11 07:51:01,808 - Pre-cat
2025-02-11 07:51:01,808 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002,    374,
          52635,     13,    576,   4935]], device='cuda:0')
2025-02-11 07:51:01,809 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:51:01,810 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:51:01,810 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,810 - Step 18: Generated next token
2025-02-11 07:51:01,810 - Step 18: Updated current_ids
2025-02-11 07:51:01,810 - Step 18: Decoded token text: 's
2025-02-11 07:51:01,810 - Step 18: Updated current_phrase
2025-02-11 07:51:01,810 - Step 18: Created step_acts
2025-02-11 07:51:01,811 - Step 18: Added to generation_acts
2025-02-11 07:51:01,811 - Step 18: Updated recent_tokens
2025-02-11 07:51:01,812 - Step 18: Decoded current text
2025-02-11 07:51:01,812 - Step 18: Reset consecutive_fillers
2025-02-11 07:51:01,812 - Step 18: Calculated unique_ratio: 1.0
2025-02-11 07:51:01,812 - 
Starting step 19
2025-02-11 07:51:01,812 - Current_ids device: cuda:0
2025-02-11 07:51:01,812 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,836 - Model output complete
2025-02-11 07:51:01,836 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:51:01,836 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,836 - Next token logits device: cuda:0
2025-02-11 07:51:01,836 - Entered do_sample
2025-02-11 07:51:01,836 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,839 - Probs max: 0.398681640625
2025-02-11 07:51:01,839 - Pre-cat
2025-02-11 07:51:01,839 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002,    374,
          52635,     13,    576,   4935,    594]], device='cuda:0')
2025-02-11 07:51:01,841 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:51:01,841 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:51:01,841 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,841 - Step 19: Generated next token
2025-02-11 07:51:01,842 - Step 19: Updated current_ids
2025-02-11 07:51:01,842 - Step 19: Decoded token text:  speed
2025-02-11 07:51:01,842 - Step 19: Updated current_phrase
2025-02-11 07:51:01,842 - Step 19: Created step_acts
2025-02-11 07:51:01,842 - Step 19: Added to generation_acts
2025-02-11 07:51:01,842 - Step 19: Updated recent_tokens
2025-02-11 07:51:01,844 - Step 19: Decoded current text
2025-02-11 07:51:01,844 - Step 19: Reset consecutive_fillers
2025-02-11 07:51:01,844 - Step 19: Calculated unique_ratio: 0.9375
2025-02-11 07:51:01,844 - 
Starting step 20
2025-02-11 07:51:01,844 - Current_ids device: cuda:0
2025-02-11 07:51:01,844 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,901 - Model output complete
2025-02-11 07:51:01,901 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:51:01,901 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,901 - Next token logits device: cuda:0
2025-02-11 07:51:01,901 - Entered do_sample
2025-02-11 07:51:01,901 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,904 - Probs max: 0.74853515625
2025-02-11 07:51:01,904 - Pre-cat
2025-02-11 07:51:01,904 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002,    374,
          52635,     13,    576,   4935,    594,   4628]], device='cuda:0')
2025-02-11 07:51:01,906 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:51:01,906 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:51:01,906 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,906 - Step 20: Generated next token
2025-02-11 07:51:01,907 - Step 20: Updated current_ids
2025-02-11 07:51:01,907 - Step 20: Decoded token text:  is
2025-02-11 07:51:01,907 - Step 20: Updated current_phrase
2025-02-11 07:51:01,907 - Step 20: Created step_acts
2025-02-11 07:51:01,907 - Step 20: Added to generation_acts
2025-02-11 07:51:01,908 - Step 20: Updated generated_texts
2025-02-11 07:51:01,909 - Step 20: Updated recent_tokens
2025-02-11 07:51:01,909 - Step 20: Decoded current text
2025-02-11 07:51:01,909 - Step 20: Reset consecutive_fillers
2025-02-11 07:51:01,909 - Step 20: Calculated unique_ratio: 0.875
2025-02-11 07:51:01,909 - 
Starting step 21
2025-02-11 07:51:01,909 - Current_ids device: cuda:0
2025-02-11 07:51:01,909 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,932 - Model output complete
2025-02-11 07:51:01,933 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:51:01,933 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,933 - Next token logits device: cuda:0
2025-02-11 07:51:01,933 - Entered do_sample
2025-02-11 07:51:01,933 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,935 - Probs max: 0.89599609375
2025-02-11 07:51:01,936 - Pre-cat
2025-02-11 07:51:01,936 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002,    374,
          52635,     13,    576,   4935,    594,   4628,    374]],
       device='cuda:0')
2025-02-11 07:51:01,938 - Next token: tensor([[348]], device='cuda:0')
2025-02-11 07:51:01,938 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:51:01,938 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,938 - Step 21: Generated next token
2025-02-11 07:51:01,938 - Step 21: Updated current_ids
2025-02-11 07:51:01,939 - Step 21: Decoded token text:  v
2025-02-11 07:51:01,939 - Step 21: Updated current_phrase
2025-02-11 07:51:01,939 - Step 21: Created step_acts
2025-02-11 07:51:01,939 - Step 21: Added to generation_acts
2025-02-11 07:51:01,939 - Step 21: Updated recent_tokens
2025-02-11 07:51:01,941 - Step 21: Decoded current text
2025-02-11 07:51:01,941 - Step 21: Reset consecutive_fillers
2025-02-11 07:51:01,941 - Step 21: Calculated unique_ratio: 0.8125
2025-02-11 07:51:01,941 - 
Starting step 22
2025-02-11 07:51:01,941 - Current_ids device: cuda:0
2025-02-11 07:51:01,941 - Current_ids dtype: torch.int64
2025-02-11 07:51:01,964 - Model output complete
2025-02-11 07:51:01,964 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:51:01,964 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,964 - Next token logits device: cuda:0
2025-02-11 07:51:01,964 - Entered do_sample
2025-02-11 07:51:01,964 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:01,967 - Probs max: 0.8212890625
2025-02-11 07:51:01,967 - Pre-cat
2025-02-11 07:51:01,967 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002,    374,
          52635,     13,    576,   4935,    594,   4628,    374,    348]],
       device='cuda:0')
2025-02-11 07:51:01,969 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:51:01,970 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:51:01,970 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:01,970 - Step 22: Generated next token
2025-02-11 07:51:01,970 - Step 22: Updated current_ids
2025-02-11 07:51:01,970 - Step 22: Decoded token text: ,
2025-02-11 07:51:01,970 - Step 22: Updated current_phrase
2025-02-11 07:51:01,971 - Step 22: Created step_acts
2025-02-11 07:51:01,971 - Step 22: Added to generation_acts
2025-02-11 07:51:01,971 - Step 22: Updated recent_tokens
2025-02-11 07:51:01,972 - Step 22: Found phrase end token
2025-02-11 07:51:01,972 - Step 22: Updated recent_phrases
2025-02-11 07:51:01,972 - Step 22: Calculated similarity: 0.5
2025-02-11 07:51:01,972 - Step 22: Decoded current text
2025-02-11 07:51:01,973 - Step 22: Reset consecutive_fillers
2025-02-11 07:51:01,973 - Step 22: Calculated unique_ratio: 0.75
2025-02-11 07:51:01,973 - 
Starting step 23
2025-02-11 07:51:01,973 - Current_ids device: cuda:0
2025-02-11 07:51:01,973 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,001 - Model output complete
2025-02-11 07:51:02,001 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:51:02,001 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,001 - Next token logits device: cuda:0
2025-02-11 07:51:02,001 - Entered do_sample
2025-02-11 07:51:02,001 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,003 - Probs max: 0.45654296875
2025-02-11 07:51:02,004 - Pre-cat
2025-02-11 07:51:02,004 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002,    374,
          52635,     13,    576,   4935,    594,   4628,    374,    348,     11]],
       device='cuda:0')
2025-02-11 07:51:02,006 - Next token: tensor([[773]], device='cuda:0')
2025-02-11 07:51:02,007 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:51:02,007 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,007 - Step 23: Generated next token
2025-02-11 07:51:02,007 - Step 23: Updated current_ids
2025-02-11 07:51:02,007 - Step 23: Decoded token text:  so
2025-02-11 07:51:02,007 - Step 23: Updated current_phrase
2025-02-11 07:51:02,007 - Step 23: Created step_acts
2025-02-11 07:51:02,007 - Step 23: Added to generation_acts
2025-02-11 07:51:02,008 - Step 23: Updated recent_tokens
2025-02-11 07:51:02,009 - Step 23: Decoded current text
2025-02-11 07:51:02,009 - Step 23: Reset consecutive_fillers
2025-02-11 07:51:02,010 - Step 23: Calculated unique_ratio: 0.8125
2025-02-11 07:51:02,010 - 
Starting step 24
2025-02-11 07:51:02,010 - Current_ids device: cuda:0
2025-02-11 07:51:02,010 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,040 - Model output complete
2025-02-11 07:51:02,040 - Logits shape: torch.Size([1, 55, 151936])
2025-02-11 07:51:02,040 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,040 - Next token logits device: cuda:0
2025-02-11 07:51:02,040 - Entered do_sample
2025-02-11 07:51:02,040 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,043 - Probs max: 0.82421875
2025-02-11 07:51:02,044 - Pre-cat
2025-02-11 07:51:02,044 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002,    374,
          52635,     13,    576,   4935,    594,   4628,    374,    348,     11,
            773]], device='cuda:0')
2025-02-11 07:51:02,047 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:51:02,047 - Current_ids shape: torch.Size([1, 55])
2025-02-11 07:51:02,047 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,047 - Step 24: Generated next token
2025-02-11 07:51:02,047 - Step 24: Updated current_ids
2025-02-11 07:51:02,048 - Step 24: Decoded token text:  the
2025-02-11 07:51:02,048 - Step 24: Updated current_phrase
2025-02-11 07:51:02,048 - Step 24: Created step_acts
2025-02-11 07:51:02,048 - Step 24: Added to generation_acts
2025-02-11 07:51:02,048 - Step 24: Updated recent_tokens
2025-02-11 07:51:02,050 - Step 24: Decoded current text
2025-02-11 07:51:02,050 - Step 24: Incremented consecutive_fillers to 1
2025-02-11 07:51:02,050 - Step 24: Calculated unique_ratio: 0.75
2025-02-11 07:51:02,050 - 
Starting step 25
2025-02-11 07:51:02,050 - Current_ids device: cuda:0
2025-02-11 07:51:02,050 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,075 - Model output complete
2025-02-11 07:51:02,075 - Logits shape: torch.Size([1, 56, 151936])
2025-02-11 07:51:02,075 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,075 - Next token logits device: cuda:0
2025-02-11 07:51:02,075 - Entered do_sample
2025-02-11 07:51:02,075 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,078 - Probs max: 0.39501953125
2025-02-11 07:51:02,079 - Pre-cat
2025-02-11 07:51:02,079 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002,    374,
          52635,     13,    576,   4935,    594,   4628,    374,    348,     11,
            773,    279]], device='cuda:0')
2025-02-11 07:51:02,082 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:51:02,082 - Current_ids shape: torch.Size([1, 56])
2025-02-11 07:51:02,082 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,082 - Step 25: Generated next token
2025-02-11 07:51:02,082 - Step 25: Updated current_ids
2025-02-11 07:51:02,082 - Step 25: Decoded token text:  ball
2025-02-11 07:51:02,083 - Step 25: Updated current_phrase
2025-02-11 07:51:02,083 - Step 25: Created step_acts
2025-02-11 07:51:02,083 - Step 25: Added to generation_acts
2025-02-11 07:51:02,084 - Step 25: Updated generated_texts
2025-02-11 07:51:02,084 - Step 25: Updated recent_tokens
2025-02-11 07:51:02,085 - Step 25: Decoded current text
2025-02-11 07:51:02,085 - Step 25: Incremented consecutive_fillers to 2
2025-02-11 07:51:02,085 - Step 25: Calculated unique_ratio: 0.75
2025-02-11 07:51:02,085 - 
Starting step 26
2025-02-11 07:51:02,085 - Current_ids device: cuda:0
2025-02-11 07:51:02,085 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,119 - Model output complete
2025-02-11 07:51:02,120 - Logits shape: torch.Size([1, 57, 151936])
2025-02-11 07:51:02,120 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,120 - Next token logits device: cuda:0
2025-02-11 07:51:02,120 - Entered do_sample
2025-02-11 07:51:02,120 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,122 - Probs max: 0.318359375
2025-02-11 07:51:02,123 - Pre-cat
2025-02-11 07:51:02,123 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,   4637,
            518,    264,   4628,    315,    348,     11,    279,   7002,    374,
          52635,     13,    576,   4935,    594,   4628,    374,    348,     11,
            773,    279,   4935]], device='cuda:0')
2025-02-11 07:51:02,126 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:51:02,126 - Current_ids shape: torch.Size([1, 57])
2025-02-11 07:51:02,126 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,127 - Step 26: Generated next token
2025-02-11 07:51:02,127 - Step 26: Updated current_ids
2025-02-11 07:51:02,127 - Step 26: Decoded token text:  is
2025-02-11 07:51:02,127 - Step 26: Updated current_phrase
2025-02-11 07:51:02,127 - Step 26: Created step_acts
2025-02-11 07:51:02,127 - Step 26: Added to generation_acts
2025-02-11 07:51:02,127 - Step 26: Updated recent_tokens
2025-02-11 07:51:02,129 - Step 26: Decoded current text
2025-02-11 07:51:02,129 - Step 26: Incremented consecutive_fillers to 3
2025-02-11 07:51:02,258 - 
Starting step 0
2025-02-11 07:51:02,258 - Current_ids device: cuda:0
2025-02-11 07:51:02,258 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,304 - Model output complete
2025-02-11 07:51:02,304 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:51:02,304 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,304 - Next token logits device: cuda:0
2025-02-11 07:51:02,304 - Entered do_sample
2025-02-11 07:51:02,305 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,307 - Probs max: 0.50341796875
2025-02-11 07:51:02,307 - Pre-cat
2025-02-11 07:51:02,308 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:51:02,309 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:51:02,309 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:51:02,309 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,310 - Step 0: Generated next token
2025-02-11 07:51:02,310 - Step 0: Updated current_ids
2025-02-11 07:51:02,310 - Step 0: Decoded token text:  If
2025-02-11 07:51:02,310 - Step 0: Updated current_phrase
2025-02-11 07:51:02,310 - Step 0: Created step_acts
2025-02-11 07:51:02,310 - Step 0: Added to generation_acts
2025-02-11 07:51:02,312 - Step 0: Updated generated_texts
2025-02-11 07:51:02,312 - Step 0: Updated recent_tokens
2025-02-11 07:51:02,312 - Step 0: Decoded current text
2025-02-11 07:51:02,312 - Step 0: Reset consecutive_fillers
2025-02-11 07:51:02,312 - 
Starting step 1
2025-02-11 07:51:02,312 - Current_ids device: cuda:0
2025-02-11 07:51:02,312 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,337 - Model output complete
2025-02-11 07:51:02,337 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:51:02,338 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,338 - Next token logits device: cuda:0
2025-02-11 07:51:02,338 - Entered do_sample
2025-02-11 07:51:02,338 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,340 - Probs max: 0.7099609375
2025-02-11 07:51:02,341 - Pre-cat
2025-02-11 07:51:02,341 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:51:02,342 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:51:02,343 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:51:02,343 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,343 - Step 1: Generated next token
2025-02-11 07:51:02,343 - Step 1: Updated current_ids
2025-02-11 07:51:02,343 - Step 1: Decoded token text:  the
2025-02-11 07:51:02,343 - Step 1: Updated current_phrase
2025-02-11 07:51:02,344 - Step 1: Created step_acts
2025-02-11 07:51:02,344 - Step 1: Added to generation_acts
2025-02-11 07:51:02,344 - Step 1: Updated recent_tokens
2025-02-11 07:51:02,345 - Step 1: Decoded current text
2025-02-11 07:51:02,345 - Step 1: Reset consecutive_fillers
2025-02-11 07:51:02,345 - 
Starting step 2
2025-02-11 07:51:02,345 - Current_ids device: cuda:0
2025-02-11 07:51:02,345 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,368 - Model output complete
2025-02-11 07:51:02,368 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:51:02,368 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,369 - Next token logits device: cuda:0
2025-02-11 07:51:02,369 - Entered do_sample
2025-02-11 07:51:02,369 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,371 - Probs max: 0.76123046875
2025-02-11 07:51:02,371 - Pre-cat
2025-02-11 07:51:02,372 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279]], device='cuda:0')
2025-02-11 07:51:02,373 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:51:02,373 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:51:02,373 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,373 - Step 2: Generated next token
2025-02-11 07:51:02,373 - Step 2: Updated current_ids
2025-02-11 07:51:02,374 - Step 2: Decoded token text:  wall
2025-02-11 07:51:02,374 - Step 2: Updated current_phrase
2025-02-11 07:51:02,374 - Step 2: Created step_acts
2025-02-11 07:51:02,374 - Step 2: Added to generation_acts
2025-02-11 07:51:02,374 - Step 2: Updated recent_tokens
2025-02-11 07:51:02,375 - Step 2: Decoded current text
2025-02-11 07:51:02,375 - Step 2: Reset consecutive_fillers
2025-02-11 07:51:02,375 - 
Starting step 3
2025-02-11 07:51:02,375 - Current_ids device: cuda:0
2025-02-11 07:51:02,376 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,398 - Model output complete
2025-02-11 07:51:02,399 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:51:02,399 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,399 - Next token logits device: cuda:0
2025-02-11 07:51:02,399 - Entered do_sample
2025-02-11 07:51:02,399 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,401 - Probs max: 0.978515625
2025-02-11 07:51:02,402 - Pre-cat
2025-02-11 07:51:02,402 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   7002]],
       device='cuda:0')
2025-02-11 07:51:02,403 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:51:02,404 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:51:02,404 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,404 - Step 3: Generated next token
2025-02-11 07:51:02,404 - Step 3: Updated current_ids
2025-02-11 07:51:02,404 - Step 3: Decoded token text:  is
2025-02-11 07:51:02,404 - Step 3: Updated current_phrase
2025-02-11 07:51:02,404 - Step 3: Created step_acts
2025-02-11 07:51:02,404 - Step 3: Added to generation_acts
2025-02-11 07:51:02,404 - Step 3: Updated recent_tokens
2025-02-11 07:51:02,406 - Step 3: Decoded current text
2025-02-11 07:51:02,406 - Step 3: Reset consecutive_fillers
2025-02-11 07:51:02,406 - 
Starting step 4
2025-02-11 07:51:02,406 - Current_ids device: cuda:0
2025-02-11 07:51:02,406 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,431 - Model output complete
2025-02-11 07:51:02,431 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:51:02,431 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,431 - Next token logits device: cuda:0
2025-02-11 07:51:02,431 - Entered do_sample
2025-02-11 07:51:02,432 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,434 - Probs max: 0.67431640625
2025-02-11 07:51:02,435 - Pre-cat
2025-02-11 07:51:02,435 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   7002,    374]],
       device='cuda:0')
2025-02-11 07:51:02,436 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:51:02,437 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:51:02,437 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,437 - Step 4: Generated next token
2025-02-11 07:51:02,437 - Step 4: Updated current_ids
2025-02-11 07:51:02,437 - Step 4: Decoded token text:  very
2025-02-11 07:51:02,437 - Step 4: Updated current_phrase
2025-02-11 07:51:02,438 - Step 4: Created step_acts
2025-02-11 07:51:02,438 - Step 4: Added to generation_acts
2025-02-11 07:51:02,438 - Step 4: Updated recent_tokens
2025-02-11 07:51:02,439 - Step 4: Decoded current text
2025-02-11 07:51:02,439 - Step 4: Reset consecutive_fillers
2025-02-11 07:51:02,439 - 
Starting step 5
2025-02-11 07:51:02,439 - Current_ids device: cuda:0
2025-02-11 07:51:02,439 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,464 - Model output complete
2025-02-11 07:51:02,464 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:51:02,464 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,464 - Next token logits device: cuda:0
2025-02-11 07:51:02,464 - Entered do_sample
2025-02-11 07:51:02,464 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,466 - Probs max: 0.97412109375
2025-02-11 07:51:02,467 - Pre-cat
2025-02-11 07:51:02,467 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   7002,    374,   1602]],
       device='cuda:0')
2025-02-11 07:51:02,469 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:51:02,469 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:51:02,469 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,469 - Step 5: Generated next token
2025-02-11 07:51:02,469 - Step 5: Updated current_ids
2025-02-11 07:51:02,469 - Step 5: Decoded token text:  fast
2025-02-11 07:51:02,469 - Step 5: Updated current_phrase
2025-02-11 07:51:02,470 - Step 5: Created step_acts
2025-02-11 07:51:02,470 - Step 5: Added to generation_acts
2025-02-11 07:51:02,471 - Step 5: Updated generated_texts
2025-02-11 07:51:02,471 - Step 5: Updated recent_tokens
2025-02-11 07:51:02,472 - Step 5: Decoded current text
2025-02-11 07:51:02,472 - Step 5: Reset consecutive_fillers
2025-02-11 07:51:02,472 - 
Starting step 6
2025-02-11 07:51:02,472 - Current_ids device: cuda:0
2025-02-11 07:51:02,472 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,529 - Model output complete
2025-02-11 07:51:02,529 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:51:02,529 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,529 - Next token logits device: cuda:0
2025-02-11 07:51:02,529 - Entered do_sample
2025-02-11 07:51:02,530 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,531 - Probs max: 0.9873046875
2025-02-11 07:51:02,534 - Pre-cat
2025-02-11 07:51:02,534 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   7002,    374,   1602,
           4937]], device='cuda:0')
2025-02-11 07:51:02,537 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:51:02,537 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:51:02,537 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,537 - Step 6: Generated next token
2025-02-11 07:51:02,538 - Step 6: Updated current_ids
2025-02-11 07:51:02,538 - Step 6: Decoded token text: ,
2025-02-11 07:51:02,538 - Step 6: Updated current_phrase
2025-02-11 07:51:02,538 - Step 6: Created step_acts
2025-02-11 07:51:02,539 - Step 6: Added to generation_acts
2025-02-11 07:51:02,539 - Step 6: Updated recent_tokens
2025-02-11 07:51:02,540 - Step 6: Found phrase end token
2025-02-11 07:51:02,540 - Step 6: Updated recent_phrases
2025-02-11 07:51:02,540 - Step 6: Decoded current text
2025-02-11 07:51:02,540 - Step 6: Reset consecutive_fillers
2025-02-11 07:51:02,540 - 
Starting step 7
2025-02-11 07:51:02,540 - Current_ids device: cuda:0
2025-02-11 07:51:02,540 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,578 - Model output complete
2025-02-11 07:51:02,578 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:51:02,578 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,578 - Next token logits device: cuda:0
2025-02-11 07:51:02,578 - Entered do_sample
2025-02-11 07:51:02,578 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,581 - Probs max: 0.525390625
2025-02-11 07:51:02,581 - Pre-cat
2025-02-11 07:51:02,581 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   7002,    374,   1602,
           4937,     11]], device='cuda:0')
2025-02-11 07:51:02,583 - Next token: tensor([[1221]], device='cuda:0')
2025-02-11 07:51:02,583 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:51:02,583 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,583 - Step 7: Generated next token
2025-02-11 07:51:02,583 - Step 7: Updated current_ids
2025-02-11 07:51:02,583 - Step 7: Decoded token text:  then
2025-02-11 07:51:02,583 - Step 7: Updated current_phrase
2025-02-11 07:51:02,584 - Step 7: Created step_acts
2025-02-11 07:51:02,584 - Step 7: Added to generation_acts
2025-02-11 07:51:02,584 - Step 7: Updated recent_tokens
2025-02-11 07:51:02,585 - Step 7: Decoded current text
2025-02-11 07:51:02,585 - Step 7: Reset consecutive_fillers
2025-02-11 07:51:02,585 - 
Starting step 8
2025-02-11 07:51:02,585 - Current_ids device: cuda:0
2025-02-11 07:51:02,585 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,610 - Model output complete
2025-02-11 07:51:02,610 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:51:02,611 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,611 - Next token logits device: cuda:0
2025-02-11 07:51:02,611 - Entered do_sample
2025-02-11 07:51:02,611 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,613 - Probs max: 0.8828125
2025-02-11 07:51:02,613 - Pre-cat
2025-02-11 07:51:02,614 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   7002,    374,   1602,
           4937,     11,   1221]], device='cuda:0')
2025-02-11 07:51:02,615 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:51:02,616 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:51:02,616 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,616 - Step 8: Generated next token
2025-02-11 07:51:02,616 - Step 8: Updated current_ids
2025-02-11 07:51:02,616 - Step 8: Decoded token text:  the
2025-02-11 07:51:02,616 - Step 8: Updated current_phrase
2025-02-11 07:51:02,616 - Step 8: Created step_acts
2025-02-11 07:51:02,616 - Step 8: Added to generation_acts
2025-02-11 07:51:02,617 - Step 8: Updated recent_tokens
2025-02-11 07:51:02,618 - Step 8: Decoded current text
2025-02-11 07:51:02,618 - Step 8: Incremented consecutive_fillers to 1
2025-02-11 07:51:02,618 - 
Starting step 9
2025-02-11 07:51:02,618 - Current_ids device: cuda:0
2025-02-11 07:51:02,618 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,642 - Model output complete
2025-02-11 07:51:02,642 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:51:02,642 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,643 - Next token logits device: cuda:0
2025-02-11 07:51:02,643 - Entered do_sample
2025-02-11 07:51:02,643 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,645 - Probs max: 0.8642578125
2025-02-11 07:51:02,646 - Pre-cat
2025-02-11 07:51:02,646 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   7002,    374,   1602,
           4937,     11,   1221,    279]], device='cuda:0')
2025-02-11 07:51:02,647 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:51:02,648 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:51:02,648 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,648 - Step 9: Generated next token
2025-02-11 07:51:02,648 - Step 9: Updated current_ids
2025-02-11 07:51:02,648 - Step 9: Decoded token text:  ball
2025-02-11 07:51:02,648 - Step 9: Updated current_phrase
2025-02-11 07:51:02,648 - Step 9: Created step_acts
2025-02-11 07:51:02,649 - Step 9: Added to generation_acts
2025-02-11 07:51:02,649 - Step 9: Updated recent_tokens
2025-02-11 07:51:02,650 - Step 9: Decoded current text
2025-02-11 07:51:02,650 - Step 9: Incremented consecutive_fillers to 2
2025-02-11 07:51:02,650 - 
Starting step 10
2025-02-11 07:51:02,650 - Current_ids device: cuda:0
2025-02-11 07:51:02,650 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,673 - Model output complete
2025-02-11 07:51:02,673 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:51:02,673 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,673 - Next token logits device: cuda:0
2025-02-11 07:51:02,673 - Entered do_sample
2025-02-11 07:51:02,673 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,676 - Probs max: 0.6142578125
2025-02-11 07:51:02,676 - Pre-cat
2025-02-11 07:51:02,676 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   7002,    374,   1602,
           4937,     11,   1221,    279,   4935]], device='cuda:0')
2025-02-11 07:51:02,678 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:51:02,678 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:51:02,678 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,678 - Step 10: Generated next token
2025-02-11 07:51:02,678 - Step 10: Updated current_ids
2025-02-11 07:51:02,679 - Step 10: Decoded token text: 's
2025-02-11 07:51:02,679 - Step 10: Updated current_phrase
2025-02-11 07:51:02,679 - Step 10: Created step_acts
2025-02-11 07:51:02,679 - Step 10: Added to generation_acts
2025-02-11 07:51:02,680 - Step 10: Updated generated_texts
2025-02-11 07:51:02,680 - Step 10: Updated recent_tokens
2025-02-11 07:51:02,681 - Step 10: Decoded current text
2025-02-11 07:51:02,681 - Step 10: Incremented consecutive_fillers to 3
2025-02-11 07:51:02,775 - 
Starting step 0
2025-02-11 07:51:02,775 - Current_ids device: cuda:0
2025-02-11 07:51:02,775 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,805 - Model output complete
2025-02-11 07:51:02,805 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:51:02,805 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,805 - Next token logits device: cuda:0
2025-02-11 07:51:02,805 - Entered do_sample
2025-02-11 07:51:02,806 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,808 - Probs max: 0.50341796875
2025-02-11 07:51:02,809 - Pre-cat
2025-02-11 07:51:02,809 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:51:02,810 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:51:02,810 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:51:02,811 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,811 - Step 0: Generated next token
2025-02-11 07:51:02,811 - Step 0: Updated current_ids
2025-02-11 07:51:02,811 - Step 0: Decoded token text:  The
2025-02-11 07:51:02,811 - Step 0: Updated current_phrase
2025-02-11 07:51:02,811 - Step 0: Created step_acts
2025-02-11 07:51:02,811 - Step 0: Added to generation_acts
2025-02-11 07:51:02,813 - Step 0: Updated generated_texts
2025-02-11 07:51:02,813 - Step 0: Updated recent_tokens
2025-02-11 07:51:02,813 - Step 0: Decoded current text
2025-02-11 07:51:02,813 - Step 0: Reset consecutive_fillers
2025-02-11 07:51:02,813 - 
Starting step 1
2025-02-11 07:51:02,813 - Current_ids device: cuda:0
2025-02-11 07:51:02,813 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,840 - Model output complete
2025-02-11 07:51:02,840 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:51:02,841 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,841 - Next token logits device: cuda:0
2025-02-11 07:51:02,841 - Entered do_sample
2025-02-11 07:51:02,841 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,843 - Probs max: 0.83837890625
2025-02-11 07:51:02,844 - Pre-cat
2025-02-11 07:51:02,844 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:51:02,845 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:51:02,846 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:51:02,846 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,846 - Step 1: Generated next token
2025-02-11 07:51:02,846 - Step 1: Updated current_ids
2025-02-11 07:51:02,846 - Step 1: Decoded token text:  ball
2025-02-11 07:51:02,846 - Step 1: Updated current_phrase
2025-02-11 07:51:02,846 - Step 1: Created step_acts
2025-02-11 07:51:02,846 - Step 1: Added to generation_acts
2025-02-11 07:51:02,847 - Step 1: Updated recent_tokens
2025-02-11 07:51:02,848 - Step 1: Decoded current text
2025-02-11 07:51:02,848 - Step 1: Reset consecutive_fillers
2025-02-11 07:51:02,848 - 
Starting step 2
2025-02-11 07:51:02,848 - Current_ids device: cuda:0
2025-02-11 07:51:02,848 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,871 - Model output complete
2025-02-11 07:51:02,871 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:51:02,872 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,872 - Next token logits device: cuda:0
2025-02-11 07:51:02,872 - Entered do_sample
2025-02-11 07:51:02,872 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,874 - Probs max: 0.583984375
2025-02-11 07:51:02,875 - Pre-cat
2025-02-11 07:51:02,875 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:51:02,877 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:51:02,877 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:51:02,877 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,877 - Step 2: Generated next token
2025-02-11 07:51:02,877 - Step 2: Updated current_ids
2025-02-11 07:51:02,877 - Step 2: Decoded token text:  is
2025-02-11 07:51:02,877 - Step 2: Updated current_phrase
2025-02-11 07:51:02,878 - Step 2: Created step_acts
2025-02-11 07:51:02,878 - Step 2: Added to generation_acts
2025-02-11 07:51:02,878 - Step 2: Updated recent_tokens
2025-02-11 07:51:02,879 - Step 2: Decoded current text
2025-02-11 07:51:02,879 - Step 2: Reset consecutive_fillers
2025-02-11 07:51:02,879 - 
Starting step 3
2025-02-11 07:51:02,879 - Current_ids device: cuda:0
2025-02-11 07:51:02,879 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,918 - Model output complete
2025-02-11 07:51:02,918 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:51:02,918 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,918 - Next token logits device: cuda:0
2025-02-11 07:51:02,918 - Entered do_sample
2025-02-11 07:51:02,918 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,920 - Probs max: 0.59521484375
2025-02-11 07:51:02,921 - Pre-cat
2025-02-11 07:51:02,921 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374]],
       device='cuda:0')
2025-02-11 07:51:02,923 - Next token: tensor([[7218]], device='cuda:0')
2025-02-11 07:51:02,923 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:51:02,923 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,923 - Step 3: Generated next token
2025-02-11 07:51:02,923 - Step 3: Updated current_ids
2025-02-11 07:51:02,923 - Step 3: Decoded token text:  moving
2025-02-11 07:51:02,923 - Step 3: Updated current_phrase
2025-02-11 07:51:02,924 - Step 3: Created step_acts
2025-02-11 07:51:02,924 - Step 3: Added to generation_acts
2025-02-11 07:51:02,924 - Step 3: Updated recent_tokens
2025-02-11 07:51:02,925 - Step 3: Decoded current text
2025-02-11 07:51:02,925 - Step 3: Reset consecutive_fillers
2025-02-11 07:51:02,925 - 
Starting step 4
2025-02-11 07:51:02,925 - Current_ids device: cuda:0
2025-02-11 07:51:02,925 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,948 - Model output complete
2025-02-11 07:51:02,948 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:51:02,948 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,948 - Next token logits device: cuda:0
2025-02-11 07:51:02,948 - Entered do_sample
2025-02-11 07:51:02,948 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,950 - Probs max: 0.677734375
2025-02-11 07:51:02,951 - Pre-cat
2025-02-11 07:51:02,951 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218]],
       device='cuda:0')
2025-02-11 07:51:02,953 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:51:02,953 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:51:02,953 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,953 - Step 4: Generated next token
2025-02-11 07:51:02,953 - Step 4: Updated current_ids
2025-02-11 07:51:02,953 - Step 4: Decoded token text:  at
2025-02-11 07:51:02,954 - Step 4: Updated current_phrase
2025-02-11 07:51:02,954 - Step 4: Created step_acts
2025-02-11 07:51:02,954 - Step 4: Added to generation_acts
2025-02-11 07:51:02,954 - Step 4: Updated recent_tokens
2025-02-11 07:51:02,955 - Step 4: Decoded current text
2025-02-11 07:51:02,955 - Step 4: Reset consecutive_fillers
2025-02-11 07:51:02,955 - 
Starting step 5
2025-02-11 07:51:02,955 - Current_ids device: cuda:0
2025-02-11 07:51:02,956 - Current_ids dtype: torch.int64
2025-02-11 07:51:02,980 - Model output complete
2025-02-11 07:51:02,980 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:51:02,980 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,980 - Next token logits device: cuda:0
2025-02-11 07:51:02,980 - Entered do_sample
2025-02-11 07:51:02,980 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:02,982 - Probs max: 0.50146484375
2025-02-11 07:51:02,983 - Pre-cat
2025-02-11 07:51:02,983 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518]],
       device='cuda:0')
2025-02-11 07:51:02,984 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:51:02,984 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:51:02,984 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:02,985 - Step 5: Generated next token
2025-02-11 07:51:02,985 - Step 5: Updated current_ids
2025-02-11 07:51:02,985 - Step 5: Decoded token text:  a
2025-02-11 07:51:02,985 - Step 5: Updated current_phrase
2025-02-11 07:51:02,985 - Step 5: Created step_acts
2025-02-11 07:51:02,985 - Step 5: Added to generation_acts
2025-02-11 07:51:02,987 - Step 5: Updated generated_texts
2025-02-11 07:51:02,987 - Step 5: Updated recent_tokens
2025-02-11 07:51:02,987 - Step 5: Decoded current text
2025-02-11 07:51:02,987 - Step 5: Reset consecutive_fillers
2025-02-11 07:51:02,987 - 
Starting step 6
2025-02-11 07:51:02,987 - Current_ids device: cuda:0
2025-02-11 07:51:02,987 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,010 - Model output complete
2025-02-11 07:51:03,011 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:51:03,011 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,011 - Next token logits device: cuda:0
2025-02-11 07:51:03,011 - Entered do_sample
2025-02-11 07:51:03,011 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,013 - Probs max: 0.3330078125
2025-02-11 07:51:03,014 - Pre-cat
2025-02-11 07:51:03,014 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264]], device='cuda:0')
2025-02-11 07:51:03,016 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:51:03,016 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:51:03,016 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,016 - Step 6: Generated next token
2025-02-11 07:51:03,016 - Step 6: Updated current_ids
2025-02-11 07:51:03,016 - Step 6: Decoded token text:  speed
2025-02-11 07:51:03,016 - Step 6: Updated current_phrase
2025-02-11 07:51:03,017 - Step 6: Created step_acts
2025-02-11 07:51:03,017 - Step 6: Added to generation_acts
2025-02-11 07:51:03,017 - Step 6: Updated recent_tokens
2025-02-11 07:51:03,018 - Step 6: Decoded current text
2025-02-11 07:51:03,018 - Step 6: Reset consecutive_fillers
2025-02-11 07:51:03,018 - 
Starting step 7
2025-02-11 07:51:03,018 - Current_ids device: cuda:0
2025-02-11 07:51:03,018 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,041 - Model output complete
2025-02-11 07:51:03,042 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:51:03,042 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,042 - Next token logits device: cuda:0
2025-02-11 07:51:03,042 - Entered do_sample
2025-02-11 07:51:03,042 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,044 - Probs max: 0.357666015625
2025-02-11 07:51:03,045 - Pre-cat
2025-02-11 07:51:03,045 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628]], device='cuda:0')
2025-02-11 07:51:03,047 - Next token: tensor([[315]], device='cuda:0')
2025-02-11 07:51:03,047 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:51:03,047 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,047 - Step 7: Generated next token
2025-02-11 07:51:03,047 - Step 7: Updated current_ids
2025-02-11 07:51:03,048 - Step 7: Decoded token text:  of
2025-02-11 07:51:03,048 - Step 7: Updated current_phrase
2025-02-11 07:51:03,048 - Step 7: Created step_acts
2025-02-11 07:51:03,048 - Step 7: Added to generation_acts
2025-02-11 07:51:03,048 - Step 7: Updated recent_tokens
2025-02-11 07:51:03,049 - Step 7: Decoded current text
2025-02-11 07:51:03,050 - Step 7: Reset consecutive_fillers
2025-02-11 07:51:03,050 - 
Starting step 8
2025-02-11 07:51:03,050 - Current_ids device: cuda:0
2025-02-11 07:51:03,050 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,072 - Model output complete
2025-02-11 07:51:03,073 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:51:03,073 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,073 - Next token logits device: cuda:0
2025-02-11 07:51:03,073 - Entered do_sample
2025-02-11 07:51:03,073 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,075 - Probs max: 0.86669921875
2025-02-11 07:51:03,076 - Pre-cat
2025-02-11 07:51:03,076 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315]], device='cuda:0')
2025-02-11 07:51:03,078 - Next token: tensor([[348]], device='cuda:0')
2025-02-11 07:51:03,078 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:51:03,078 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,078 - Step 8: Generated next token
2025-02-11 07:51:03,078 - Step 8: Updated current_ids
2025-02-11 07:51:03,079 - Step 8: Decoded token text:  v
2025-02-11 07:51:03,079 - Step 8: Updated current_phrase
2025-02-11 07:51:03,079 - Step 8: Created step_acts
2025-02-11 07:51:03,079 - Step 8: Added to generation_acts
2025-02-11 07:51:03,079 - Step 8: Updated recent_tokens
2025-02-11 07:51:03,080 - Step 8: Decoded current text
2025-02-11 07:51:03,081 - Step 8: Reset consecutive_fillers
2025-02-11 07:51:03,081 - 
Starting step 9
2025-02-11 07:51:03,081 - Current_ids device: cuda:0
2025-02-11 07:51:03,081 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,107 - Model output complete
2025-02-11 07:51:03,107 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:51:03,107 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,108 - Next token logits device: cuda:0
2025-02-11 07:51:03,108 - Entered do_sample
2025-02-11 07:51:03,108 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,110 - Probs max: 0.52001953125
2025-02-11 07:51:03,111 - Pre-cat
2025-02-11 07:51:03,111 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348]], device='cuda:0')
2025-02-11 07:51:03,113 - Next token: tensor([[8674]], device='cuda:0')
2025-02-11 07:51:03,114 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:51:03,114 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,114 - Step 9: Generated next token
2025-02-11 07:51:03,114 - Step 9: Updated current_ids
2025-02-11 07:51:03,114 - Step 9: Decoded token text:  relative
2025-02-11 07:51:03,114 - Step 9: Updated current_phrase
2025-02-11 07:51:03,115 - Step 9: Created step_acts
2025-02-11 07:51:03,115 - Step 9: Added to generation_acts
2025-02-11 07:51:03,115 - Step 9: Updated recent_tokens
2025-02-11 07:51:03,116 - Step 9: Decoded current text
2025-02-11 07:51:03,116 - Step 9: Reset consecutive_fillers
2025-02-11 07:51:03,116 - 
Starting step 10
2025-02-11 07:51:03,116 - Current_ids device: cuda:0
2025-02-11 07:51:03,116 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,149 - Model output complete
2025-02-11 07:51:03,149 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:51:03,149 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,149 - Next token logits device: cuda:0
2025-02-11 07:51:03,149 - Entered do_sample
2025-02-11 07:51:03,150 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,152 - Probs max: 0.99951171875
2025-02-11 07:51:03,153 - Pre-cat
2025-02-11 07:51:03,153 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,   8674]], device='cuda:0')
2025-02-11 07:51:03,155 - Next token: tensor([[311]], device='cuda:0')
2025-02-11 07:51:03,155 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:51:03,155 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,155 - Step 10: Generated next token
2025-02-11 07:51:03,155 - Step 10: Updated current_ids
2025-02-11 07:51:03,155 - Step 10: Decoded token text:  to
2025-02-11 07:51:03,155 - Step 10: Updated current_phrase
2025-02-11 07:51:03,156 - Step 10: Created step_acts
2025-02-11 07:51:03,156 - Step 10: Added to generation_acts
2025-02-11 07:51:03,157 - Step 10: Updated generated_texts
2025-02-11 07:51:03,157 - Step 10: Updated recent_tokens
2025-02-11 07:51:03,157 - Step 10: Decoded current text
2025-02-11 07:51:03,157 - Step 10: Reset consecutive_fillers
2025-02-11 07:51:03,158 - 
Starting step 11
2025-02-11 07:51:03,158 - Current_ids device: cuda:0
2025-02-11 07:51:03,158 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,182 - Model output complete
2025-02-11 07:51:03,182 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:51:03,182 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,182 - Next token logits device: cuda:0
2025-02-11 07:51:03,182 - Entered do_sample
2025-02-11 07:51:03,182 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,185 - Probs max: 0.994140625
2025-02-11 07:51:03,185 - Pre-cat
2025-02-11 07:51:03,185 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,   8674,    311]], device='cuda:0')
2025-02-11 07:51:03,187 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:51:03,187 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:51:03,187 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,187 - Step 11: Generated next token
2025-02-11 07:51:03,187 - Step 11: Updated current_ids
2025-02-11 07:51:03,187 - Step 11: Decoded token text:  the
2025-02-11 07:51:03,187 - Step 11: Updated current_phrase
2025-02-11 07:51:03,188 - Step 11: Created step_acts
2025-02-11 07:51:03,188 - Step 11: Added to generation_acts
2025-02-11 07:51:03,188 - Step 11: Updated recent_tokens
2025-02-11 07:51:03,189 - Step 11: Decoded current text
2025-02-11 07:51:03,189 - Step 11: Incremented consecutive_fillers to 1
2025-02-11 07:51:03,189 - 
Starting step 12
2025-02-11 07:51:03,189 - Current_ids device: cuda:0
2025-02-11 07:51:03,190 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,212 - Model output complete
2025-02-11 07:51:03,212 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:51:03,213 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,213 - Next token logits device: cuda:0
2025-02-11 07:51:03,213 - Entered do_sample
2025-02-11 07:51:03,213 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,216 - Probs max: 0.99853515625
2025-02-11 07:51:03,216 - Pre-cat
2025-02-11 07:51:03,216 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,   8674,    311,    279]],
       device='cuda:0')
2025-02-11 07:51:03,218 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:51:03,219 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:51:03,219 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,219 - Step 12: Generated next token
2025-02-11 07:51:03,219 - Step 12: Updated current_ids
2025-02-11 07:51:03,219 - Step 12: Decoded token text:  wall
2025-02-11 07:51:03,219 - Step 12: Updated current_phrase
2025-02-11 07:51:03,219 - Step 12: Created step_acts
2025-02-11 07:51:03,219 - Step 12: Added to generation_acts
2025-02-11 07:51:03,219 - Step 12: Updated recent_tokens
2025-02-11 07:51:03,221 - Step 12: Decoded current text
2025-02-11 07:51:03,221 - Step 12: Incremented consecutive_fillers to 2
2025-02-11 07:51:03,221 - 
Starting step 13
2025-02-11 07:51:03,221 - Current_ids device: cuda:0
2025-02-11 07:51:03,221 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,243 - Model output complete
2025-02-11 07:51:03,243 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:51:03,244 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,244 - Next token logits device: cuda:0
2025-02-11 07:51:03,244 - Entered do_sample
2025-02-11 07:51:03,244 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,247 - Probs max: 0.82373046875
2025-02-11 07:51:03,247 - Pre-cat
2025-02-11 07:51:03,247 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   4628,    315,    348,   8674,    311,    279,   7002]],
       device='cuda:0')
2025-02-11 07:51:03,249 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:51:03,249 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:51:03,249 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,249 - Step 13: Generated next token
2025-02-11 07:51:03,249 - Step 13: Updated current_ids
2025-02-11 07:51:03,249 - Step 13: Decoded token text: ,
2025-02-11 07:51:03,250 - Step 13: Updated current_phrase
2025-02-11 07:51:03,250 - Step 13: Created step_acts
2025-02-11 07:51:03,250 - Step 13: Added to generation_acts
2025-02-11 07:51:03,250 - Step 13: Updated recent_tokens
2025-02-11 07:51:03,251 - Step 13: Found phrase end token
2025-02-11 07:51:03,251 - Step 13: Updated recent_phrases
2025-02-11 07:51:03,251 - Step 13: Decoded current text
2025-02-11 07:51:03,251 - Step 13: Incremented consecutive_fillers to 3
2025-02-11 07:51:03,359 - 
Starting step 0
2025-02-11 07:51:03,360 - Current_ids device: cuda:0
2025-02-11 07:51:03,360 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,395 - Model output complete
2025-02-11 07:51:03,395 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:51:03,395 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,396 - Next token logits device: cuda:0
2025-02-11 07:51:03,396 - Entered do_sample
2025-02-11 07:51:03,396 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,398 - Probs max: 0.50341796875
2025-02-11 07:51:03,399 - Pre-cat
2025-02-11 07:51:03,399 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:51:03,401 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:51:03,402 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:51:03,402 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,402 - Step 0: Generated next token
2025-02-11 07:51:03,402 - Step 0: Updated current_ids
2025-02-11 07:51:03,402 - Step 0: Decoded token text:  If
2025-02-11 07:51:03,402 - Step 0: Updated current_phrase
2025-02-11 07:51:03,402 - Step 0: Created step_acts
2025-02-11 07:51:03,403 - Step 0: Added to generation_acts
2025-02-11 07:51:03,404 - Step 0: Updated generated_texts
2025-02-11 07:51:03,404 - Step 0: Updated recent_tokens
2025-02-11 07:51:03,404 - Step 0: Decoded current text
2025-02-11 07:51:03,404 - Step 0: Reset consecutive_fillers
2025-02-11 07:51:03,404 - 
Starting step 1
2025-02-11 07:51:03,404 - Current_ids device: cuda:0
2025-02-11 07:51:03,404 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,429 - Model output complete
2025-02-11 07:51:03,429 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:51:03,429 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,429 - Next token logits device: cuda:0
2025-02-11 07:51:03,429 - Entered do_sample
2025-02-11 07:51:03,429 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,431 - Probs max: 0.7099609375
2025-02-11 07:51:03,432 - Pre-cat
2025-02-11 07:51:03,432 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:51:03,434 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:51:03,434 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:51:03,434 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,434 - Step 1: Generated next token
2025-02-11 07:51:03,434 - Step 1: Updated current_ids
2025-02-11 07:51:03,434 - Step 1: Decoded token text:  a
2025-02-11 07:51:03,434 - Step 1: Updated current_phrase
2025-02-11 07:51:03,435 - Step 1: Created step_acts
2025-02-11 07:51:03,435 - Step 1: Added to generation_acts
2025-02-11 07:51:03,435 - Step 1: Updated recent_tokens
2025-02-11 07:51:03,436 - Step 1: Decoded current text
2025-02-11 07:51:03,436 - Step 1: Reset consecutive_fillers
2025-02-11 07:51:03,436 - 
Starting step 2
2025-02-11 07:51:03,436 - Current_ids device: cuda:0
2025-02-11 07:51:03,436 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,461 - Model output complete
2025-02-11 07:51:03,461 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:51:03,461 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,461 - Next token logits device: cuda:0
2025-02-11 07:51:03,461 - Entered do_sample
2025-02-11 07:51:03,461 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,463 - Probs max: 0.9970703125
2025-02-11 07:51:03,464 - Pre-cat
2025-02-11 07:51:03,464 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264]], device='cuda:0')
2025-02-11 07:51:03,466 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:51:03,466 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:51:03,466 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,466 - Step 2: Generated next token
2025-02-11 07:51:03,466 - Step 2: Updated current_ids
2025-02-11 07:51:03,466 - Step 2: Decoded token text:  ball
2025-02-11 07:51:03,466 - Step 2: Updated current_phrase
2025-02-11 07:51:03,467 - Step 2: Created step_acts
2025-02-11 07:51:03,467 - Step 2: Added to generation_acts
2025-02-11 07:51:03,467 - Step 2: Updated recent_tokens
2025-02-11 07:51:03,468 - Step 2: Decoded current text
2025-02-11 07:51:03,468 - Step 2: Reset consecutive_fillers
2025-02-11 07:51:03,468 - 
Starting step 3
2025-02-11 07:51:03,468 - Current_ids device: cuda:0
2025-02-11 07:51:03,468 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,493 - Model output complete
2025-02-11 07:51:03,493 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:51:03,494 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,494 - Next token logits device: cuda:0
2025-02-11 07:51:03,494 - Entered do_sample
2025-02-11 07:51:03,494 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,496 - Probs max: 0.99951171875
2025-02-11 07:51:03,497 - Pre-cat
2025-02-11 07:51:03,497 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935]],
       device='cuda:0')
2025-02-11 07:51:03,498 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:51:03,499 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:51:03,499 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,499 - Step 3: Generated next token
2025-02-11 07:51:03,499 - Step 3: Updated current_ids
2025-02-11 07:51:03,499 - Step 3: Decoded token text:  is
2025-02-11 07:51:03,499 - Step 3: Updated current_phrase
2025-02-11 07:51:03,499 - Step 3: Created step_acts
2025-02-11 07:51:03,499 - Step 3: Added to generation_acts
2025-02-11 07:51:03,499 - Step 3: Updated recent_tokens
2025-02-11 07:51:03,501 - Step 3: Decoded current text
2025-02-11 07:51:03,501 - Step 3: Reset consecutive_fillers
2025-02-11 07:51:03,501 - 
Starting step 4
2025-02-11 07:51:03,501 - Current_ids device: cuda:0
2025-02-11 07:51:03,501 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,524 - Model output complete
2025-02-11 07:51:03,524 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:51:03,524 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,525 - Next token logits device: cuda:0
2025-02-11 07:51:03,525 - Entered do_sample
2025-02-11 07:51:03,525 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,527 - Probs max: 0.998046875
2025-02-11 07:51:03,528 - Pre-cat
2025-02-11 07:51:03,528 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374]],
       device='cuda:0')
2025-02-11 07:51:03,529 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:51:03,529 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:51:03,529 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,530 - Step 4: Generated next token
2025-02-11 07:51:03,530 - Step 4: Updated current_ids
2025-02-11 07:51:03,530 - Step 4: Decoded token text:  thrown
2025-02-11 07:51:03,530 - Step 4: Updated current_phrase
2025-02-11 07:51:03,530 - Step 4: Created step_acts
2025-02-11 07:51:03,530 - Step 4: Added to generation_acts
2025-02-11 07:51:03,530 - Step 4: Updated recent_tokens
2025-02-11 07:51:03,532 - Step 4: Decoded current text
2025-02-11 07:51:03,532 - Step 4: Reset consecutive_fillers
2025-02-11 07:51:03,532 - 
Starting step 5
2025-02-11 07:51:03,532 - Current_ids device: cuda:0
2025-02-11 07:51:03,532 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,556 - Model output complete
2025-02-11 07:51:03,556 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:51:03,556 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,556 - Next token logits device: cuda:0
2025-02-11 07:51:03,557 - Entered do_sample
2025-02-11 07:51:03,557 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,559 - Probs max: 0.9697265625
2025-02-11 07:51:03,560 - Pre-cat
2025-02-11 07:51:03,560 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:51:03,561 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:51:03,562 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:51:03,562 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,562 - Step 5: Generated next token
2025-02-11 07:51:03,562 - Step 5: Updated current_ids
2025-02-11 07:51:03,562 - Step 5: Decoded token text:  at
2025-02-11 07:51:03,562 - Step 5: Updated current_phrase
2025-02-11 07:51:03,563 - Step 5: Created step_acts
2025-02-11 07:51:03,563 - Step 5: Added to generation_acts
2025-02-11 07:51:03,564 - Step 5: Updated generated_texts
2025-02-11 07:51:03,564 - Step 5: Updated recent_tokens
2025-02-11 07:51:03,565 - Step 5: Decoded current text
2025-02-11 07:51:03,565 - Step 5: Reset consecutive_fillers
2025-02-11 07:51:03,565 - 
Starting step 6
2025-02-11 07:51:03,565 - Current_ids device: cuda:0
2025-02-11 07:51:03,565 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,589 - Model output complete
2025-02-11 07:51:03,590 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:51:03,590 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,590 - Next token logits device: cuda:0
2025-02-11 07:51:03,590 - Entered do_sample
2025-02-11 07:51:03,590 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,592 - Probs max: 0.99951171875
2025-02-11 07:51:03,593 - Pre-cat
2025-02-11 07:51:03,593 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:51:03,594 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:51:03,595 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:51:03,595 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,595 - Step 6: Generated next token
2025-02-11 07:51:03,595 - Step 6: Updated current_ids
2025-02-11 07:51:03,595 - Step 6: Decoded token text:  a
2025-02-11 07:51:03,595 - Step 6: Updated current_phrase
2025-02-11 07:51:03,595 - Step 6: Created step_acts
2025-02-11 07:51:03,595 - Step 6: Added to generation_acts
2025-02-11 07:51:03,596 - Step 6: Updated recent_tokens
2025-02-11 07:51:03,597 - Step 6: Decoded current text
2025-02-11 07:51:03,597 - Step 6: Reset consecutive_fillers
2025-02-11 07:51:03,597 - 
Starting step 7
2025-02-11 07:51:03,597 - Current_ids device: cuda:0
2025-02-11 07:51:03,597 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,621 - Model output complete
2025-02-11 07:51:03,621 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:51:03,621 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,621 - Next token logits device: cuda:0
2025-02-11 07:51:03,621 - Entered do_sample
2025-02-11 07:51:03,622 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,624 - Probs max: 0.9990234375
2025-02-11 07:51:03,624 - Pre-cat
2025-02-11 07:51:03,624 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:51:03,626 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:51:03,626 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:51:03,626 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,626 - Step 7: Generated next token
2025-02-11 07:51:03,626 - Step 7: Updated current_ids
2025-02-11 07:51:03,627 - Step 7: Decoded token text:  wall
2025-02-11 07:51:03,627 - Step 7: Updated current_phrase
2025-02-11 07:51:03,627 - Step 7: Created step_acts
2025-02-11 07:51:03,627 - Step 7: Added to generation_acts
2025-02-11 07:51:03,627 - Step 7: Updated recent_tokens
2025-02-11 07:51:03,628 - Step 7: Decoded current text
2025-02-11 07:51:03,628 - Step 7: Reset consecutive_fillers
2025-02-11 07:51:03,628 - 
Starting step 8
2025-02-11 07:51:03,629 - Current_ids device: cuda:0
2025-02-11 07:51:03,629 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,652 - Model output complete
2025-02-11 07:51:03,652 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:51:03,652 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,652 - Next token logits device: cuda:0
2025-02-11 07:51:03,652 - Entered do_sample
2025-02-11 07:51:03,652 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,654 - Probs max: 0.9912109375
2025-02-11 07:51:03,655 - Pre-cat
2025-02-11 07:51:03,655 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:51:03,657 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:51:03,657 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:51:03,657 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,657 - Step 8: Generated next token
2025-02-11 07:51:03,657 - Step 8: Updated current_ids
2025-02-11 07:51:03,657 - Step 8: Decoded token text:  very
2025-02-11 07:51:03,657 - Step 8: Updated current_phrase
2025-02-11 07:51:03,658 - Step 8: Created step_acts
2025-02-11 07:51:03,658 - Step 8: Added to generation_acts
2025-02-11 07:51:03,658 - Step 8: Updated recent_tokens
2025-02-11 07:51:03,659 - Step 8: Decoded current text
2025-02-11 07:51:03,659 - Step 8: Reset consecutive_fillers
2025-02-11 07:51:03,659 - 
Starting step 9
2025-02-11 07:51:03,659 - Current_ids device: cuda:0
2025-02-11 07:51:03,659 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,684 - Model output complete
2025-02-11 07:51:03,684 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:51:03,684 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,684 - Next token logits device: cuda:0
2025-02-11 07:51:03,684 - Entered do_sample
2025-02-11 07:51:03,684 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,686 - Probs max: 0.998046875
2025-02-11 07:51:03,687 - Pre-cat
2025-02-11 07:51:03,687 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:51:03,689 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:51:03,689 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:51:03,689 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,689 - Step 9: Generated next token
2025-02-11 07:51:03,689 - Step 9: Updated current_ids
2025-02-11 07:51:03,689 - Step 9: Decoded token text:  fast
2025-02-11 07:51:03,689 - Step 9: Updated current_phrase
2025-02-11 07:51:03,690 - Step 9: Created step_acts
2025-02-11 07:51:03,690 - Step 9: Added to generation_acts
2025-02-11 07:51:03,690 - Step 9: Updated recent_tokens
2025-02-11 07:51:03,691 - Step 9: Decoded current text
2025-02-11 07:51:03,691 - Step 9: Reset consecutive_fillers
2025-02-11 07:51:03,691 - 
Starting step 10
2025-02-11 07:51:03,691 - Current_ids device: cuda:0
2025-02-11 07:51:03,691 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,714 - Model output complete
2025-02-11 07:51:03,714 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:51:03,714 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,714 - Next token logits device: cuda:0
2025-02-11 07:51:03,715 - Entered do_sample
2025-02-11 07:51:03,715 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,717 - Probs max: 0.99853515625
2025-02-11 07:51:03,718 - Pre-cat
2025-02-11 07:51:03,718 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:51:03,720 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:51:03,720 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:51:03,720 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,720 - Step 10: Generated next token
2025-02-11 07:51:03,720 - Step 10: Updated current_ids
2025-02-11 07:51:03,720 - Step 10: Decoded token text: ,
2025-02-11 07:51:03,720 - Step 10: Updated current_phrase
2025-02-11 07:51:03,721 - Step 10: Created step_acts
2025-02-11 07:51:03,721 - Step 10: Added to generation_acts
2025-02-11 07:51:03,722 - Step 10: Updated generated_texts
2025-02-11 07:51:03,722 - Step 10: Updated recent_tokens
2025-02-11 07:51:03,722 - Step 10: Found phrase end token
2025-02-11 07:51:03,722 - Step 10: Updated recent_phrases
2025-02-11 07:51:03,722 - Step 10: Decoded current text
2025-02-11 07:51:03,722 - Step 10: Reset consecutive_fillers
2025-02-11 07:51:03,723 - 
Starting step 11
2025-02-11 07:51:03,723 - Current_ids device: cuda:0
2025-02-11 07:51:03,723 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,747 - Model output complete
2025-02-11 07:51:03,748 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:51:03,748 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,748 - Next token logits device: cuda:0
2025-02-11 07:51:03,748 - Entered do_sample
2025-02-11 07:51:03,748 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,750 - Probs max: 0.58154296875
2025-02-11 07:51:03,751 - Pre-cat
2025-02-11 07:51:03,751 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:51:03,752 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:51:03,753 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:51:03,753 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,753 - Step 11: Generated next token
2025-02-11 07:51:03,753 - Step 11: Updated current_ids
2025-02-11 07:51:03,753 - Step 11: Decoded token text:  the
2025-02-11 07:51:03,753 - Step 11: Updated current_phrase
2025-02-11 07:51:03,754 - Step 11: Created step_acts
2025-02-11 07:51:03,754 - Step 11: Added to generation_acts
2025-02-11 07:51:03,754 - Step 11: Updated recent_tokens
2025-02-11 07:51:03,755 - Step 11: Decoded current text
2025-02-11 07:51:03,755 - Step 11: Reset consecutive_fillers
2025-02-11 07:51:03,755 - 
Starting step 12
2025-02-11 07:51:03,755 - Current_ids device: cuda:0
2025-02-11 07:51:03,755 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,778 - Model output complete
2025-02-11 07:51:03,778 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:51:03,779 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,779 - Next token logits device: cuda:0
2025-02-11 07:51:03,779 - Entered do_sample
2025-02-11 07:51:03,779 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,781 - Probs max: 0.6064453125
2025-02-11 07:51:03,782 - Pre-cat
2025-02-11 07:51:03,782 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279]],
       device='cuda:0')
2025-02-11 07:51:03,784 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:51:03,784 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:51:03,784 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,784 - Step 12: Generated next token
2025-02-11 07:51:03,784 - Step 12: Updated current_ids
2025-02-11 07:51:03,785 - Step 12: Decoded token text:  ball
2025-02-11 07:51:03,785 - Step 12: Updated current_phrase
2025-02-11 07:51:03,785 - Step 12: Created step_acts
2025-02-11 07:51:03,785 - Step 12: Added to generation_acts
2025-02-11 07:51:03,785 - Step 12: Updated recent_tokens
2025-02-11 07:51:03,786 - Step 12: Decoded current text
2025-02-11 07:51:03,787 - Step 12: Reset consecutive_fillers
2025-02-11 07:51:03,787 - 
Starting step 13
2025-02-11 07:51:03,787 - Current_ids device: cuda:0
2025-02-11 07:51:03,787 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,810 - Model output complete
2025-02-11 07:51:03,810 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:51:03,811 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,811 - Next token logits device: cuda:0
2025-02-11 07:51:03,811 - Entered do_sample
2025-02-11 07:51:03,811 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,813 - Probs max: 0.76953125
2025-02-11 07:51:03,814 - Pre-cat
2025-02-11 07:51:03,814 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935]],
       device='cuda:0')
2025-02-11 07:51:03,816 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:51:03,816 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:51:03,816 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,816 - Step 13: Generated next token
2025-02-11 07:51:03,816 - Step 13: Updated current_ids
2025-02-11 07:51:03,816 - Step 13: Decoded token text:  will
2025-02-11 07:51:03,816 - Step 13: Updated current_phrase
2025-02-11 07:51:03,817 - Step 13: Created step_acts
2025-02-11 07:51:03,817 - Step 13: Added to generation_acts
2025-02-11 07:51:03,817 - Step 13: Updated recent_tokens
2025-02-11 07:51:03,818 - Step 13: Decoded current text
2025-02-11 07:51:03,818 - Step 13: Reset consecutive_fillers
2025-02-11 07:51:03,818 - 
Starting step 14
2025-02-11 07:51:03,818 - Current_ids device: cuda:0
2025-02-11 07:51:03,818 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,842 - Model output complete
2025-02-11 07:51:03,842 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:51:03,842 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,842 - Next token logits device: cuda:0
2025-02-11 07:51:03,842 - Entered do_sample
2025-02-11 07:51:03,842 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,845 - Probs max: 0.43408203125
2025-02-11 07:51:03,846 - Pre-cat
2025-02-11 07:51:03,846 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686]],
       device='cuda:0')
2025-02-11 07:51:03,848 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:51:03,848 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:51:03,849 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,849 - Step 14: Generated next token
2025-02-11 07:51:03,849 - Step 14: Updated current_ids
2025-02-11 07:51:03,849 - Step 14: Decoded token text:  hit
2025-02-11 07:51:03,849 - Step 14: Updated current_phrase
2025-02-11 07:51:03,849 - Step 14: Created step_acts
2025-02-11 07:51:03,849 - Step 14: Added to generation_acts
2025-02-11 07:51:03,849 - Step 14: Updated recent_tokens
2025-02-11 07:51:03,851 - Step 14: Decoded current text
2025-02-11 07:51:03,851 - Step 14: Reset consecutive_fillers
2025-02-11 07:51:03,851 - 
Starting step 15
2025-02-11 07:51:03,851 - Current_ids device: cuda:0
2025-02-11 07:51:03,851 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,878 - Model output complete
2025-02-11 07:51:03,878 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:51:03,878 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,878 - Next token logits device: cuda:0
2025-02-11 07:51:03,879 - Entered do_sample
2025-02-11 07:51:03,879 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,882 - Probs max: 0.9951171875
2025-02-11 07:51:03,883 - Pre-cat
2025-02-11 07:51:03,883 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201]], device='cuda:0')
2025-02-11 07:51:03,885 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:51:03,885 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:51:03,885 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,885 - Step 15: Generated next token
2025-02-11 07:51:03,886 - Step 15: Updated current_ids
2025-02-11 07:51:03,886 - Step 15: Decoded token text:  the
2025-02-11 07:51:03,886 - Step 15: Updated current_phrase
2025-02-11 07:51:03,886 - Step 15: Created step_acts
2025-02-11 07:51:03,886 - Step 15: Added to generation_acts
2025-02-11 07:51:03,888 - Step 15: Updated generated_texts
2025-02-11 07:51:03,888 - Step 15: Updated recent_tokens
2025-02-11 07:51:03,888 - Step 15: Decoded current text
2025-02-11 07:51:03,888 - Step 15: Reset consecutive_fillers
2025-02-11 07:51:03,888 - 
Starting step 16
2025-02-11 07:51:03,888 - Current_ids device: cuda:0
2025-02-11 07:51:03,888 - Current_ids dtype: torch.int64
2025-02-11 07:51:03,916 - Model output complete
2025-02-11 07:51:03,916 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:51:03,916 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,916 - Next token logits device: cuda:0
2025-02-11 07:51:03,916 - Entered do_sample
2025-02-11 07:51:03,916 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:03,919 - Probs max: 0.9990234375
2025-02-11 07:51:03,919 - Pre-cat
2025-02-11 07:51:03,919 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279]], device='cuda:0')
2025-02-11 07:51:03,921 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:51:03,921 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:51:03,921 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:03,921 - Step 16: Generated next token
2025-02-11 07:51:03,922 - Step 16: Updated current_ids
2025-02-11 07:51:03,922 - Step 16: Decoded token text:  wall
2025-02-11 07:51:03,922 - Step 16: Updated current_phrase
2025-02-11 07:51:03,922 - Step 16: Created step_acts
2025-02-11 07:51:03,922 - Step 16: Added to generation_acts
2025-02-11 07:51:03,922 - Step 16: Updated recent_tokens
2025-02-11 07:51:03,924 - Step 16: Decoded current text
2025-02-11 07:51:03,924 - Step 16: Reset consecutive_fillers
2025-02-11 07:51:03,924 - Step 16: Calculated unique_ratio: 0.75
2025-02-11 07:51:03,924 - 
Starting step 17
2025-02-11 07:51:03,924 - Current_ids device: cuda:0
2025-02-11 07:51:03,924 - Current_ids dtype: torch.int64
2025-02-11 07:51:05,030 - Model output complete
2025-02-11 07:51:05,030 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:51:05,030 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,031 - Next token logits device: cuda:0
2025-02-11 07:51:05,031 - Entered do_sample
2025-02-11 07:51:05,031 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,034 - Probs max: 0.422119140625
2025-02-11 07:51:05,034 - Pre-cat
2025-02-11 07:51:05,034 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002]], device='cuda:0')
2025-02-11 07:51:05,038 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:51:05,039 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:51:05,039 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:05,039 - Step 17: Generated next token
2025-02-11 07:51:05,039 - Step 17: Updated current_ids
2025-02-11 07:51:05,039 - Step 17: Decoded token text:  very
2025-02-11 07:51:05,039 - Step 17: Updated current_phrase
2025-02-11 07:51:05,040 - Step 17: Created step_acts
2025-02-11 07:51:05,040 - Step 17: Added to generation_acts
2025-02-11 07:51:05,040 - Step 17: Updated recent_tokens
2025-02-11 07:51:05,042 - Step 17: Decoded current text
2025-02-11 07:51:05,042 - Step 17: Reset consecutive_fillers
2025-02-11 07:51:05,042 - Step 17: Calculated unique_ratio: 0.75
2025-02-11 07:51:05,042 - 
Starting step 18
2025-02-11 07:51:05,042 - Current_ids device: cuda:0
2025-02-11 07:51:05,042 - Current_ids dtype: torch.int64
2025-02-11 07:51:05,091 - Model output complete
2025-02-11 07:51:05,091 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:51:05,091 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,091 - Next token logits device: cuda:0
2025-02-11 07:51:05,091 - Entered do_sample
2025-02-11 07:51:05,092 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,094 - Probs max: 0.73095703125
2025-02-11 07:51:05,095 - Pre-cat
2025-02-11 07:51:05,095 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002,   1602]], device='cuda:0')
2025-02-11 07:51:05,097 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:51:05,098 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:51:05,098 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:05,098 - Step 18: Generated next token
2025-02-11 07:51:05,098 - Step 18: Updated current_ids
2025-02-11 07:51:05,098 - Step 18: Decoded token text:  fast
2025-02-11 07:51:05,098 - Step 18: Updated current_phrase
2025-02-11 07:51:05,099 - Step 18: Created step_acts
2025-02-11 07:51:05,099 - Step 18: Added to generation_acts
2025-02-11 07:51:05,099 - Step 18: Updated recent_tokens
2025-02-11 07:51:05,101 - Step 18: Decoded current text
2025-02-11 07:51:05,101 - Step 18: Reset consecutive_fillers
2025-02-11 07:51:05,101 - Step 18: Calculated unique_ratio: 0.75
2025-02-11 07:51:05,101 - 
Starting step 19
2025-02-11 07:51:05,101 - Current_ids device: cuda:0
2025-02-11 07:51:05,101 - Current_ids dtype: torch.int64
2025-02-11 07:51:05,129 - Model output complete
2025-02-11 07:51:05,129 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:51:05,129 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,130 - Next token logits device: cuda:0
2025-02-11 07:51:05,130 - Entered do_sample
2025-02-11 07:51:05,130 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,132 - Probs max: 0.705078125
2025-02-11 07:51:05,134 - Pre-cat
2025-02-11 07:51:05,134 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:51:05,138 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:51:05,139 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:51:05,139 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:05,139 - Step 19: Generated next token
2025-02-11 07:51:05,139 - Step 19: Updated current_ids
2025-02-11 07:51:05,139 - Step 19: Decoded token text: ,
2025-02-11 07:51:05,139 - Step 19: Updated current_phrase
2025-02-11 07:51:05,140 - Step 19: Created step_acts
2025-02-11 07:51:05,140 - Step 19: Added to generation_acts
2025-02-11 07:51:05,140 - Step 19: Updated recent_tokens
2025-02-11 07:51:05,141 - Step 19: Found phrase end token
2025-02-11 07:51:05,141 - Step 19: Updated recent_phrases
2025-02-11 07:51:05,141 - Step 19: Calculated similarity: 0.5714285714285714
2025-02-11 07:51:05,141 - Step 19: Decoded current text
2025-02-11 07:51:05,142 - Step 19: Reset consecutive_fillers
2025-02-11 07:51:05,142 - Step 19: Calculated unique_ratio: 0.6875
2025-02-11 07:51:05,142 - 
Starting step 20
2025-02-11 07:51:05,142 - Current_ids device: cuda:0
2025-02-11 07:51:05,142 - Current_ids dtype: torch.int64
2025-02-11 07:51:05,177 - Model output complete
2025-02-11 07:51:05,177 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:51:05,177 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,177 - Next token logits device: cuda:0
2025-02-11 07:51:05,177 - Entered do_sample
2025-02-11 07:51:05,177 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,180 - Probs max: 0.47412109375
2025-02-11 07:51:05,180 - Pre-cat
2025-02-11 07:51:05,180 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:51:05,183 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:51:05,184 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:51:05,184 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:05,184 - Step 20: Generated next token
2025-02-11 07:51:05,184 - Step 20: Updated current_ids
2025-02-11 07:51:05,184 - Step 20: Decoded token text:  the
2025-02-11 07:51:05,184 - Step 20: Updated current_phrase
2025-02-11 07:51:05,185 - Step 20: Created step_acts
2025-02-11 07:51:05,185 - Step 20: Added to generation_acts
2025-02-11 07:51:05,186 - Step 20: Updated generated_texts
2025-02-11 07:51:05,186 - Step 20: Updated recent_tokens
2025-02-11 07:51:05,187 - Step 20: Decoded current text
2025-02-11 07:51:05,187 - Step 20: Reset consecutive_fillers
2025-02-11 07:51:05,187 - Step 20: Calculated unique_ratio: 0.625
2025-02-11 07:51:05,187 - 
Starting step 21
2025-02-11 07:51:05,187 - Current_ids device: cuda:0
2025-02-11 07:51:05,187 - Current_ids dtype: torch.int64
2025-02-11 07:51:05,210 - Model output complete
2025-02-11 07:51:05,210 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:51:05,211 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,211 - Next token logits device: cuda:0
2025-02-11 07:51:05,211 - Entered do_sample
2025-02-11 07:51:05,211 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,213 - Probs max: 0.8583984375
2025-02-11 07:51:05,214 - Pre-cat
2025-02-11 07:51:05,214 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11,    279]],
       device='cuda:0')
2025-02-11 07:51:05,217 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:51:05,217 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:51:05,217 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:05,217 - Step 21: Generated next token
2025-02-11 07:51:05,218 - Step 21: Updated current_ids
2025-02-11 07:51:05,218 - Step 21: Decoded token text:  ball
2025-02-11 07:51:05,218 - Step 21: Updated current_phrase
2025-02-11 07:51:05,218 - Step 21: Created step_acts
2025-02-11 07:51:05,218 - Step 21: Added to generation_acts
2025-02-11 07:51:05,218 - Step 21: Updated recent_tokens
2025-02-11 07:51:05,220 - Step 21: Decoded current text
2025-02-11 07:51:05,220 - Step 21: Reset consecutive_fillers
2025-02-11 07:51:05,220 - Step 21: Calculated unique_ratio: 0.5625
2025-02-11 07:51:05,220 - 
Starting step 22
2025-02-11 07:51:05,220 - Current_ids device: cuda:0
2025-02-11 07:51:05,220 - Current_ids dtype: torch.int64
2025-02-11 07:51:05,247 - Model output complete
2025-02-11 07:51:05,247 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:51:05,247 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,247 - Next token logits device: cuda:0
2025-02-11 07:51:05,247 - Entered do_sample
2025-02-11 07:51:05,247 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,249 - Probs max: 0.96630859375
2025-02-11 07:51:05,250 - Pre-cat
2025-02-11 07:51:05,250 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11,    279,   4935]],
       device='cuda:0')
2025-02-11 07:51:05,253 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:51:05,253 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:51:05,253 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:05,253 - Step 22: Generated next token
2025-02-11 07:51:05,253 - Step 22: Updated current_ids
2025-02-11 07:51:05,253 - Step 22: Decoded token text:  will
2025-02-11 07:51:05,253 - Step 22: Updated current_phrase
2025-02-11 07:51:05,254 - Step 22: Created step_acts
2025-02-11 07:51:05,254 - Step 22: Added to generation_acts
2025-02-11 07:51:05,254 - Step 22: Updated recent_tokens
2025-02-11 07:51:05,255 - Step 22: Decoded current text
2025-02-11 07:51:05,255 - Step 22: Reset consecutive_fillers
2025-02-11 07:51:05,256 - Step 22: Calculated unique_ratio: 0.5
2025-02-11 07:51:05,256 - 
Starting step 23
2025-02-11 07:51:05,256 - Current_ids device: cuda:0
2025-02-11 07:51:05,256 - Current_ids dtype: torch.int64
2025-02-11 07:51:05,281 - Model output complete
2025-02-11 07:51:05,281 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:51:05,281 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,281 - Next token logits device: cuda:0
2025-02-11 07:51:05,281 - Entered do_sample
2025-02-11 07:51:05,282 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,284 - Probs max: 0.89990234375
2025-02-11 07:51:05,285 - Pre-cat
2025-02-11 07:51:05,285 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11,    279,   4935,    686]],
       device='cuda:0')
2025-02-11 07:51:05,287 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:51:05,287 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:51:05,288 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:05,288 - Step 23: Generated next token
2025-02-11 07:51:05,288 - Step 23: Updated current_ids
2025-02-11 07:51:05,288 - Step 23: Decoded token text:  hit
2025-02-11 07:51:05,288 - Step 23: Updated current_phrase
2025-02-11 07:51:05,288 - Step 23: Created step_acts
2025-02-11 07:51:05,288 - Step 23: Added to generation_acts
2025-02-11 07:51:05,288 - Step 23: Updated recent_tokens
2025-02-11 07:51:05,290 - Step 23: Decoded current text
2025-02-11 07:51:05,290 - Step 23: Reset consecutive_fillers
2025-02-11 07:51:05,290 - Step 23: Calculated unique_ratio: 0.5
2025-02-11 07:51:05,290 - 
Starting step 24
2025-02-11 07:51:05,290 - Current_ids device: cuda:0
2025-02-11 07:51:05,290 - Current_ids dtype: torch.int64
2025-02-11 07:51:05,316 - Model output complete
2025-02-11 07:51:05,316 - Logits shape: torch.Size([1, 55, 151936])
2025-02-11 07:51:05,316 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,316 - Next token logits device: cuda:0
2025-02-11 07:51:05,316 - Entered do_sample
2025-02-11 07:51:05,316 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,318 - Probs max: 0.99462890625
2025-02-11 07:51:05,319 - Pre-cat
2025-02-11 07:51:05,319 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201]], device='cuda:0')
2025-02-11 07:51:05,322 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:51:05,322 - Current_ids shape: torch.Size([1, 55])
2025-02-11 07:51:05,322 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:05,323 - Step 24: Generated next token
2025-02-11 07:51:05,323 - Step 24: Updated current_ids
2025-02-11 07:51:05,323 - Step 24: Decoded token text:  the
2025-02-11 07:51:05,323 - Step 24: Updated current_phrase
2025-02-11 07:51:05,323 - Step 24: Created step_acts
2025-02-11 07:51:05,323 - Step 24: Added to generation_acts
2025-02-11 07:51:05,323 - Step 24: Updated recent_tokens
2025-02-11 07:51:05,325 - Step 24: Decoded current text
2025-02-11 07:51:05,325 - Step 24: Reset consecutive_fillers
2025-02-11 07:51:05,325 - Step 24: Calculated unique_ratio: 0.5
2025-02-11 07:51:05,325 - 
Starting step 25
2025-02-11 07:51:05,325 - Current_ids device: cuda:0
2025-02-11 07:51:05,325 - Current_ids dtype: torch.int64
2025-02-11 07:51:05,350 - Model output complete
2025-02-11 07:51:05,350 - Logits shape: torch.Size([1, 56, 151936])
2025-02-11 07:51:05,350 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,350 - Next token logits device: cuda:0
2025-02-11 07:51:05,350 - Entered do_sample
2025-02-11 07:51:05,350 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,352 - Probs max: 0.99755859375
2025-02-11 07:51:05,353 - Pre-cat
2025-02-11 07:51:05,353 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279]], device='cuda:0')
2025-02-11 07:51:05,355 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:51:05,356 - Current_ids shape: torch.Size([1, 56])
2025-02-11 07:51:05,356 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:05,356 - Step 25: Generated next token
2025-02-11 07:51:05,356 - Step 25: Updated current_ids
2025-02-11 07:51:05,356 - Step 25: Decoded token text:  wall
2025-02-11 07:51:05,356 - Step 25: Updated current_phrase
2025-02-11 07:51:05,356 - Step 25: Created step_acts
2025-02-11 07:51:05,356 - Step 25: Added to generation_acts
2025-02-11 07:51:05,358 - Step 25: Updated generated_texts
2025-02-11 07:51:05,358 - Step 25: Updated recent_tokens
2025-02-11 07:51:05,358 - Step 25: Decoded current text
2025-02-11 07:51:05,358 - Step 25: Reset consecutive_fillers
2025-02-11 07:51:05,358 - Step 25: Calculated unique_ratio: 0.5
2025-02-11 07:51:05,359 - 
Starting step 26
2025-02-11 07:51:05,359 - Current_ids device: cuda:0
2025-02-11 07:51:05,359 - Current_ids dtype: torch.int64
2025-02-11 07:51:05,381 - Model output complete
2025-02-11 07:51:05,381 - Logits shape: torch.Size([1, 57, 151936])
2025-02-11 07:51:05,382 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,382 - Next token logits device: cuda:0
2025-02-11 07:51:05,382 - Entered do_sample
2025-02-11 07:51:05,382 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,385 - Probs max: 0.88330078125
2025-02-11 07:51:05,385 - Pre-cat
2025-02-11 07:51:05,386 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002]], device='cuda:0')
2025-02-11 07:51:05,388 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:51:05,388 - Current_ids shape: torch.Size([1, 57])
2025-02-11 07:51:05,388 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:05,388 - Step 26: Generated next token
2025-02-11 07:51:05,388 - Step 26: Updated current_ids
2025-02-11 07:51:05,388 - Step 26: Decoded token text:  very
2025-02-11 07:51:05,388 - Step 26: Updated current_phrase
2025-02-11 07:51:05,389 - Step 26: Created step_acts
2025-02-11 07:51:05,389 - Step 26: Added to generation_acts
2025-02-11 07:51:05,389 - Step 26: Updated recent_tokens
2025-02-11 07:51:05,390 - Step 26: Decoded current text
2025-02-11 07:51:05,390 - Step 26: Reset consecutive_fillers
2025-02-11 07:51:05,391 - Step 26: Calculated unique_ratio: 0.5
2025-02-11 07:51:05,391 - 
Starting step 27
2025-02-11 07:51:05,391 - Current_ids device: cuda:0
2025-02-11 07:51:05,391 - Current_ids dtype: torch.int64
2025-02-11 07:51:05,414 - Model output complete
2025-02-11 07:51:05,414 - Logits shape: torch.Size([1, 58, 151936])
2025-02-11 07:51:05,414 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,415 - Next token logits device: cuda:0
2025-02-11 07:51:05,415 - Entered do_sample
2025-02-11 07:51:05,415 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,417 - Probs max: 0.8779296875
2025-02-11 07:51:05,418 - Pre-cat
2025-02-11 07:51:05,418 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002,   1602]], device='cuda:0')
2025-02-11 07:51:05,420 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:51:05,420 - Current_ids shape: torch.Size([1, 58])
2025-02-11 07:51:05,420 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:05,420 - Step 27: Generated next token
2025-02-11 07:51:05,420 - Step 27: Updated current_ids
2025-02-11 07:51:05,420 - Step 27: Decoded token text:  fast
2025-02-11 07:51:05,420 - Step 27: Updated current_phrase
2025-02-11 07:51:05,421 - Step 27: Created step_acts
2025-02-11 07:51:05,421 - Step 27: Added to generation_acts
2025-02-11 07:51:05,421 - Step 27: Updated recent_tokens
2025-02-11 07:51:05,422 - Step 27: Decoded current text
2025-02-11 07:51:05,422 - Step 27: Reset consecutive_fillers
2025-02-11 07:51:05,423 - Step 27: Calculated unique_ratio: 0.5
2025-02-11 07:51:05,423 - 
Starting step 28
2025-02-11 07:51:05,423 - Current_ids device: cuda:0
2025-02-11 07:51:05,423 - Current_ids dtype: torch.int64
2025-02-11 07:51:05,451 - Model output complete
2025-02-11 07:51:05,451 - Logits shape: torch.Size([1, 59, 151936])
2025-02-11 07:51:05,452 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,452 - Next token logits device: cuda:0
2025-02-11 07:51:05,452 - Entered do_sample
2025-02-11 07:51:05,452 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:51:05,454 - Probs max: 0.98095703125
2025-02-11 07:51:05,459 - Pre-cat
2025-02-11 07:51:05,459 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002,   1602,   4937,     11,    279,   4935,    686,
           4201,    279,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:51:05,463 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:51:05,464 - Current_ids shape: torch.Size([1, 59])
2025-02-11 07:51:05,464 - Next token shape: torch.Size([1, 1])
2025-02-11 07:51:05,464 - Step 28: Generated next token
2025-02-11 07:51:05,464 - Step 28: Updated current_ids
2025-02-11 07:51:05,465 - Step 28: Decoded token text: ,
2025-02-11 07:51:05,465 - Step 28: Updated current_phrase
2025-02-11 07:51:05,466 - Step 28: Created step_acts
2025-02-11 07:51:05,466 - Step 28: Added to generation_acts
2025-02-11 07:51:05,466 - Step 28: Updated recent_tokens
2025-02-11 07:51:05,467 - Step 28: Found phrase end token
2025-02-11 07:51:05,468 - Step 28: Updated recent_phrases
2025-02-11 07:51:05,468 - Step 28: Calculated similarity: 1.0
2025-02-11 07:55:04,771 - Starting new HTTPS connection (1): huggingface.co:443
2025-02-11 07:55:04,847 - https://huggingface.co:443 "GET /api/models/EleutherAI/sae-DeepSeek-R1-Distill-Qwen-1.5B-65k/revision/main HTTP/1.1" 200 3415
2025-02-11 07:55:04,861 - from_dict for <class 'sparsify.config.SaeConfig'>, drop extra fields: True
2025-02-11 07:55:04,861 - name = expansion_factor, field_type = <class 'int'>
2025-02-11 07:55:04,861 - Getting the decoding function for <class 'int'>
2025-02-11 07:55:04,861 - <class 'int'> -> <class 'int'>
2025-02-11 07:55:04,861 - name = normalize_decoder, field_type = <class 'bool'>
2025-02-11 07:55:04,861 - Getting the decoding function for <class 'bool'>
2025-02-11 07:55:04,861 - <class 'bool'> -> <class 'bool'>
2025-02-11 07:55:04,861 - name = num_latents, field_type = <class 'int'>
2025-02-11 07:55:04,861 - Getting the decoding function for <class 'int'>
2025-02-11 07:55:04,861 - <class 'int'> -> <class 'int'>
2025-02-11 07:55:04,861 - name = k, field_type = <class 'int'>
2025-02-11 07:55:04,861 - Getting the decoding function for <class 'int'>
2025-02-11 07:55:04,861 - <class 'int'> -> <class 'int'>
2025-02-11 07:55:04,861 - name = multi_topk, field_type = <class 'bool'>
2025-02-11 07:55:04,862 - Getting the decoding function for <class 'bool'>
2025-02-11 07:55:04,862 - <class 'bool'> -> <class 'bool'>
2025-02-11 07:55:04,862 - name = skip_connection, field_type = <class 'bool'>
2025-02-11 07:55:04,862 - Getting the decoding function for <class 'bool'>
2025-02-11 07:55:04,862 - <class 'bool'> -> <class 'bool'>
2025-02-11 07:55:05,347 - https://huggingface.co:443 "HEAD /deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-02-11 07:55:05,768 - https://huggingface.co:443 "HEAD /deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/config.json HTTP/1.1" 200 0
2025-02-11 07:55:08,144 - https://huggingface.co:443 "HEAD /deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/resolve/main/generation_config.json HTTP/1.1" 200 0
2025-02-11 07:55:08,234 - 
Starting step 0
2025-02-11 07:55:08,234 - Current_ids device: cuda:0
2025-02-11 07:55:08,234 - Current_ids dtype: torch.int64
2025-02-11 07:55:08,434 - Model output complete
2025-02-11 07:55:08,434 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:08,434 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,434 - Next token logits device: cuda:0
2025-02-11 07:55:08,434 - Entered do_sample
2025-02-11 07:55:08,512 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,520 - Probs max: 0.50341796875
2025-02-11 07:55:08,559 - Pre-cat
2025-02-11 07:55:08,559 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:08,562 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:55:08,562 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:08,562 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:08,563 - Step 0: Generated next token
2025-02-11 07:55:08,563 - Step 0: Updated current_ids
2025-02-11 07:55:08,563 - Step 0: Decoded token text:  If
2025-02-11 07:55:08,564 - Step 0: Updated current_phrase
2025-02-11 07:55:08,571 - Step 0: Created step_acts
2025-02-11 07:55:08,572 - Step 0: Added to generation_acts
2025-02-11 07:55:08,572 - Step 0: Updated generated_texts
2025-02-11 07:55:08,572 - Step 0: Updated recent_tokens
2025-02-11 07:55:08,573 - Step 0: Decoded current text
2025-02-11 07:55:08,573 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:08,573 - 
Starting step 1
2025-02-11 07:55:08,573 - Current_ids device: cuda:0
2025-02-11 07:55:08,573 - Current_ids dtype: torch.int64
2025-02-11 07:55:08,609 - Model output complete
2025-02-11 07:55:08,609 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:08,609 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,609 - Next token logits device: cuda:0
2025-02-11 07:55:08,609 - Entered do_sample
2025-02-11 07:55:08,610 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,612 - Probs max: 0.7099609375
2025-02-11 07:55:08,613 - Pre-cat
2025-02-11 07:55:08,613 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:55:08,615 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:08,615 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:08,615 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:08,615 - Step 1: Generated next token
2025-02-11 07:55:08,615 - Step 1: Updated current_ids
2025-02-11 07:55:08,615 - Step 1: Decoded token text:  a
2025-02-11 07:55:08,615 - Step 1: Updated current_phrase
2025-02-11 07:55:08,616 - Step 1: Created step_acts
2025-02-11 07:55:08,616 - Step 1: Added to generation_acts
2025-02-11 07:55:08,616 - Step 1: Updated recent_tokens
2025-02-11 07:55:08,617 - Step 1: Decoded current text
2025-02-11 07:55:08,617 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:08,617 - 
Starting step 2
2025-02-11 07:55:08,617 - Current_ids device: cuda:0
2025-02-11 07:55:08,617 - Current_ids dtype: torch.int64
2025-02-11 07:55:08,649 - Model output complete
2025-02-11 07:55:08,649 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:08,649 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,649 - Next token logits device: cuda:0
2025-02-11 07:55:08,649 - Entered do_sample
2025-02-11 07:55:08,650 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,652 - Probs max: 0.9970703125
2025-02-11 07:55:08,653 - Pre-cat
2025-02-11 07:55:08,653 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264]], device='cuda:0')
2025-02-11 07:55:08,655 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:08,656 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:08,656 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:08,656 - Step 2: Generated next token
2025-02-11 07:55:08,656 - Step 2: Updated current_ids
2025-02-11 07:55:08,656 - Step 2: Decoded token text:  ball
2025-02-11 07:55:08,656 - Step 2: Updated current_phrase
2025-02-11 07:55:08,657 - Step 2: Created step_acts
2025-02-11 07:55:08,657 - Step 2: Added to generation_acts
2025-02-11 07:55:08,657 - Step 2: Updated recent_tokens
2025-02-11 07:55:08,658 - Step 2: Decoded current text
2025-02-11 07:55:08,658 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:08,658 - 
Starting step 3
2025-02-11 07:55:08,658 - Current_ids device: cuda:0
2025-02-11 07:55:08,658 - Current_ids dtype: torch.int64
2025-02-11 07:55:08,694 - Model output complete
2025-02-11 07:55:08,694 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:08,694 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,694 - Next token logits device: cuda:0
2025-02-11 07:55:08,694 - Entered do_sample
2025-02-11 07:55:08,695 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,697 - Probs max: 0.99951171875
2025-02-11 07:55:08,698 - Pre-cat
2025-02-11 07:55:08,698 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935]],
       device='cuda:0')
2025-02-11 07:55:08,700 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:08,700 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:08,700 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:08,701 - Step 3: Generated next token
2025-02-11 07:55:08,701 - Step 3: Updated current_ids
2025-02-11 07:55:08,701 - Step 3: Decoded token text:  is
2025-02-11 07:55:08,701 - Step 3: Updated current_phrase
2025-02-11 07:55:08,701 - Step 3: Created step_acts
2025-02-11 07:55:08,701 - Step 3: Added to generation_acts
2025-02-11 07:55:08,702 - Step 3: Updated recent_tokens
2025-02-11 07:55:08,703 - Step 3: Decoded current text
2025-02-11 07:55:08,703 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:08,703 - 
Starting step 4
2025-02-11 07:55:08,703 - Current_ids device: cuda:0
2025-02-11 07:55:08,703 - Current_ids dtype: torch.int64
2025-02-11 07:55:08,751 - Model output complete
2025-02-11 07:55:08,751 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:08,751 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,751 - Next token logits device: cuda:0
2025-02-11 07:55:08,751 - Entered do_sample
2025-02-11 07:55:08,752 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,753 - Probs max: 0.998046875
2025-02-11 07:55:08,754 - Pre-cat
2025-02-11 07:55:08,754 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:08,757 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:08,758 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:08,758 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:08,758 - Step 4: Generated next token
2025-02-11 07:55:08,758 - Step 4: Updated current_ids
2025-02-11 07:55:08,758 - Step 4: Decoded token text:  thrown
2025-02-11 07:55:08,758 - Step 4: Updated current_phrase
2025-02-11 07:55:08,759 - Step 4: Created step_acts
2025-02-11 07:55:08,759 - Step 4: Added to generation_acts
2025-02-11 07:55:08,759 - Step 4: Updated recent_tokens
2025-02-11 07:55:08,761 - Step 4: Decoded current text
2025-02-11 07:55:08,761 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:08,761 - 
Starting step 5
2025-02-11 07:55:08,761 - Current_ids device: cuda:0
2025-02-11 07:55:08,761 - Current_ids dtype: torch.int64
2025-02-11 07:55:08,796 - Model output complete
2025-02-11 07:55:08,796 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:08,797 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,797 - Next token logits device: cuda:0
2025-02-11 07:55:08,797 - Entered do_sample
2025-02-11 07:55:08,797 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,799 - Probs max: 0.9697265625
2025-02-11 07:55:08,800 - Pre-cat
2025-02-11 07:55:08,800 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:55:08,803 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:55:08,804 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:08,804 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:08,804 - Step 5: Generated next token
2025-02-11 07:55:08,804 - Step 5: Updated current_ids
2025-02-11 07:55:08,805 - Step 5: Decoded token text:  at
2025-02-11 07:55:08,805 - Step 5: Updated current_phrase
2025-02-11 07:55:08,805 - Step 5: Created step_acts
2025-02-11 07:55:08,805 - Step 5: Added to generation_acts
2025-02-11 07:55:08,807 - Step 5: Updated generated_texts
2025-02-11 07:55:08,808 - Step 5: Updated recent_tokens
2025-02-11 07:55:08,808 - Step 5: Decoded current text
2025-02-11 07:55:08,808 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:08,808 - 
Starting step 6
2025-02-11 07:55:08,808 - Current_ids device: cuda:0
2025-02-11 07:55:08,809 - Current_ids dtype: torch.int64
2025-02-11 07:55:08,849 - Model output complete
2025-02-11 07:55:08,849 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:08,849 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,849 - Next token logits device: cuda:0
2025-02-11 07:55:08,849 - Entered do_sample
2025-02-11 07:55:08,850 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,852 - Probs max: 0.99951171875
2025-02-11 07:55:08,853 - Pre-cat
2025-02-11 07:55:08,853 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:55:08,856 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:08,856 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:08,856 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:08,856 - Step 6: Generated next token
2025-02-11 07:55:08,857 - Step 6: Updated current_ids
2025-02-11 07:55:08,857 - Step 6: Decoded token text:  a
2025-02-11 07:55:08,857 - Step 6: Updated current_phrase
2025-02-11 07:55:08,858 - Step 6: Created step_acts
2025-02-11 07:55:08,858 - Step 6: Added to generation_acts
2025-02-11 07:55:08,858 - Step 6: Updated recent_tokens
2025-02-11 07:55:08,859 - Step 6: Decoded current text
2025-02-11 07:55:08,859 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:08,859 - 
Starting step 7
2025-02-11 07:55:08,859 - Current_ids device: cuda:0
2025-02-11 07:55:08,859 - Current_ids dtype: torch.int64
2025-02-11 07:55:08,900 - Model output complete
2025-02-11 07:55:08,900 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:08,900 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,900 - Next token logits device: cuda:0
2025-02-11 07:55:08,901 - Entered do_sample
2025-02-11 07:55:08,901 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,902 - Probs max: 0.9990234375
2025-02-11 07:55:08,903 - Pre-cat
2025-02-11 07:55:08,903 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:55:08,905 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:08,906 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:08,906 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:08,906 - Step 7: Generated next token
2025-02-11 07:55:08,906 - Step 7: Updated current_ids
2025-02-11 07:55:08,906 - Step 7: Decoded token text:  wall
2025-02-11 07:55:08,906 - Step 7: Updated current_phrase
2025-02-11 07:55:08,906 - Step 7: Created step_acts
2025-02-11 07:55:08,907 - Step 7: Added to generation_acts
2025-02-11 07:55:08,907 - Step 7: Updated recent_tokens
2025-02-11 07:55:08,908 - Step 7: Decoded current text
2025-02-11 07:55:08,908 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:08,909 - 
Starting step 8
2025-02-11 07:55:08,909 - Current_ids device: cuda:0
2025-02-11 07:55:08,909 - Current_ids dtype: torch.int64
2025-02-11 07:55:08,944 - Model output complete
2025-02-11 07:55:08,944 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:08,944 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,945 - Next token logits device: cuda:0
2025-02-11 07:55:08,945 - Entered do_sample
2025-02-11 07:55:08,945 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,946 - Probs max: 0.9912109375
2025-02-11 07:55:08,948 - Pre-cat
2025-02-11 07:55:08,948 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:55:08,951 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:08,951 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:08,951 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:08,951 - Step 8: Generated next token
2025-02-11 07:55:08,951 - Step 8: Updated current_ids
2025-02-11 07:55:08,952 - Step 8: Decoded token text:  very
2025-02-11 07:55:08,952 - Step 8: Updated current_phrase
2025-02-11 07:55:08,952 - Step 8: Created step_acts
2025-02-11 07:55:08,953 - Step 8: Added to generation_acts
2025-02-11 07:55:08,953 - Step 8: Updated recent_tokens
2025-02-11 07:55:08,954 - Step 8: Decoded current text
2025-02-11 07:55:08,955 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:08,955 - 
Starting step 9
2025-02-11 07:55:08,955 - Current_ids device: cuda:0
2025-02-11 07:55:08,955 - Current_ids dtype: torch.int64
2025-02-11 07:55:08,989 - Model output complete
2025-02-11 07:55:08,990 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:08,990 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,990 - Next token logits device: cuda:0
2025-02-11 07:55:08,990 - Entered do_sample
2025-02-11 07:55:08,990 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:08,992 - Probs max: 0.998046875
2025-02-11 07:55:08,992 - Pre-cat
2025-02-11 07:55:08,992 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:55:08,994 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:55:08,995 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:08,995 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:08,995 - Step 9: Generated next token
2025-02-11 07:55:08,995 - Step 9: Updated current_ids
2025-02-11 07:55:08,995 - Step 9: Decoded token text:  fast
2025-02-11 07:55:08,995 - Step 9: Updated current_phrase
2025-02-11 07:55:08,995 - Step 9: Created step_acts
2025-02-11 07:55:08,995 - Step 9: Added to generation_acts
2025-02-11 07:55:08,996 - Step 9: Updated recent_tokens
2025-02-11 07:55:08,997 - Step 9: Decoded current text
2025-02-11 07:55:08,997 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:08,997 - 
Starting step 10
2025-02-11 07:55:08,997 - Current_ids device: cuda:0
2025-02-11 07:55:08,997 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,026 - Model output complete
2025-02-11 07:55:09,027 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:09,027 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,027 - Next token logits device: cuda:0
2025-02-11 07:55:09,027 - Entered do_sample
2025-02-11 07:55:09,027 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,029 - Probs max: 0.99853515625
2025-02-11 07:55:09,030 - Pre-cat
2025-02-11 07:55:09,030 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:55:09,032 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:09,032 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:09,032 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,033 - Step 10: Generated next token
2025-02-11 07:55:09,033 - Step 10: Updated current_ids
2025-02-11 07:55:09,033 - Step 10: Decoded token text: ,
2025-02-11 07:55:09,033 - Step 10: Updated current_phrase
2025-02-11 07:55:09,033 - Step 10: Created step_acts
2025-02-11 07:55:09,033 - Step 10: Added to generation_acts
2025-02-11 07:55:09,035 - Step 10: Updated generated_texts
2025-02-11 07:55:09,035 - Step 10: Updated recent_tokens
2025-02-11 07:55:09,035 - Step 10: Found phrase end token
2025-02-11 07:55:09,035 - Step 10: Updated recent_phrases
2025-02-11 07:55:09,035 - Step 10: Decoded current text
2025-02-11 07:55:09,035 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:09,035 - 
Starting step 11
2025-02-11 07:55:09,035 - Current_ids device: cuda:0
2025-02-11 07:55:09,035 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,066 - Model output complete
2025-02-11 07:55:09,066 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:09,066 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,066 - Next token logits device: cuda:0
2025-02-11 07:55:09,067 - Entered do_sample
2025-02-11 07:55:09,067 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,069 - Probs max: 0.58154296875
2025-02-11 07:55:09,070 - Pre-cat
2025-02-11 07:55:09,070 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:55:09,074 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:09,075 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:09,075 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,075 - Step 11: Generated next token
2025-02-11 07:55:09,075 - Step 11: Updated current_ids
2025-02-11 07:55:09,075 - Step 11: Decoded token text:  the
2025-02-11 07:55:09,075 - Step 11: Updated current_phrase
2025-02-11 07:55:09,076 - Step 11: Created step_acts
2025-02-11 07:55:09,076 - Step 11: Added to generation_acts
2025-02-11 07:55:09,076 - Step 11: Updated recent_tokens
2025-02-11 07:55:09,078 - Step 11: Decoded current text
2025-02-11 07:55:09,078 - Step 11: Reset consecutive_fillers
2025-02-11 07:55:09,078 - 
Starting step 12
2025-02-11 07:55:09,078 - Current_ids device: cuda:0
2025-02-11 07:55:09,078 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,109 - Model output complete
2025-02-11 07:55:09,109 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:09,110 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,110 - Next token logits device: cuda:0
2025-02-11 07:55:09,110 - Entered do_sample
2025-02-11 07:55:09,110 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,113 - Probs max: 0.6064453125
2025-02-11 07:55:09,113 - Pre-cat
2025-02-11 07:55:09,113 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279]],
       device='cuda:0')
2025-02-11 07:55:09,116 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:09,116 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:09,116 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,116 - Step 12: Generated next token
2025-02-11 07:55:09,116 - Step 12: Updated current_ids
2025-02-11 07:55:09,117 - Step 12: Decoded token text:  wall
2025-02-11 07:55:09,117 - Step 12: Updated current_phrase
2025-02-11 07:55:09,117 - Step 12: Created step_acts
2025-02-11 07:55:09,117 - Step 12: Added to generation_acts
2025-02-11 07:55:09,117 - Step 12: Updated recent_tokens
2025-02-11 07:55:09,119 - Step 12: Decoded current text
2025-02-11 07:55:09,119 - Step 12: Reset consecutive_fillers
2025-02-11 07:55:09,119 - 
Starting step 13
2025-02-11 07:55:09,119 - Current_ids device: cuda:0
2025-02-11 07:55:09,119 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,155 - Model output complete
2025-02-11 07:55:09,155 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:09,155 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,156 - Next token logits device: cuda:0
2025-02-11 07:55:09,156 - Entered do_sample
2025-02-11 07:55:09,156 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,157 - Probs max: 0.85498046875
2025-02-11 07:55:09,158 - Pre-cat
2025-02-11 07:55:09,158 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   7002]],
       device='cuda:0')
2025-02-11 07:55:09,160 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:09,161 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:09,161 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,161 - Step 13: Generated next token
2025-02-11 07:55:09,161 - Step 13: Updated current_ids
2025-02-11 07:55:09,161 - Step 13: Decoded token text:  will
2025-02-11 07:55:09,161 - Step 13: Updated current_phrase
2025-02-11 07:55:09,162 - Step 13: Created step_acts
2025-02-11 07:55:09,162 - Step 13: Added to generation_acts
2025-02-11 07:55:09,162 - Step 13: Updated recent_tokens
2025-02-11 07:55:09,163 - Step 13: Decoded current text
2025-02-11 07:55:09,163 - Step 13: Reset consecutive_fillers
2025-02-11 07:55:09,164 - 
Starting step 14
2025-02-11 07:55:09,164 - Current_ids device: cuda:0
2025-02-11 07:55:09,164 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,193 - Model output complete
2025-02-11 07:55:09,193 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:55:09,194 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,194 - Next token logits device: cuda:0
2025-02-11 07:55:09,194 - Entered do_sample
2025-02-11 07:55:09,194 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,196 - Probs max: 0.365234375
2025-02-11 07:55:09,197 - Pre-cat
2025-02-11 07:55:09,197 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   7002,    686]],
       device='cuda:0')
2025-02-11 07:55:09,200 - Next token: tensor([[42744]], device='cuda:0')
2025-02-11 07:55:09,200 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:55:09,200 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,200 - Step 14: Generated next token
2025-02-11 07:55:09,200 - Step 14: Updated current_ids
2025-02-11 07:55:09,200 - Step 14: Decoded token text:  exert
2025-02-11 07:55:09,200 - Step 14: Updated current_phrase
2025-02-11 07:55:09,201 - Step 14: Created step_acts
2025-02-11 07:55:09,201 - Step 14: Added to generation_acts
2025-02-11 07:55:09,201 - Step 14: Updated recent_tokens
2025-02-11 07:55:09,202 - Step 14: Decoded current text
2025-02-11 07:55:09,202 - Step 14: Reset consecutive_fillers
2025-02-11 07:55:09,202 - 
Starting step 15
2025-02-11 07:55:09,203 - Current_ids device: cuda:0
2025-02-11 07:55:09,203 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,232 - Model output complete
2025-02-11 07:55:09,233 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:55:09,233 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,233 - Next token logits device: cuda:0
2025-02-11 07:55:09,233 - Entered do_sample
2025-02-11 07:55:09,233 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,235 - Probs max: 0.7919921875
2025-02-11 07:55:09,236 - Pre-cat
2025-02-11 07:55:09,236 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   7002,    686,
          42744]], device='cuda:0')
2025-02-11 07:55:09,239 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:09,239 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:55:09,239 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,239 - Step 15: Generated next token
2025-02-11 07:55:09,239 - Step 15: Updated current_ids
2025-02-11 07:55:09,240 - Step 15: Decoded token text:  a
2025-02-11 07:55:09,240 - Step 15: Updated current_phrase
2025-02-11 07:55:09,240 - Step 15: Created step_acts
2025-02-11 07:55:09,240 - Step 15: Added to generation_acts
2025-02-11 07:55:09,242 - Step 15: Updated generated_texts
2025-02-11 07:55:09,242 - Step 15: Updated recent_tokens
2025-02-11 07:55:09,242 - Step 15: Decoded current text
2025-02-11 07:55:09,242 - Step 15: Reset consecutive_fillers
2025-02-11 07:55:09,242 - 
Starting step 16
2025-02-11 07:55:09,242 - Current_ids device: cuda:0
2025-02-11 07:55:09,242 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,277 - Model output complete
2025-02-11 07:55:09,277 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:55:09,277 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,277 - Next token logits device: cuda:0
2025-02-11 07:55:09,277 - Entered do_sample
2025-02-11 07:55:09,277 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,279 - Probs max: 0.9052734375
2025-02-11 07:55:09,280 - Pre-cat
2025-02-11 07:55:09,280 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   7002,    686,
          42744,    264]], device='cuda:0')
2025-02-11 07:55:09,282 - Next token: tensor([[5344]], device='cuda:0')
2025-02-11 07:55:09,282 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:55:09,283 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,283 - Step 16: Generated next token
2025-02-11 07:55:09,283 - Step 16: Updated current_ids
2025-02-11 07:55:09,283 - Step 16: Decoded token text:  force
2025-02-11 07:55:09,283 - Step 16: Updated current_phrase
2025-02-11 07:55:09,284 - Step 16: Created step_acts
2025-02-11 07:55:09,284 - Step 16: Added to generation_acts
2025-02-11 07:55:09,284 - Step 16: Updated recent_tokens
2025-02-11 07:55:09,286 - Step 16: Decoded current text
2025-02-11 07:55:09,286 - Step 16: Reset consecutive_fillers
2025-02-11 07:55:09,294 - Step 16: Calculated unique_ratio: 0.8125
2025-02-11 07:55:09,294 - 
Starting step 17
2025-02-11 07:55:09,294 - Current_ids device: cuda:0
2025-02-11 07:55:09,294 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,359 - Model output complete
2025-02-11 07:55:09,359 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:55:09,359 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,360 - Next token logits device: cuda:0
2025-02-11 07:55:09,360 - Entered do_sample
2025-02-11 07:55:09,360 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,362 - Probs max: 0.97607421875
2025-02-11 07:55:09,363 - Pre-cat
2025-02-11 07:55:09,363 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   7002,    686,
          42744,    264,   5344]], device='cuda:0')
2025-02-11 07:55:09,366 - Next token: tensor([[389]], device='cuda:0')
2025-02-11 07:55:09,367 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:55:09,367 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,367 - Step 17: Generated next token
2025-02-11 07:55:09,367 - Step 17: Updated current_ids
2025-02-11 07:55:09,367 - Step 17: Decoded token text:  on
2025-02-11 07:55:09,368 - Step 17: Updated current_phrase
2025-02-11 07:55:09,368 - Step 17: Created step_acts
2025-02-11 07:55:09,368 - Step 17: Added to generation_acts
2025-02-11 07:55:09,368 - Step 17: Updated recent_tokens
2025-02-11 07:55:09,369 - Step 17: Decoded current text
2025-02-11 07:55:09,370 - Step 17: Reset consecutive_fillers
2025-02-11 07:55:09,370 - Step 17: Calculated unique_ratio: 0.875
2025-02-11 07:55:09,370 - 
Starting step 18
2025-02-11 07:55:09,370 - Current_ids device: cuda:0
2025-02-11 07:55:09,370 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,439 - Model output complete
2025-02-11 07:55:09,439 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:55:09,439 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,440 - Next token logits device: cuda:0
2025-02-11 07:55:09,440 - Entered do_sample
2025-02-11 07:55:09,440 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,441 - Probs max: 0.99755859375
2025-02-11 07:55:09,444 - Pre-cat
2025-02-11 07:55:09,445 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   7002,    686,
          42744,    264,   5344,    389]], device='cuda:0')
2025-02-11 07:55:09,450 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:09,451 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:55:09,451 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,451 - Step 18: Generated next token
2025-02-11 07:55:09,451 - Step 18: Updated current_ids
2025-02-11 07:55:09,451 - Step 18: Decoded token text:  the
2025-02-11 07:55:09,451 - Step 18: Updated current_phrase
2025-02-11 07:55:09,452 - Step 18: Created step_acts
2025-02-11 07:55:09,452 - Step 18: Added to generation_acts
2025-02-11 07:55:09,452 - Step 18: Updated recent_tokens
2025-02-11 07:55:09,453 - Step 18: Decoded current text
2025-02-11 07:55:09,453 - Step 18: Incremented consecutive_fillers to 1
2025-02-11 07:55:09,454 - Step 18: Calculated unique_ratio: 0.8125
2025-02-11 07:55:09,454 - 
Starting step 19
2025-02-11 07:55:09,454 - Current_ids device: cuda:0
2025-02-11 07:55:09,454 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,488 - Model output complete
2025-02-11 07:55:09,489 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:55:09,489 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,489 - Next token logits device: cuda:0
2025-02-11 07:55:09,489 - Entered do_sample
2025-02-11 07:55:09,490 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,490 - Probs max: 0.93896484375
2025-02-11 07:55:09,492 - Pre-cat
2025-02-11 07:55:09,492 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   7002,    686,
          42744,    264,   5344,    389,    279]], device='cuda:0')
2025-02-11 07:55:09,497 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:09,498 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:55:09,499 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,499 - Step 19: Generated next token
2025-02-11 07:55:09,499 - Step 19: Updated current_ids
2025-02-11 07:55:09,499 - Step 19: Decoded token text:  ball
2025-02-11 07:55:09,499 - Step 19: Updated current_phrase
2025-02-11 07:55:09,500 - Step 19: Created step_acts
2025-02-11 07:55:09,500 - Step 19: Added to generation_acts
2025-02-11 07:55:09,501 - Step 19: Updated recent_tokens
2025-02-11 07:55:09,502 - Step 19: Decoded current text
2025-02-11 07:55:09,502 - Step 19: Incremented consecutive_fillers to 2
2025-02-11 07:55:09,502 - Step 19: Calculated unique_ratio: 0.8125
2025-02-11 07:55:09,503 - 
Starting step 20
2025-02-11 07:55:09,503 - Current_ids device: cuda:0
2025-02-11 07:55:09,503 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,557 - Model output complete
2025-02-11 07:55:09,557 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:55:09,557 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,558 - Next token logits device: cuda:0
2025-02-11 07:55:09,558 - Entered do_sample
2025-02-11 07:55:09,558 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,560 - Probs max: 0.47607421875
2025-02-11 07:55:09,560 - Pre-cat
2025-02-11 07:55:09,560 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   7002,    686,
          42744,    264,   5344,    389,    279,   4935]], device='cuda:0')
2025-02-11 07:55:09,563 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:09,563 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:55:09,563 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,563 - Step 20: Generated next token
2025-02-11 07:55:09,563 - Step 20: Updated current_ids
2025-02-11 07:55:09,563 - Step 20: Decoded token text: ,
2025-02-11 07:55:09,563 - Step 20: Updated current_phrase
2025-02-11 07:55:09,564 - Step 20: Created step_acts
2025-02-11 07:55:09,564 - Step 20: Added to generation_acts
2025-02-11 07:55:09,565 - Step 20: Updated generated_texts
2025-02-11 07:55:09,565 - Step 20: Updated recent_tokens
2025-02-11 07:55:09,565 - Step 20: Found phrase end token
2025-02-11 07:55:09,566 - Step 20: Updated recent_phrases
2025-02-11 07:55:09,566 - Step 20: Calculated similarity: 0.25
2025-02-11 07:55:09,566 - Step 20: Decoded current text
2025-02-11 07:55:09,566 - Step 20: Incremented consecutive_fillers to 3
2025-02-11 07:55:09,707 - 
Starting step 0
2025-02-11 07:55:09,707 - Current_ids device: cuda:0
2025-02-11 07:55:09,707 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,754 - Model output complete
2025-02-11 07:55:09,754 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:09,754 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,755 - Next token logits device: cuda:0
2025-02-11 07:55:09,755 - Entered do_sample
2025-02-11 07:55:09,755 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,757 - Probs max: 0.50341796875
2025-02-11 07:55:09,758 - Pre-cat
2025-02-11 07:55:09,758 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:09,761 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:09,761 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:09,761 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,761 - Step 0: Generated next token
2025-02-11 07:55:09,761 - Step 0: Updated current_ids
2025-02-11 07:55:09,762 - Step 0: Decoded token text:  The
2025-02-11 07:55:09,762 - Step 0: Updated current_phrase
2025-02-11 07:55:09,762 - Step 0: Created step_acts
2025-02-11 07:55:09,762 - Step 0: Added to generation_acts
2025-02-11 07:55:09,763 - Step 0: Updated generated_texts
2025-02-11 07:55:09,763 - Step 0: Updated recent_tokens
2025-02-11 07:55:09,764 - Step 0: Decoded current text
2025-02-11 07:55:09,764 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:09,764 - 
Starting step 1
2025-02-11 07:55:09,764 - Current_ids device: cuda:0
2025-02-11 07:55:09,764 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,801 - Model output complete
2025-02-11 07:55:09,801 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:09,801 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,801 - Next token logits device: cuda:0
2025-02-11 07:55:09,802 - Entered do_sample
2025-02-11 07:55:09,802 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,804 - Probs max: 0.83837890625
2025-02-11 07:55:09,804 - Pre-cat
2025-02-11 07:55:09,804 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:55:09,806 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:09,807 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:09,807 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,807 - Step 1: Generated next token
2025-02-11 07:55:09,807 - Step 1: Updated current_ids
2025-02-11 07:55:09,807 - Step 1: Decoded token text:  ball
2025-02-11 07:55:09,807 - Step 1: Updated current_phrase
2025-02-11 07:55:09,808 - Step 1: Created step_acts
2025-02-11 07:55:09,808 - Step 1: Added to generation_acts
2025-02-11 07:55:09,808 - Step 1: Updated recent_tokens
2025-02-11 07:55:09,809 - Step 1: Decoded current text
2025-02-11 07:55:09,809 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:09,809 - 
Starting step 2
2025-02-11 07:55:09,809 - Current_ids device: cuda:0
2025-02-11 07:55:09,810 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,847 - Model output complete
2025-02-11 07:55:09,847 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:09,847 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,847 - Next token logits device: cuda:0
2025-02-11 07:55:09,847 - Entered do_sample
2025-02-11 07:55:09,847 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,849 - Probs max: 0.583984375
2025-02-11 07:55:09,850 - Pre-cat
2025-02-11 07:55:09,850 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:55:09,852 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:09,852 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:09,852 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,852 - Step 2: Generated next token
2025-02-11 07:55:09,852 - Step 2: Updated current_ids
2025-02-11 07:55:09,853 - Step 2: Decoded token text:  is
2025-02-11 07:55:09,853 - Step 2: Updated current_phrase
2025-02-11 07:55:09,853 - Step 2: Created step_acts
2025-02-11 07:55:09,853 - Step 2: Added to generation_acts
2025-02-11 07:55:09,853 - Step 2: Updated recent_tokens
2025-02-11 07:55:09,855 - Step 2: Decoded current text
2025-02-11 07:55:09,855 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:09,855 - 
Starting step 3
2025-02-11 07:55:09,855 - Current_ids device: cuda:0
2025-02-11 07:55:09,855 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,896 - Model output complete
2025-02-11 07:55:09,896 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:09,896 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,896 - Next token logits device: cuda:0
2025-02-11 07:55:09,896 - Entered do_sample
2025-02-11 07:55:09,896 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,898 - Probs max: 0.59521484375
2025-02-11 07:55:09,899 - Pre-cat
2025-02-11 07:55:09,899 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:09,902 - Next token: tensor([[7218]], device='cuda:0')
2025-02-11 07:55:09,902 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:09,902 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,902 - Step 3: Generated next token
2025-02-11 07:55:09,902 - Step 3: Updated current_ids
2025-02-11 07:55:09,903 - Step 3: Decoded token text:  moving
2025-02-11 07:55:09,903 - Step 3: Updated current_phrase
2025-02-11 07:55:09,903 - Step 3: Created step_acts
2025-02-11 07:55:09,903 - Step 3: Added to generation_acts
2025-02-11 07:55:09,903 - Step 3: Updated recent_tokens
2025-02-11 07:55:09,904 - Step 3: Decoded current text
2025-02-11 07:55:09,904 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:09,904 - 
Starting step 4
2025-02-11 07:55:09,905 - Current_ids device: cuda:0
2025-02-11 07:55:09,905 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,932 - Model output complete
2025-02-11 07:55:09,933 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:09,933 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,933 - Next token logits device: cuda:0
2025-02-11 07:55:09,933 - Entered do_sample
2025-02-11 07:55:09,933 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,935 - Probs max: 0.677734375
2025-02-11 07:55:09,936 - Pre-cat
2025-02-11 07:55:09,936 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218]],
       device='cuda:0')
2025-02-11 07:55:09,937 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:55:09,938 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:09,938 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,938 - Step 4: Generated next token
2025-02-11 07:55:09,938 - Step 4: Updated current_ids
2025-02-11 07:55:09,938 - Step 4: Decoded token text:  at
2025-02-11 07:55:09,938 - Step 4: Updated current_phrase
2025-02-11 07:55:09,939 - Step 4: Created step_acts
2025-02-11 07:55:09,939 - Step 4: Added to generation_acts
2025-02-11 07:55:09,939 - Step 4: Updated recent_tokens
2025-02-11 07:55:09,940 - Step 4: Decoded current text
2025-02-11 07:55:09,940 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:09,940 - 
Starting step 5
2025-02-11 07:55:09,940 - Current_ids device: cuda:0
2025-02-11 07:55:09,940 - Current_ids dtype: torch.int64
2025-02-11 07:55:09,970 - Model output complete
2025-02-11 07:55:09,971 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:09,971 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,971 - Next token logits device: cuda:0
2025-02-11 07:55:09,971 - Entered do_sample
2025-02-11 07:55:09,971 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:09,973 - Probs max: 0.50146484375
2025-02-11 07:55:09,974 - Pre-cat
2025-02-11 07:55:09,974 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518]],
       device='cuda:0')
2025-02-11 07:55:09,977 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:09,978 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:09,978 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:09,978 - Step 5: Generated next token
2025-02-11 07:55:09,978 - Step 5: Updated current_ids
2025-02-11 07:55:09,978 - Step 5: Decoded token text:  a
2025-02-11 07:55:09,978 - Step 5: Updated current_phrase
2025-02-11 07:55:09,979 - Step 5: Created step_acts
2025-02-11 07:55:09,979 - Step 5: Added to generation_acts
2025-02-11 07:55:09,980 - Step 5: Updated generated_texts
2025-02-11 07:55:09,980 - Step 5: Updated recent_tokens
2025-02-11 07:55:09,981 - Step 5: Decoded current text
2025-02-11 07:55:09,981 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:09,981 - 
Starting step 6
2025-02-11 07:55:09,981 - Current_ids device: cuda:0
2025-02-11 07:55:09,981 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,027 - Model output complete
2025-02-11 07:55:10,027 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:10,027 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,027 - Next token logits device: cuda:0
2025-02-11 07:55:10,028 - Entered do_sample
2025-02-11 07:55:10,028 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,029 - Probs max: 0.3330078125
2025-02-11 07:55:10,031 - Pre-cat
2025-02-11 07:55:10,031 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264]], device='cuda:0')
2025-02-11 07:55:10,034 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:10,034 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:10,034 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,034 - Step 6: Generated next token
2025-02-11 07:55:10,035 - Step 6: Updated current_ids
2025-02-11 07:55:10,035 - Step 6: Decoded token text:  very
2025-02-11 07:55:10,035 - Step 6: Updated current_phrase
2025-02-11 07:55:10,036 - Step 6: Created step_acts
2025-02-11 07:55:10,036 - Step 6: Added to generation_acts
2025-02-11 07:55:10,036 - Step 6: Updated recent_tokens
2025-02-11 07:55:10,038 - Step 6: Decoded current text
2025-02-11 07:55:10,038 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:10,038 - 
Starting step 7
2025-02-11 07:55:10,038 - Current_ids device: cuda:0
2025-02-11 07:55:10,038 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,085 - Model output complete
2025-02-11 07:55:10,086 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:10,086 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,087 - Next token logits device: cuda:0
2025-02-11 07:55:10,087 - Entered do_sample
2025-02-11 07:55:10,087 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,087 - Probs max: 0.669921875
2025-02-11 07:55:10,089 - Pre-cat
2025-02-11 07:55:10,089 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   1602]], device='cuda:0')
2025-02-11 07:55:10,091 - Next token: tensor([[1550]], device='cuda:0')
2025-02-11 07:55:10,091 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:10,092 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,092 - Step 7: Generated next token
2025-02-11 07:55:10,092 - Step 7: Updated current_ids
2025-02-11 07:55:10,092 - Step 7: Decoded token text:  high
2025-02-11 07:55:10,092 - Step 7: Updated current_phrase
2025-02-11 07:55:10,092 - Step 7: Created step_acts
2025-02-11 07:55:10,093 - Step 7: Added to generation_acts
2025-02-11 07:55:10,093 - Step 7: Updated recent_tokens
2025-02-11 07:55:10,094 - Step 7: Decoded current text
2025-02-11 07:55:10,094 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:10,094 - 
Starting step 8
2025-02-11 07:55:10,094 - Current_ids device: cuda:0
2025-02-11 07:55:10,094 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,123 - Model output complete
2025-02-11 07:55:10,123 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:10,123 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,123 - Next token logits device: cuda:0
2025-02-11 07:55:10,123 - Entered do_sample
2025-02-11 07:55:10,123 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,125 - Probs max: 0.95458984375
2025-02-11 07:55:10,127 - Pre-cat
2025-02-11 07:55:10,127 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   1602,   1550]], device='cuda:0')
2025-02-11 07:55:10,130 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:10,130 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:10,130 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,130 - Step 8: Generated next token
2025-02-11 07:55:10,130 - Step 8: Updated current_ids
2025-02-11 07:55:10,131 - Step 8: Decoded token text:  speed
2025-02-11 07:55:10,131 - Step 8: Updated current_phrase
2025-02-11 07:55:10,131 - Step 8: Created step_acts
2025-02-11 07:55:10,131 - Step 8: Added to generation_acts
2025-02-11 07:55:10,131 - Step 8: Updated recent_tokens
2025-02-11 07:55:10,132 - Step 8: Decoded current text
2025-02-11 07:55:10,132 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:10,132 - 
Starting step 9
2025-02-11 07:55:10,132 - Current_ids device: cuda:0
2025-02-11 07:55:10,133 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,164 - Model output complete
2025-02-11 07:55:10,164 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:10,164 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,164 - Next token logits device: cuda:0
2025-02-11 07:55:10,164 - Entered do_sample
2025-02-11 07:55:10,164 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,166 - Probs max: 0.546875
2025-02-11 07:55:10,167 - Pre-cat
2025-02-11 07:55:10,167 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   1602,   1550,   4628]], device='cuda:0')
2025-02-11 07:55:10,169 - Next token: tensor([[6974]], device='cuda:0')
2025-02-11 07:55:10,169 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:10,169 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,169 - Step 9: Generated next token
2025-02-11 07:55:10,170 - Step 9: Updated current_ids
2025-02-11 07:55:10,170 - Step 9: Decoded token text:  towards
2025-02-11 07:55:10,170 - Step 9: Updated current_phrase
2025-02-11 07:55:10,170 - Step 9: Created step_acts
2025-02-11 07:55:10,170 - Step 9: Added to generation_acts
2025-02-11 07:55:10,171 - Step 9: Updated recent_tokens
2025-02-11 07:55:10,173 - Step 9: Decoded current text
2025-02-11 07:55:10,173 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:10,173 - 
Starting step 10
2025-02-11 07:55:10,173 - Current_ids device: cuda:0
2025-02-11 07:55:10,173 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,207 - Model output complete
2025-02-11 07:55:10,208 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:10,208 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,208 - Next token logits device: cuda:0
2025-02-11 07:55:10,208 - Entered do_sample
2025-02-11 07:55:10,208 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,210 - Probs max: 0.9736328125
2025-02-11 07:55:10,211 - Pre-cat
2025-02-11 07:55:10,211 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   1602,   1550,   4628,   6974]], device='cuda:0')
2025-02-11 07:55:10,214 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:10,214 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:10,215 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,215 - Step 10: Generated next token
2025-02-11 07:55:10,215 - Step 10: Updated current_ids
2025-02-11 07:55:10,215 - Step 10: Decoded token text:  the
2025-02-11 07:55:10,215 - Step 10: Updated current_phrase
2025-02-11 07:55:10,215 - Step 10: Created step_acts
2025-02-11 07:55:10,215 - Step 10: Added to generation_acts
2025-02-11 07:55:10,217 - Step 10: Updated generated_texts
2025-02-11 07:55:10,217 - Step 10: Updated recent_tokens
2025-02-11 07:55:10,218 - Step 10: Decoded current text
2025-02-11 07:55:10,218 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:10,218 - 
Starting step 11
2025-02-11 07:55:10,218 - Current_ids device: cuda:0
2025-02-11 07:55:10,218 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,254 - Model output complete
2025-02-11 07:55:10,254 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:10,254 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,254 - Next token logits device: cuda:0
2025-02-11 07:55:10,254 - Entered do_sample
2025-02-11 07:55:10,255 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,256 - Probs max: 0.9990234375
2025-02-11 07:55:10,257 - Pre-cat
2025-02-11 07:55:10,257 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   1602,   1550,   4628,   6974,    279]], device='cuda:0')
2025-02-11 07:55:10,260 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:10,260 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:10,260 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,260 - Step 11: Generated next token
2025-02-11 07:55:10,261 - Step 11: Updated current_ids
2025-02-11 07:55:10,261 - Step 11: Decoded token text:  wall
2025-02-11 07:55:10,261 - Step 11: Updated current_phrase
2025-02-11 07:55:10,262 - Step 11: Created step_acts
2025-02-11 07:55:10,262 - Step 11: Added to generation_acts
2025-02-11 07:55:10,262 - Step 11: Updated recent_tokens
2025-02-11 07:55:10,264 - Step 11: Decoded current text
2025-02-11 07:55:10,264 - Step 11: Reset consecutive_fillers
2025-02-11 07:55:10,264 - 
Starting step 12
2025-02-11 07:55:10,264 - Current_ids device: cuda:0
2025-02-11 07:55:10,264 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,296 - Model output complete
2025-02-11 07:55:10,297 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:10,297 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,297 - Next token logits device: cuda:0
2025-02-11 07:55:10,297 - Entered do_sample
2025-02-11 07:55:10,297 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,299 - Probs max: 0.70703125
2025-02-11 07:55:10,300 - Pre-cat
2025-02-11 07:55:10,300 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   1602,   1550,   4628,   6974,    279,   7002]],
       device='cuda:0')
2025-02-11 07:55:10,301 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:55:10,302 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:10,302 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,302 - Step 12: Generated next token
2025-02-11 07:55:10,302 - Step 12: Updated current_ids
2025-02-11 07:55:10,302 - Step 12: Decoded token text: .
2025-02-11 07:55:10,302 - Step 12: Updated current_phrase
2025-02-11 07:55:10,302 - Step 12: Created step_acts
2025-02-11 07:55:10,302 - Step 12: Added to generation_acts
2025-02-11 07:55:10,303 - Step 12: Updated recent_tokens
2025-02-11 07:55:10,304 - Step 12: Found phrase end token
2025-02-11 07:55:10,304 - Step 12: Updated recent_phrases
2025-02-11 07:55:10,304 - Step 12: Decoded current text
2025-02-11 07:55:10,304 - Step 12: Reset consecutive_fillers
2025-02-11 07:55:10,304 - 
Starting step 13
2025-02-11 07:55:10,304 - Current_ids device: cuda:0
2025-02-11 07:55:10,304 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,339 - Model output complete
2025-02-11 07:55:10,339 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:10,339 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,339 - Next token logits device: cuda:0
2025-02-11 07:55:10,339 - Entered do_sample
2025-02-11 07:55:10,339 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,341 - Probs max: 0.708984375
2025-02-11 07:55:10,343 - Pre-cat
2025-02-11 07:55:10,344 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   1602,   1550,   4628,   6974,    279,   7002,     13]],
       device='cuda:0')
2025-02-11 07:55:10,348 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:10,348 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:10,348 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,348 - Step 13: Generated next token
2025-02-11 07:55:10,348 - Step 13: Updated current_ids
2025-02-11 07:55:10,349 - Step 13: Decoded token text:  The
2025-02-11 07:55:10,349 - Step 13: Updated current_phrase
2025-02-11 07:55:10,350 - Step 13: Created step_acts
2025-02-11 07:55:10,350 - Step 13: Added to generation_acts
2025-02-11 07:55:10,350 - Step 13: Updated recent_tokens
2025-02-11 07:55:10,351 - Step 13: Decoded current text
2025-02-11 07:55:10,351 - Step 13: Reset consecutive_fillers
2025-02-11 07:55:10,351 - 
Starting step 14
2025-02-11 07:55:10,351 - Current_ids device: cuda:0
2025-02-11 07:55:10,351 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,387 - Model output complete
2025-02-11 07:55:10,387 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:55:10,387 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,387 - Next token logits device: cuda:0
2025-02-11 07:55:10,387 - Entered do_sample
2025-02-11 07:55:10,387 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,389 - Probs max: 0.94287109375
2025-02-11 07:55:10,390 - Pre-cat
2025-02-11 07:55:10,390 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   1602,   1550,   4628,   6974,    279,   7002,     13,    576]],
       device='cuda:0')
2025-02-11 07:55:10,393 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:10,393 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:55:10,393 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,394 - Step 14: Generated next token
2025-02-11 07:55:10,394 - Step 14: Updated current_ids
2025-02-11 07:55:10,394 - Step 14: Decoded token text:  wall
2025-02-11 07:55:10,394 - Step 14: Updated current_phrase
2025-02-11 07:55:10,394 - Step 14: Created step_acts
2025-02-11 07:55:10,394 - Step 14: Added to generation_acts
2025-02-11 07:55:10,394 - Step 14: Updated recent_tokens
2025-02-11 07:55:10,396 - Step 14: Decoded current text
2025-02-11 07:55:10,396 - Step 14: Reset consecutive_fillers
2025-02-11 07:55:10,396 - 
Starting step 15
2025-02-11 07:55:10,396 - Current_ids device: cuda:0
2025-02-11 07:55:10,396 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,436 - Model output complete
2025-02-11 07:55:10,436 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:55:10,436 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,436 - Next token logits device: cuda:0
2025-02-11 07:55:10,436 - Entered do_sample
2025-02-11 07:55:10,436 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,438 - Probs max: 0.94921875
2025-02-11 07:55:10,439 - Pre-cat
2025-02-11 07:55:10,439 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   1602,   1550,   4628,   6974,    279,   7002,     13,    576,
           7002]], device='cuda:0')
2025-02-11 07:55:10,443 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:10,444 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:55:10,444 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,444 - Step 15: Generated next token
2025-02-11 07:55:10,444 - Step 15: Updated current_ids
2025-02-11 07:55:10,444 - Step 15: Decoded token text:  is
2025-02-11 07:55:10,444 - Step 15: Updated current_phrase
2025-02-11 07:55:10,445 - Step 15: Created step_acts
2025-02-11 07:55:10,445 - Step 15: Added to generation_acts
2025-02-11 07:55:10,446 - Step 15: Updated generated_texts
2025-02-11 07:55:10,446 - Step 15: Updated recent_tokens
2025-02-11 07:55:10,447 - Step 15: Decoded current text
2025-02-11 07:55:10,447 - Step 15: Reset consecutive_fillers
2025-02-11 07:55:10,447 - 
Starting step 16
2025-02-11 07:55:10,447 - Current_ids device: cuda:0
2025-02-11 07:55:10,447 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,479 - Model output complete
2025-02-11 07:55:10,479 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:55:10,479 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,479 - Next token logits device: cuda:0
2025-02-11 07:55:10,479 - Entered do_sample
2025-02-11 07:55:10,479 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,481 - Probs max: 0.6552734375
2025-02-11 07:55:10,482 - Pre-cat
2025-02-11 07:55:10,482 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            264,   1602,   1550,   4628,   6974,    279,   7002,     13,    576,
           7002,    374]], device='cuda:0')
2025-02-11 07:55:10,485 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:10,485 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:55:10,485 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,485 - Step 16: Generated next token
2025-02-11 07:55:10,485 - Step 16: Updated current_ids
2025-02-11 07:55:10,486 - Step 16: Decoded token text:  very
2025-02-11 07:55:10,486 - Step 16: Updated current_phrase
2025-02-11 07:55:10,486 - Step 16: Created step_acts
2025-02-11 07:55:10,486 - Step 16: Added to generation_acts
2025-02-11 07:55:10,486 - Step 16: Updated recent_tokens
2025-02-11 07:55:10,488 - Step 16: Decoded current text
2025-02-11 07:55:10,488 - Step 16: Reset consecutive_fillers
2025-02-11 07:55:10,489 - Step 16: Calculated unique_ratio: 0.8125
2025-02-11 07:55:10,489 - 
Starting step 17
2025-02-11 07:55:10,489 - Current_ids device: cuda:0
2025-02-11 07:55:10,489 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,534 - Model output complete
2025-02-11 07:55:10,534 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:55:10,535 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,535 - Next token logits device: cuda:0
2025-02-11 07:55:10,535 - Entered do_sample
2025-02-11 07:55:10,535 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,537 - Probs max: 0.1375732421875
2025-02-11 07:55:10,661 - 
Starting step 0
2025-02-11 07:55:10,661 - Current_ids device: cuda:0
2025-02-11 07:55:10,661 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,733 - Model output complete
2025-02-11 07:55:10,733 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:10,734 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,734 - Next token logits device: cuda:0
2025-02-11 07:55:10,734 - Entered do_sample
2025-02-11 07:55:10,734 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,736 - Probs max: 0.50341796875
2025-02-11 07:55:10,736 - Pre-cat
2025-02-11 07:55:10,737 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:10,738 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:55:10,738 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:10,738 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,738 - Step 0: Generated next token
2025-02-11 07:55:10,738 - Step 0: Updated current_ids
2025-02-11 07:55:10,739 - Step 0: Decoded token text:  If
2025-02-11 07:55:10,739 - Step 0: Updated current_phrase
2025-02-11 07:55:10,739 - Step 0: Created step_acts
2025-02-11 07:55:10,739 - Step 0: Added to generation_acts
2025-02-11 07:55:10,740 - Step 0: Updated generated_texts
2025-02-11 07:55:10,740 - Step 0: Updated recent_tokens
2025-02-11 07:55:10,741 - Step 0: Decoded current text
2025-02-11 07:55:10,741 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:10,741 - 
Starting step 1
2025-02-11 07:55:10,741 - Current_ids device: cuda:0
2025-02-11 07:55:10,741 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,765 - Model output complete
2025-02-11 07:55:10,765 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:10,765 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,765 - Next token logits device: cuda:0
2025-02-11 07:55:10,765 - Entered do_sample
2025-02-11 07:55:10,765 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,768 - Probs max: 0.7099609375
2025-02-11 07:55:10,768 - Pre-cat
2025-02-11 07:55:10,768 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:55:10,770 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:10,770 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:10,770 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,771 - Step 1: Generated next token
2025-02-11 07:55:10,771 - Step 1: Updated current_ids
2025-02-11 07:55:10,771 - Step 1: Decoded token text:  a
2025-02-11 07:55:10,771 - Step 1: Updated current_phrase
2025-02-11 07:55:10,771 - Step 1: Created step_acts
2025-02-11 07:55:10,771 - Step 1: Added to generation_acts
2025-02-11 07:55:10,771 - Step 1: Updated recent_tokens
2025-02-11 07:55:10,773 - Step 1: Decoded current text
2025-02-11 07:55:10,773 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:10,773 - 
Starting step 2
2025-02-11 07:55:10,773 - Current_ids device: cuda:0
2025-02-11 07:55:10,773 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,805 - Model output complete
2025-02-11 07:55:10,805 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:10,805 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,805 - Next token logits device: cuda:0
2025-02-11 07:55:10,805 - Entered do_sample
2025-02-11 07:55:10,806 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,807 - Probs max: 0.9970703125
2025-02-11 07:55:10,808 - Pre-cat
2025-02-11 07:55:10,808 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264]], device='cuda:0')
2025-02-11 07:55:10,810 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:10,810 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:10,810 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,810 - Step 2: Generated next token
2025-02-11 07:55:10,810 - Step 2: Updated current_ids
2025-02-11 07:55:10,811 - Step 2: Decoded token text:  ball
2025-02-11 07:55:10,811 - Step 2: Updated current_phrase
2025-02-11 07:55:10,811 - Step 2: Created step_acts
2025-02-11 07:55:10,811 - Step 2: Added to generation_acts
2025-02-11 07:55:10,811 - Step 2: Updated recent_tokens
2025-02-11 07:55:10,813 - Step 2: Decoded current text
2025-02-11 07:55:10,813 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:10,813 - 
Starting step 3
2025-02-11 07:55:10,813 - Current_ids device: cuda:0
2025-02-11 07:55:10,813 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,842 - Model output complete
2025-02-11 07:55:10,842 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:10,842 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,842 - Next token logits device: cuda:0
2025-02-11 07:55:10,842 - Entered do_sample
2025-02-11 07:55:10,842 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,845 - Probs max: 0.99951171875
2025-02-11 07:55:10,845 - Pre-cat
2025-02-11 07:55:10,845 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935]],
       device='cuda:0')
2025-02-11 07:55:10,846 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:10,847 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:10,847 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,847 - Step 3: Generated next token
2025-02-11 07:55:10,847 - Step 3: Updated current_ids
2025-02-11 07:55:10,847 - Step 3: Decoded token text:  is
2025-02-11 07:55:10,847 - Step 3: Updated current_phrase
2025-02-11 07:55:10,847 - Step 3: Created step_acts
2025-02-11 07:55:10,847 - Step 3: Added to generation_acts
2025-02-11 07:55:10,847 - Step 3: Updated recent_tokens
2025-02-11 07:55:10,849 - Step 3: Decoded current text
2025-02-11 07:55:10,849 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:10,849 - 
Starting step 4
2025-02-11 07:55:10,849 - Current_ids device: cuda:0
2025-02-11 07:55:10,850 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,872 - Model output complete
2025-02-11 07:55:10,873 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:10,873 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,873 - Next token logits device: cuda:0
2025-02-11 07:55:10,873 - Entered do_sample
2025-02-11 07:55:10,873 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,875 - Probs max: 0.998046875
2025-02-11 07:55:10,875 - Pre-cat
2025-02-11 07:55:10,876 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:10,877 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:10,877 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:10,877 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,877 - Step 4: Generated next token
2025-02-11 07:55:10,877 - Step 4: Updated current_ids
2025-02-11 07:55:10,877 - Step 4: Decoded token text:  thrown
2025-02-11 07:55:10,877 - Step 4: Updated current_phrase
2025-02-11 07:55:10,878 - Step 4: Created step_acts
2025-02-11 07:55:10,878 - Step 4: Added to generation_acts
2025-02-11 07:55:10,878 - Step 4: Updated recent_tokens
2025-02-11 07:55:10,880 - Step 4: Decoded current text
2025-02-11 07:55:10,880 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:10,880 - 
Starting step 5
2025-02-11 07:55:10,880 - Current_ids device: cuda:0
2025-02-11 07:55:10,880 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,902 - Model output complete
2025-02-11 07:55:10,903 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:10,903 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,903 - Next token logits device: cuda:0
2025-02-11 07:55:10,903 - Entered do_sample
2025-02-11 07:55:10,903 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,905 - Probs max: 0.9697265625
2025-02-11 07:55:10,906 - Pre-cat
2025-02-11 07:55:10,906 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:55:10,907 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:55:10,907 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:10,907 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,907 - Step 5: Generated next token
2025-02-11 07:55:10,907 - Step 5: Updated current_ids
2025-02-11 07:55:10,908 - Step 5: Decoded token text:  at
2025-02-11 07:55:10,908 - Step 5: Updated current_phrase
2025-02-11 07:55:10,908 - Step 5: Created step_acts
2025-02-11 07:55:10,908 - Step 5: Added to generation_acts
2025-02-11 07:55:10,910 - Step 5: Updated generated_texts
2025-02-11 07:55:10,910 - Step 5: Updated recent_tokens
2025-02-11 07:55:10,910 - Step 5: Decoded current text
2025-02-11 07:55:10,910 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:10,910 - 
Starting step 6
2025-02-11 07:55:10,910 - Current_ids device: cuda:0
2025-02-11 07:55:10,911 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,934 - Model output complete
2025-02-11 07:55:10,934 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:10,934 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,934 - Next token logits device: cuda:0
2025-02-11 07:55:10,934 - Entered do_sample
2025-02-11 07:55:10,934 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,936 - Probs max: 0.99951171875
2025-02-11 07:55:10,937 - Pre-cat
2025-02-11 07:55:10,937 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:55:10,940 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:10,941 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:10,941 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,941 - Step 6: Generated next token
2025-02-11 07:55:10,941 - Step 6: Updated current_ids
2025-02-11 07:55:10,941 - Step 6: Decoded token text:  a
2025-02-11 07:55:10,941 - Step 6: Updated current_phrase
2025-02-11 07:55:10,941 - Step 6: Created step_acts
2025-02-11 07:55:10,941 - Step 6: Added to generation_acts
2025-02-11 07:55:10,942 - Step 6: Updated recent_tokens
2025-02-11 07:55:10,943 - Step 6: Decoded current text
2025-02-11 07:55:10,943 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:10,943 - 
Starting step 7
2025-02-11 07:55:10,943 - Current_ids device: cuda:0
2025-02-11 07:55:10,943 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,964 - Model output complete
2025-02-11 07:55:10,964 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:10,964 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,964 - Next token logits device: cuda:0
2025-02-11 07:55:10,964 - Entered do_sample
2025-02-11 07:55:10,964 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,967 - Probs max: 0.9990234375
2025-02-11 07:55:10,968 - Pre-cat
2025-02-11 07:55:10,968 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:55:10,970 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:10,970 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:10,971 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:10,971 - Step 7: Generated next token
2025-02-11 07:55:10,971 - Step 7: Updated current_ids
2025-02-11 07:55:10,971 - Step 7: Decoded token text:  wall
2025-02-11 07:55:10,971 - Step 7: Updated current_phrase
2025-02-11 07:55:10,971 - Step 7: Created step_acts
2025-02-11 07:55:10,971 - Step 7: Added to generation_acts
2025-02-11 07:55:10,971 - Step 7: Updated recent_tokens
2025-02-11 07:55:10,973 - Step 7: Decoded current text
2025-02-11 07:55:10,973 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:10,973 - 
Starting step 8
2025-02-11 07:55:10,973 - Current_ids device: cuda:0
2025-02-11 07:55:10,973 - Current_ids dtype: torch.int64
2025-02-11 07:55:10,994 - Model output complete
2025-02-11 07:55:10,994 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:10,994 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,994 - Next token logits device: cuda:0
2025-02-11 07:55:10,994 - Entered do_sample
2025-02-11 07:55:10,994 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:10,997 - Probs max: 0.9912109375
2025-02-11 07:55:10,997 - Pre-cat
2025-02-11 07:55:10,997 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:55:11,000 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:11,000 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:11,000 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,000 - Step 8: Generated next token
2025-02-11 07:55:11,000 - Step 8: Updated current_ids
2025-02-11 07:55:11,000 - Step 8: Decoded token text:  very
2025-02-11 07:55:11,000 - Step 8: Updated current_phrase
2025-02-11 07:55:11,001 - Step 8: Created step_acts
2025-02-11 07:55:11,001 - Step 8: Added to generation_acts
2025-02-11 07:55:11,001 - Step 8: Updated recent_tokens
2025-02-11 07:55:11,002 - Step 8: Decoded current text
2025-02-11 07:55:11,002 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:11,002 - 
Starting step 9
2025-02-11 07:55:11,002 - Current_ids device: cuda:0
2025-02-11 07:55:11,002 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,037 - Model output complete
2025-02-11 07:55:11,037 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:11,038 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,038 - Next token logits device: cuda:0
2025-02-11 07:55:11,038 - Entered do_sample
2025-02-11 07:55:11,038 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,040 - Probs max: 0.998046875
2025-02-11 07:55:11,041 - Pre-cat
2025-02-11 07:55:11,041 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:55:11,043 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:55:11,044 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:11,044 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,044 - Step 9: Generated next token
2025-02-11 07:55:11,044 - Step 9: Updated current_ids
2025-02-11 07:55:11,045 - Step 9: Decoded token text:  fast
2025-02-11 07:55:11,045 - Step 9: Updated current_phrase
2025-02-11 07:55:11,045 - Step 9: Created step_acts
2025-02-11 07:55:11,045 - Step 9: Added to generation_acts
2025-02-11 07:55:11,045 - Step 9: Updated recent_tokens
2025-02-11 07:55:11,047 - Step 9: Decoded current text
2025-02-11 07:55:11,047 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:11,047 - 
Starting step 10
2025-02-11 07:55:11,047 - Current_ids device: cuda:0
2025-02-11 07:55:11,047 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,084 - Model output complete
2025-02-11 07:55:11,084 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:11,084 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,084 - Next token logits device: cuda:0
2025-02-11 07:55:11,084 - Entered do_sample
2025-02-11 07:55:11,084 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,087 - Probs max: 0.99853515625
2025-02-11 07:55:11,087 - Pre-cat
2025-02-11 07:55:11,087 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:55:11,090 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:11,090 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:11,090 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,090 - Step 10: Generated next token
2025-02-11 07:55:11,090 - Step 10: Updated current_ids
2025-02-11 07:55:11,091 - Step 10: Decoded token text: ,
2025-02-11 07:55:11,091 - Step 10: Updated current_phrase
2025-02-11 07:55:11,091 - Step 10: Created step_acts
2025-02-11 07:55:11,091 - Step 10: Added to generation_acts
2025-02-11 07:55:11,092 - Step 10: Updated generated_texts
2025-02-11 07:55:11,092 - Step 10: Updated recent_tokens
2025-02-11 07:55:11,093 - Step 10: Found phrase end token
2025-02-11 07:55:11,093 - Step 10: Updated recent_phrases
2025-02-11 07:55:11,093 - Step 10: Decoded current text
2025-02-11 07:55:11,093 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:11,093 - 
Starting step 11
2025-02-11 07:55:11,093 - Current_ids device: cuda:0
2025-02-11 07:55:11,093 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,114 - Model output complete
2025-02-11 07:55:11,114 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:11,114 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,114 - Next token logits device: cuda:0
2025-02-11 07:55:11,114 - Entered do_sample
2025-02-11 07:55:11,114 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,117 - Probs max: 0.58154296875
2025-02-11 07:55:11,118 - Pre-cat
2025-02-11 07:55:11,118 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:55:11,121 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:11,121 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:11,121 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,121 - Step 11: Generated next token
2025-02-11 07:55:11,121 - Step 11: Updated current_ids
2025-02-11 07:55:11,121 - Step 11: Decoded token text:  the
2025-02-11 07:55:11,121 - Step 11: Updated current_phrase
2025-02-11 07:55:11,122 - Step 11: Created step_acts
2025-02-11 07:55:11,122 - Step 11: Added to generation_acts
2025-02-11 07:55:11,122 - Step 11: Updated recent_tokens
2025-02-11 07:55:11,123 - Step 11: Decoded current text
2025-02-11 07:55:11,123 - Step 11: Reset consecutive_fillers
2025-02-11 07:55:11,123 - 
Starting step 12
2025-02-11 07:55:11,123 - Current_ids device: cuda:0
2025-02-11 07:55:11,123 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,152 - Model output complete
2025-02-11 07:55:11,152 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:11,153 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,153 - Next token logits device: cuda:0
2025-02-11 07:55:11,153 - Entered do_sample
2025-02-11 07:55:11,153 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,155 - Probs max: 0.6064453125
2025-02-11 07:55:11,155 - Pre-cat
2025-02-11 07:55:11,156 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279]],
       device='cuda:0')
2025-02-11 07:55:11,158 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:11,158 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:11,158 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,158 - Step 12: Generated next token
2025-02-11 07:55:11,158 - Step 12: Updated current_ids
2025-02-11 07:55:11,159 - Step 12: Decoded token text:  ball
2025-02-11 07:55:11,159 - Step 12: Updated current_phrase
2025-02-11 07:55:11,159 - Step 12: Created step_acts
2025-02-11 07:55:11,159 - Step 12: Added to generation_acts
2025-02-11 07:55:11,159 - Step 12: Updated recent_tokens
2025-02-11 07:55:11,160 - Step 12: Decoded current text
2025-02-11 07:55:11,160 - Step 12: Reset consecutive_fillers
2025-02-11 07:55:11,160 - 
Starting step 13
2025-02-11 07:55:11,160 - Current_ids device: cuda:0
2025-02-11 07:55:11,161 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,183 - Model output complete
2025-02-11 07:55:11,183 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:11,183 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,183 - Next token logits device: cuda:0
2025-02-11 07:55:11,183 - Entered do_sample
2025-02-11 07:55:11,184 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,186 - Probs max: 0.76953125
2025-02-11 07:55:11,186 - Pre-cat
2025-02-11 07:55:11,186 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935]],
       device='cuda:0')
2025-02-11 07:55:11,188 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:11,188 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:11,188 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,188 - Step 13: Generated next token
2025-02-11 07:55:11,188 - Step 13: Updated current_ids
2025-02-11 07:55:11,188 - Step 13: Decoded token text:  is
2025-02-11 07:55:11,188 - Step 13: Updated current_phrase
2025-02-11 07:55:11,189 - Step 13: Created step_acts
2025-02-11 07:55:11,189 - Step 13: Added to generation_acts
2025-02-11 07:55:11,189 - Step 13: Updated recent_tokens
2025-02-11 07:55:11,190 - Step 13: Decoded current text
2025-02-11 07:55:11,190 - Step 13: Reset consecutive_fillers
2025-02-11 07:55:11,190 - 
Starting step 14
2025-02-11 07:55:11,190 - Current_ids device: cuda:0
2025-02-11 07:55:11,190 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,212 - Model output complete
2025-02-11 07:55:11,212 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:55:11,212 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,212 - Next token logits device: cuda:0
2025-02-11 07:55:11,212 - Entered do_sample
2025-02-11 07:55:11,212 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,215 - Probs max: 0.61083984375
2025-02-11 07:55:11,216 - Pre-cat
2025-02-11 07:55:11,216 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:11,218 - Next token: tensor([[3832]], device='cuda:0')
2025-02-11 07:55:11,218 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:55:11,218 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,218 - Step 14: Generated next token
2025-02-11 07:55:11,218 - Step 14: Updated current_ids
2025-02-11 07:55:11,219 - Step 14: Decoded token text:  subject
2025-02-11 07:55:11,219 - Step 14: Updated current_phrase
2025-02-11 07:55:11,219 - Step 14: Created step_acts
2025-02-11 07:55:11,219 - Step 14: Added to generation_acts
2025-02-11 07:55:11,219 - Step 14: Updated recent_tokens
2025-02-11 07:55:11,220 - Step 14: Decoded current text
2025-02-11 07:55:11,220 - Step 14: Reset consecutive_fillers
2025-02-11 07:55:11,220 - 
Starting step 15
2025-02-11 07:55:11,221 - Current_ids device: cuda:0
2025-02-11 07:55:11,221 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,243 - Model output complete
2025-02-11 07:55:11,243 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:55:11,243 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,243 - Next token logits device: cuda:0
2025-02-11 07:55:11,243 - Entered do_sample
2025-02-11 07:55:11,243 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,247 - Probs max: 0.9990234375
2025-02-11 07:55:11,247 - Pre-cat
2025-02-11 07:55:11,248 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           3832]], device='cuda:0')
2025-02-11 07:55:11,249 - Next token: tensor([[311]], device='cuda:0')
2025-02-11 07:55:11,249 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:55:11,249 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,249 - Step 15: Generated next token
2025-02-11 07:55:11,249 - Step 15: Updated current_ids
2025-02-11 07:55:11,250 - Step 15: Decoded token text:  to
2025-02-11 07:55:11,250 - Step 15: Updated current_phrase
2025-02-11 07:55:11,250 - Step 15: Created step_acts
2025-02-11 07:55:11,250 - Step 15: Added to generation_acts
2025-02-11 07:55:11,251 - Step 15: Updated generated_texts
2025-02-11 07:55:11,251 - Step 15: Updated recent_tokens
2025-02-11 07:55:11,252 - Step 15: Decoded current text
2025-02-11 07:55:11,252 - Step 15: Reset consecutive_fillers
2025-02-11 07:55:11,252 - 
Starting step 16
2025-02-11 07:55:11,252 - Current_ids device: cuda:0
2025-02-11 07:55:11,252 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,274 - Model output complete
2025-02-11 07:55:11,274 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:55:11,274 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,274 - Next token logits device: cuda:0
2025-02-11 07:55:11,274 - Entered do_sample
2025-02-11 07:55:11,274 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,277 - Probs max: 0.411865234375
2025-02-11 07:55:11,278 - Pre-cat
2025-02-11 07:55:11,278 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           3832,    311]], device='cuda:0')
2025-02-11 07:55:11,280 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:11,281 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:55:11,281 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,281 - Step 16: Generated next token
2025-02-11 07:55:11,281 - Step 16: Updated current_ids
2025-02-11 07:55:11,281 - Step 16: Decoded token text:  a
2025-02-11 07:55:11,281 - Step 16: Updated current_phrase
2025-02-11 07:55:11,281 - Step 16: Created step_acts
2025-02-11 07:55:11,281 - Step 16: Added to generation_acts
2025-02-11 07:55:11,281 - Step 16: Updated recent_tokens
2025-02-11 07:55:11,283 - Step 16: Decoded current text
2025-02-11 07:55:11,283 - Step 16: Reset consecutive_fillers
2025-02-11 07:55:11,283 - Step 16: Calculated unique_ratio: 0.75
2025-02-11 07:55:11,283 - 
Starting step 17
2025-02-11 07:55:11,283 - Current_ids device: cuda:0
2025-02-11 07:55:11,283 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,315 - Model output complete
2025-02-11 07:55:11,316 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:55:11,316 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,316 - Next token logits device: cuda:0
2025-02-11 07:55:11,316 - Entered do_sample
2025-02-11 07:55:11,316 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,318 - Probs max: 0.306640625
2025-02-11 07:55:11,319 - Pre-cat
2025-02-11 07:55:11,319 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           3832,    311,    264]], device='cuda:0')
2025-02-11 07:55:11,320 - Next token: tensor([[2297]], device='cuda:0')
2025-02-11 07:55:11,321 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:55:11,321 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,321 - Step 17: Generated next token
2025-02-11 07:55:11,321 - Step 17: Updated current_ids
2025-02-11 07:55:11,321 - Step 17: Decoded token text:  change
2025-02-11 07:55:11,321 - Step 17: Updated current_phrase
2025-02-11 07:55:11,321 - Step 17: Created step_acts
2025-02-11 07:55:11,322 - Step 17: Added to generation_acts
2025-02-11 07:55:11,322 - Step 17: Updated recent_tokens
2025-02-11 07:55:11,323 - Step 17: Decoded current text
2025-02-11 07:55:11,323 - Step 17: Reset consecutive_fillers
2025-02-11 07:55:11,323 - Step 17: Calculated unique_ratio: 0.8125
2025-02-11 07:55:11,323 - 
Starting step 18
2025-02-11 07:55:11,323 - Current_ids device: cuda:0
2025-02-11 07:55:11,323 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,346 - Model output complete
2025-02-11 07:55:11,347 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:55:11,347 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,347 - Next token logits device: cuda:0
2025-02-11 07:55:11,347 - Entered do_sample
2025-02-11 07:55:11,347 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,349 - Probs max: 0.9921875
2025-02-11 07:55:11,350 - Pre-cat
2025-02-11 07:55:11,350 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           3832,    311,    264,   2297]], device='cuda:0')
2025-02-11 07:55:11,352 - Next token: tensor([[304]], device='cuda:0')
2025-02-11 07:55:11,352 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:55:11,352 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,352 - Step 18: Generated next token
2025-02-11 07:55:11,352 - Step 18: Updated current_ids
2025-02-11 07:55:11,352 - Step 18: Decoded token text:  in
2025-02-11 07:55:11,352 - Step 18: Updated current_phrase
2025-02-11 07:55:11,352 - Step 18: Created step_acts
2025-02-11 07:55:11,353 - Step 18: Added to generation_acts
2025-02-11 07:55:11,353 - Step 18: Updated recent_tokens
2025-02-11 07:55:11,354 - Step 18: Decoded current text
2025-02-11 07:55:11,354 - Step 18: Reset consecutive_fillers
2025-02-11 07:55:11,354 - Step 18: Calculated unique_ratio: 0.875
2025-02-11 07:55:11,354 - 
Starting step 19
2025-02-11 07:55:11,354 - Current_ids device: cuda:0
2025-02-11 07:55:11,354 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,375 - Model output complete
2025-02-11 07:55:11,375 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:55:11,375 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,375 - Next token logits device: cuda:0
2025-02-11 07:55:11,375 - Entered do_sample
2025-02-11 07:55:11,376 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,379 - Probs max: 0.83642578125
2025-02-11 07:55:11,380 - Pre-cat
2025-02-11 07:55:11,380 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           3832,    311,    264,   2297,    304]], device='cuda:0')
2025-02-11 07:55:11,382 - Next token: tensor([[23270]], device='cuda:0')
2025-02-11 07:55:11,382 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:55:11,382 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,382 - Step 19: Generated next token
2025-02-11 07:55:11,382 - Step 19: Updated current_ids
2025-02-11 07:55:11,382 - Step 19: Decoded token text:  momentum
2025-02-11 07:55:11,382 - Step 19: Updated current_phrase
2025-02-11 07:55:11,383 - Step 19: Created step_acts
2025-02-11 07:55:11,383 - Step 19: Added to generation_acts
2025-02-11 07:55:11,383 - Step 19: Updated recent_tokens
2025-02-11 07:55:11,384 - Step 19: Decoded current text
2025-02-11 07:55:11,384 - Step 19: Reset consecutive_fillers
2025-02-11 07:55:11,384 - Step 19: Calculated unique_ratio: 0.9375
2025-02-11 07:55:11,385 - 
Starting step 20
2025-02-11 07:55:11,385 - Current_ids device: cuda:0
2025-02-11 07:55:11,385 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,406 - Model output complete
2025-02-11 07:55:11,406 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:55:11,406 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,406 - Next token logits device: cuda:0
2025-02-11 07:55:11,406 - Entered do_sample
2025-02-11 07:55:11,406 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,410 - Probs max: 0.7353515625
2025-02-11 07:55:11,411 - Pre-cat
2025-02-11 07:55:11,411 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           3832,    311,    264,   2297,    304,  23270]], device='cuda:0')
2025-02-11 07:55:11,413 - Next token: tensor([[320]], device='cuda:0')
2025-02-11 07:55:11,413 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:55:11,413 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,413 - Step 20: Generated next token
2025-02-11 07:55:11,413 - Step 20: Updated current_ids
2025-02-11 07:55:11,413 - Step 20: Decoded token text:  (
2025-02-11 07:55:11,413 - Step 20: Updated current_phrase
2025-02-11 07:55:11,414 - Step 20: Created step_acts
2025-02-11 07:55:11,414 - Step 20: Added to generation_acts
2025-02-11 07:55:11,415 - Step 20: Updated generated_texts
2025-02-11 07:55:11,415 - Step 20: Updated recent_tokens
2025-02-11 07:55:11,415 - Step 20: Decoded current text
2025-02-11 07:55:11,416 - Step 20: Reset consecutive_fillers
2025-02-11 07:55:11,416 - Step 20: Calculated unique_ratio: 0.9375
2025-02-11 07:55:11,416 - 
Starting step 21
2025-02-11 07:55:11,416 - Current_ids device: cuda:0
2025-02-11 07:55:11,416 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,438 - Model output complete
2025-02-11 07:55:11,438 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:55:11,438 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,438 - Next token logits device: cuda:0
2025-02-11 07:55:11,438 - Entered do_sample
2025-02-11 07:55:11,438 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,441 - Probs max: 0.91796875
2025-02-11 07:55:11,442 - Pre-cat
2025-02-11 07:55:11,442 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           3832,    311,    264,   2297,    304,  23270,    320]],
       device='cuda:0')
2025-02-11 07:55:11,444 - Next token: tensor([[144665]], device='cuda:0')
2025-02-11 07:55:11,444 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:55:11,444 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,444 - Step 21: Generated next token
2025-02-11 07:55:11,444 - Step 21: Updated current_ids
2025-02-11 07:55:11,444 - Step 21: Decoded token text: 
2025-02-11 07:55:11,444 - Step 21: Updated current_phrase
2025-02-11 07:55:11,445 - Step 21: Created step_acts
2025-02-11 07:55:11,445 - Step 21: Added to generation_acts
2025-02-11 07:55:11,445 - Step 21: Updated recent_tokens
2025-02-11 07:55:11,446 - Step 21: Decoded current text
2025-02-11 07:55:11,446 - Step 21: Reset consecutive_fillers
2025-02-11 07:55:11,446 - Step 21: Calculated unique_ratio: 0.9375
2025-02-11 07:55:11,446 - 
Starting step 22
2025-02-11 07:55:11,446 - Current_ids device: cuda:0
2025-02-11 07:55:11,446 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,469 - Model output complete
2025-02-11 07:55:11,469 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:55:11,469 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,469 - Next token logits device: cuda:0
2025-02-11 07:55:11,469 - Entered do_sample
2025-02-11 07:55:11,469 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,472 - Probs max: 0.9931640625
2025-02-11 07:55:11,472 - Pre-cat
2025-02-11 07:55:11,472 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           3832,    311,    264,   2297,    304,  23270,    320, 144665]],
       device='cuda:0')
2025-02-11 07:55:11,474 - Next token: tensor([[79]], device='cuda:0')
2025-02-11 07:55:11,475 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:55:11,475 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,475 - Step 22: Generated next token
2025-02-11 07:55:11,475 - Step 22: Updated current_ids
2025-02-11 07:55:11,475 - Step 22: Decoded token text: p
2025-02-11 07:55:11,475 - Step 22: Updated current_phrase
2025-02-11 07:55:11,475 - Step 22: Created step_acts
2025-02-11 07:55:11,476 - Step 22: Added to generation_acts
2025-02-11 07:55:11,476 - Step 22: Updated recent_tokens
2025-02-11 07:55:11,477 - Step 22: Decoded current text
2025-02-11 07:55:11,477 - Step 22: Reset consecutive_fillers
2025-02-11 07:55:11,477 - Step 22: Calculated unique_ratio: 1.0
2025-02-11 07:55:11,477 - 
Starting step 23
2025-02-11 07:55:11,477 - Current_ids device: cuda:0
2025-02-11 07:55:11,477 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,500 - Model output complete
2025-02-11 07:55:11,500 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:55:11,500 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,500 - Next token logits device: cuda:0
2025-02-11 07:55:11,500 - Entered do_sample
2025-02-11 07:55:11,500 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,503 - Probs max: 0.5302734375
2025-02-11 07:55:11,503 - Pre-cat
2025-02-11 07:55:11,503 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           3832,    311,    264,   2297,    304,  23270,    320, 144665,     79]],
       device='cuda:0')
2025-02-11 07:55:11,505 - Next token: tensor([[8]], device='cuda:0')
2025-02-11 07:55:11,505 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:55:11,506 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,506 - Step 23: Generated next token
2025-02-11 07:55:11,506 - Step 23: Updated current_ids
2025-02-11 07:55:11,506 - Step 23: Decoded token text: )
2025-02-11 07:55:11,506 - Step 23: Updated current_phrase
2025-02-11 07:55:11,506 - Step 23: Created step_acts
2025-02-11 07:55:11,506 - Step 23: Added to generation_acts
2025-02-11 07:55:11,506 - Step 23: Updated recent_tokens
2025-02-11 07:55:11,508 - Step 23: Decoded current text
2025-02-11 07:55:11,508 - Step 23: Reset consecutive_fillers
2025-02-11 07:55:11,508 - Step 23: Calculated unique_ratio: 1.0
2025-02-11 07:55:11,508 - 
Starting step 24
2025-02-11 07:55:11,508 - Current_ids device: cuda:0
2025-02-11 07:55:11,508 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,533 - Model output complete
2025-02-11 07:55:11,533 - Logits shape: torch.Size([1, 55, 151936])
2025-02-11 07:55:11,533 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,533 - Next token logits device: cuda:0
2025-02-11 07:55:11,533 - Entered do_sample
2025-02-11 07:55:11,533 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,535 - Probs max: 0.5107421875
2025-02-11 07:55:11,536 - Pre-cat
2025-02-11 07:55:11,536 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           3832,    311,    264,   2297,    304,  23270,    320, 144665,     79,
              8]], device='cuda:0')
2025-02-11 07:55:11,538 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:55:11,538 - Current_ids shape: torch.Size([1, 55])
2025-02-11 07:55:11,538 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,538 - Step 24: Generated next token
2025-02-11 07:55:11,538 - Step 24: Updated current_ids
2025-02-11 07:55:11,539 - Step 24: Decoded token text:  and
2025-02-11 07:55:11,539 - Step 24: Updated current_phrase
2025-02-11 07:55:11,539 - Step 24: Created step_acts
2025-02-11 07:55:11,539 - Step 24: Added to generation_acts
2025-02-11 07:55:11,539 - Step 24: Updated recent_tokens
2025-02-11 07:55:11,540 - Step 24: Decoded current text
2025-02-11 07:55:11,541 - Step 24: Reset consecutive_fillers
2025-02-11 07:55:11,541 - Step 24: Calculated unique_ratio: 1.0
2025-02-11 07:55:11,541 - 
Starting step 25
2025-02-11 07:55:11,541 - Current_ids device: cuda:0
2025-02-11 07:55:11,541 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,564 - Model output complete
2025-02-11 07:55:11,564 - Logits shape: torch.Size([1, 56, 151936])
2025-02-11 07:55:11,564 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,564 - Next token logits device: cuda:0
2025-02-11 07:55:11,564 - Entered do_sample
2025-02-11 07:55:11,564 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,569 - Probs max: 0.41015625
2025-02-11 07:55:11,569 - Pre-cat
2025-02-11 07:55:11,569 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           3832,    311,    264,   2297,    304,  23270,    320, 144665,     79,
              8,    323]], device='cuda:0')
2025-02-11 07:55:11,571 - Next token: tensor([[8916]], device='cuda:0')
2025-02-11 07:55:11,571 - Current_ids shape: torch.Size([1, 56])
2025-02-11 07:55:11,571 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,571 - Step 25: Generated next token
2025-02-11 07:55:11,572 - Step 25: Updated current_ids
2025-02-11 07:55:11,572 - Step 25: Decoded token text:  therefore
2025-02-11 07:55:11,572 - Step 25: Updated current_phrase
2025-02-11 07:55:11,572 - Step 25: Created step_acts
2025-02-11 07:55:11,572 - Step 25: Added to generation_acts
2025-02-11 07:55:11,574 - Step 25: Updated generated_texts
2025-02-11 07:55:11,574 - Step 25: Updated recent_tokens
2025-02-11 07:55:11,574 - Step 25: Decoded current text
2025-02-11 07:55:11,574 - Step 25: Incremented consecutive_fillers to 1
2025-02-11 07:55:11,574 - Step 25: Calculated unique_ratio: 1.0
2025-02-11 07:55:11,574 - 
Starting step 26
2025-02-11 07:55:11,574 - Current_ids device: cuda:0
2025-02-11 07:55:11,574 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,598 - Model output complete
2025-02-11 07:55:11,598 - Logits shape: torch.Size([1, 57, 151936])
2025-02-11 07:55:11,598 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,598 - Next token logits device: cuda:0
2025-02-11 07:55:11,598 - Entered do_sample
2025-02-11 07:55:11,598 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,602 - Probs max: 0.447509765625
2025-02-11 07:55:11,603 - Pre-cat
2025-02-11 07:55:11,603 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           3832,    311,    264,   2297,    304,  23270,    320, 144665,     79,
              8,    323,   8916]], device='cuda:0')
2025-02-11 07:55:11,605 - Next token: tensor([[702]], device='cuda:0')
2025-02-11 07:55:11,605 - Current_ids shape: torch.Size([1, 57])
2025-02-11 07:55:11,605 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,605 - Step 26: Generated next token
2025-02-11 07:55:11,605 - Step 26: Updated current_ids
2025-02-11 07:55:11,606 - Step 26: Decoded token text:  has
2025-02-11 07:55:11,606 - Step 26: Updated current_phrase
2025-02-11 07:55:11,606 - Step 26: Created step_acts
2025-02-11 07:55:11,606 - Step 26: Added to generation_acts
2025-02-11 07:55:11,606 - Step 26: Updated recent_tokens
2025-02-11 07:55:11,608 - Step 26: Decoded current text
2025-02-11 07:55:11,608 - Step 26: Incremented consecutive_fillers to 2
2025-02-11 07:55:11,609 - Step 26: Calculated unique_ratio: 1.0
2025-02-11 07:55:11,609 - 
Starting step 27
2025-02-11 07:55:11,609 - Current_ids device: cuda:0
2025-02-11 07:55:11,609 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,633 - Model output complete
2025-02-11 07:55:11,633 - Logits shape: torch.Size([1, 58, 151936])
2025-02-11 07:55:11,633 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,633 - Next token logits device: cuda:0
2025-02-11 07:55:11,633 - Entered do_sample
2025-02-11 07:55:11,633 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,636 - Probs max: 0.97509765625
2025-02-11 07:55:11,637 - Pre-cat
2025-02-11 07:55:11,637 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    374,
           3832,    311,    264,   2297,    304,  23270,    320, 144665,     79,
              8,    323,   8916,    702]], device='cuda:0')
2025-02-11 07:55:11,639 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:11,640 - Current_ids shape: torch.Size([1, 58])
2025-02-11 07:55:11,640 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,640 - Step 27: Generated next token
2025-02-11 07:55:11,640 - Step 27: Updated current_ids
2025-02-11 07:55:11,640 - Step 27: Decoded token text:  a
2025-02-11 07:55:11,640 - Step 27: Updated current_phrase
2025-02-11 07:55:11,641 - Step 27: Created step_acts
2025-02-11 07:55:11,641 - Step 27: Added to generation_acts
2025-02-11 07:55:11,641 - Step 27: Updated recent_tokens
2025-02-11 07:55:11,643 - Step 27: Decoded current text
2025-02-11 07:55:11,643 - Step 27: Incremented consecutive_fillers to 3
2025-02-11 07:55:11,748 - 
Starting step 0
2025-02-11 07:55:11,748 - Current_ids device: cuda:0
2025-02-11 07:55:11,748 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,774 - Model output complete
2025-02-11 07:55:11,774 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:11,774 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,774 - Next token logits device: cuda:0
2025-02-11 07:55:11,774 - Entered do_sample
2025-02-11 07:55:11,774 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,776 - Probs max: 0.50341796875
2025-02-11 07:55:11,777 - Pre-cat
2025-02-11 07:55:11,777 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:11,779 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:55:11,779 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:11,779 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,779 - Step 0: Generated next token
2025-02-11 07:55:11,779 - Step 0: Updated current_ids
2025-02-11 07:55:11,779 - Step 0: Decoded token text:  It
2025-02-11 07:55:11,779 - Step 0: Updated current_phrase
2025-02-11 07:55:11,780 - Step 0: Created step_acts
2025-02-11 07:55:11,780 - Step 0: Added to generation_acts
2025-02-11 07:55:11,781 - Step 0: Updated generated_texts
2025-02-11 07:55:11,781 - Step 0: Updated recent_tokens
2025-02-11 07:55:11,782 - Step 0: Decoded current text
2025-02-11 07:55:11,782 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:11,782 - 
Starting step 1
2025-02-11 07:55:11,782 - Current_ids device: cuda:0
2025-02-11 07:55:11,782 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,808 - Model output complete
2025-02-11 07:55:11,808 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:11,808 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,808 - Next token logits device: cuda:0
2025-02-11 07:55:11,808 - Entered do_sample
2025-02-11 07:55:11,808 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,811 - Probs max: 0.9384765625
2025-02-11 07:55:11,812 - Pre-cat
2025-02-11 07:55:11,812 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084]], device='cuda:0')
2025-02-11 07:55:11,813 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:11,814 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:11,814 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,814 - Step 1: Generated next token
2025-02-11 07:55:11,814 - Step 1: Updated current_ids
2025-02-11 07:55:11,814 - Step 1: Decoded token text:  will
2025-02-11 07:55:11,814 - Step 1: Updated current_phrase
2025-02-11 07:55:11,814 - Step 1: Created step_acts
2025-02-11 07:55:11,814 - Step 1: Added to generation_acts
2025-02-11 07:55:11,815 - Step 1: Updated recent_tokens
2025-02-11 07:55:11,816 - Step 1: Decoded current text
2025-02-11 07:55:11,816 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:11,816 - 
Starting step 2
2025-02-11 07:55:11,816 - Current_ids device: cuda:0
2025-02-11 07:55:11,816 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,875 - Model output complete
2025-02-11 07:55:11,875 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:11,875 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,875 - Next token logits device: cuda:0
2025-02-11 07:55:11,875 - Entered do_sample
2025-02-11 07:55:11,875 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,877 - Probs max: 0.2442626953125
2025-02-11 07:55:11,879 - Pre-cat
2025-02-11 07:55:11,879 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686]], device='cuda:0')
2025-02-11 07:55:11,882 - Next token: tensor([[537]], device='cuda:0')
2025-02-11 07:55:11,882 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:11,882 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:11,882 - Step 2: Generated next token
2025-02-11 07:55:11,882 - Step 2: Updated current_ids
2025-02-11 07:55:11,883 - Step 2: Decoded token text:  not
2025-02-11 07:55:11,883 - Step 2: Updated current_phrase
2025-02-11 07:55:11,883 - Step 2: Created step_acts
2025-02-11 07:55:11,883 - Step 2: Added to generation_acts
2025-02-11 07:55:11,883 - Step 2: Updated recent_tokens
2025-02-11 07:55:11,885 - Step 2: Decoded current text
2025-02-11 07:55:11,885 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:11,885 - 
Starting step 3
2025-02-11 07:55:11,885 - Current_ids device: cuda:0
2025-02-11 07:55:11,885 - Current_ids dtype: torch.int64
2025-02-11 07:55:11,926 - Model output complete
2025-02-11 07:55:11,926 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:11,926 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,927 - Next token logits device: cuda:0
2025-02-11 07:55:11,927 - Entered do_sample
2025-02-11 07:55:11,927 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:11,929 - Probs max: 0.1690673828125
2025-02-11 07:55:12,060 - 
Starting step 0
2025-02-11 07:55:12,061 - Current_ids device: cuda:0
2025-02-11 07:55:12,061 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,100 - Model output complete
2025-02-11 07:55:12,100 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:12,100 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,100 - Next token logits device: cuda:0
2025-02-11 07:55:12,100 - Entered do_sample
2025-02-11 07:55:12,100 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,103 - Probs max: 0.50341796875
2025-02-11 07:55:12,104 - Pre-cat
2025-02-11 07:55:12,104 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:12,105 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:55:12,106 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:12,106 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,106 - Step 0: Generated next token
2025-02-11 07:55:12,106 - Step 0: Updated current_ids
2025-02-11 07:55:12,106 - Step 0: Decoded token text:  If
2025-02-11 07:55:12,106 - Step 0: Updated current_phrase
2025-02-11 07:55:12,106 - Step 0: Created step_acts
2025-02-11 07:55:12,107 - Step 0: Added to generation_acts
2025-02-11 07:55:12,108 - Step 0: Updated generated_texts
2025-02-11 07:55:12,108 - Step 0: Updated recent_tokens
2025-02-11 07:55:12,108 - Step 0: Decoded current text
2025-02-11 07:55:12,108 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:12,108 - 
Starting step 1
2025-02-11 07:55:12,108 - Current_ids device: cuda:0
2025-02-11 07:55:12,108 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,134 - Model output complete
2025-02-11 07:55:12,134 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:12,134 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,134 - Next token logits device: cuda:0
2025-02-11 07:55:12,134 - Entered do_sample
2025-02-11 07:55:12,134 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,136 - Probs max: 0.7099609375
2025-02-11 07:55:12,138 - Pre-cat
2025-02-11 07:55:12,138 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:55:12,141 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:12,141 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:12,141 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,141 - Step 1: Generated next token
2025-02-11 07:55:12,141 - Step 1: Updated current_ids
2025-02-11 07:55:12,142 - Step 1: Decoded token text:  a
2025-02-11 07:55:12,142 - Step 1: Updated current_phrase
2025-02-11 07:55:12,142 - Step 1: Created step_acts
2025-02-11 07:55:12,142 - Step 1: Added to generation_acts
2025-02-11 07:55:12,142 - Step 1: Updated recent_tokens
2025-02-11 07:55:12,144 - Step 1: Decoded current text
2025-02-11 07:55:12,144 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:12,144 - 
Starting step 2
2025-02-11 07:55:12,144 - Current_ids device: cuda:0
2025-02-11 07:55:12,144 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,221 - Model output complete
2025-02-11 07:55:12,221 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:12,221 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,221 - Next token logits device: cuda:0
2025-02-11 07:55:12,221 - Entered do_sample
2025-02-11 07:55:12,222 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,224 - Probs max: 0.9970703125
2025-02-11 07:55:12,225 - Pre-cat
2025-02-11 07:55:12,225 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264]], device='cuda:0')
2025-02-11 07:55:12,228 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:12,228 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:12,228 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,228 - Step 2: Generated next token
2025-02-11 07:55:12,228 - Step 2: Updated current_ids
2025-02-11 07:55:12,228 - Step 2: Decoded token text:  ball
2025-02-11 07:55:12,228 - Step 2: Updated current_phrase
2025-02-11 07:55:12,229 - Step 2: Created step_acts
2025-02-11 07:55:12,229 - Step 2: Added to generation_acts
2025-02-11 07:55:12,229 - Step 2: Updated recent_tokens
2025-02-11 07:55:12,230 - Step 2: Decoded current text
2025-02-11 07:55:12,230 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:12,230 - 
Starting step 3
2025-02-11 07:55:12,230 - Current_ids device: cuda:0
2025-02-11 07:55:12,230 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,253 - Model output complete
2025-02-11 07:55:12,254 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:12,254 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,254 - Next token logits device: cuda:0
2025-02-11 07:55:12,254 - Entered do_sample
2025-02-11 07:55:12,254 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,256 - Probs max: 0.99951171875
2025-02-11 07:55:12,257 - Pre-cat
2025-02-11 07:55:12,257 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935]],
       device='cuda:0')
2025-02-11 07:55:12,259 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:12,260 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:12,260 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,260 - Step 3: Generated next token
2025-02-11 07:55:12,260 - Step 3: Updated current_ids
2025-02-11 07:55:12,260 - Step 3: Decoded token text:  is
2025-02-11 07:55:12,260 - Step 3: Updated current_phrase
2025-02-11 07:55:12,261 - Step 3: Created step_acts
2025-02-11 07:55:12,261 - Step 3: Added to generation_acts
2025-02-11 07:55:12,261 - Step 3: Updated recent_tokens
2025-02-11 07:55:12,262 - Step 3: Decoded current text
2025-02-11 07:55:12,262 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:12,262 - 
Starting step 4
2025-02-11 07:55:12,262 - Current_ids device: cuda:0
2025-02-11 07:55:12,262 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,288 - Model output complete
2025-02-11 07:55:12,288 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:12,288 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,288 - Next token logits device: cuda:0
2025-02-11 07:55:12,288 - Entered do_sample
2025-02-11 07:55:12,289 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,291 - Probs max: 0.998046875
2025-02-11 07:55:12,292 - Pre-cat
2025-02-11 07:55:12,292 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:12,295 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:12,295 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:12,295 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,295 - Step 4: Generated next token
2025-02-11 07:55:12,296 - Step 4: Updated current_ids
2025-02-11 07:55:12,296 - Step 4: Decoded token text:  thrown
2025-02-11 07:55:12,296 - Step 4: Updated current_phrase
2025-02-11 07:55:12,296 - Step 4: Created step_acts
2025-02-11 07:55:12,296 - Step 4: Added to generation_acts
2025-02-11 07:55:12,296 - Step 4: Updated recent_tokens
2025-02-11 07:55:12,298 - Step 4: Decoded current text
2025-02-11 07:55:12,298 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:12,298 - 
Starting step 5
2025-02-11 07:55:12,298 - Current_ids device: cuda:0
2025-02-11 07:55:12,298 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,321 - Model output complete
2025-02-11 07:55:12,321 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:12,321 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,321 - Next token logits device: cuda:0
2025-02-11 07:55:12,321 - Entered do_sample
2025-02-11 07:55:12,322 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,325 - Probs max: 0.9697265625
2025-02-11 07:55:12,326 - Pre-cat
2025-02-11 07:55:12,326 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:55:12,328 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:55:12,329 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:12,329 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,329 - Step 5: Generated next token
2025-02-11 07:55:12,329 - Step 5: Updated current_ids
2025-02-11 07:55:12,329 - Step 5: Decoded token text:  at
2025-02-11 07:55:12,329 - Step 5: Updated current_phrase
2025-02-11 07:55:12,329 - Step 5: Created step_acts
2025-02-11 07:55:12,330 - Step 5: Added to generation_acts
2025-02-11 07:55:12,331 - Step 5: Updated generated_texts
2025-02-11 07:55:12,331 - Step 5: Updated recent_tokens
2025-02-11 07:55:12,331 - Step 5: Decoded current text
2025-02-11 07:55:12,331 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:12,331 - 
Starting step 6
2025-02-11 07:55:12,331 - Current_ids device: cuda:0
2025-02-11 07:55:12,331 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,362 - Model output complete
2025-02-11 07:55:12,362 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:12,362 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,362 - Next token logits device: cuda:0
2025-02-11 07:55:12,362 - Entered do_sample
2025-02-11 07:55:12,363 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,365 - Probs max: 0.99951171875
2025-02-11 07:55:12,366 - Pre-cat
2025-02-11 07:55:12,366 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:55:12,368 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:12,368 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:12,368 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,368 - Step 6: Generated next token
2025-02-11 07:55:12,368 - Step 6: Updated current_ids
2025-02-11 07:55:12,368 - Step 6: Decoded token text:  a
2025-02-11 07:55:12,368 - Step 6: Updated current_phrase
2025-02-11 07:55:12,369 - Step 6: Created step_acts
2025-02-11 07:55:12,369 - Step 6: Added to generation_acts
2025-02-11 07:55:12,369 - Step 6: Updated recent_tokens
2025-02-11 07:55:12,370 - Step 6: Decoded current text
2025-02-11 07:55:12,371 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:12,371 - 
Starting step 7
2025-02-11 07:55:12,371 - Current_ids device: cuda:0
2025-02-11 07:55:12,371 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,395 - Model output complete
2025-02-11 07:55:12,395 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:12,395 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,396 - Next token logits device: cuda:0
2025-02-11 07:55:12,396 - Entered do_sample
2025-02-11 07:55:12,396 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,399 - Probs max: 0.9990234375
2025-02-11 07:55:12,399 - Pre-cat
2025-02-11 07:55:12,399 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:55:12,401 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:12,401 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:12,401 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,401 - Step 7: Generated next token
2025-02-11 07:55:12,401 - Step 7: Updated current_ids
2025-02-11 07:55:12,402 - Step 7: Decoded token text:  wall
2025-02-11 07:55:12,402 - Step 7: Updated current_phrase
2025-02-11 07:55:12,402 - Step 7: Created step_acts
2025-02-11 07:55:12,402 - Step 7: Added to generation_acts
2025-02-11 07:55:12,402 - Step 7: Updated recent_tokens
2025-02-11 07:55:12,403 - Step 7: Decoded current text
2025-02-11 07:55:12,403 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:12,404 - 
Starting step 8
2025-02-11 07:55:12,404 - Current_ids device: cuda:0
2025-02-11 07:55:12,404 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,430 - Model output complete
2025-02-11 07:55:12,430 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:12,430 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,430 - Next token logits device: cuda:0
2025-02-11 07:55:12,430 - Entered do_sample
2025-02-11 07:55:12,430 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,432 - Probs max: 0.9912109375
2025-02-11 07:55:12,433 - Pre-cat
2025-02-11 07:55:12,433 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:55:12,435 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:12,435 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:12,435 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,435 - Step 8: Generated next token
2025-02-11 07:55:12,435 - Step 8: Updated current_ids
2025-02-11 07:55:12,436 - Step 8: Decoded token text:  very
2025-02-11 07:55:12,436 - Step 8: Updated current_phrase
2025-02-11 07:55:12,436 - Step 8: Created step_acts
2025-02-11 07:55:12,436 - Step 8: Added to generation_acts
2025-02-11 07:55:12,436 - Step 8: Updated recent_tokens
2025-02-11 07:55:12,438 - Step 8: Decoded current text
2025-02-11 07:55:12,438 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:12,438 - 
Starting step 9
2025-02-11 07:55:12,438 - Current_ids device: cuda:0
2025-02-11 07:55:12,438 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,463 - Model output complete
2025-02-11 07:55:12,464 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:12,464 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,464 - Next token logits device: cuda:0
2025-02-11 07:55:12,464 - Entered do_sample
2025-02-11 07:55:12,464 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,466 - Probs max: 0.998046875
2025-02-11 07:55:12,467 - Pre-cat
2025-02-11 07:55:12,467 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:55:12,469 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:55:12,469 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:12,469 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,469 - Step 9: Generated next token
2025-02-11 07:55:12,469 - Step 9: Updated current_ids
2025-02-11 07:55:12,469 - Step 9: Decoded token text:  fast
2025-02-11 07:55:12,470 - Step 9: Updated current_phrase
2025-02-11 07:55:12,470 - Step 9: Created step_acts
2025-02-11 07:55:12,470 - Step 9: Added to generation_acts
2025-02-11 07:55:12,470 - Step 9: Updated recent_tokens
2025-02-11 07:55:12,472 - Step 9: Decoded current text
2025-02-11 07:55:12,472 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:12,472 - 
Starting step 10
2025-02-11 07:55:12,473 - Current_ids device: cuda:0
2025-02-11 07:55:12,473 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,539 - Model output complete
2025-02-11 07:55:12,539 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:12,539 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,539 - Next token logits device: cuda:0
2025-02-11 07:55:12,539 - Entered do_sample
2025-02-11 07:55:12,540 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,541 - Probs max: 0.99853515625
2025-02-11 07:55:12,544 - Pre-cat
2025-02-11 07:55:12,544 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:55:12,546 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:12,546 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:12,546 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,546 - Step 10: Generated next token
2025-02-11 07:55:12,546 - Step 10: Updated current_ids
2025-02-11 07:55:12,547 - Step 10: Decoded token text: ,
2025-02-11 07:55:12,547 - Step 10: Updated current_phrase
2025-02-11 07:55:12,547 - Step 10: Created step_acts
2025-02-11 07:55:12,547 - Step 10: Added to generation_acts
2025-02-11 07:55:12,548 - Step 10: Updated generated_texts
2025-02-11 07:55:12,548 - Step 10: Updated recent_tokens
2025-02-11 07:55:12,549 - Step 10: Found phrase end token
2025-02-11 07:55:12,549 - Step 10: Updated recent_phrases
2025-02-11 07:55:12,549 - Step 10: Decoded current text
2025-02-11 07:55:12,549 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:12,549 - 
Starting step 11
2025-02-11 07:55:12,549 - Current_ids device: cuda:0
2025-02-11 07:55:12,549 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,577 - Model output complete
2025-02-11 07:55:12,577 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:12,577 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,577 - Next token logits device: cuda:0
2025-02-11 07:55:12,577 - Entered do_sample
2025-02-11 07:55:12,577 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,579 - Probs max: 0.58154296875
2025-02-11 07:55:12,581 - Pre-cat
2025-02-11 07:55:12,581 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:55:12,585 - Next token: tensor([[1221]], device='cuda:0')
2025-02-11 07:55:12,585 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:12,585 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,586 - Step 11: Generated next token
2025-02-11 07:55:12,586 - Step 11: Updated current_ids
2025-02-11 07:55:12,586 - Step 11: Decoded token text:  then
2025-02-11 07:55:12,586 - Step 11: Updated current_phrase
2025-02-11 07:55:12,587 - Step 11: Created step_acts
2025-02-11 07:55:12,587 - Step 11: Added to generation_acts
2025-02-11 07:55:12,587 - Step 11: Updated recent_tokens
2025-02-11 07:55:12,588 - Step 11: Decoded current text
2025-02-11 07:55:12,588 - Step 11: Reset consecutive_fillers
2025-02-11 07:55:12,588 - 
Starting step 12
2025-02-11 07:55:12,588 - Current_ids device: cuda:0
2025-02-11 07:55:12,588 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,640 - Model output complete
2025-02-11 07:55:12,640 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:12,640 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,640 - Next token logits device: cuda:0
2025-02-11 07:55:12,640 - Entered do_sample
2025-02-11 07:55:12,640 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,643 - Probs max: 0.87744140625
2025-02-11 07:55:12,643 - Pre-cat
2025-02-11 07:55:12,643 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,   1221]],
       device='cuda:0')
2025-02-11 07:55:12,645 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:12,645 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:12,645 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,646 - Step 12: Generated next token
2025-02-11 07:55:12,646 - Step 12: Updated current_ids
2025-02-11 07:55:12,646 - Step 12: Decoded token text:  the
2025-02-11 07:55:12,646 - Step 12: Updated current_phrase
2025-02-11 07:55:12,646 - Step 12: Created step_acts
2025-02-11 07:55:12,646 - Step 12: Added to generation_acts
2025-02-11 07:55:12,646 - Step 12: Updated recent_tokens
2025-02-11 07:55:12,648 - Step 12: Decoded current text
2025-02-11 07:55:12,648 - Step 12: Incremented consecutive_fillers to 1
2025-02-11 07:55:12,648 - 
Starting step 13
2025-02-11 07:55:12,648 - Current_ids device: cuda:0
2025-02-11 07:55:12,648 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,691 - Model output complete
2025-02-11 07:55:12,691 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:12,691 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,691 - Next token logits device: cuda:0
2025-02-11 07:55:12,691 - Entered do_sample
2025-02-11 07:55:12,691 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,693 - Probs max: 0.5595703125
2025-02-11 07:55:12,695 - Pre-cat
2025-02-11 07:55:12,695 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,   1221,    279]],
       device='cuda:0')
2025-02-11 07:55:12,699 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:12,700 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:12,700 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,700 - Step 13: Generated next token
2025-02-11 07:55:12,700 - Step 13: Updated current_ids
2025-02-11 07:55:12,700 - Step 13: Decoded token text:  ball
2025-02-11 07:55:12,700 - Step 13: Updated current_phrase
2025-02-11 07:55:12,701 - Step 13: Created step_acts
2025-02-11 07:55:12,701 - Step 13: Added to generation_acts
2025-02-11 07:55:12,701 - Step 13: Updated recent_tokens
2025-02-11 07:55:12,703 - Step 13: Decoded current text
2025-02-11 07:55:12,703 - Step 13: Incremented consecutive_fillers to 2
2025-02-11 07:55:12,703 - 
Starting step 14
2025-02-11 07:55:12,703 - Current_ids device: cuda:0
2025-02-11 07:55:12,703 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,753 - Model output complete
2025-02-11 07:55:12,753 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:55:12,753 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,753 - Next token logits device: cuda:0
2025-02-11 07:55:12,753 - Entered do_sample
2025-02-11 07:55:12,753 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,755 - Probs max: 0.5419921875
2025-02-11 07:55:12,756 - Pre-cat
2025-02-11 07:55:12,756 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,   1221,    279,   4935]],
       device='cuda:0')
2025-02-11 07:55:12,759 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:55:12,759 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:55:12,759 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,759 - Step 14: Generated next token
2025-02-11 07:55:12,760 - Step 14: Updated current_ids
2025-02-11 07:55:12,760 - Step 14: Decoded token text: 's
2025-02-11 07:55:12,760 - Step 14: Updated current_phrase
2025-02-11 07:55:12,760 - Step 14: Created step_acts
2025-02-11 07:55:12,760 - Step 14: Added to generation_acts
2025-02-11 07:55:12,760 - Step 14: Updated recent_tokens
2025-02-11 07:55:12,762 - Step 14: Decoded current text
2025-02-11 07:55:12,762 - Step 14: Incremented consecutive_fillers to 3
2025-02-11 07:55:12,908 - 
Starting step 0
2025-02-11 07:55:12,908 - Current_ids device: cuda:0
2025-02-11 07:55:12,908 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,937 - Model output complete
2025-02-11 07:55:12,937 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:12,937 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,937 - Next token logits device: cuda:0
2025-02-11 07:55:12,937 - Entered do_sample
2025-02-11 07:55:12,938 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,940 - Probs max: 0.50341796875
2025-02-11 07:55:12,941 - Pre-cat
2025-02-11 07:55:12,941 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:12,942 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:12,943 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:12,943 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,943 - Step 0: Generated next token
2025-02-11 07:55:12,943 - Step 0: Updated current_ids
2025-02-11 07:55:12,943 - Step 0: Decoded token text:  The
2025-02-11 07:55:12,943 - Step 0: Updated current_phrase
2025-02-11 07:55:12,944 - Step 0: Created step_acts
2025-02-11 07:55:12,944 - Step 0: Added to generation_acts
2025-02-11 07:55:12,945 - Step 0: Updated generated_texts
2025-02-11 07:55:12,945 - Step 0: Updated recent_tokens
2025-02-11 07:55:12,946 - Step 0: Decoded current text
2025-02-11 07:55:12,946 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:12,946 - 
Starting step 1
2025-02-11 07:55:12,946 - Current_ids device: cuda:0
2025-02-11 07:55:12,946 - Current_ids dtype: torch.int64
2025-02-11 07:55:12,970 - Model output complete
2025-02-11 07:55:12,970 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:12,970 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,970 - Next token logits device: cuda:0
2025-02-11 07:55:12,970 - Entered do_sample
2025-02-11 07:55:12,970 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:12,974 - Probs max: 0.83837890625
2025-02-11 07:55:12,974 - Pre-cat
2025-02-11 07:55:12,974 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:55:12,976 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:12,977 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:12,977 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:12,977 - Step 1: Generated next token
2025-02-11 07:55:12,977 - Step 1: Updated current_ids
2025-02-11 07:55:12,977 - Step 1: Decoded token text:  ball
2025-02-11 07:55:12,977 - Step 1: Updated current_phrase
2025-02-11 07:55:12,978 - Step 1: Created step_acts
2025-02-11 07:55:12,978 - Step 1: Added to generation_acts
2025-02-11 07:55:12,978 - Step 1: Updated recent_tokens
2025-02-11 07:55:12,979 - Step 1: Decoded current text
2025-02-11 07:55:12,979 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:12,979 - 
Starting step 2
2025-02-11 07:55:12,979 - Current_ids device: cuda:0
2025-02-11 07:55:12,979 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,002 - Model output complete
2025-02-11 07:55:13,002 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:13,002 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,002 - Next token logits device: cuda:0
2025-02-11 07:55:13,002 - Entered do_sample
2025-02-11 07:55:13,003 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,005 - Probs max: 0.583984375
2025-02-11 07:55:13,006 - Pre-cat
2025-02-11 07:55:13,006 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:55:13,008 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:13,008 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:13,009 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,009 - Step 2: Generated next token
2025-02-11 07:55:13,009 - Step 2: Updated current_ids
2025-02-11 07:55:13,009 - Step 2: Decoded token text:  is
2025-02-11 07:55:13,009 - Step 2: Updated current_phrase
2025-02-11 07:55:13,009 - Step 2: Created step_acts
2025-02-11 07:55:13,009 - Step 2: Added to generation_acts
2025-02-11 07:55:13,010 - Step 2: Updated recent_tokens
2025-02-11 07:55:13,011 - Step 2: Decoded current text
2025-02-11 07:55:13,011 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:13,011 - 
Starting step 3
2025-02-11 07:55:13,011 - Current_ids device: cuda:0
2025-02-11 07:55:13,011 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,047 - Model output complete
2025-02-11 07:55:13,047 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:13,047 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,047 - Next token logits device: cuda:0
2025-02-11 07:55:13,047 - Entered do_sample
2025-02-11 07:55:13,048 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,050 - Probs max: 0.59521484375
2025-02-11 07:55:13,050 - Pre-cat
2025-02-11 07:55:13,051 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:13,052 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:13,052 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:13,052 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,052 - Step 3: Generated next token
2025-02-11 07:55:13,052 - Step 3: Updated current_ids
2025-02-11 07:55:13,052 - Step 3: Decoded token text:  thrown
2025-02-11 07:55:13,053 - Step 3: Updated current_phrase
2025-02-11 07:55:13,053 - Step 3: Created step_acts
2025-02-11 07:55:13,053 - Step 3: Added to generation_acts
2025-02-11 07:55:13,053 - Step 3: Updated recent_tokens
2025-02-11 07:55:13,054 - Step 3: Decoded current text
2025-02-11 07:55:13,054 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:13,054 - 
Starting step 4
2025-02-11 07:55:13,054 - Current_ids device: cuda:0
2025-02-11 07:55:13,054 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,084 - Model output complete
2025-02-11 07:55:13,084 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:13,084 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,084 - Next token logits device: cuda:0
2025-02-11 07:55:13,084 - Entered do_sample
2025-02-11 07:55:13,084 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,086 - Probs max: 0.67431640625
2025-02-11 07:55:13,088 - Pre-cat
2025-02-11 07:55:13,089 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:55:13,092 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:13,093 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:13,093 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,093 - Step 4: Generated next token
2025-02-11 07:55:13,093 - Step 4: Updated current_ids
2025-02-11 07:55:13,093 - Step 4: Decoded token text:  very
2025-02-11 07:55:13,093 - Step 4: Updated current_phrase
2025-02-11 07:55:13,094 - Step 4: Created step_acts
2025-02-11 07:55:13,094 - Step 4: Added to generation_acts
2025-02-11 07:55:13,094 - Step 4: Updated recent_tokens
2025-02-11 07:55:13,095 - Step 4: Decoded current text
2025-02-11 07:55:13,095 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:13,096 - 
Starting step 5
2025-02-11 07:55:13,096 - Current_ids device: cuda:0
2025-02-11 07:55:13,096 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,156 - Model output complete
2025-02-11 07:55:13,156 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:13,156 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,156 - Next token logits device: cuda:0
2025-02-11 07:55:13,156 - Entered do_sample
2025-02-11 07:55:13,156 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,159 - Probs max: 0.9658203125
2025-02-11 07:55:13,160 - Pre-cat
2025-02-11 07:55:13,160 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,   1602]],
       device='cuda:0')
2025-02-11 07:55:13,161 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:55:13,161 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:13,161 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,161 - Step 5: Generated next token
2025-02-11 07:55:13,162 - Step 5: Updated current_ids
2025-02-11 07:55:13,162 - Step 5: Decoded token text:  fast
2025-02-11 07:55:13,162 - Step 5: Updated current_phrase
2025-02-11 07:55:13,162 - Step 5: Created step_acts
2025-02-11 07:55:13,162 - Step 5: Added to generation_acts
2025-02-11 07:55:13,163 - Step 5: Updated generated_texts
2025-02-11 07:55:13,164 - Step 5: Updated recent_tokens
2025-02-11 07:55:13,164 - Step 5: Decoded current text
2025-02-11 07:55:13,164 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:13,164 - 
Starting step 6
2025-02-11 07:55:13,164 - Current_ids device: cuda:0
2025-02-11 07:55:13,164 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,192 - Model output complete
2025-02-11 07:55:13,192 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:13,192 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,192 - Next token logits device: cuda:0
2025-02-11 07:55:13,192 - Entered do_sample
2025-02-11 07:55:13,192 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,194 - Probs max: 0.8232421875
2025-02-11 07:55:13,195 - Pre-cat
2025-02-11 07:55:13,195 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,   1602,
           4937]], device='cuda:0')
2025-02-11 07:55:13,197 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:13,198 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:13,198 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,198 - Step 6: Generated next token
2025-02-11 07:55:13,198 - Step 6: Updated current_ids
2025-02-11 07:55:13,199 - Step 6: Decoded token text: ,
2025-02-11 07:55:13,199 - Step 6: Updated current_phrase
2025-02-11 07:55:13,199 - Step 6: Created step_acts
2025-02-11 07:55:13,199 - Step 6: Added to generation_acts
2025-02-11 07:55:13,199 - Step 6: Updated recent_tokens
2025-02-11 07:55:13,200 - Step 6: Found phrase end token
2025-02-11 07:55:13,200 - Step 6: Updated recent_phrases
2025-02-11 07:55:13,201 - Step 6: Decoded current text
2025-02-11 07:55:13,201 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:13,201 - 
Starting step 7
2025-02-11 07:55:13,201 - Current_ids device: cuda:0
2025-02-11 07:55:13,201 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,226 - Model output complete
2025-02-11 07:55:13,226 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:13,227 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,227 - Next token logits device: cuda:0
2025-02-11 07:55:13,227 - Entered do_sample
2025-02-11 07:55:13,227 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,230 - Probs max: 0.6064453125
2025-02-11 07:55:13,232 - Pre-cat
2025-02-11 07:55:13,232 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,   1602,
           4937,     11]], device='cuda:0')
2025-02-11 07:55:13,238 - Next token: tensor([[773]], device='cuda:0')
2025-02-11 07:55:13,238 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:13,238 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,238 - Step 7: Generated next token
2025-02-11 07:55:13,239 - Step 7: Updated current_ids
2025-02-11 07:55:13,239 - Step 7: Decoded token text:  so
2025-02-11 07:55:13,239 - Step 7: Updated current_phrase
2025-02-11 07:55:13,240 - Step 7: Created step_acts
2025-02-11 07:55:13,240 - Step 7: Added to generation_acts
2025-02-11 07:55:13,240 - Step 7: Updated recent_tokens
2025-02-11 07:55:13,241 - Step 7: Decoded current text
2025-02-11 07:55:13,241 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:13,241 - 
Starting step 8
2025-02-11 07:55:13,241 - Current_ids device: cuda:0
2025-02-11 07:55:13,241 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,276 - Model output complete
2025-02-11 07:55:13,276 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:13,276 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,276 - Next token logits device: cuda:0
2025-02-11 07:55:13,276 - Entered do_sample
2025-02-11 07:55:13,276 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,279 - Probs max: 0.75830078125
2025-02-11 07:55:13,279 - Pre-cat
2025-02-11 07:55:13,279 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,   1602,
           4937,     11,    773]], device='cuda:0')
2025-02-11 07:55:13,281 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:13,281 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:13,281 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,281 - Step 8: Generated next token
2025-02-11 07:55:13,282 - Step 8: Updated current_ids
2025-02-11 07:55:13,282 - Step 8: Decoded token text:  the
2025-02-11 07:55:13,282 - Step 8: Updated current_phrase
2025-02-11 07:55:13,282 - Step 8: Created step_acts
2025-02-11 07:55:13,282 - Step 8: Added to generation_acts
2025-02-11 07:55:13,282 - Step 8: Updated recent_tokens
2025-02-11 07:55:13,284 - Step 8: Decoded current text
2025-02-11 07:55:13,284 - Step 8: Incremented consecutive_fillers to 1
2025-02-11 07:55:13,284 - 
Starting step 9
2025-02-11 07:55:13,284 - Current_ids device: cuda:0
2025-02-11 07:55:13,284 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,307 - Model output complete
2025-02-11 07:55:13,307 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:13,307 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,307 - Next token logits device: cuda:0
2025-02-11 07:55:13,307 - Entered do_sample
2025-02-11 07:55:13,307 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,310 - Probs max: 0.7060546875
2025-02-11 07:55:13,311 - Pre-cat
2025-02-11 07:55:13,311 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,   1602,
           4937,     11,    773,    279]], device='cuda:0')
2025-02-11 07:55:13,312 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:13,313 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:13,313 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,313 - Step 9: Generated next token
2025-02-11 07:55:13,313 - Step 9: Updated current_ids
2025-02-11 07:55:13,313 - Step 9: Decoded token text:  wall
2025-02-11 07:55:13,313 - Step 9: Updated current_phrase
2025-02-11 07:55:13,313 - Step 9: Created step_acts
2025-02-11 07:55:13,313 - Step 9: Added to generation_acts
2025-02-11 07:55:13,313 - Step 9: Updated recent_tokens
2025-02-11 07:55:13,315 - Step 9: Decoded current text
2025-02-11 07:55:13,315 - Step 9: Incremented consecutive_fillers to 2
2025-02-11 07:55:13,315 - 
Starting step 10
2025-02-11 07:55:13,315 - Current_ids device: cuda:0
2025-02-11 07:55:13,315 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,341 - Model output complete
2025-02-11 07:55:13,341 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:13,342 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,342 - Next token logits device: cuda:0
2025-02-11 07:55:13,342 - Entered do_sample
2025-02-11 07:55:13,342 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,344 - Probs max: 0.47705078125
2025-02-11 07:55:13,345 - Pre-cat
2025-02-11 07:55:13,345 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,   1602,
           4937,     11,    773,    279,   7002]], device='cuda:0')
2025-02-11 07:55:13,347 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:13,348 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:13,348 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,348 - Step 10: Generated next token
2025-02-11 07:55:13,348 - Step 10: Updated current_ids
2025-02-11 07:55:13,348 - Step 10: Decoded token text:  is
2025-02-11 07:55:13,348 - Step 10: Updated current_phrase
2025-02-11 07:55:13,349 - Step 10: Created step_acts
2025-02-11 07:55:13,349 - Step 10: Added to generation_acts
2025-02-11 07:55:13,351 - Step 10: Updated generated_texts
2025-02-11 07:55:13,351 - Step 10: Updated recent_tokens
2025-02-11 07:55:13,352 - Step 10: Decoded current text
2025-02-11 07:55:13,352 - Step 10: Incremented consecutive_fillers to 3
2025-02-11 07:55:13,459 - 
Starting step 0
2025-02-11 07:55:13,459 - Current_ids device: cuda:0
2025-02-11 07:55:13,459 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,532 - Model output complete
2025-02-11 07:55:13,532 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:13,532 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,533 - Next token logits device: cuda:0
2025-02-11 07:55:13,533 - Entered do_sample
2025-02-11 07:55:13,533 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,535 - Probs max: 0.50341796875
2025-02-11 07:55:13,536 - Pre-cat
2025-02-11 07:55:13,536 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:13,538 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:55:13,538 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:13,538 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,538 - Step 0: Generated next token
2025-02-11 07:55:13,538 - Step 0: Updated current_ids
2025-02-11 07:55:13,539 - Step 0: Decoded token text:  It
2025-02-11 07:55:13,539 - Step 0: Updated current_phrase
2025-02-11 07:55:13,539 - Step 0: Created step_acts
2025-02-11 07:55:13,539 - Step 0: Added to generation_acts
2025-02-11 07:55:13,540 - Step 0: Updated generated_texts
2025-02-11 07:55:13,540 - Step 0: Updated recent_tokens
2025-02-11 07:55:13,541 - Step 0: Decoded current text
2025-02-11 07:55:13,541 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:13,541 - 
Starting step 1
2025-02-11 07:55:13,541 - Current_ids device: cuda:0
2025-02-11 07:55:13,541 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,631 - Model output complete
2025-02-11 07:55:13,632 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:13,632 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,632 - Next token logits device: cuda:0
2025-02-11 07:55:13,632 - Entered do_sample
2025-02-11 07:55:13,632 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,634 - Probs max: 0.9384765625
2025-02-11 07:55:13,635 - Pre-cat
2025-02-11 07:55:13,635 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084]], device='cuda:0')
2025-02-11 07:55:13,639 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:13,640 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:13,640 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,640 - Step 1: Generated next token
2025-02-11 07:55:13,640 - Step 1: Updated current_ids
2025-02-11 07:55:13,640 - Step 1: Decoded token text:  will
2025-02-11 07:55:13,640 - Step 1: Updated current_phrase
2025-02-11 07:55:13,641 - Step 1: Created step_acts
2025-02-11 07:55:13,641 - Step 1: Added to generation_acts
2025-02-11 07:55:13,641 - Step 1: Updated recent_tokens
2025-02-11 07:55:13,643 - Step 1: Decoded current text
2025-02-11 07:55:13,643 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:13,643 - 
Starting step 2
2025-02-11 07:55:13,643 - Current_ids device: cuda:0
2025-02-11 07:55:13,643 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,684 - Model output complete
2025-02-11 07:55:13,684 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:13,685 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,685 - Next token logits device: cuda:0
2025-02-11 07:55:13,685 - Entered do_sample
2025-02-11 07:55:13,685 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,687 - Probs max: 0.2442626953125
2025-02-11 07:55:13,688 - Pre-cat
2025-02-11 07:55:13,688 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686]], device='cuda:0')
2025-02-11 07:55:13,690 - Next token: tensor([[1112]], device='cuda:0')
2025-02-11 07:55:13,690 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:13,690 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,690 - Step 2: Generated next token
2025-02-11 07:55:13,690 - Step 2: Updated current_ids
2025-02-11 07:55:13,691 - Step 2: Decoded token text: ...
2025-02-11 07:55:13,691 - Step 2: Updated current_phrase
2025-02-11 07:55:13,691 - Step 2: Created step_acts
2025-02-11 07:55:13,692 - Step 2: Added to generation_acts
2025-02-11 07:55:13,692 - Step 2: Updated recent_tokens
2025-02-11 07:55:13,693 - Step 2: Found phrase end token
2025-02-11 07:55:13,693 - Step 2: Updated recent_phrases
2025-02-11 07:55:13,693 - Step 2: Decoded current text
2025-02-11 07:55:13,693 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:13,694 - 
Starting step 3
2025-02-11 07:55:13,694 - Current_ids device: cuda:0
2025-02-11 07:55:13,694 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,724 - Model output complete
2025-02-11 07:55:13,724 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:13,724 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,725 - Next token logits device: cuda:0
2025-02-11 07:55:13,725 - Entered do_sample
2025-02-11 07:55:13,725 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,727 - Probs max: 0.167236328125
2025-02-11 07:55:13,842 - 
Starting step 0
2025-02-11 07:55:13,842 - Current_ids device: cuda:0
2025-02-11 07:55:13,842 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,883 - Model output complete
2025-02-11 07:55:13,883 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:13,883 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,883 - Next token logits device: cuda:0
2025-02-11 07:55:13,883 - Entered do_sample
2025-02-11 07:55:13,883 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,885 - Probs max: 0.50341796875
2025-02-11 07:55:13,886 - Pre-cat
2025-02-11 07:55:13,886 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:13,889 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:55:13,889 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:13,889 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,889 - Step 0: Generated next token
2025-02-11 07:55:13,889 - Step 0: Updated current_ids
2025-02-11 07:55:13,890 - Step 0: Decoded token text:  If
2025-02-11 07:55:13,890 - Step 0: Updated current_phrase
2025-02-11 07:55:13,890 - Step 0: Created step_acts
2025-02-11 07:55:13,890 - Step 0: Added to generation_acts
2025-02-11 07:55:13,891 - Step 0: Updated generated_texts
2025-02-11 07:55:13,891 - Step 0: Updated recent_tokens
2025-02-11 07:55:13,892 - Step 0: Decoded current text
2025-02-11 07:55:13,892 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:13,892 - 
Starting step 1
2025-02-11 07:55:13,892 - Current_ids device: cuda:0
2025-02-11 07:55:13,892 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,926 - Model output complete
2025-02-11 07:55:13,926 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:13,926 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,926 - Next token logits device: cuda:0
2025-02-11 07:55:13,926 - Entered do_sample
2025-02-11 07:55:13,926 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,928 - Probs max: 0.7099609375
2025-02-11 07:55:13,929 - Pre-cat
2025-02-11 07:55:13,929 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:55:13,931 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:13,931 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:13,931 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,931 - Step 1: Generated next token
2025-02-11 07:55:13,931 - Step 1: Updated current_ids
2025-02-11 07:55:13,931 - Step 1: Decoded token text:  the
2025-02-11 07:55:13,931 - Step 1: Updated current_phrase
2025-02-11 07:55:13,932 - Step 1: Created step_acts
2025-02-11 07:55:13,932 - Step 1: Added to generation_acts
2025-02-11 07:55:13,932 - Step 1: Updated recent_tokens
2025-02-11 07:55:13,933 - Step 1: Decoded current text
2025-02-11 07:55:13,933 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:13,933 - 
Starting step 2
2025-02-11 07:55:13,934 - Current_ids device: cuda:0
2025-02-11 07:55:13,934 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,959 - Model output complete
2025-02-11 07:55:13,959 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:13,960 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,960 - Next token logits device: cuda:0
2025-02-11 07:55:13,960 - Entered do_sample
2025-02-11 07:55:13,960 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,962 - Probs max: 0.76123046875
2025-02-11 07:55:13,963 - Pre-cat
2025-02-11 07:55:13,963 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279]], device='cuda:0')
2025-02-11 07:55:13,964 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:13,965 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:13,965 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,965 - Step 2: Generated next token
2025-02-11 07:55:13,965 - Step 2: Updated current_ids
2025-02-11 07:55:13,965 - Step 2: Decoded token text:  ball
2025-02-11 07:55:13,965 - Step 2: Updated current_phrase
2025-02-11 07:55:13,965 - Step 2: Created step_acts
2025-02-11 07:55:13,965 - Step 2: Added to generation_acts
2025-02-11 07:55:13,966 - Step 2: Updated recent_tokens
2025-02-11 07:55:13,967 - Step 2: Decoded current text
2025-02-11 07:55:13,967 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:13,967 - 
Starting step 3
2025-02-11 07:55:13,967 - Current_ids device: cuda:0
2025-02-11 07:55:13,967 - Current_ids dtype: torch.int64
2025-02-11 07:55:13,991 - Model output complete
2025-02-11 07:55:13,991 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:13,991 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,991 - Next token logits device: cuda:0
2025-02-11 07:55:13,991 - Entered do_sample
2025-02-11 07:55:13,991 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:13,994 - Probs max: 0.9921875
2025-02-11 07:55:13,995 - Pre-cat
2025-02-11 07:55:13,995 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935]],
       device='cuda:0')
2025-02-11 07:55:13,996 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:13,996 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:13,996 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:13,997 - Step 3: Generated next token
2025-02-11 07:55:13,997 - Step 3: Updated current_ids
2025-02-11 07:55:13,997 - Step 3: Decoded token text:  is
2025-02-11 07:55:13,997 - Step 3: Updated current_phrase
2025-02-11 07:55:13,997 - Step 3: Created step_acts
2025-02-11 07:55:13,997 - Step 3: Added to generation_acts
2025-02-11 07:55:13,997 - Step 3: Updated recent_tokens
2025-02-11 07:55:13,999 - Step 3: Decoded current text
2025-02-11 07:55:13,999 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:13,999 - 
Starting step 4
2025-02-11 07:55:13,999 - Current_ids device: cuda:0
2025-02-11 07:55:13,999 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,037 - Model output complete
2025-02-11 07:55:14,038 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:14,038 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,038 - Next token logits device: cuda:0
2025-02-11 07:55:14,038 - Entered do_sample
2025-02-11 07:55:14,038 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,040 - Probs max: 0.94921875
2025-02-11 07:55:14,042 - Pre-cat
2025-02-11 07:55:14,042 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:14,045 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:14,045 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:14,045 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,045 - Step 4: Generated next token
2025-02-11 07:55:14,045 - Step 4: Updated current_ids
2025-02-11 07:55:14,045 - Step 4: Decoded token text:  thrown
2025-02-11 07:55:14,045 - Step 4: Updated current_phrase
2025-02-11 07:55:14,046 - Step 4: Created step_acts
2025-02-11 07:55:14,046 - Step 4: Added to generation_acts
2025-02-11 07:55:14,046 - Step 4: Updated recent_tokens
2025-02-11 07:55:14,047 - Step 4: Decoded current text
2025-02-11 07:55:14,047 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:14,047 - 
Starting step 5
2025-02-11 07:55:14,047 - Current_ids device: cuda:0
2025-02-11 07:55:14,048 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,095 - Model output complete
2025-02-11 07:55:14,095 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:14,095 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,095 - Next token logits device: cuda:0
2025-02-11 07:55:14,095 - Entered do_sample
2025-02-11 07:55:14,095 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,097 - Probs max: 0.7861328125
2025-02-11 07:55:14,098 - Pre-cat
2025-02-11 07:55:14,098 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:55:14,100 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:55:14,100 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:14,101 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,101 - Step 5: Generated next token
2025-02-11 07:55:14,101 - Step 5: Updated current_ids
2025-02-11 07:55:14,101 - Step 5: Decoded token text:  at
2025-02-11 07:55:14,101 - Step 5: Updated current_phrase
2025-02-11 07:55:14,101 - Step 5: Created step_acts
2025-02-11 07:55:14,101 - Step 5: Added to generation_acts
2025-02-11 07:55:14,103 - Step 5: Updated generated_texts
2025-02-11 07:55:14,103 - Step 5: Updated recent_tokens
2025-02-11 07:55:14,103 - Step 5: Decoded current text
2025-02-11 07:55:14,103 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:14,103 - 
Starting step 6
2025-02-11 07:55:14,103 - Current_ids device: cuda:0
2025-02-11 07:55:14,103 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,136 - Model output complete
2025-02-11 07:55:14,136 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:14,136 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,136 - Next token logits device: cuda:0
2025-02-11 07:55:14,136 - Entered do_sample
2025-02-11 07:55:14,136 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,138 - Probs max: 0.8837890625
2025-02-11 07:55:14,140 - Pre-cat
2025-02-11 07:55:14,140 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:55:14,143 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:14,144 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:14,144 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,144 - Step 6: Generated next token
2025-02-11 07:55:14,144 - Step 6: Updated current_ids
2025-02-11 07:55:14,144 - Step 6: Decoded token text:  a
2025-02-11 07:55:14,144 - Step 6: Updated current_phrase
2025-02-11 07:55:14,145 - Step 6: Created step_acts
2025-02-11 07:55:14,145 - Step 6: Added to generation_acts
2025-02-11 07:55:14,145 - Step 6: Updated recent_tokens
2025-02-11 07:55:14,146 - Step 6: Decoded current text
2025-02-11 07:55:14,146 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:14,146 - 
Starting step 7
2025-02-11 07:55:14,146 - Current_ids device: cuda:0
2025-02-11 07:55:14,146 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,222 - Model output complete
2025-02-11 07:55:14,223 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:14,223 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,223 - Next token logits device: cuda:0
2025-02-11 07:55:14,223 - Entered do_sample
2025-02-11 07:55:14,223 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,225 - Probs max: 0.98974609375
2025-02-11 07:55:14,226 - Pre-cat
2025-02-11 07:55:14,226 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:55:14,227 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:14,227 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:14,228 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,228 - Step 7: Generated next token
2025-02-11 07:55:14,228 - Step 7: Updated current_ids
2025-02-11 07:55:14,228 - Step 7: Decoded token text:  wall
2025-02-11 07:55:14,228 - Step 7: Updated current_phrase
2025-02-11 07:55:14,228 - Step 7: Created step_acts
2025-02-11 07:55:14,228 - Step 7: Added to generation_acts
2025-02-11 07:55:14,228 - Step 7: Updated recent_tokens
2025-02-11 07:55:14,230 - Step 7: Decoded current text
2025-02-11 07:55:14,230 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:14,230 - 
Starting step 8
2025-02-11 07:55:14,230 - Current_ids device: cuda:0
2025-02-11 07:55:14,230 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,258 - Model output complete
2025-02-11 07:55:14,258 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:14,258 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,258 - Next token logits device: cuda:0
2025-02-11 07:55:14,258 - Entered do_sample
2025-02-11 07:55:14,259 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,261 - Probs max: 0.99462890625
2025-02-11 07:55:14,262 - Pre-cat
2025-02-11 07:55:14,263 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:55:14,266 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:14,266 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:14,266 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,266 - Step 8: Generated next token
2025-02-11 07:55:14,266 - Step 8: Updated current_ids
2025-02-11 07:55:14,266 - Step 8: Decoded token text:  very
2025-02-11 07:55:14,266 - Step 8: Updated current_phrase
2025-02-11 07:55:14,267 - Step 8: Created step_acts
2025-02-11 07:55:14,267 - Step 8: Added to generation_acts
2025-02-11 07:55:14,267 - Step 8: Updated recent_tokens
2025-02-11 07:55:14,268 - Step 8: Decoded current text
2025-02-11 07:55:14,268 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:14,269 - 
Starting step 9
2025-02-11 07:55:14,269 - Current_ids device: cuda:0
2025-02-11 07:55:14,269 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,304 - Model output complete
2025-02-11 07:55:14,304 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:14,304 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,304 - Next token logits device: cuda:0
2025-02-11 07:55:14,304 - Entered do_sample
2025-02-11 07:55:14,305 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,307 - Probs max: 0.99755859375
2025-02-11 07:55:14,308 - Pre-cat
2025-02-11 07:55:14,308 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:55:14,311 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:55:14,312 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:14,312 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,312 - Step 9: Generated next token
2025-02-11 07:55:14,312 - Step 9: Updated current_ids
2025-02-11 07:55:14,312 - Step 9: Decoded token text:  fast
2025-02-11 07:55:14,312 - Step 9: Updated current_phrase
2025-02-11 07:55:14,313 - Step 9: Created step_acts
2025-02-11 07:55:14,313 - Step 9: Added to generation_acts
2025-02-11 07:55:14,313 - Step 9: Updated recent_tokens
2025-02-11 07:55:14,314 - Step 9: Decoded current text
2025-02-11 07:55:14,314 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:14,315 - 
Starting step 10
2025-02-11 07:55:14,315 - Current_ids device: cuda:0
2025-02-11 07:55:14,315 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,343 - Model output complete
2025-02-11 07:55:14,343 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:14,343 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,343 - Next token logits device: cuda:0
2025-02-11 07:55:14,343 - Entered do_sample
2025-02-11 07:55:14,343 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,346 - Probs max: 0.99853515625
2025-02-11 07:55:14,347 - Pre-cat
2025-02-11 07:55:14,347 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:55:14,349 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:14,349 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:14,349 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,349 - Step 10: Generated next token
2025-02-11 07:55:14,349 - Step 10: Updated current_ids
2025-02-11 07:55:14,350 - Step 10: Decoded token text: ,
2025-02-11 07:55:14,350 - Step 10: Updated current_phrase
2025-02-11 07:55:14,350 - Step 10: Created step_acts
2025-02-11 07:55:14,350 - Step 10: Added to generation_acts
2025-02-11 07:55:14,351 - Step 10: Updated generated_texts
2025-02-11 07:55:14,352 - Step 10: Updated recent_tokens
2025-02-11 07:55:14,352 - Step 10: Found phrase end token
2025-02-11 07:55:14,352 - Step 10: Updated recent_phrases
2025-02-11 07:55:14,352 - Step 10: Decoded current text
2025-02-11 07:55:14,352 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:14,352 - 
Starting step 11
2025-02-11 07:55:14,352 - Current_ids device: cuda:0
2025-02-11 07:55:14,352 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,375 - Model output complete
2025-02-11 07:55:14,375 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:14,375 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,375 - Next token logits device: cuda:0
2025-02-11 07:55:14,375 - Entered do_sample
2025-02-11 07:55:14,375 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,378 - Probs max: 0.66357421875
2025-02-11 07:55:14,378 - Pre-cat
2025-02-11 07:55:14,378 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:55:14,380 - Next token: tensor([[1221]], device='cuda:0')
2025-02-11 07:55:14,380 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:14,380 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,380 - Step 11: Generated next token
2025-02-11 07:55:14,380 - Step 11: Updated current_ids
2025-02-11 07:55:14,380 - Step 11: Decoded token text:  then
2025-02-11 07:55:14,380 - Step 11: Updated current_phrase
2025-02-11 07:55:14,381 - Step 11: Created step_acts
2025-02-11 07:55:14,381 - Step 11: Added to generation_acts
2025-02-11 07:55:14,381 - Step 11: Updated recent_tokens
2025-02-11 07:55:14,382 - Step 11: Decoded current text
2025-02-11 07:55:14,382 - Step 11: Reset consecutive_fillers
2025-02-11 07:55:14,382 - 
Starting step 12
2025-02-11 07:55:14,382 - Current_ids device: cuda:0
2025-02-11 07:55:14,382 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,418 - Model output complete
2025-02-11 07:55:14,418 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:14,418 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,418 - Next token logits device: cuda:0
2025-02-11 07:55:14,418 - Entered do_sample
2025-02-11 07:55:14,418 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,420 - Probs max: 0.9365234375
2025-02-11 07:55:14,421 - Pre-cat
2025-02-11 07:55:14,421 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,   1221]],
       device='cuda:0')
2025-02-11 07:55:14,422 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:14,423 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:14,423 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,423 - Step 12: Generated next token
2025-02-11 07:55:14,423 - Step 12: Updated current_ids
2025-02-11 07:55:14,423 - Step 12: Decoded token text:  the
2025-02-11 07:55:14,423 - Step 12: Updated current_phrase
2025-02-11 07:55:14,424 - Step 12: Created step_acts
2025-02-11 07:55:14,424 - Step 12: Added to generation_acts
2025-02-11 07:55:14,424 - Step 12: Updated recent_tokens
2025-02-11 07:55:14,425 - Step 12: Decoded current text
2025-02-11 07:55:14,425 - Step 12: Incremented consecutive_fillers to 1
2025-02-11 07:55:14,425 - 
Starting step 13
2025-02-11 07:55:14,425 - Current_ids device: cuda:0
2025-02-11 07:55:14,425 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,449 - Model output complete
2025-02-11 07:55:14,449 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:14,449 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,449 - Next token logits device: cuda:0
2025-02-11 07:55:14,449 - Entered do_sample
2025-02-11 07:55:14,449 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,452 - Probs max: 0.6162109375
2025-02-11 07:55:14,452 - Pre-cat
2025-02-11 07:55:14,452 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,   1221,    279]],
       device='cuda:0')
2025-02-11 07:55:14,454 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:14,454 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:14,454 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,455 - Step 13: Generated next token
2025-02-11 07:55:14,455 - Step 13: Updated current_ids
2025-02-11 07:55:14,455 - Step 13: Decoded token text:  ball
2025-02-11 07:55:14,455 - Step 13: Updated current_phrase
2025-02-11 07:55:14,455 - Step 13: Created step_acts
2025-02-11 07:55:14,455 - Step 13: Added to generation_acts
2025-02-11 07:55:14,455 - Step 13: Updated recent_tokens
2025-02-11 07:55:14,457 - Step 13: Decoded current text
2025-02-11 07:55:14,457 - Step 13: Incremented consecutive_fillers to 2
2025-02-11 07:55:14,457 - 
Starting step 14
2025-02-11 07:55:14,457 - Current_ids device: cuda:0
2025-02-11 07:55:14,457 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,480 - Model output complete
2025-02-11 07:55:14,480 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:55:14,480 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,480 - Next token logits device: cuda:0
2025-02-11 07:55:14,480 - Entered do_sample
2025-02-11 07:55:14,480 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,483 - Probs max: 0.4248046875
2025-02-11 07:55:14,483 - Pre-cat
2025-02-11 07:55:14,483 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,   1221,    279,   4935]],
       device='cuda:0')
2025-02-11 07:55:14,485 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:55:14,486 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:55:14,486 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,486 - Step 14: Generated next token
2025-02-11 07:55:14,486 - Step 14: Updated current_ids
2025-02-11 07:55:14,486 - Step 14: Decoded token text: 's
2025-02-11 07:55:14,486 - Step 14: Updated current_phrase
2025-02-11 07:55:14,486 - Step 14: Created step_acts
2025-02-11 07:55:14,486 - Step 14: Added to generation_acts
2025-02-11 07:55:14,486 - Step 14: Updated recent_tokens
2025-02-11 07:55:14,488 - Step 14: Decoded current text
2025-02-11 07:55:14,488 - Step 14: Incremented consecutive_fillers to 3
2025-02-11 07:55:14,595 - 
Starting step 0
2025-02-11 07:55:14,595 - Current_ids device: cuda:0
2025-02-11 07:55:14,595 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,624 - Model output complete
2025-02-11 07:55:14,624 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:14,624 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,624 - Next token logits device: cuda:0
2025-02-11 07:55:14,624 - Entered do_sample
2025-02-11 07:55:14,624 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,626 - Probs max: 0.50341796875
2025-02-11 07:55:14,627 - Pre-cat
2025-02-11 07:55:14,627 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:14,629 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:14,630 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:14,630 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,630 - Step 0: Generated next token
2025-02-11 07:55:14,630 - Step 0: Updated current_ids
2025-02-11 07:55:14,630 - Step 0: Decoded token text:  The
2025-02-11 07:55:14,630 - Step 0: Updated current_phrase
2025-02-11 07:55:14,631 - Step 0: Created step_acts
2025-02-11 07:55:14,631 - Step 0: Added to generation_acts
2025-02-11 07:55:14,633 - Step 0: Updated generated_texts
2025-02-11 07:55:14,633 - Step 0: Updated recent_tokens
2025-02-11 07:55:14,634 - Step 0: Decoded current text
2025-02-11 07:55:14,634 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:14,634 - 
Starting step 1
2025-02-11 07:55:14,634 - Current_ids device: cuda:0
2025-02-11 07:55:14,634 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,663 - Model output complete
2025-02-11 07:55:14,663 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:14,663 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,663 - Next token logits device: cuda:0
2025-02-11 07:55:14,663 - Entered do_sample
2025-02-11 07:55:14,663 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,665 - Probs max: 0.83837890625
2025-02-11 07:55:14,666 - Pre-cat
2025-02-11 07:55:14,666 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:55:14,669 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:14,669 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:14,669 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,669 - Step 1: Generated next token
2025-02-11 07:55:14,669 - Step 1: Updated current_ids
2025-02-11 07:55:14,670 - Step 1: Decoded token text:  ball
2025-02-11 07:55:14,670 - Step 1: Updated current_phrase
2025-02-11 07:55:14,670 - Step 1: Created step_acts
2025-02-11 07:55:14,670 - Step 1: Added to generation_acts
2025-02-11 07:55:14,670 - Step 1: Updated recent_tokens
2025-02-11 07:55:14,672 - Step 1: Decoded current text
2025-02-11 07:55:14,672 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:14,672 - 
Starting step 2
2025-02-11 07:55:14,672 - Current_ids device: cuda:0
2025-02-11 07:55:14,672 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,718 - Model output complete
2025-02-11 07:55:14,719 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:14,719 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,719 - Next token logits device: cuda:0
2025-02-11 07:55:14,719 - Entered do_sample
2025-02-11 07:55:14,719 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,721 - Probs max: 0.583984375
2025-02-11 07:55:14,722 - Pre-cat
2025-02-11 07:55:14,722 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:55:14,725 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:14,725 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:14,725 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,725 - Step 2: Generated next token
2025-02-11 07:55:14,726 - Step 2: Updated current_ids
2025-02-11 07:55:14,726 - Step 2: Decoded token text:  will
2025-02-11 07:55:14,726 - Step 2: Updated current_phrase
2025-02-11 07:55:14,726 - Step 2: Created step_acts
2025-02-11 07:55:14,726 - Step 2: Added to generation_acts
2025-02-11 07:55:14,726 - Step 2: Updated recent_tokens
2025-02-11 07:55:14,728 - Step 2: Decoded current text
2025-02-11 07:55:14,728 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:14,728 - 
Starting step 3
2025-02-11 07:55:14,728 - Current_ids device: cuda:0
2025-02-11 07:55:14,728 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,755 - Model output complete
2025-02-11 07:55:14,755 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:14,755 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,755 - Next token logits device: cuda:0
2025-02-11 07:55:14,755 - Entered do_sample
2025-02-11 07:55:14,755 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,758 - Probs max: 0.56884765625
2025-02-11 07:55:14,759 - Pre-cat
2025-02-11 07:55:14,759 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686]],
       device='cuda:0')
2025-02-11 07:55:14,760 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:55:14,760 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:14,760 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,760 - Step 3: Generated next token
2025-02-11 07:55:14,761 - Step 3: Updated current_ids
2025-02-11 07:55:14,761 - Step 3: Decoded token text:  hit
2025-02-11 07:55:14,761 - Step 3: Updated current_phrase
2025-02-11 07:55:14,761 - Step 3: Created step_acts
2025-02-11 07:55:14,761 - Step 3: Added to generation_acts
2025-02-11 07:55:14,761 - Step 3: Updated recent_tokens
2025-02-11 07:55:14,763 - Step 3: Decoded current text
2025-02-11 07:55:14,763 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:14,763 - 
Starting step 4
2025-02-11 07:55:14,763 - Current_ids device: cuda:0
2025-02-11 07:55:14,763 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,785 - Model output complete
2025-02-11 07:55:14,785 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:14,785 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,786 - Next token logits device: cuda:0
2025-02-11 07:55:14,786 - Entered do_sample
2025-02-11 07:55:14,786 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,789 - Probs max: 0.99609375
2025-02-11 07:55:14,789 - Pre-cat
2025-02-11 07:55:14,790 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201]],
       device='cuda:0')
2025-02-11 07:55:14,791 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:14,791 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:14,791 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,791 - Step 4: Generated next token
2025-02-11 07:55:14,792 - Step 4: Updated current_ids
2025-02-11 07:55:14,792 - Step 4: Decoded token text:  the
2025-02-11 07:55:14,792 - Step 4: Updated current_phrase
2025-02-11 07:55:14,792 - Step 4: Created step_acts
2025-02-11 07:55:14,792 - Step 4: Added to generation_acts
2025-02-11 07:55:14,792 - Step 4: Updated recent_tokens
2025-02-11 07:55:14,793 - Step 4: Decoded current text
2025-02-11 07:55:14,793 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:14,794 - 
Starting step 5
2025-02-11 07:55:14,794 - Current_ids device: cuda:0
2025-02-11 07:55:14,794 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,816 - Model output complete
2025-02-11 07:55:14,816 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:14,816 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,816 - Next token logits device: cuda:0
2025-02-11 07:55:14,816 - Entered do_sample
2025-02-11 07:55:14,816 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,819 - Probs max: 0.99853515625
2025-02-11 07:55:14,820 - Pre-cat
2025-02-11 07:55:14,820 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279]],
       device='cuda:0')
2025-02-11 07:55:14,821 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:14,821 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:14,821 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,821 - Step 5: Generated next token
2025-02-11 07:55:14,821 - Step 5: Updated current_ids
2025-02-11 07:55:14,822 - Step 5: Decoded token text:  wall
2025-02-11 07:55:14,822 - Step 5: Updated current_phrase
2025-02-11 07:55:14,822 - Step 5: Created step_acts
2025-02-11 07:55:14,822 - Step 5: Added to generation_acts
2025-02-11 07:55:14,823 - Step 5: Updated generated_texts
2025-02-11 07:55:14,823 - Step 5: Updated recent_tokens
2025-02-11 07:55:14,824 - Step 5: Decoded current text
2025-02-11 07:55:14,824 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:14,824 - 
Starting step 6
2025-02-11 07:55:14,824 - Current_ids device: cuda:0
2025-02-11 07:55:14,824 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,878 - Model output complete
2025-02-11 07:55:14,878 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:14,878 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,878 - Next token logits device: cuda:0
2025-02-11 07:55:14,878 - Entered do_sample
2025-02-11 07:55:14,879 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,881 - Probs max: 0.376708984375
2025-02-11 07:55:14,883 - Pre-cat
2025-02-11 07:55:14,883 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002]], device='cuda:0')
2025-02-11 07:55:14,886 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:55:14,887 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:14,887 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,887 - Step 6: Generated next token
2025-02-11 07:55:14,887 - Step 6: Updated current_ids
2025-02-11 07:55:14,887 - Step 6: Decoded token text:  and
2025-02-11 07:55:14,887 - Step 6: Updated current_phrase
2025-02-11 07:55:14,888 - Step 6: Created step_acts
2025-02-11 07:55:14,888 - Step 6: Added to generation_acts
2025-02-11 07:55:14,888 - Step 6: Updated recent_tokens
2025-02-11 07:55:14,889 - Step 6: Decoded current text
2025-02-11 07:55:14,889 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:14,889 - 
Starting step 7
2025-02-11 07:55:14,889 - Current_ids device: cuda:0
2025-02-11 07:55:14,889 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,922 - Model output complete
2025-02-11 07:55:14,923 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:14,923 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,923 - Next token logits device: cuda:0
2025-02-11 07:55:14,923 - Entered do_sample
2025-02-11 07:55:14,923 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,925 - Probs max: 0.416748046875
2025-02-11 07:55:14,926 - Pre-cat
2025-02-11 07:55:14,926 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323]], device='cuda:0')
2025-02-11 07:55:14,929 - Next token: tensor([[1221]], device='cuda:0')
2025-02-11 07:55:14,930 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:14,930 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,930 - Step 7: Generated next token
2025-02-11 07:55:14,930 - Step 7: Updated current_ids
2025-02-11 07:55:14,930 - Step 7: Decoded token text:  then
2025-02-11 07:55:14,930 - Step 7: Updated current_phrase
2025-02-11 07:55:14,931 - Step 7: Created step_acts
2025-02-11 07:55:14,931 - Step 7: Added to generation_acts
2025-02-11 07:55:14,931 - Step 7: Updated recent_tokens
2025-02-11 07:55:14,933 - Step 7: Decoded current text
2025-02-11 07:55:14,933 - Step 7: Incremented consecutive_fillers to 1
2025-02-11 07:55:14,933 - 
Starting step 8
2025-02-11 07:55:14,933 - Current_ids device: cuda:0
2025-02-11 07:55:14,933 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,957 - Model output complete
2025-02-11 07:55:14,958 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:14,958 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,958 - Next token logits device: cuda:0
2025-02-11 07:55:14,958 - Entered do_sample
2025-02-11 07:55:14,959 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,960 - Probs max: 0.27587890625
2025-02-11 07:55:14,961 - Pre-cat
2025-02-11 07:55:14,961 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,   1221]], device='cuda:0')
2025-02-11 07:55:14,965 - Next token: tensor([[2936]], device='cuda:0')
2025-02-11 07:55:14,965 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:14,966 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,966 - Step 8: Generated next token
2025-02-11 07:55:14,966 - Step 8: Updated current_ids
2025-02-11 07:55:14,966 - Step 8: Decoded token text:  stop
2025-02-11 07:55:14,966 - Step 8: Updated current_phrase
2025-02-11 07:55:14,966 - Step 8: Created step_acts
2025-02-11 07:55:14,966 - Step 8: Added to generation_acts
2025-02-11 07:55:14,967 - Step 8: Updated recent_tokens
2025-02-11 07:55:14,968 - Step 8: Decoded current text
2025-02-11 07:55:14,968 - Step 8: Incremented consecutive_fillers to 2
2025-02-11 07:55:14,968 - 
Starting step 9
2025-02-11 07:55:14,968 - Current_ids device: cuda:0
2025-02-11 07:55:14,968 - Current_ids dtype: torch.int64
2025-02-11 07:55:14,994 - Model output complete
2025-02-11 07:55:14,994 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:14,994 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,994 - Next token logits device: cuda:0
2025-02-11 07:55:14,994 - Entered do_sample
2025-02-11 07:55:14,994 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:14,997 - Probs max: 0.64599609375
2025-02-11 07:55:14,997 - Pre-cat
2025-02-11 07:55:14,997 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,   1221,   2936]], device='cuda:0')
2025-02-11 07:55:14,999 - Next token: tensor([[624]], device='cuda:0')
2025-02-11 07:55:14,999 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:14,999 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:14,999 - Step 9: Generated next token
2025-02-11 07:55:14,999 - Step 9: Updated current_ids
2025-02-11 07:55:15,000 - Step 9: Decoded token text: .

2025-02-11 07:55:15,000 - Step 9: Updated current_phrase
2025-02-11 07:55:15,000 - Step 9: Created step_acts
2025-02-11 07:55:15,000 - Step 9: Added to generation_acts
2025-02-11 07:55:15,000 - Step 9: Updated recent_tokens
2025-02-11 07:55:15,001 - Step 9: Decoded current text
2025-02-11 07:55:15,001 - Step 9: Incremented consecutive_fillers to 3
2025-02-11 07:55:15,108 - 
Starting step 0
2025-02-11 07:55:15,108 - Current_ids device: cuda:0
2025-02-11 07:55:15,108 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,140 - Model output complete
2025-02-11 07:55:15,141 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:15,141 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,141 - Next token logits device: cuda:0
2025-02-11 07:55:15,141 - Entered do_sample
2025-02-11 07:55:15,141 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,144 - Probs max: 0.50341796875
2025-02-11 07:55:15,145 - Pre-cat
2025-02-11 07:55:15,145 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:15,146 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:55:15,146 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:15,146 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,146 - Step 0: Generated next token
2025-02-11 07:55:15,147 - Step 0: Updated current_ids
2025-02-11 07:55:15,147 - Step 0: Decoded token text:  If
2025-02-11 07:55:15,147 - Step 0: Updated current_phrase
2025-02-11 07:55:15,147 - Step 0: Created step_acts
2025-02-11 07:55:15,147 - Step 0: Added to generation_acts
2025-02-11 07:55:15,148 - Step 0: Updated generated_texts
2025-02-11 07:55:15,149 - Step 0: Updated recent_tokens
2025-02-11 07:55:15,149 - Step 0: Decoded current text
2025-02-11 07:55:15,149 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:15,149 - 
Starting step 1
2025-02-11 07:55:15,149 - Current_ids device: cuda:0
2025-02-11 07:55:15,149 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,205 - Model output complete
2025-02-11 07:55:15,205 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:15,206 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,206 - Next token logits device: cuda:0
2025-02-11 07:55:15,206 - Entered do_sample
2025-02-11 07:55:15,206 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,208 - Probs max: 0.7099609375
2025-02-11 07:55:15,209 - Pre-cat
2025-02-11 07:55:15,209 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:55:15,210 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:15,210 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:15,210 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,210 - Step 1: Generated next token
2025-02-11 07:55:15,210 - Step 1: Updated current_ids
2025-02-11 07:55:15,211 - Step 1: Decoded token text:  a
2025-02-11 07:55:15,211 - Step 1: Updated current_phrase
2025-02-11 07:55:15,211 - Step 1: Created step_acts
2025-02-11 07:55:15,211 - Step 1: Added to generation_acts
2025-02-11 07:55:15,211 - Step 1: Updated recent_tokens
2025-02-11 07:55:15,212 - Step 1: Decoded current text
2025-02-11 07:55:15,212 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:15,212 - 
Starting step 2
2025-02-11 07:55:15,212 - Current_ids device: cuda:0
2025-02-11 07:55:15,213 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,235 - Model output complete
2025-02-11 07:55:15,235 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:15,235 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,235 - Next token logits device: cuda:0
2025-02-11 07:55:15,235 - Entered do_sample
2025-02-11 07:55:15,235 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,238 - Probs max: 0.9970703125
2025-02-11 07:55:15,239 - Pre-cat
2025-02-11 07:55:15,239 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264]], device='cuda:0')
2025-02-11 07:55:15,240 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:15,241 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:15,241 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,241 - Step 2: Generated next token
2025-02-11 07:55:15,241 - Step 2: Updated current_ids
2025-02-11 07:55:15,241 - Step 2: Decoded token text:  ball
2025-02-11 07:55:15,241 - Step 2: Updated current_phrase
2025-02-11 07:55:15,241 - Step 2: Created step_acts
2025-02-11 07:55:15,241 - Step 2: Added to generation_acts
2025-02-11 07:55:15,242 - Step 2: Updated recent_tokens
2025-02-11 07:55:15,243 - Step 2: Decoded current text
2025-02-11 07:55:15,243 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:15,243 - 
Starting step 3
2025-02-11 07:55:15,243 - Current_ids device: cuda:0
2025-02-11 07:55:15,243 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,267 - Model output complete
2025-02-11 07:55:15,267 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:15,267 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,267 - Next token logits device: cuda:0
2025-02-11 07:55:15,267 - Entered do_sample
2025-02-11 07:55:15,267 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,269 - Probs max: 0.99951171875
2025-02-11 07:55:15,270 - Pre-cat
2025-02-11 07:55:15,271 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935]],
       device='cuda:0')
2025-02-11 07:55:15,273 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:15,273 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:15,273 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,273 - Step 3: Generated next token
2025-02-11 07:55:15,273 - Step 3: Updated current_ids
2025-02-11 07:55:15,274 - Step 3: Decoded token text:  is
2025-02-11 07:55:15,274 - Step 3: Updated current_phrase
2025-02-11 07:55:15,274 - Step 3: Created step_acts
2025-02-11 07:55:15,274 - Step 3: Added to generation_acts
2025-02-11 07:55:15,274 - Step 3: Updated recent_tokens
2025-02-11 07:55:15,275 - Step 3: Decoded current text
2025-02-11 07:55:15,276 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:15,276 - 
Starting step 4
2025-02-11 07:55:15,276 - Current_ids device: cuda:0
2025-02-11 07:55:15,276 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,302 - Model output complete
2025-02-11 07:55:15,302 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:15,302 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,302 - Next token logits device: cuda:0
2025-02-11 07:55:15,302 - Entered do_sample
2025-02-11 07:55:15,302 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,305 - Probs max: 0.998046875
2025-02-11 07:55:15,306 - Pre-cat
2025-02-11 07:55:15,306 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:15,310 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:15,311 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:15,311 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,311 - Step 4: Generated next token
2025-02-11 07:55:15,311 - Step 4: Updated current_ids
2025-02-11 07:55:15,312 - Step 4: Decoded token text:  thrown
2025-02-11 07:55:15,312 - Step 4: Updated current_phrase
2025-02-11 07:55:15,315 - Step 4: Created step_acts
2025-02-11 07:55:15,315 - Step 4: Added to generation_acts
2025-02-11 07:55:15,316 - Step 4: Updated recent_tokens
2025-02-11 07:55:15,316 - Step 4: Decoded current text
2025-02-11 07:55:15,316 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:15,317 - 
Starting step 5
2025-02-11 07:55:15,317 - Current_ids device: cuda:0
2025-02-11 07:55:15,317 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,363 - Model output complete
2025-02-11 07:55:15,363 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:15,363 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,363 - Next token logits device: cuda:0
2025-02-11 07:55:15,363 - Entered do_sample
2025-02-11 07:55:15,363 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,366 - Probs max: 0.9697265625
2025-02-11 07:55:15,369 - Pre-cat
2025-02-11 07:55:15,369 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:55:15,373 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:55:15,374 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:15,374 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,374 - Step 5: Generated next token
2025-02-11 07:55:15,374 - Step 5: Updated current_ids
2025-02-11 07:55:15,375 - Step 5: Decoded token text:  at
2025-02-11 07:55:15,375 - Step 5: Updated current_phrase
2025-02-11 07:55:15,376 - Step 5: Created step_acts
2025-02-11 07:55:15,376 - Step 5: Added to generation_acts
2025-02-11 07:55:15,377 - Step 5: Updated generated_texts
2025-02-11 07:55:15,377 - Step 5: Updated recent_tokens
2025-02-11 07:55:15,378 - Step 5: Decoded current text
2025-02-11 07:55:15,378 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:15,378 - 
Starting step 6
2025-02-11 07:55:15,378 - Current_ids device: cuda:0
2025-02-11 07:55:15,378 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,417 - Model output complete
2025-02-11 07:55:15,417 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:15,418 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,418 - Next token logits device: cuda:0
2025-02-11 07:55:15,418 - Entered do_sample
2025-02-11 07:55:15,418 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,420 - Probs max: 0.99951171875
2025-02-11 07:55:15,421 - Pre-cat
2025-02-11 07:55:15,421 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:55:15,424 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:15,424 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:15,424 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,424 - Step 6: Generated next token
2025-02-11 07:55:15,424 - Step 6: Updated current_ids
2025-02-11 07:55:15,425 - Step 6: Decoded token text:  a
2025-02-11 07:55:15,425 - Step 6: Updated current_phrase
2025-02-11 07:55:15,425 - Step 6: Created step_acts
2025-02-11 07:55:15,425 - Step 6: Added to generation_acts
2025-02-11 07:55:15,425 - Step 6: Updated recent_tokens
2025-02-11 07:55:15,426 - Step 6: Decoded current text
2025-02-11 07:55:15,427 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:15,427 - 
Starting step 7
2025-02-11 07:55:15,427 - Current_ids device: cuda:0
2025-02-11 07:55:15,427 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,485 - Model output complete
2025-02-11 07:55:15,485 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:15,485 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,485 - Next token logits device: cuda:0
2025-02-11 07:55:15,486 - Entered do_sample
2025-02-11 07:55:15,486 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,488 - Probs max: 0.9990234375
2025-02-11 07:55:15,489 - Pre-cat
2025-02-11 07:55:15,489 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:55:15,492 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:15,492 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:15,492 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,492 - Step 7: Generated next token
2025-02-11 07:55:15,492 - Step 7: Updated current_ids
2025-02-11 07:55:15,493 - Step 7: Decoded token text:  wall
2025-02-11 07:55:15,493 - Step 7: Updated current_phrase
2025-02-11 07:55:15,493 - Step 7: Created step_acts
2025-02-11 07:55:15,493 - Step 7: Added to generation_acts
2025-02-11 07:55:15,493 - Step 7: Updated recent_tokens
2025-02-11 07:55:15,495 - Step 7: Decoded current text
2025-02-11 07:55:15,495 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:15,495 - 
Starting step 8
2025-02-11 07:55:15,495 - Current_ids device: cuda:0
2025-02-11 07:55:15,495 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,577 - Model output complete
2025-02-11 07:55:15,577 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:15,577 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,577 - Next token logits device: cuda:0
2025-02-11 07:55:15,577 - Entered do_sample
2025-02-11 07:55:15,577 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,579 - Probs max: 0.9912109375
2025-02-11 07:55:15,580 - Pre-cat
2025-02-11 07:55:15,580 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:55:15,582 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:15,582 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:15,582 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,582 - Step 8: Generated next token
2025-02-11 07:55:15,583 - Step 8: Updated current_ids
2025-02-11 07:55:15,583 - Step 8: Decoded token text:  very
2025-02-11 07:55:15,583 - Step 8: Updated current_phrase
2025-02-11 07:55:15,583 - Step 8: Created step_acts
2025-02-11 07:55:15,583 - Step 8: Added to generation_acts
2025-02-11 07:55:15,583 - Step 8: Updated recent_tokens
2025-02-11 07:55:15,585 - Step 8: Decoded current text
2025-02-11 07:55:15,585 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:15,585 - 
Starting step 9
2025-02-11 07:55:15,585 - Current_ids device: cuda:0
2025-02-11 07:55:15,585 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,617 - Model output complete
2025-02-11 07:55:15,617 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:15,617 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,617 - Next token logits device: cuda:0
2025-02-11 07:55:15,617 - Entered do_sample
2025-02-11 07:55:15,617 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,620 - Probs max: 0.998046875
2025-02-11 07:55:15,621 - Pre-cat
2025-02-11 07:55:15,621 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:55:15,622 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:55:15,623 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:15,623 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,623 - Step 9: Generated next token
2025-02-11 07:55:15,623 - Step 9: Updated current_ids
2025-02-11 07:55:15,623 - Step 9: Decoded token text:  fast
2025-02-11 07:55:15,623 - Step 9: Updated current_phrase
2025-02-11 07:55:15,623 - Step 9: Created step_acts
2025-02-11 07:55:15,624 - Step 9: Added to generation_acts
2025-02-11 07:55:15,624 - Step 9: Updated recent_tokens
2025-02-11 07:55:15,625 - Step 9: Decoded current text
2025-02-11 07:55:15,625 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:15,625 - 
Starting step 10
2025-02-11 07:55:15,625 - Current_ids device: cuda:0
2025-02-11 07:55:15,625 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,652 - Model output complete
2025-02-11 07:55:15,652 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:15,652 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,652 - Next token logits device: cuda:0
2025-02-11 07:55:15,652 - Entered do_sample
2025-02-11 07:55:15,652 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,655 - Probs max: 0.99853515625
2025-02-11 07:55:15,655 - Pre-cat
2025-02-11 07:55:15,656 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:55:15,657 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:15,658 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:15,658 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,658 - Step 10: Generated next token
2025-02-11 07:55:15,658 - Step 10: Updated current_ids
2025-02-11 07:55:15,658 - Step 10: Decoded token text: ,
2025-02-11 07:55:15,658 - Step 10: Updated current_phrase
2025-02-11 07:55:15,659 - Step 10: Created step_acts
2025-02-11 07:55:15,659 - Step 10: Added to generation_acts
2025-02-11 07:55:15,660 - Step 10: Updated generated_texts
2025-02-11 07:55:15,660 - Step 10: Updated recent_tokens
2025-02-11 07:55:15,660 - Step 10: Found phrase end token
2025-02-11 07:55:15,660 - Step 10: Updated recent_phrases
2025-02-11 07:55:15,661 - Step 10: Decoded current text
2025-02-11 07:55:15,661 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:15,661 - 
Starting step 11
2025-02-11 07:55:15,661 - Current_ids device: cuda:0
2025-02-11 07:55:15,661 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,684 - Model output complete
2025-02-11 07:55:15,684 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:15,684 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,684 - Next token logits device: cuda:0
2025-02-11 07:55:15,684 - Entered do_sample
2025-02-11 07:55:15,684 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,688 - Probs max: 0.58154296875
2025-02-11 07:55:15,688 - Pre-cat
2025-02-11 07:55:15,688 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:55:15,690 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:15,691 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:15,691 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,691 - Step 11: Generated next token
2025-02-11 07:55:15,691 - Step 11: Updated current_ids
2025-02-11 07:55:15,691 - Step 11: Decoded token text:  the
2025-02-11 07:55:15,691 - Step 11: Updated current_phrase
2025-02-11 07:55:15,692 - Step 11: Created step_acts
2025-02-11 07:55:15,692 - Step 11: Added to generation_acts
2025-02-11 07:55:15,692 - Step 11: Updated recent_tokens
2025-02-11 07:55:15,693 - Step 11: Decoded current text
2025-02-11 07:55:15,693 - Step 11: Reset consecutive_fillers
2025-02-11 07:55:15,693 - 
Starting step 12
2025-02-11 07:55:15,694 - Current_ids device: cuda:0
2025-02-11 07:55:15,694 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,716 - Model output complete
2025-02-11 07:55:15,716 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:15,716 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,716 - Next token logits device: cuda:0
2025-02-11 07:55:15,716 - Entered do_sample
2025-02-11 07:55:15,717 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,720 - Probs max: 0.6064453125
2025-02-11 07:55:15,721 - Pre-cat
2025-02-11 07:55:15,721 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279]],
       device='cuda:0')
2025-02-11 07:55:15,723 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:15,723 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:15,723 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,724 - Step 12: Generated next token
2025-02-11 07:55:15,724 - Step 12: Updated current_ids
2025-02-11 07:55:15,724 - Step 12: Decoded token text:  ball
2025-02-11 07:55:15,724 - Step 12: Updated current_phrase
2025-02-11 07:55:15,724 - Step 12: Created step_acts
2025-02-11 07:55:15,724 - Step 12: Added to generation_acts
2025-02-11 07:55:15,725 - Step 12: Updated recent_tokens
2025-02-11 07:55:15,726 - Step 12: Decoded current text
2025-02-11 07:55:15,726 - Step 12: Reset consecutive_fillers
2025-02-11 07:55:15,726 - 
Starting step 13
2025-02-11 07:55:15,726 - Current_ids device: cuda:0
2025-02-11 07:55:15,726 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,749 - Model output complete
2025-02-11 07:55:15,750 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:15,750 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,750 - Next token logits device: cuda:0
2025-02-11 07:55:15,750 - Entered do_sample
2025-02-11 07:55:15,750 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,753 - Probs max: 0.76953125
2025-02-11 07:55:15,754 - Pre-cat
2025-02-11 07:55:15,754 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935]],
       device='cuda:0')
2025-02-11 07:55:15,756 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:15,757 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:15,757 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,757 - Step 13: Generated next token
2025-02-11 07:55:15,757 - Step 13: Updated current_ids
2025-02-11 07:55:15,757 - Step 13: Decoded token text:  will
2025-02-11 07:55:15,757 - Step 13: Updated current_phrase
2025-02-11 07:55:15,757 - Step 13: Created step_acts
2025-02-11 07:55:15,757 - Step 13: Added to generation_acts
2025-02-11 07:55:15,758 - Step 13: Updated recent_tokens
2025-02-11 07:55:15,759 - Step 13: Decoded current text
2025-02-11 07:55:15,759 - Step 13: Reset consecutive_fillers
2025-02-11 07:55:15,759 - 
Starting step 14
2025-02-11 07:55:15,759 - Current_ids device: cuda:0
2025-02-11 07:55:15,759 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,783 - Model output complete
2025-02-11 07:55:15,783 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:55:15,784 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,784 - Next token logits device: cuda:0
2025-02-11 07:55:15,784 - Entered do_sample
2025-02-11 07:55:15,784 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,786 - Probs max: 0.43408203125
2025-02-11 07:55:15,787 - Pre-cat
2025-02-11 07:55:15,787 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686]],
       device='cuda:0')
2025-02-11 07:55:15,789 - Next token: tensor([[387]], device='cuda:0')
2025-02-11 07:55:15,790 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:55:15,790 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,790 - Step 14: Generated next token
2025-02-11 07:55:15,790 - Step 14: Updated current_ids
2025-02-11 07:55:15,790 - Step 14: Decoded token text:  be
2025-02-11 07:55:15,790 - Step 14: Updated current_phrase
2025-02-11 07:55:15,790 - Step 14: Created step_acts
2025-02-11 07:55:15,790 - Step 14: Added to generation_acts
2025-02-11 07:55:15,791 - Step 14: Updated recent_tokens
2025-02-11 07:55:15,792 - Step 14: Decoded current text
2025-02-11 07:55:15,792 - Step 14: Reset consecutive_fillers
2025-02-11 07:55:15,792 - 
Starting step 15
2025-02-11 07:55:15,792 - Current_ids device: cuda:0
2025-02-11 07:55:15,792 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,817 - Model output complete
2025-02-11 07:55:15,817 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:55:15,818 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,818 - Next token logits device: cuda:0
2025-02-11 07:55:15,818 - Entered do_sample
2025-02-11 07:55:15,818 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,821 - Probs max: 0.533203125
2025-02-11 07:55:15,822 - Pre-cat
2025-02-11 07:55:15,822 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387]], device='cuda:0')
2025-02-11 07:55:15,824 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:55:15,825 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:55:15,825 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,825 - Step 15: Generated next token
2025-02-11 07:55:15,825 - Step 15: Updated current_ids
2025-02-11 07:55:15,825 - Step 15: Decoded token text:  hit
2025-02-11 07:55:15,825 - Step 15: Updated current_phrase
2025-02-11 07:55:15,826 - Step 15: Created step_acts
2025-02-11 07:55:15,826 - Step 15: Added to generation_acts
2025-02-11 07:55:15,827 - Step 15: Updated generated_texts
2025-02-11 07:55:15,828 - Step 15: Updated recent_tokens
2025-02-11 07:55:15,828 - Step 15: Decoded current text
2025-02-11 07:55:15,828 - Step 15: Reset consecutive_fillers
2025-02-11 07:55:15,828 - 
Starting step 16
2025-02-11 07:55:15,828 - Current_ids device: cuda:0
2025-02-11 07:55:15,828 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,853 - Model output complete
2025-02-11 07:55:15,853 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:55:15,853 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,853 - Next token logits device: cuda:0
2025-02-11 07:55:15,854 - Entered do_sample
2025-02-11 07:55:15,854 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,856 - Probs max: 0.568359375
2025-02-11 07:55:15,857 - Pre-cat
2025-02-11 07:55:15,857 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   4201]], device='cuda:0')
2025-02-11 07:55:15,860 - Next token: tensor([[553]], device='cuda:0')
2025-02-11 07:55:15,861 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:55:15,861 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,861 - Step 16: Generated next token
2025-02-11 07:55:15,861 - Step 16: Updated current_ids
2025-02-11 07:55:15,861 - Step 16: Decoded token text:  by
2025-02-11 07:55:15,861 - Step 16: Updated current_phrase
2025-02-11 07:55:15,862 - Step 16: Created step_acts
2025-02-11 07:55:15,862 - Step 16: Added to generation_acts
2025-02-11 07:55:15,862 - Step 16: Updated recent_tokens
2025-02-11 07:55:15,863 - Step 16: Decoded current text
2025-02-11 07:55:15,863 - Step 16: Reset consecutive_fillers
2025-02-11 07:55:15,863 - Step 16: Calculated unique_ratio: 0.875
2025-02-11 07:55:15,863 - 
Starting step 17
2025-02-11 07:55:15,864 - Current_ids device: cuda:0
2025-02-11 07:55:15,864 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,907 - Model output complete
2025-02-11 07:55:15,907 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:55:15,907 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,908 - Next token logits device: cuda:0
2025-02-11 07:55:15,908 - Entered do_sample
2025-02-11 07:55:15,908 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,910 - Probs max: 0.96142578125
2025-02-11 07:55:15,912 - Pre-cat
2025-02-11 07:55:15,912 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   4201,    553]], device='cuda:0')
2025-02-11 07:55:15,916 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:15,916 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:55:15,916 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,916 - Step 17: Generated next token
2025-02-11 07:55:15,917 - Step 17: Updated current_ids
2025-02-11 07:55:15,917 - Step 17: Decoded token text:  the
2025-02-11 07:55:15,917 - Step 17: Updated current_phrase
2025-02-11 07:55:15,918 - Step 17: Created step_acts
2025-02-11 07:55:15,918 - Step 17: Added to generation_acts
2025-02-11 07:55:15,918 - Step 17: Updated recent_tokens
2025-02-11 07:55:15,919 - Step 17: Decoded current text
2025-02-11 07:55:15,919 - Step 17: Reset consecutive_fillers
2025-02-11 07:55:15,920 - Step 17: Calculated unique_ratio: 0.875
2025-02-11 07:55:15,920 - 
Starting step 18
2025-02-11 07:55:15,920 - Current_ids device: cuda:0
2025-02-11 07:55:15,920 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,952 - Model output complete
2025-02-11 07:55:15,952 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:55:15,952 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,952 - Next token logits device: cuda:0
2025-02-11 07:55:15,952 - Entered do_sample
2025-02-11 07:55:15,952 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,956 - Probs max: 0.998046875
2025-02-11 07:55:15,957 - Pre-cat
2025-02-11 07:55:15,957 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   4201,    553,    279]], device='cuda:0')
2025-02-11 07:55:15,959 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:15,959 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:55:15,959 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,959 - Step 18: Generated next token
2025-02-11 07:55:15,959 - Step 18: Updated current_ids
2025-02-11 07:55:15,959 - Step 18: Decoded token text:  wall
2025-02-11 07:55:15,959 - Step 18: Updated current_phrase
2025-02-11 07:55:15,960 - Step 18: Created step_acts
2025-02-11 07:55:15,960 - Step 18: Added to generation_acts
2025-02-11 07:55:15,960 - Step 18: Updated recent_tokens
2025-02-11 07:55:15,961 - Step 18: Decoded current text
2025-02-11 07:55:15,961 - Step 18: Reset consecutive_fillers
2025-02-11 07:55:15,961 - Step 18: Calculated unique_ratio: 0.875
2025-02-11 07:55:15,962 - 
Starting step 19
2025-02-11 07:55:15,962 - Current_ids device: cuda:0
2025-02-11 07:55:15,962 - Current_ids dtype: torch.int64
2025-02-11 07:55:15,985 - Model output complete
2025-02-11 07:55:15,985 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:55:15,985 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,985 - Next token logits device: cuda:0
2025-02-11 07:55:15,985 - Entered do_sample
2025-02-11 07:55:15,985 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:15,988 - Probs max: 0.362548828125
2025-02-11 07:55:15,989 - Pre-cat
2025-02-11 07:55:15,989 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   4201,    553,    279,   7002]], device='cuda:0')
2025-02-11 07:55:15,992 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:55:15,992 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:55:15,992 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:15,992 - Step 19: Generated next token
2025-02-11 07:55:15,993 - Step 19: Updated current_ids
2025-02-11 07:55:15,993 - Step 19: Decoded token text: .
2025-02-11 07:55:15,993 - Step 19: Updated current_phrase
2025-02-11 07:55:15,993 - Step 19: Created step_acts
2025-02-11 07:55:15,993 - Step 19: Added to generation_acts
2025-02-11 07:55:15,993 - Step 19: Updated recent_tokens
2025-02-11 07:55:15,995 - Step 19: Found phrase end token
2025-02-11 07:55:15,995 - Step 19: Updated recent_phrases
2025-02-11 07:55:15,995 - Step 19: Calculated similarity: 0.14285714285714285
2025-02-11 07:55:15,995 - Step 19: Decoded current text
2025-02-11 07:55:15,995 - Step 19: Reset consecutive_fillers
2025-02-11 07:55:15,995 - Step 19: Calculated unique_ratio: 0.875
2025-02-11 07:55:15,995 - 
Starting step 20
2025-02-11 07:55:15,995 - Current_ids device: cuda:0
2025-02-11 07:55:15,995 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,021 - Model output complete
2025-02-11 07:55:16,022 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:55:16,022 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,022 - Next token logits device: cuda:0
2025-02-11 07:55:16,022 - Entered do_sample
2025-02-11 07:55:16,022 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,025 - Probs max: 0.5224609375
2025-02-11 07:55:16,026 - Pre-cat
2025-02-11 07:55:16,026 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   4201,    553,    279,   7002,     13]], device='cuda:0')
2025-02-11 07:55:16,028 - Next token: tensor([[2055]], device='cuda:0')
2025-02-11 07:55:16,028 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:55:16,028 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,028 - Step 20: Generated next token
2025-02-11 07:55:16,028 - Step 20: Updated current_ids
2025-02-11 07:55:16,029 - Step 20: Decoded token text:  So
2025-02-11 07:55:16,029 - Step 20: Updated current_phrase
2025-02-11 07:55:16,029 - Step 20: Created step_acts
2025-02-11 07:55:16,029 - Step 20: Added to generation_acts
2025-02-11 07:55:16,031 - Step 20: Updated generated_texts
2025-02-11 07:55:16,031 - Step 20: Updated recent_tokens
2025-02-11 07:55:16,031 - Step 20: Decoded current text
2025-02-11 07:55:16,031 - Step 20: Reset consecutive_fillers
2025-02-11 07:55:16,031 - Step 20: Calculated unique_ratio: 0.875
2025-02-11 07:55:16,032 - 
Starting step 21
2025-02-11 07:55:16,032 - Current_ids device: cuda:0
2025-02-11 07:55:16,032 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,055 - Model output complete
2025-02-11 07:55:16,055 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:55:16,055 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,055 - Next token logits device: cuda:0
2025-02-11 07:55:16,055 - Entered do_sample
2025-02-11 07:55:16,055 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,059 - Probs max: 0.7578125
2025-02-11 07:55:16,059 - Pre-cat
2025-02-11 07:55:16,059 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   4201,    553,    279,   7002,     13,   2055]],
       device='cuda:0')
2025-02-11 07:55:16,061 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:16,062 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:55:16,062 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,062 - Step 21: Generated next token
2025-02-11 07:55:16,062 - Step 21: Updated current_ids
2025-02-11 07:55:16,062 - Step 21: Decoded token text: ,
2025-02-11 07:55:16,062 - Step 21: Updated current_phrase
2025-02-11 07:55:16,063 - Step 21: Created step_acts
2025-02-11 07:55:16,063 - Step 21: Added to generation_acts
2025-02-11 07:55:16,063 - Step 21: Updated recent_tokens
2025-02-11 07:55:16,064 - Step 21: Found phrase end token
2025-02-11 07:55:16,065 - Step 21: Updated recent_phrases
2025-02-11 07:55:16,065 - Step 21: Calculated similarity: 0.0
2025-02-11 07:55:16,065 - Step 21: Decoded current text
2025-02-11 07:55:16,065 - Step 21: Reset consecutive_fillers
2025-02-11 07:55:16,065 - Step 21: Calculated unique_ratio: 0.8125
2025-02-11 07:55:16,065 - 
Starting step 22
2025-02-11 07:55:16,065 - Current_ids device: cuda:0
2025-02-11 07:55:16,065 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,124 - Model output complete
2025-02-11 07:55:16,124 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:55:16,125 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,125 - Next token logits device: cuda:0
2025-02-11 07:55:16,125 - Entered do_sample
2025-02-11 07:55:16,125 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,127 - Probs max: 0.97314453125
2025-02-11 07:55:16,128 - Pre-cat
2025-02-11 07:55:16,128 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   4201,    553,    279,   7002,     13,   2055,     11]],
       device='cuda:0')
2025-02-11 07:55:16,130 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:16,130 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:55:16,130 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,131 - Step 22: Generated next token
2025-02-11 07:55:16,131 - Step 22: Updated current_ids
2025-02-11 07:55:16,131 - Step 22: Decoded token text:  the
2025-02-11 07:55:16,131 - Step 22: Updated current_phrase
2025-02-11 07:55:16,131 - Step 22: Created step_acts
2025-02-11 07:55:16,131 - Step 22: Added to generation_acts
2025-02-11 07:55:16,131 - Step 22: Updated recent_tokens
2025-02-11 07:55:16,133 - Step 22: Decoded current text
2025-02-11 07:55:16,133 - Step 22: Reset consecutive_fillers
2025-02-11 07:55:16,133 - Step 22: Calculated unique_ratio: 0.75
2025-02-11 07:55:16,133 - 
Starting step 23
2025-02-11 07:55:16,133 - Current_ids device: cuda:0
2025-02-11 07:55:16,133 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,160 - Model output complete
2025-02-11 07:55:16,160 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:55:16,160 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,161 - Next token logits device: cuda:0
2025-02-11 07:55:16,161 - Entered do_sample
2025-02-11 07:55:16,161 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,163 - Probs max: 0.385986328125
2025-02-11 07:55:16,164 - Pre-cat
2025-02-11 07:55:16,164 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   4201,    553,    279,   7002,     13,   2055,     11,    279]],
       device='cuda:0')
2025-02-11 07:55:16,167 - Next token: tensor([[4226]], device='cuda:0')
2025-02-11 07:55:16,167 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:55:16,167 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,167 - Step 23: Generated next token
2025-02-11 07:55:16,167 - Step 23: Updated current_ids
2025-02-11 07:55:16,168 - Step 23: Decoded token text:  answer
2025-02-11 07:55:16,168 - Step 23: Updated current_phrase
2025-02-11 07:55:16,168 - Step 23: Created step_acts
2025-02-11 07:55:16,168 - Step 23: Added to generation_acts
2025-02-11 07:55:16,168 - Step 23: Updated recent_tokens
2025-02-11 07:55:16,170 - Step 23: Decoded current text
2025-02-11 07:55:16,170 - Step 23: Reset consecutive_fillers
2025-02-11 07:55:16,170 - Step 23: Calculated unique_ratio: 0.8125
2025-02-11 07:55:16,170 - 
Starting step 24
2025-02-11 07:55:16,170 - Current_ids device: cuda:0
2025-02-11 07:55:16,170 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,193 - Model output complete
2025-02-11 07:55:16,194 - Logits shape: torch.Size([1, 55, 151936])
2025-02-11 07:55:16,194 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,194 - Next token logits device: cuda:0
2025-02-11 07:55:16,194 - Entered do_sample
2025-02-11 07:55:16,194 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,198 - Probs max: 0.916015625
2025-02-11 07:55:16,198 - Pre-cat
2025-02-11 07:55:16,198 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   4201,    553,    279,   7002,     13,   2055,     11,    279,
           4226]], device='cuda:0')
2025-02-11 07:55:16,200 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:16,201 - Current_ids shape: torch.Size([1, 55])
2025-02-11 07:55:16,201 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,201 - Step 24: Generated next token
2025-02-11 07:55:16,201 - Step 24: Updated current_ids
2025-02-11 07:55:16,201 - Step 24: Decoded token text:  is
2025-02-11 07:55:16,201 - Step 24: Updated current_phrase
2025-02-11 07:55:16,201 - Step 24: Created step_acts
2025-02-11 07:55:16,201 - Step 24: Added to generation_acts
2025-02-11 07:55:16,202 - Step 24: Updated recent_tokens
2025-02-11 07:55:16,203 - Step 24: Decoded current text
2025-02-11 07:55:16,203 - Step 24: Reset consecutive_fillers
2025-02-11 07:55:16,203 - Step 24: Calculated unique_ratio: 0.8125
2025-02-11 07:55:16,203 - 
Starting step 25
2025-02-11 07:55:16,203 - Current_ids device: cuda:0
2025-02-11 07:55:16,203 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,242 - Model output complete
2025-02-11 07:55:16,242 - Logits shape: torch.Size([1, 56, 151936])
2025-02-11 07:55:16,242 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,242 - Next token logits device: cuda:0
2025-02-11 07:55:16,242 - Entered do_sample
2025-02-11 07:55:16,242 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,245 - Probs max: 0.5029296875
2025-02-11 07:55:16,246 - Pre-cat
2025-02-11 07:55:16,246 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   4201,    553,    279,   7002,     13,   2055,     11,    279,
           4226,    374]], device='cuda:0')
2025-02-11 07:55:16,249 - Next token: tensor([[362]], device='cuda:0')
2025-02-11 07:55:16,249 - Current_ids shape: torch.Size([1, 56])
2025-02-11 07:55:16,249 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,249 - Step 25: Generated next token
2025-02-11 07:55:16,249 - Step 25: Updated current_ids
2025-02-11 07:55:16,250 - Step 25: Decoded token text:  A
2025-02-11 07:55:16,250 - Step 25: Updated current_phrase
2025-02-11 07:55:16,250 - Step 25: Created step_acts
2025-02-11 07:55:16,250 - Step 25: Added to generation_acts
2025-02-11 07:55:16,251 - Step 25: Updated generated_texts
2025-02-11 07:55:16,252 - Step 25: Updated recent_tokens
2025-02-11 07:55:16,252 - Step 25: Decoded current text
2025-02-11 07:55:16,252 - Step 25: Incremented consecutive_fillers to 1
2025-02-11 07:55:16,252 - Step 25: Calculated unique_ratio: 0.8125
2025-02-11 07:55:16,252 - 
Starting step 26
2025-02-11 07:55:16,252 - Current_ids device: cuda:0
2025-02-11 07:55:16,252 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,275 - Model output complete
2025-02-11 07:55:16,275 - Logits shape: torch.Size([1, 57, 151936])
2025-02-11 07:55:16,275 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,275 - Next token logits device: cuda:0
2025-02-11 07:55:16,275 - Entered do_sample
2025-02-11 07:55:16,275 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,280 - Probs max: 0.28369140625
2025-02-11 07:55:16,280 - Pre-cat
2025-02-11 07:55:16,280 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   4201,    553,    279,   7002,     13,   2055,     11,    279,
           4226,    374,    362]], device='cuda:0')
2025-02-11 07:55:16,283 - Next token: tensor([[25]], device='cuda:0')
2025-02-11 07:55:16,283 - Current_ids shape: torch.Size([1, 57])
2025-02-11 07:55:16,283 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,283 - Step 26: Generated next token
2025-02-11 07:55:16,283 - Step 26: Updated current_ids
2025-02-11 07:55:16,283 - Step 26: Decoded token text: :
2025-02-11 07:55:16,283 - Step 26: Updated current_phrase
2025-02-11 07:55:16,284 - Step 26: Created step_acts
2025-02-11 07:55:16,284 - Step 26: Added to generation_acts
2025-02-11 07:55:16,284 - Step 26: Updated recent_tokens
2025-02-11 07:55:16,285 - Step 26: Found phrase end token
2025-02-11 07:55:16,285 - Step 26: Updated recent_phrases
2025-02-11 07:55:16,286 - Step 26: Calculated similarity: 0.0
2025-02-11 07:55:16,286 - Step 26: Decoded current text
2025-02-11 07:55:16,286 - Step 26: Incremented consecutive_fillers to 2
2025-02-11 07:55:16,286 - Step 26: Calculated unique_ratio: 0.875
2025-02-11 07:55:16,286 - 
Starting step 27
2025-02-11 07:55:16,286 - Current_ids device: cuda:0
2025-02-11 07:55:16,286 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,313 - Model output complete
2025-02-11 07:55:16,314 - Logits shape: torch.Size([1, 58, 151936])
2025-02-11 07:55:16,314 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,314 - Next token logits device: cuda:0
2025-02-11 07:55:16,314 - Entered do_sample
2025-02-11 07:55:16,314 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,317 - Probs max: 0.89404296875
2025-02-11 07:55:16,318 - Pre-cat
2025-02-11 07:55:16,318 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   4201,    553,    279,   7002,     13,   2055,     11,    279,
           4226,    374,    362,     25]], device='cuda:0')
2025-02-11 07:55:16,320 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:55:16,320 - Current_ids shape: torch.Size([1, 58])
2025-02-11 07:55:16,320 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,320 - Step 27: Generated next token
2025-02-11 07:55:16,320 - Step 27: Updated current_ids
2025-02-11 07:55:16,320 - Step 27: Decoded token text:  If
2025-02-11 07:55:16,320 - Step 27: Updated current_phrase
2025-02-11 07:55:16,321 - Step 27: Created step_acts
2025-02-11 07:55:16,321 - Step 27: Added to generation_acts
2025-02-11 07:55:16,321 - Step 27: Updated recent_tokens
2025-02-11 07:55:16,323 - Step 27: Decoded current text
2025-02-11 07:55:16,323 - Step 27: Incremented consecutive_fillers to 3
2025-02-11 07:55:16,431 - 
Starting step 0
2025-02-11 07:55:16,432 - Current_ids device: cuda:0
2025-02-11 07:55:16,432 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,465 - Model output complete
2025-02-11 07:55:16,465 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:16,465 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,465 - Next token logits device: cuda:0
2025-02-11 07:55:16,465 - Entered do_sample
2025-02-11 07:55:16,466 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,468 - Probs max: 0.50341796875
2025-02-11 07:55:16,471 - Pre-cat
2025-02-11 07:55:16,471 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:16,475 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:16,475 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:16,475 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,475 - Step 0: Generated next token
2025-02-11 07:55:16,476 - Step 0: Updated current_ids
2025-02-11 07:55:16,476 - Step 0: Decoded token text:  The
2025-02-11 07:55:16,476 - Step 0: Updated current_phrase
2025-02-11 07:55:16,476 - Step 0: Created step_acts
2025-02-11 07:55:16,477 - Step 0: Added to generation_acts
2025-02-11 07:55:16,478 - Step 0: Updated generated_texts
2025-02-11 07:55:16,478 - Step 0: Updated recent_tokens
2025-02-11 07:55:16,478 - Step 0: Decoded current text
2025-02-11 07:55:16,478 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:16,478 - 
Starting step 1
2025-02-11 07:55:16,478 - Current_ids device: cuda:0
2025-02-11 07:55:16,479 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,507 - Model output complete
2025-02-11 07:55:16,507 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:16,507 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,507 - Next token logits device: cuda:0
2025-02-11 07:55:16,507 - Entered do_sample
2025-02-11 07:55:16,507 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,509 - Probs max: 0.83837890625
2025-02-11 07:55:16,510 - Pre-cat
2025-02-11 07:55:16,511 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:55:16,512 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:16,513 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:16,513 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,513 - Step 1: Generated next token
2025-02-11 07:55:16,513 - Step 1: Updated current_ids
2025-02-11 07:55:16,513 - Step 1: Decoded token text:  ball
2025-02-11 07:55:16,513 - Step 1: Updated current_phrase
2025-02-11 07:55:16,513 - Step 1: Created step_acts
2025-02-11 07:55:16,513 - Step 1: Added to generation_acts
2025-02-11 07:55:16,514 - Step 1: Updated recent_tokens
2025-02-11 07:55:16,515 - Step 1: Decoded current text
2025-02-11 07:55:16,515 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:16,515 - 
Starting step 2
2025-02-11 07:55:16,515 - Current_ids device: cuda:0
2025-02-11 07:55:16,515 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,542 - Model output complete
2025-02-11 07:55:16,542 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:16,542 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,542 - Next token logits device: cuda:0
2025-02-11 07:55:16,542 - Entered do_sample
2025-02-11 07:55:16,543 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,545 - Probs max: 0.583984375
2025-02-11 07:55:16,545 - Pre-cat
2025-02-11 07:55:16,545 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:55:16,547 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:16,547 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:16,547 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,547 - Step 2: Generated next token
2025-02-11 07:55:16,548 - Step 2: Updated current_ids
2025-02-11 07:55:16,548 - Step 2: Decoded token text:  will
2025-02-11 07:55:16,548 - Step 2: Updated current_phrase
2025-02-11 07:55:16,548 - Step 2: Created step_acts
2025-02-11 07:55:16,548 - Step 2: Added to generation_acts
2025-02-11 07:55:16,548 - Step 2: Updated recent_tokens
2025-02-11 07:55:16,551 - Step 2: Decoded current text
2025-02-11 07:55:16,551 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:16,551 - 
Starting step 3
2025-02-11 07:55:16,551 - Current_ids device: cuda:0
2025-02-11 07:55:16,551 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,575 - Model output complete
2025-02-11 07:55:16,575 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:16,575 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,575 - Next token logits device: cuda:0
2025-02-11 07:55:16,575 - Entered do_sample
2025-02-11 07:55:16,576 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,578 - Probs max: 0.56884765625
2025-02-11 07:55:16,579 - Pre-cat
2025-02-11 07:55:16,579 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686]],
       device='cuda:0')
2025-02-11 07:55:16,580 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:55:16,581 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:16,581 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,581 - Step 3: Generated next token
2025-02-11 07:55:16,581 - Step 3: Updated current_ids
2025-02-11 07:55:16,582 - Step 3: Decoded token text:  hit
2025-02-11 07:55:16,582 - Step 3: Updated current_phrase
2025-02-11 07:55:16,583 - Step 3: Created step_acts
2025-02-11 07:55:16,583 - Step 3: Added to generation_acts
2025-02-11 07:55:16,583 - Step 3: Updated recent_tokens
2025-02-11 07:55:16,584 - Step 3: Decoded current text
2025-02-11 07:55:16,584 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:16,584 - 
Starting step 4
2025-02-11 07:55:16,584 - Current_ids device: cuda:0
2025-02-11 07:55:16,584 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,608 - Model output complete
2025-02-11 07:55:16,608 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:16,608 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,609 - Next token logits device: cuda:0
2025-02-11 07:55:16,609 - Entered do_sample
2025-02-11 07:55:16,609 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,611 - Probs max: 0.99609375
2025-02-11 07:55:16,612 - Pre-cat
2025-02-11 07:55:16,612 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201]],
       device='cuda:0')
2025-02-11 07:55:16,615 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:16,615 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:16,615 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,615 - Step 4: Generated next token
2025-02-11 07:55:16,615 - Step 4: Updated current_ids
2025-02-11 07:55:16,615 - Step 4: Decoded token text:  the
2025-02-11 07:55:16,616 - Step 4: Updated current_phrase
2025-02-11 07:55:16,616 - Step 4: Created step_acts
2025-02-11 07:55:16,616 - Step 4: Added to generation_acts
2025-02-11 07:55:16,616 - Step 4: Updated recent_tokens
2025-02-11 07:55:16,617 - Step 4: Decoded current text
2025-02-11 07:55:16,617 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:16,617 - 
Starting step 5
2025-02-11 07:55:16,617 - Current_ids device: cuda:0
2025-02-11 07:55:16,618 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,663 - Model output complete
2025-02-11 07:55:16,663 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:16,663 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,663 - Next token logits device: cuda:0
2025-02-11 07:55:16,664 - Entered do_sample
2025-02-11 07:55:16,664 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,666 - Probs max: 0.99853515625
2025-02-11 07:55:16,667 - Pre-cat
2025-02-11 07:55:16,667 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279]],
       device='cuda:0')
2025-02-11 07:55:16,669 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:16,669 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:16,669 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,669 - Step 5: Generated next token
2025-02-11 07:55:16,669 - Step 5: Updated current_ids
2025-02-11 07:55:16,669 - Step 5: Decoded token text:  wall
2025-02-11 07:55:16,669 - Step 5: Updated current_phrase
2025-02-11 07:55:16,670 - Step 5: Created step_acts
2025-02-11 07:55:16,670 - Step 5: Added to generation_acts
2025-02-11 07:55:16,671 - Step 5: Updated generated_texts
2025-02-11 07:55:16,672 - Step 5: Updated recent_tokens
2025-02-11 07:55:16,672 - Step 5: Decoded current text
2025-02-11 07:55:16,672 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:16,673 - 
Starting step 6
2025-02-11 07:55:16,673 - Current_ids device: cuda:0
2025-02-11 07:55:16,673 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,707 - Model output complete
2025-02-11 07:55:16,708 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:16,708 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,708 - Next token logits device: cuda:0
2025-02-11 07:55:16,708 - Entered do_sample
2025-02-11 07:55:16,708 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,710 - Probs max: 0.376708984375
2025-02-11 07:55:16,711 - Pre-cat
2025-02-11 07:55:16,711 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002]], device='cuda:0')
2025-02-11 07:55:16,714 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:16,714 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:16,714 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,714 - Step 6: Generated next token
2025-02-11 07:55:16,715 - Step 6: Updated current_ids
2025-02-11 07:55:16,715 - Step 6: Decoded token text: ,
2025-02-11 07:55:16,715 - Step 6: Updated current_phrase
2025-02-11 07:55:16,715 - Step 6: Created step_acts
2025-02-11 07:55:16,715 - Step 6: Added to generation_acts
2025-02-11 07:55:16,716 - Step 6: Updated recent_tokens
2025-02-11 07:55:16,717 - Step 6: Found phrase end token
2025-02-11 07:55:16,717 - Step 6: Updated recent_phrases
2025-02-11 07:55:16,717 - Step 6: Decoded current text
2025-02-11 07:55:16,717 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:16,717 - 
Starting step 7
2025-02-11 07:55:16,717 - Current_ids device: cuda:0
2025-02-11 07:55:16,717 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,740 - Model output complete
2025-02-11 07:55:16,741 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:16,741 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,741 - Next token logits device: cuda:0
2025-02-11 07:55:16,741 - Entered do_sample
2025-02-11 07:55:16,741 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,743 - Probs max: 0.4912109375
2025-02-11 07:55:16,745 - Pre-cat
2025-02-11 07:55:16,745 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,     11]], device='cuda:0')
2025-02-11 07:55:16,746 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:55:16,747 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:16,747 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,747 - Step 7: Generated next token
2025-02-11 07:55:16,747 - Step 7: Updated current_ids
2025-02-11 07:55:16,747 - Step 7: Decoded token text:  and
2025-02-11 07:55:16,747 - Step 7: Updated current_phrase
2025-02-11 07:55:16,747 - Step 7: Created step_acts
2025-02-11 07:55:16,747 - Step 7: Added to generation_acts
2025-02-11 07:55:16,747 - Step 7: Updated recent_tokens
2025-02-11 07:55:16,749 - Step 7: Decoded current text
2025-02-11 07:55:16,749 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:16,749 - 
Starting step 8
2025-02-11 07:55:16,749 - Current_ids device: cuda:0
2025-02-11 07:55:16,749 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,772 - Model output complete
2025-02-11 07:55:16,772 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:16,772 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,772 - Next token logits device: cuda:0
2025-02-11 07:55:16,772 - Entered do_sample
2025-02-11 07:55:16,772 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,775 - Probs max: 0.6806640625
2025-02-11 07:55:16,776 - Pre-cat
2025-02-11 07:55:16,776 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,     11,    323]], device='cuda:0')
2025-02-11 07:55:16,778 - Next token: tensor([[1221]], device='cuda:0')
2025-02-11 07:55:16,778 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:16,778 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,779 - Step 8: Generated next token
2025-02-11 07:55:16,779 - Step 8: Updated current_ids
2025-02-11 07:55:16,779 - Step 8: Decoded token text:  then
2025-02-11 07:55:16,779 - Step 8: Updated current_phrase
2025-02-11 07:55:16,779 - Step 8: Created step_acts
2025-02-11 07:55:16,779 - Step 8: Added to generation_acts
2025-02-11 07:55:16,780 - Step 8: Updated recent_tokens
2025-02-11 07:55:16,781 - Step 8: Decoded current text
2025-02-11 07:55:16,781 - Step 8: Incremented consecutive_fillers to 1
2025-02-11 07:55:16,781 - 
Starting step 9
2025-02-11 07:55:16,781 - Current_ids device: cuda:0
2025-02-11 07:55:16,781 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,804 - Model output complete
2025-02-11 07:55:16,804 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:16,804 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,804 - Next token logits device: cuda:0
2025-02-11 07:55:16,804 - Entered do_sample
2025-02-11 07:55:16,804 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,807 - Probs max: 0.7080078125
2025-02-11 07:55:16,808 - Pre-cat
2025-02-11 07:55:16,808 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,     11,    323,   1221]], device='cuda:0')
2025-02-11 07:55:16,810 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:16,810 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:16,811 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,811 - Step 9: Generated next token
2025-02-11 07:55:16,811 - Step 9: Updated current_ids
2025-02-11 07:55:16,811 - Step 9: Decoded token text:  the
2025-02-11 07:55:16,811 - Step 9: Updated current_phrase
2025-02-11 07:55:16,811 - Step 9: Created step_acts
2025-02-11 07:55:16,811 - Step 9: Added to generation_acts
2025-02-11 07:55:16,812 - Step 9: Updated recent_tokens
2025-02-11 07:55:16,813 - Step 9: Decoded current text
2025-02-11 07:55:16,813 - Step 9: Incremented consecutive_fillers to 2
2025-02-11 07:55:16,813 - 
Starting step 10
2025-02-11 07:55:16,813 - Current_ids device: cuda:0
2025-02-11 07:55:16,813 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,836 - Model output complete
2025-02-11 07:55:16,836 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:16,836 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,836 - Next token logits device: cuda:0
2025-02-11 07:55:16,836 - Entered do_sample
2025-02-11 07:55:16,836 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,839 - Probs max: 0.94287109375
2025-02-11 07:55:16,840 - Pre-cat
2025-02-11 07:55:16,840 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,     11,    323,   1221,    279]], device='cuda:0')
2025-02-11 07:55:16,842 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:16,843 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:16,843 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,843 - Step 10: Generated next token
2025-02-11 07:55:16,843 - Step 10: Updated current_ids
2025-02-11 07:55:16,843 - Step 10: Decoded token text:  wall
2025-02-11 07:55:16,843 - Step 10: Updated current_phrase
2025-02-11 07:55:16,844 - Step 10: Created step_acts
2025-02-11 07:55:16,844 - Step 10: Added to generation_acts
2025-02-11 07:55:16,845 - Step 10: Updated generated_texts
2025-02-11 07:55:16,845 - Step 10: Updated recent_tokens
2025-02-11 07:55:16,845 - Step 10: Decoded current text
2025-02-11 07:55:16,846 - Step 10: Incremented consecutive_fillers to 3
2025-02-11 07:55:16,949 - 
Starting step 0
2025-02-11 07:55:16,949 - Current_ids device: cuda:0
2025-02-11 07:55:16,949 - Current_ids dtype: torch.int64
2025-02-11 07:55:16,976 - Model output complete
2025-02-11 07:55:16,976 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:16,976 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,976 - Next token logits device: cuda:0
2025-02-11 07:55:16,976 - Entered do_sample
2025-02-11 07:55:16,976 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:16,979 - Probs max: 0.50341796875
2025-02-11 07:55:16,979 - Pre-cat
2025-02-11 07:55:16,979 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:16,981 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:55:16,982 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:16,982 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:16,982 - Step 0: Generated next token
2025-02-11 07:55:16,982 - Step 0: Updated current_ids
2025-02-11 07:55:16,982 - Step 0: Decoded token text:  If
2025-02-11 07:55:16,982 - Step 0: Updated current_phrase
2025-02-11 07:55:16,983 - Step 0: Created step_acts
2025-02-11 07:55:16,983 - Step 0: Added to generation_acts
2025-02-11 07:55:16,984 - Step 0: Updated generated_texts
2025-02-11 07:55:16,984 - Step 0: Updated recent_tokens
2025-02-11 07:55:16,984 - Step 0: Decoded current text
2025-02-11 07:55:16,984 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:16,985 - 
Starting step 1
2025-02-11 07:55:16,985 - Current_ids device: cuda:0
2025-02-11 07:55:16,985 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,021 - Model output complete
2025-02-11 07:55:17,021 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:17,021 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,022 - Next token logits device: cuda:0
2025-02-11 07:55:17,022 - Entered do_sample
2025-02-11 07:55:17,022 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,024 - Probs max: 0.7099609375
2025-02-11 07:55:17,025 - Pre-cat
2025-02-11 07:55:17,025 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:55:17,027 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:17,027 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:17,027 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,027 - Step 1: Generated next token
2025-02-11 07:55:17,028 - Step 1: Updated current_ids
2025-02-11 07:55:17,028 - Step 1: Decoded token text:  a
2025-02-11 07:55:17,028 - Step 1: Updated current_phrase
2025-02-11 07:55:17,028 - Step 1: Created step_acts
2025-02-11 07:55:17,028 - Step 1: Added to generation_acts
2025-02-11 07:55:17,028 - Step 1: Updated recent_tokens
2025-02-11 07:55:17,030 - Step 1: Decoded current text
2025-02-11 07:55:17,030 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:17,030 - 
Starting step 2
2025-02-11 07:55:17,030 - Current_ids device: cuda:0
2025-02-11 07:55:17,030 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,079 - Model output complete
2025-02-11 07:55:17,079 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:17,079 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,079 - Next token logits device: cuda:0
2025-02-11 07:55:17,079 - Entered do_sample
2025-02-11 07:55:17,080 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,082 - Probs max: 0.9970703125
2025-02-11 07:55:17,084 - Pre-cat
2025-02-11 07:55:17,084 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264]], device='cuda:0')
2025-02-11 07:55:17,087 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:17,088 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:17,088 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,088 - Step 2: Generated next token
2025-02-11 07:55:17,088 - Step 2: Updated current_ids
2025-02-11 07:55:17,088 - Step 2: Decoded token text:  ball
2025-02-11 07:55:17,088 - Step 2: Updated current_phrase
2025-02-11 07:55:17,089 - Step 2: Created step_acts
2025-02-11 07:55:17,089 - Step 2: Added to generation_acts
2025-02-11 07:55:17,089 - Step 2: Updated recent_tokens
2025-02-11 07:55:17,090 - Step 2: Decoded current text
2025-02-11 07:55:17,090 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:17,090 - 
Starting step 3
2025-02-11 07:55:17,090 - Current_ids device: cuda:0
2025-02-11 07:55:17,090 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,120 - Model output complete
2025-02-11 07:55:17,120 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:17,120 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,120 - Next token logits device: cuda:0
2025-02-11 07:55:17,120 - Entered do_sample
2025-02-11 07:55:17,120 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,123 - Probs max: 0.99951171875
2025-02-11 07:55:17,124 - Pre-cat
2025-02-11 07:55:17,124 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935]],
       device='cuda:0')
2025-02-11 07:55:17,126 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:17,126 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:17,126 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,126 - Step 3: Generated next token
2025-02-11 07:55:17,126 - Step 3: Updated current_ids
2025-02-11 07:55:17,126 - Step 3: Decoded token text:  is
2025-02-11 07:55:17,126 - Step 3: Updated current_phrase
2025-02-11 07:55:17,127 - Step 3: Created step_acts
2025-02-11 07:55:17,127 - Step 3: Added to generation_acts
2025-02-11 07:55:17,127 - Step 3: Updated recent_tokens
2025-02-11 07:55:17,128 - Step 3: Decoded current text
2025-02-11 07:55:17,128 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:17,128 - 
Starting step 4
2025-02-11 07:55:17,128 - Current_ids device: cuda:0
2025-02-11 07:55:17,128 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,169 - Model output complete
2025-02-11 07:55:17,169 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:17,169 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,169 - Next token logits device: cuda:0
2025-02-11 07:55:17,169 - Entered do_sample
2025-02-11 07:55:17,170 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,171 - Probs max: 0.998046875
2025-02-11 07:55:17,172 - Pre-cat
2025-02-11 07:55:17,172 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:17,174 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:17,174 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:17,174 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,174 - Step 4: Generated next token
2025-02-11 07:55:17,174 - Step 4: Updated current_ids
2025-02-11 07:55:17,174 - Step 4: Decoded token text:  thrown
2025-02-11 07:55:17,174 - Step 4: Updated current_phrase
2025-02-11 07:55:17,175 - Step 4: Created step_acts
2025-02-11 07:55:17,175 - Step 4: Added to generation_acts
2025-02-11 07:55:17,175 - Step 4: Updated recent_tokens
2025-02-11 07:55:17,176 - Step 4: Decoded current text
2025-02-11 07:55:17,176 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:17,176 - 
Starting step 5
2025-02-11 07:55:17,176 - Current_ids device: cuda:0
2025-02-11 07:55:17,176 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,207 - Model output complete
2025-02-11 07:55:17,207 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:17,207 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,207 - Next token logits device: cuda:0
2025-02-11 07:55:17,207 - Entered do_sample
2025-02-11 07:55:17,207 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,209 - Probs max: 0.9697265625
2025-02-11 07:55:17,210 - Pre-cat
2025-02-11 07:55:17,210 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:55:17,211 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:55:17,212 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:17,212 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,212 - Step 5: Generated next token
2025-02-11 07:55:17,212 - Step 5: Updated current_ids
2025-02-11 07:55:17,212 - Step 5: Decoded token text:  at
2025-02-11 07:55:17,212 - Step 5: Updated current_phrase
2025-02-11 07:55:17,213 - Step 5: Created step_acts
2025-02-11 07:55:17,213 - Step 5: Added to generation_acts
2025-02-11 07:55:17,215 - Step 5: Updated generated_texts
2025-02-11 07:55:17,215 - Step 5: Updated recent_tokens
2025-02-11 07:55:17,215 - Step 5: Decoded current text
2025-02-11 07:55:17,215 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:17,215 - 
Starting step 6
2025-02-11 07:55:17,215 - Current_ids device: cuda:0
2025-02-11 07:55:17,215 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,257 - Model output complete
2025-02-11 07:55:17,257 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:17,257 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,257 - Next token logits device: cuda:0
2025-02-11 07:55:17,257 - Entered do_sample
2025-02-11 07:55:17,257 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,259 - Probs max: 0.99951171875
2025-02-11 07:55:17,261 - Pre-cat
2025-02-11 07:55:17,262 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:55:17,264 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:17,265 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:17,265 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,265 - Step 6: Generated next token
2025-02-11 07:55:17,265 - Step 6: Updated current_ids
2025-02-11 07:55:17,265 - Step 6: Decoded token text:  a
2025-02-11 07:55:17,265 - Step 6: Updated current_phrase
2025-02-11 07:55:17,266 - Step 6: Created step_acts
2025-02-11 07:55:17,266 - Step 6: Added to generation_acts
2025-02-11 07:55:17,266 - Step 6: Updated recent_tokens
2025-02-11 07:55:17,267 - Step 6: Decoded current text
2025-02-11 07:55:17,267 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:17,267 - 
Starting step 7
2025-02-11 07:55:17,267 - Current_ids device: cuda:0
2025-02-11 07:55:17,267 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,290 - Model output complete
2025-02-11 07:55:17,290 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:17,290 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,291 - Next token logits device: cuda:0
2025-02-11 07:55:17,291 - Entered do_sample
2025-02-11 07:55:17,291 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,293 - Probs max: 0.9990234375
2025-02-11 07:55:17,295 - Pre-cat
2025-02-11 07:55:17,295 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:55:17,297 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:17,297 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:17,297 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,297 - Step 7: Generated next token
2025-02-11 07:55:17,297 - Step 7: Updated current_ids
2025-02-11 07:55:17,297 - Step 7: Decoded token text:  wall
2025-02-11 07:55:17,297 - Step 7: Updated current_phrase
2025-02-11 07:55:17,298 - Step 7: Created step_acts
2025-02-11 07:55:17,298 - Step 7: Added to generation_acts
2025-02-11 07:55:17,298 - Step 7: Updated recent_tokens
2025-02-11 07:55:17,299 - Step 7: Decoded current text
2025-02-11 07:55:17,299 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:17,299 - 
Starting step 8
2025-02-11 07:55:17,299 - Current_ids device: cuda:0
2025-02-11 07:55:17,299 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,323 - Model output complete
2025-02-11 07:55:17,323 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:17,323 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,323 - Next token logits device: cuda:0
2025-02-11 07:55:17,323 - Entered do_sample
2025-02-11 07:55:17,323 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,326 - Probs max: 0.9912109375
2025-02-11 07:55:17,327 - Pre-cat
2025-02-11 07:55:17,327 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:55:17,328 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:17,329 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:17,329 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,329 - Step 8: Generated next token
2025-02-11 07:55:17,329 - Step 8: Updated current_ids
2025-02-11 07:55:17,329 - Step 8: Decoded token text:  very
2025-02-11 07:55:17,329 - Step 8: Updated current_phrase
2025-02-11 07:55:17,329 - Step 8: Created step_acts
2025-02-11 07:55:17,329 - Step 8: Added to generation_acts
2025-02-11 07:55:17,329 - Step 8: Updated recent_tokens
2025-02-11 07:55:17,331 - Step 8: Decoded current text
2025-02-11 07:55:17,331 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:17,331 - 
Starting step 9
2025-02-11 07:55:17,331 - Current_ids device: cuda:0
2025-02-11 07:55:17,331 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,397 - Model output complete
2025-02-11 07:55:17,397 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:17,397 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,398 - Next token logits device: cuda:0
2025-02-11 07:55:17,398 - Entered do_sample
2025-02-11 07:55:17,398 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,400 - Probs max: 0.998046875
2025-02-11 07:55:17,400 - Pre-cat
2025-02-11 07:55:17,400 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:55:17,402 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:55:17,402 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:17,402 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,402 - Step 9: Generated next token
2025-02-11 07:55:17,402 - Step 9: Updated current_ids
2025-02-11 07:55:17,402 - Step 9: Decoded token text:  fast
2025-02-11 07:55:17,402 - Step 9: Updated current_phrase
2025-02-11 07:55:17,403 - Step 9: Created step_acts
2025-02-11 07:55:17,403 - Step 9: Added to generation_acts
2025-02-11 07:55:17,403 - Step 9: Updated recent_tokens
2025-02-11 07:55:17,404 - Step 9: Decoded current text
2025-02-11 07:55:17,404 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:17,404 - 
Starting step 10
2025-02-11 07:55:17,404 - Current_ids device: cuda:0
2025-02-11 07:55:17,404 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,427 - Model output complete
2025-02-11 07:55:17,427 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:17,427 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,427 - Next token logits device: cuda:0
2025-02-11 07:55:17,427 - Entered do_sample
2025-02-11 07:55:17,427 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,430 - Probs max: 0.99853515625
2025-02-11 07:55:17,431 - Pre-cat
2025-02-11 07:55:17,431 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:55:17,432 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:17,433 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:17,433 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,433 - Step 10: Generated next token
2025-02-11 07:55:17,433 - Step 10: Updated current_ids
2025-02-11 07:55:17,433 - Step 10: Decoded token text: ,
2025-02-11 07:55:17,433 - Step 10: Updated current_phrase
2025-02-11 07:55:17,433 - Step 10: Created step_acts
2025-02-11 07:55:17,433 - Step 10: Added to generation_acts
2025-02-11 07:55:17,435 - Step 10: Updated generated_texts
2025-02-11 07:55:17,435 - Step 10: Updated recent_tokens
2025-02-11 07:55:17,435 - Step 10: Found phrase end token
2025-02-11 07:55:17,435 - Step 10: Updated recent_phrases
2025-02-11 07:55:17,435 - Step 10: Decoded current text
2025-02-11 07:55:17,435 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:17,435 - 
Starting step 11
2025-02-11 07:55:17,435 - Current_ids device: cuda:0
2025-02-11 07:55:17,436 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,468 - Model output complete
2025-02-11 07:55:17,468 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:17,468 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,468 - Next token logits device: cuda:0
2025-02-11 07:55:17,468 - Entered do_sample
2025-02-11 07:55:17,469 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,470 - Probs max: 0.58154296875
2025-02-11 07:55:17,471 - Pre-cat
2025-02-11 07:55:17,471 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:55:17,475 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:17,475 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:17,475 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,475 - Step 11: Generated next token
2025-02-11 07:55:17,475 - Step 11: Updated current_ids
2025-02-11 07:55:17,476 - Step 11: Decoded token text:  the
2025-02-11 07:55:17,476 - Step 11: Updated current_phrase
2025-02-11 07:55:17,476 - Step 11: Created step_acts
2025-02-11 07:55:17,476 - Step 11: Added to generation_acts
2025-02-11 07:55:17,476 - Step 11: Updated recent_tokens
2025-02-11 07:55:17,478 - Step 11: Decoded current text
2025-02-11 07:55:17,478 - Step 11: Reset consecutive_fillers
2025-02-11 07:55:17,478 - 
Starting step 12
2025-02-11 07:55:17,478 - Current_ids device: cuda:0
2025-02-11 07:55:17,478 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,508 - Model output complete
2025-02-11 07:55:17,508 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:17,508 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,508 - Next token logits device: cuda:0
2025-02-11 07:55:17,509 - Entered do_sample
2025-02-11 07:55:17,509 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,511 - Probs max: 0.6064453125
2025-02-11 07:55:17,513 - Pre-cat
2025-02-11 07:55:17,513 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279]],
       device='cuda:0')
2025-02-11 07:55:17,516 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:17,516 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:17,516 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,516 - Step 12: Generated next token
2025-02-11 07:55:17,516 - Step 12: Updated current_ids
2025-02-11 07:55:17,517 - Step 12: Decoded token text:  ball
2025-02-11 07:55:17,517 - Step 12: Updated current_phrase
2025-02-11 07:55:17,517 - Step 12: Created step_acts
2025-02-11 07:55:17,517 - Step 12: Added to generation_acts
2025-02-11 07:55:17,517 - Step 12: Updated recent_tokens
2025-02-11 07:55:17,519 - Step 12: Decoded current text
2025-02-11 07:55:17,519 - Step 12: Reset consecutive_fillers
2025-02-11 07:55:17,519 - 
Starting step 13
2025-02-11 07:55:17,519 - Current_ids device: cuda:0
2025-02-11 07:55:17,519 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,541 - Model output complete
2025-02-11 07:55:17,541 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:17,541 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,541 - Next token logits device: cuda:0
2025-02-11 07:55:17,541 - Entered do_sample
2025-02-11 07:55:17,542 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,544 - Probs max: 0.76953125
2025-02-11 07:55:17,545 - Pre-cat
2025-02-11 07:55:17,545 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935]],
       device='cuda:0')
2025-02-11 07:55:17,547 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:17,547 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:17,547 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,547 - Step 13: Generated next token
2025-02-11 07:55:17,547 - Step 13: Updated current_ids
2025-02-11 07:55:17,547 - Step 13: Decoded token text:  will
2025-02-11 07:55:17,547 - Step 13: Updated current_phrase
2025-02-11 07:55:17,548 - Step 13: Created step_acts
2025-02-11 07:55:17,548 - Step 13: Added to generation_acts
2025-02-11 07:55:17,548 - Step 13: Updated recent_tokens
2025-02-11 07:55:17,549 - Step 13: Decoded current text
2025-02-11 07:55:17,549 - Step 13: Reset consecutive_fillers
2025-02-11 07:55:17,549 - 
Starting step 14
2025-02-11 07:55:17,549 - Current_ids device: cuda:0
2025-02-11 07:55:17,549 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,571 - Model output complete
2025-02-11 07:55:17,571 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:55:17,571 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,571 - Next token logits device: cuda:0
2025-02-11 07:55:17,571 - Entered do_sample
2025-02-11 07:55:17,572 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,575 - Probs max: 0.43408203125
2025-02-11 07:55:17,576 - Pre-cat
2025-02-11 07:55:17,576 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686]],
       device='cuda:0')
2025-02-11 07:55:17,578 - Next token: tensor([[537]], device='cuda:0')
2025-02-11 07:55:17,579 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:55:17,579 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,579 - Step 14: Generated next token
2025-02-11 07:55:17,579 - Step 14: Updated current_ids
2025-02-11 07:55:17,579 - Step 14: Decoded token text:  not
2025-02-11 07:55:17,579 - Step 14: Updated current_phrase
2025-02-11 07:55:17,579 - Step 14: Created step_acts
2025-02-11 07:55:17,579 - Step 14: Added to generation_acts
2025-02-11 07:55:17,579 - Step 14: Updated recent_tokens
2025-02-11 07:55:17,581 - Step 14: Decoded current text
2025-02-11 07:55:17,581 - Step 14: Reset consecutive_fillers
2025-02-11 07:55:17,581 - 
Starting step 15
2025-02-11 07:55:17,581 - Current_ids device: cuda:0
2025-02-11 07:55:17,581 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,604 - Model output complete
2025-02-11 07:55:17,605 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:55:17,605 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,605 - Next token logits device: cuda:0
2025-02-11 07:55:17,605 - Entered do_sample
2025-02-11 07:55:17,605 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,608 - Probs max: 0.351318359375
2025-02-11 07:55:17,608 - Pre-cat
2025-02-11 07:55:17,608 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            537]], device='cuda:0')
2025-02-11 07:55:17,610 - Next token: tensor([[1172]], device='cuda:0')
2025-02-11 07:55:17,610 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:55:17,610 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,611 - Step 15: Generated next token
2025-02-11 07:55:17,611 - Step 15: Updated current_ids
2025-02-11 07:55:17,611 - Step 15: Decoded token text:  only
2025-02-11 07:55:17,611 - Step 15: Updated current_phrase
2025-02-11 07:55:17,611 - Step 15: Created step_acts
2025-02-11 07:55:17,611 - Step 15: Added to generation_acts
2025-02-11 07:55:17,613 - Step 15: Updated generated_texts
2025-02-11 07:55:17,613 - Step 15: Updated recent_tokens
2025-02-11 07:55:17,613 - Step 15: Decoded current text
2025-02-11 07:55:17,613 - Step 15: Reset consecutive_fillers
2025-02-11 07:55:17,613 - 
Starting step 16
2025-02-11 07:55:17,613 - Current_ids device: cuda:0
2025-02-11 07:55:17,613 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,635 - Model output complete
2025-02-11 07:55:17,635 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:55:17,635 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,635 - Next token logits device: cuda:0
2025-02-11 07:55:17,635 - Entered do_sample
2025-02-11 07:55:17,635 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,638 - Probs max: 0.189697265625
2025-02-11 07:55:17,739 - 
Starting step 0
2025-02-11 07:55:17,740 - Current_ids device: cuda:0
2025-02-11 07:55:17,740 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,766 - Model output complete
2025-02-11 07:55:17,766 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:17,766 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,766 - Next token logits device: cuda:0
2025-02-11 07:55:17,766 - Entered do_sample
2025-02-11 07:55:17,766 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,769 - Probs max: 0.50341796875
2025-02-11 07:55:17,769 - Pre-cat
2025-02-11 07:55:17,769 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:17,771 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:55:17,771 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:17,771 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,771 - Step 0: Generated next token
2025-02-11 07:55:17,771 - Step 0: Updated current_ids
2025-02-11 07:55:17,771 - Step 0: Decoded token text:  If
2025-02-11 07:55:17,772 - Step 0: Updated current_phrase
2025-02-11 07:55:17,772 - Step 0: Created step_acts
2025-02-11 07:55:17,772 - Step 0: Added to generation_acts
2025-02-11 07:55:17,773 - Step 0: Updated generated_texts
2025-02-11 07:55:17,774 - Step 0: Updated recent_tokens
2025-02-11 07:55:17,774 - Step 0: Decoded current text
2025-02-11 07:55:17,774 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:17,774 - 
Starting step 1
2025-02-11 07:55:17,774 - Current_ids device: cuda:0
2025-02-11 07:55:17,775 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,801 - Model output complete
2025-02-11 07:55:17,801 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:17,801 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,802 - Next token logits device: cuda:0
2025-02-11 07:55:17,802 - Entered do_sample
2025-02-11 07:55:17,802 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,805 - Probs max: 0.7099609375
2025-02-11 07:55:17,806 - Pre-cat
2025-02-11 07:55:17,806 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:55:17,808 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:17,808 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:17,808 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,808 - Step 1: Generated next token
2025-02-11 07:55:17,809 - Step 1: Updated current_ids
2025-02-11 07:55:17,809 - Step 1: Decoded token text:  a
2025-02-11 07:55:17,809 - Step 1: Updated current_phrase
2025-02-11 07:55:17,809 - Step 1: Created step_acts
2025-02-11 07:55:17,809 - Step 1: Added to generation_acts
2025-02-11 07:55:17,809 - Step 1: Updated recent_tokens
2025-02-11 07:55:17,810 - Step 1: Decoded current text
2025-02-11 07:55:17,811 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:17,811 - 
Starting step 2
2025-02-11 07:55:17,811 - Current_ids device: cuda:0
2025-02-11 07:55:17,811 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,834 - Model output complete
2025-02-11 07:55:17,834 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:17,834 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,834 - Next token logits device: cuda:0
2025-02-11 07:55:17,834 - Entered do_sample
2025-02-11 07:55:17,835 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,837 - Probs max: 0.9970703125
2025-02-11 07:55:17,837 - Pre-cat
2025-02-11 07:55:17,837 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264]], device='cuda:0')
2025-02-11 07:55:17,839 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:17,839 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:17,839 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,839 - Step 2: Generated next token
2025-02-11 07:55:17,839 - Step 2: Updated current_ids
2025-02-11 07:55:17,839 - Step 2: Decoded token text:  ball
2025-02-11 07:55:17,840 - Step 2: Updated current_phrase
2025-02-11 07:55:17,840 - Step 2: Created step_acts
2025-02-11 07:55:17,840 - Step 2: Added to generation_acts
2025-02-11 07:55:17,840 - Step 2: Updated recent_tokens
2025-02-11 07:55:17,841 - Step 2: Decoded current text
2025-02-11 07:55:17,841 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:17,841 - 
Starting step 3
2025-02-11 07:55:17,841 - Current_ids device: cuda:0
2025-02-11 07:55:17,841 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,865 - Model output complete
2025-02-11 07:55:17,865 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:17,865 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,865 - Next token logits device: cuda:0
2025-02-11 07:55:17,865 - Entered do_sample
2025-02-11 07:55:17,865 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,868 - Probs max: 0.99951171875
2025-02-11 07:55:17,869 - Pre-cat
2025-02-11 07:55:17,869 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935]],
       device='cuda:0')
2025-02-11 07:55:17,870 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:17,871 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:17,871 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,871 - Step 3: Generated next token
2025-02-11 07:55:17,871 - Step 3: Updated current_ids
2025-02-11 07:55:17,871 - Step 3: Decoded token text:  is
2025-02-11 07:55:17,871 - Step 3: Updated current_phrase
2025-02-11 07:55:17,871 - Step 3: Created step_acts
2025-02-11 07:55:17,871 - Step 3: Added to generation_acts
2025-02-11 07:55:17,871 - Step 3: Updated recent_tokens
2025-02-11 07:55:17,873 - Step 3: Decoded current text
2025-02-11 07:55:17,873 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:17,873 - 
Starting step 4
2025-02-11 07:55:17,873 - Current_ids device: cuda:0
2025-02-11 07:55:17,873 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,895 - Model output complete
2025-02-11 07:55:17,895 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:17,895 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,895 - Next token logits device: cuda:0
2025-02-11 07:55:17,895 - Entered do_sample
2025-02-11 07:55:17,896 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,899 - Probs max: 0.998046875
2025-02-11 07:55:17,899 - Pre-cat
2025-02-11 07:55:17,899 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:17,901 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:17,901 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:17,901 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,901 - Step 4: Generated next token
2025-02-11 07:55:17,901 - Step 4: Updated current_ids
2025-02-11 07:55:17,902 - Step 4: Decoded token text:  thrown
2025-02-11 07:55:17,902 - Step 4: Updated current_phrase
2025-02-11 07:55:17,902 - Step 4: Created step_acts
2025-02-11 07:55:17,902 - Step 4: Added to generation_acts
2025-02-11 07:55:17,902 - Step 4: Updated recent_tokens
2025-02-11 07:55:17,903 - Step 4: Decoded current text
2025-02-11 07:55:17,903 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:17,903 - 
Starting step 5
2025-02-11 07:55:17,904 - Current_ids device: cuda:0
2025-02-11 07:55:17,904 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,934 - Model output complete
2025-02-11 07:55:17,934 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:17,935 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,935 - Next token logits device: cuda:0
2025-02-11 07:55:17,935 - Entered do_sample
2025-02-11 07:55:17,935 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,937 - Probs max: 0.9697265625
2025-02-11 07:55:17,939 - Pre-cat
2025-02-11 07:55:17,939 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:55:17,942 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:55:17,942 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:17,942 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,942 - Step 5: Generated next token
2025-02-11 07:55:17,942 - Step 5: Updated current_ids
2025-02-11 07:55:17,943 - Step 5: Decoded token text:  at
2025-02-11 07:55:17,943 - Step 5: Updated current_phrase
2025-02-11 07:55:17,943 - Step 5: Created step_acts
2025-02-11 07:55:17,943 - Step 5: Added to generation_acts
2025-02-11 07:55:17,945 - Step 5: Updated generated_texts
2025-02-11 07:55:17,945 - Step 5: Updated recent_tokens
2025-02-11 07:55:17,945 - Step 5: Decoded current text
2025-02-11 07:55:17,945 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:17,945 - 
Starting step 6
2025-02-11 07:55:17,945 - Current_ids device: cuda:0
2025-02-11 07:55:17,945 - Current_ids dtype: torch.int64
2025-02-11 07:55:17,972 - Model output complete
2025-02-11 07:55:17,972 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:17,972 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,972 - Next token logits device: cuda:0
2025-02-11 07:55:17,972 - Entered do_sample
2025-02-11 07:55:17,972 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:17,975 - Probs max: 0.99951171875
2025-02-11 07:55:17,975 - Pre-cat
2025-02-11 07:55:17,975 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518]], device='cuda:0')
2025-02-11 07:55:17,977 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:17,977 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:17,977 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:17,977 - Step 6: Generated next token
2025-02-11 07:55:17,977 - Step 6: Updated current_ids
2025-02-11 07:55:17,978 - Step 6: Decoded token text:  a
2025-02-11 07:55:17,978 - Step 6: Updated current_phrase
2025-02-11 07:55:17,978 - Step 6: Created step_acts
2025-02-11 07:55:17,978 - Step 6: Added to generation_acts
2025-02-11 07:55:17,978 - Step 6: Updated recent_tokens
2025-02-11 07:55:17,979 - Step 6: Decoded current text
2025-02-11 07:55:17,979 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:17,979 - 
Starting step 7
2025-02-11 07:55:17,980 - Current_ids device: cuda:0
2025-02-11 07:55:17,980 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,003 - Model output complete
2025-02-11 07:55:18,003 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:18,003 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,003 - Next token logits device: cuda:0
2025-02-11 07:55:18,004 - Entered do_sample
2025-02-11 07:55:18,004 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,006 - Probs max: 0.9990234375
2025-02-11 07:55:18,008 - Pre-cat
2025-02-11 07:55:18,008 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264]], device='cuda:0')
2025-02-11 07:55:18,011 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:18,011 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:18,011 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,011 - Step 7: Generated next token
2025-02-11 07:55:18,011 - Step 7: Updated current_ids
2025-02-11 07:55:18,011 - Step 7: Decoded token text:  wall
2025-02-11 07:55:18,011 - Step 7: Updated current_phrase
2025-02-11 07:55:18,012 - Step 7: Created step_acts
2025-02-11 07:55:18,012 - Step 7: Added to generation_acts
2025-02-11 07:55:18,012 - Step 7: Updated recent_tokens
2025-02-11 07:55:18,013 - Step 7: Decoded current text
2025-02-11 07:55:18,014 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:18,014 - 
Starting step 8
2025-02-11 07:55:18,014 - Current_ids device: cuda:0
2025-02-11 07:55:18,014 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,045 - Model output complete
2025-02-11 07:55:18,045 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:18,045 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,045 - Next token logits device: cuda:0
2025-02-11 07:55:18,045 - Entered do_sample
2025-02-11 07:55:18,045 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,049 - Probs max: 0.9912109375
2025-02-11 07:55:18,049 - Pre-cat
2025-02-11 07:55:18,049 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002]], device='cuda:0')
2025-02-11 07:55:18,051 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:18,051 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:18,051 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,051 - Step 8: Generated next token
2025-02-11 07:55:18,051 - Step 8: Updated current_ids
2025-02-11 07:55:18,051 - Step 8: Decoded token text:  very
2025-02-11 07:55:18,052 - Step 8: Updated current_phrase
2025-02-11 07:55:18,052 - Step 8: Created step_acts
2025-02-11 07:55:18,052 - Step 8: Added to generation_acts
2025-02-11 07:55:18,052 - Step 8: Updated recent_tokens
2025-02-11 07:55:18,053 - Step 8: Decoded current text
2025-02-11 07:55:18,053 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:18,053 - 
Starting step 9
2025-02-11 07:55:18,053 - Current_ids device: cuda:0
2025-02-11 07:55:18,053 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,116 - Model output complete
2025-02-11 07:55:18,116 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:18,116 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,116 - Next token logits device: cuda:0
2025-02-11 07:55:18,116 - Entered do_sample
2025-02-11 07:55:18,116 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,119 - Probs max: 0.998046875
2025-02-11 07:55:18,119 - Pre-cat
2025-02-11 07:55:18,119 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602]], device='cuda:0')
2025-02-11 07:55:18,121 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:55:18,121 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:18,121 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,121 - Step 9: Generated next token
2025-02-11 07:55:18,122 - Step 9: Updated current_ids
2025-02-11 07:55:18,122 - Step 9: Decoded token text:  fast
2025-02-11 07:55:18,122 - Step 9: Updated current_phrase
2025-02-11 07:55:18,122 - Step 9: Created step_acts
2025-02-11 07:55:18,122 - Step 9: Added to generation_acts
2025-02-11 07:55:18,122 - Step 9: Updated recent_tokens
2025-02-11 07:55:18,124 - Step 9: Decoded current text
2025-02-11 07:55:18,124 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:18,124 - 
Starting step 10
2025-02-11 07:55:18,124 - Current_ids device: cuda:0
2025-02-11 07:55:18,124 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,147 - Model output complete
2025-02-11 07:55:18,147 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:18,147 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,147 - Next token logits device: cuda:0
2025-02-11 07:55:18,147 - Entered do_sample
2025-02-11 07:55:18,147 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,150 - Probs max: 0.99853515625
2025-02-11 07:55:18,150 - Pre-cat
2025-02-11 07:55:18,151 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:55:18,152 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:18,153 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:18,153 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,153 - Step 10: Generated next token
2025-02-11 07:55:18,153 - Step 10: Updated current_ids
2025-02-11 07:55:18,153 - Step 10: Decoded token text: ,
2025-02-11 07:55:18,153 - Step 10: Updated current_phrase
2025-02-11 07:55:18,153 - Step 10: Created step_acts
2025-02-11 07:55:18,153 - Step 10: Added to generation_acts
2025-02-11 07:55:18,155 - Step 10: Updated generated_texts
2025-02-11 07:55:18,155 - Step 10: Updated recent_tokens
2025-02-11 07:55:18,155 - Step 10: Found phrase end token
2025-02-11 07:55:18,155 - Step 10: Updated recent_phrases
2025-02-11 07:55:18,155 - Step 10: Decoded current text
2025-02-11 07:55:18,155 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:18,155 - 
Starting step 11
2025-02-11 07:55:18,155 - Current_ids device: cuda:0
2025-02-11 07:55:18,155 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,177 - Model output complete
2025-02-11 07:55:18,177 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:18,178 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,178 - Next token logits device: cuda:0
2025-02-11 07:55:18,178 - Entered do_sample
2025-02-11 07:55:18,178 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,181 - Probs max: 0.58154296875
2025-02-11 07:55:18,181 - Pre-cat
2025-02-11 07:55:18,181 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:55:18,184 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:18,184 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:18,184 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,185 - Step 11: Generated next token
2025-02-11 07:55:18,185 - Step 11: Updated current_ids
2025-02-11 07:55:18,185 - Step 11: Decoded token text:  the
2025-02-11 07:55:18,185 - Step 11: Updated current_phrase
2025-02-11 07:55:18,185 - Step 11: Created step_acts
2025-02-11 07:55:18,186 - Step 11: Added to generation_acts
2025-02-11 07:55:18,186 - Step 11: Updated recent_tokens
2025-02-11 07:55:18,188 - Step 11: Decoded current text
2025-02-11 07:55:18,188 - Step 11: Reset consecutive_fillers
2025-02-11 07:55:18,188 - 
Starting step 12
2025-02-11 07:55:18,188 - Current_ids device: cuda:0
2025-02-11 07:55:18,188 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,213 - Model output complete
2025-02-11 07:55:18,213 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:18,213 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,213 - Next token logits device: cuda:0
2025-02-11 07:55:18,213 - Entered do_sample
2025-02-11 07:55:18,213 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,216 - Probs max: 0.6064453125
2025-02-11 07:55:18,217 - Pre-cat
2025-02-11 07:55:18,217 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279]],
       device='cuda:0')
2025-02-11 07:55:18,218 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:18,219 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:18,219 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,219 - Step 12: Generated next token
2025-02-11 07:55:18,219 - Step 12: Updated current_ids
2025-02-11 07:55:18,219 - Step 12: Decoded token text:  ball
2025-02-11 07:55:18,219 - Step 12: Updated current_phrase
2025-02-11 07:55:18,219 - Step 12: Created step_acts
2025-02-11 07:55:18,220 - Step 12: Added to generation_acts
2025-02-11 07:55:18,220 - Step 12: Updated recent_tokens
2025-02-11 07:55:18,221 - Step 12: Decoded current text
2025-02-11 07:55:18,222 - Step 12: Reset consecutive_fillers
2025-02-11 07:55:18,222 - 
Starting step 13
2025-02-11 07:55:18,222 - Current_ids device: cuda:0
2025-02-11 07:55:18,222 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,244 - Model output complete
2025-02-11 07:55:18,244 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:18,244 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,244 - Next token logits device: cuda:0
2025-02-11 07:55:18,245 - Entered do_sample
2025-02-11 07:55:18,245 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,248 - Probs max: 0.76953125
2025-02-11 07:55:18,250 - Pre-cat
2025-02-11 07:55:18,250 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935]],
       device='cuda:0')
2025-02-11 07:55:18,253 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:18,254 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:18,254 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,254 - Step 13: Generated next token
2025-02-11 07:55:18,254 - Step 13: Updated current_ids
2025-02-11 07:55:18,254 - Step 13: Decoded token text:  will
2025-02-11 07:55:18,254 - Step 13: Updated current_phrase
2025-02-11 07:55:18,255 - Step 13: Created step_acts
2025-02-11 07:55:18,255 - Step 13: Added to generation_acts
2025-02-11 07:55:18,255 - Step 13: Updated recent_tokens
2025-02-11 07:55:18,256 - Step 13: Decoded current text
2025-02-11 07:55:18,256 - Step 13: Reset consecutive_fillers
2025-02-11 07:55:18,256 - 
Starting step 14
2025-02-11 07:55:18,257 - Current_ids device: cuda:0
2025-02-11 07:55:18,257 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,280 - Model output complete
2025-02-11 07:55:18,280 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:55:18,280 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,280 - Next token logits device: cuda:0
2025-02-11 07:55:18,280 - Entered do_sample
2025-02-11 07:55:18,280 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,283 - Probs max: 0.43408203125
2025-02-11 07:55:18,284 - Pre-cat
2025-02-11 07:55:18,284 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686]],
       device='cuda:0')
2025-02-11 07:55:18,286 - Next token: tensor([[387]], device='cuda:0')
2025-02-11 07:55:18,286 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:55:18,286 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,286 - Step 14: Generated next token
2025-02-11 07:55:18,286 - Step 14: Updated current_ids
2025-02-11 07:55:18,287 - Step 14: Decoded token text:  be
2025-02-11 07:55:18,287 - Step 14: Updated current_phrase
2025-02-11 07:55:18,287 - Step 14: Created step_acts
2025-02-11 07:55:18,287 - Step 14: Added to generation_acts
2025-02-11 07:55:18,287 - Step 14: Updated recent_tokens
2025-02-11 07:55:18,288 - Step 14: Decoded current text
2025-02-11 07:55:18,289 - Step 14: Reset consecutive_fillers
2025-02-11 07:55:18,289 - 
Starting step 15
2025-02-11 07:55:18,289 - Current_ids device: cuda:0
2025-02-11 07:55:18,289 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,310 - Model output complete
2025-02-11 07:55:18,310 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:55:18,310 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,310 - Next token logits device: cuda:0
2025-02-11 07:55:18,310 - Entered do_sample
2025-02-11 07:55:18,310 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,314 - Probs max: 0.533203125
2025-02-11 07:55:18,315 - Pre-cat
2025-02-11 07:55:18,315 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387]], device='cuda:0')
2025-02-11 07:55:18,316 - Next token: tensor([[7218]], device='cuda:0')
2025-02-11 07:55:18,317 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:55:18,317 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,317 - Step 15: Generated next token
2025-02-11 07:55:18,317 - Step 15: Updated current_ids
2025-02-11 07:55:18,317 - Step 15: Decoded token text:  moving
2025-02-11 07:55:18,317 - Step 15: Updated current_phrase
2025-02-11 07:55:18,317 - Step 15: Created step_acts
2025-02-11 07:55:18,317 - Step 15: Added to generation_acts
2025-02-11 07:55:18,319 - Step 15: Updated generated_texts
2025-02-11 07:55:18,319 - Step 15: Updated recent_tokens
2025-02-11 07:55:18,319 - Step 15: Decoded current text
2025-02-11 07:55:18,319 - Step 15: Reset consecutive_fillers
2025-02-11 07:55:18,319 - 
Starting step 16
2025-02-11 07:55:18,319 - Current_ids device: cuda:0
2025-02-11 07:55:18,319 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,369 - Model output complete
2025-02-11 07:55:18,370 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:55:18,370 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,370 - Next token logits device: cuda:0
2025-02-11 07:55:18,370 - Entered do_sample
2025-02-11 07:55:18,370 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,372 - Probs max: 0.42822265625
2025-02-11 07:55:18,374 - Pre-cat
2025-02-11 07:55:18,374 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218]], device='cuda:0')
2025-02-11 07:55:18,378 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:18,378 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:55:18,379 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,379 - Step 16: Generated next token
2025-02-11 07:55:18,379 - Step 16: Updated current_ids
2025-02-11 07:55:18,379 - Step 16: Decoded token text:  very
2025-02-11 07:55:18,379 - Step 16: Updated current_phrase
2025-02-11 07:55:18,380 - Step 16: Created step_acts
2025-02-11 07:55:18,380 - Step 16: Added to generation_acts
2025-02-11 07:55:18,380 - Step 16: Updated recent_tokens
2025-02-11 07:55:18,382 - Step 16: Decoded current text
2025-02-11 07:55:18,382 - Step 16: Reset consecutive_fillers
2025-02-11 07:55:18,383 - Step 16: Calculated unique_ratio: 0.8125
2025-02-11 07:55:18,383 - 
Starting step 17
2025-02-11 07:55:18,383 - Current_ids device: cuda:0
2025-02-11 07:55:18,383 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,463 - Model output complete
2025-02-11 07:55:18,464 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:55:18,464 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,464 - Next token logits device: cuda:0
2025-02-11 07:55:18,464 - Entered do_sample
2025-02-11 07:55:18,464 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,466 - Probs max: 0.77978515625
2025-02-11 07:55:18,467 - Pre-cat
2025-02-11 07:55:18,468 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,   1602]], device='cuda:0')
2025-02-11 07:55:18,471 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:55:18,472 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:55:18,472 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,472 - Step 17: Generated next token
2025-02-11 07:55:18,472 - Step 17: Updated current_ids
2025-02-11 07:55:18,472 - Step 17: Decoded token text:  fast
2025-02-11 07:55:18,472 - Step 17: Updated current_phrase
2025-02-11 07:55:18,473 - Step 17: Created step_acts
2025-02-11 07:55:18,473 - Step 17: Added to generation_acts
2025-02-11 07:55:18,473 - Step 17: Updated recent_tokens
2025-02-11 07:55:18,474 - Step 17: Decoded current text
2025-02-11 07:55:18,475 - Step 17: Reset consecutive_fillers
2025-02-11 07:55:18,475 - Step 17: Calculated unique_ratio: 0.8125
2025-02-11 07:55:18,475 - 
Starting step 18
2025-02-11 07:55:18,475 - Current_ids device: cuda:0
2025-02-11 07:55:18,475 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,512 - Model output complete
2025-02-11 07:55:18,512 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:55:18,512 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,512 - Next token logits device: cuda:0
2025-02-11 07:55:18,512 - Entered do_sample
2025-02-11 07:55:18,512 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,515 - Probs max: 0.390380859375
2025-02-11 07:55:18,516 - Pre-cat
2025-02-11 07:55:18,516 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,   1602,   4937]], device='cuda:0')
2025-02-11 07:55:18,518 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:55:18,518 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:55:18,518 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,519 - Step 18: Generated next token
2025-02-11 07:55:18,519 - Step 18: Updated current_ids
2025-02-11 07:55:18,519 - Step 18: Decoded token text:  and
2025-02-11 07:55:18,519 - Step 18: Updated current_phrase
2025-02-11 07:55:18,520 - Step 18: Created step_acts
2025-02-11 07:55:18,520 - Step 18: Added to generation_acts
2025-02-11 07:55:18,520 - Step 18: Updated recent_tokens
2025-02-11 07:55:18,522 - Step 18: Decoded current text
2025-02-11 07:55:18,522 - Step 18: Reset consecutive_fillers
2025-02-11 07:55:18,522 - Step 18: Calculated unique_ratio: 0.875
2025-02-11 07:55:18,522 - 
Starting step 19
2025-02-11 07:55:18,523 - Current_ids device: cuda:0
2025-02-11 07:55:18,523 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,560 - Model output complete
2025-02-11 07:55:18,560 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:55:18,560 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,560 - Next token logits device: cuda:0
2025-02-11 07:55:18,560 - Entered do_sample
2025-02-11 07:55:18,561 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,563 - Probs max: 0.57568359375
2025-02-11 07:55:18,564 - Pre-cat
2025-02-11 07:55:18,565 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,   1602,   4937,    323]], device='cuda:0')
2025-02-11 07:55:18,568 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:18,569 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:55:18,569 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,569 - Step 19: Generated next token
2025-02-11 07:55:18,569 - Step 19: Updated current_ids
2025-02-11 07:55:18,569 - Step 19: Decoded token text:  the
2025-02-11 07:55:18,569 - Step 19: Updated current_phrase
2025-02-11 07:55:18,570 - Step 19: Created step_acts
2025-02-11 07:55:18,570 - Step 19: Added to generation_acts
2025-02-11 07:55:18,570 - Step 19: Updated recent_tokens
2025-02-11 07:55:18,571 - Step 19: Decoded current text
2025-02-11 07:55:18,571 - Step 19: Incremented consecutive_fillers to 1
2025-02-11 07:55:18,571 - Step 19: Calculated unique_ratio: 0.8125
2025-02-11 07:55:18,571 - 
Starting step 20
2025-02-11 07:55:18,572 - Current_ids device: cuda:0
2025-02-11 07:55:18,572 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,641 - Model output complete
2025-02-11 07:55:18,641 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:55:18,641 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,641 - Next token logits device: cuda:0
2025-02-11 07:55:18,642 - Entered do_sample
2025-02-11 07:55:18,642 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,644 - Probs max: 0.93701171875
2025-02-11 07:55:18,645 - Pre-cat
2025-02-11 07:55:18,645 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,   1602,   4937,    323,    279]], device='cuda:0')
2025-02-11 07:55:18,647 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:18,647 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:55:18,647 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,647 - Step 20: Generated next token
2025-02-11 07:55:18,647 - Step 20: Updated current_ids
2025-02-11 07:55:18,647 - Step 20: Decoded token text:  wall
2025-02-11 07:55:18,647 - Step 20: Updated current_phrase
2025-02-11 07:55:18,648 - Step 20: Created step_acts
2025-02-11 07:55:18,648 - Step 20: Added to generation_acts
2025-02-11 07:55:18,649 - Step 20: Updated generated_texts
2025-02-11 07:55:18,649 - Step 20: Updated recent_tokens
2025-02-11 07:55:18,650 - Step 20: Decoded current text
2025-02-11 07:55:18,650 - Step 20: Incremented consecutive_fillers to 2
2025-02-11 07:55:18,650 - Step 20: Calculated unique_ratio: 0.75
2025-02-11 07:55:18,650 - 
Starting step 21
2025-02-11 07:55:18,650 - Current_ids device: cuda:0
2025-02-11 07:55:18,650 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,673 - Model output complete
2025-02-11 07:55:18,673 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:55:18,673 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,673 - Next token logits device: cuda:0
2025-02-11 07:55:18,673 - Entered do_sample
2025-02-11 07:55:18,673 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,677 - Probs max: 0.919921875
2025-02-11 07:55:18,677 - Pre-cat
2025-02-11 07:55:18,677 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    264,   4935,    374,  14989,
            518,    264,   7002,   1602,   4937,     11,    279,   4935,    686,
            387,   7218,   1602,   4937,    323,    279,   7002]],
       device='cuda:0')
2025-02-11 07:55:18,680 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:18,680 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:55:18,680 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,680 - Step 21: Generated next token
2025-02-11 07:55:18,680 - Step 21: Updated current_ids
2025-02-11 07:55:18,680 - Step 21: Decoded token text:  will
2025-02-11 07:55:18,680 - Step 21: Updated current_phrase
2025-02-11 07:55:18,681 - Step 21: Created step_acts
2025-02-11 07:55:18,681 - Step 21: Added to generation_acts
2025-02-11 07:55:18,681 - Step 21: Updated recent_tokens
2025-02-11 07:55:18,682 - Step 21: Decoded current text
2025-02-11 07:55:18,682 - Step 21: Incremented consecutive_fillers to 3
2025-02-11 07:55:18,788 - 
Starting step 0
2025-02-11 07:55:18,788 - Current_ids device: cuda:0
2025-02-11 07:55:18,789 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,830 - Model output complete
2025-02-11 07:55:18,830 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:18,830 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,830 - Next token logits device: cuda:0
2025-02-11 07:55:18,830 - Entered do_sample
2025-02-11 07:55:18,830 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,832 - Probs max: 0.50341796875
2025-02-11 07:55:18,833 - Pre-cat
2025-02-11 07:55:18,833 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:18,836 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:18,836 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:18,836 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,836 - Step 0: Generated next token
2025-02-11 07:55:18,836 - Step 0: Updated current_ids
2025-02-11 07:55:18,837 - Step 0: Decoded token text:  The
2025-02-11 07:55:18,837 - Step 0: Updated current_phrase
2025-02-11 07:55:18,837 - Step 0: Created step_acts
2025-02-11 07:55:18,837 - Step 0: Added to generation_acts
2025-02-11 07:55:18,838 - Step 0: Updated generated_texts
2025-02-11 07:55:18,838 - Step 0: Updated recent_tokens
2025-02-11 07:55:18,839 - Step 0: Decoded current text
2025-02-11 07:55:18,839 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:18,839 - 
Starting step 1
2025-02-11 07:55:18,839 - Current_ids device: cuda:0
2025-02-11 07:55:18,839 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,864 - Model output complete
2025-02-11 07:55:18,865 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:18,865 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,865 - Next token logits device: cuda:0
2025-02-11 07:55:18,865 - Entered do_sample
2025-02-11 07:55:18,865 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,868 - Probs max: 0.83837890625
2025-02-11 07:55:18,869 - Pre-cat
2025-02-11 07:55:18,869 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:55:18,870 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:18,870 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:18,870 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,870 - Step 1: Generated next token
2025-02-11 07:55:18,871 - Step 1: Updated current_ids
2025-02-11 07:55:18,871 - Step 1: Decoded token text:  ball
2025-02-11 07:55:18,871 - Step 1: Updated current_phrase
2025-02-11 07:55:18,871 - Step 1: Created step_acts
2025-02-11 07:55:18,871 - Step 1: Added to generation_acts
2025-02-11 07:55:18,871 - Step 1: Updated recent_tokens
2025-02-11 07:55:18,872 - Step 1: Decoded current text
2025-02-11 07:55:18,872 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:18,873 - 
Starting step 2
2025-02-11 07:55:18,873 - Current_ids device: cuda:0
2025-02-11 07:55:18,873 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,930 - Model output complete
2025-02-11 07:55:18,930 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:18,930 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,930 - Next token logits device: cuda:0
2025-02-11 07:55:18,930 - Entered do_sample
2025-02-11 07:55:18,930 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,932 - Probs max: 0.583984375
2025-02-11 07:55:18,934 - Pre-cat
2025-02-11 07:55:18,934 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:55:18,936 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:55:18,937 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:18,937 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,937 - Step 2: Generated next token
2025-02-11 07:55:18,937 - Step 2: Updated current_ids
2025-02-11 07:55:18,937 - Step 2: Decoded token text: 's
2025-02-11 07:55:18,937 - Step 2: Updated current_phrase
2025-02-11 07:55:18,938 - Step 2: Created step_acts
2025-02-11 07:55:18,938 - Step 2: Added to generation_acts
2025-02-11 07:55:18,938 - Step 2: Updated recent_tokens
2025-02-11 07:55:18,939 - Step 2: Decoded current text
2025-02-11 07:55:18,939 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:18,939 - 
Starting step 3
2025-02-11 07:55:18,939 - Current_ids device: cuda:0
2025-02-11 07:55:18,939 - Current_ids dtype: torch.int64
2025-02-11 07:55:18,971 - Model output complete
2025-02-11 07:55:18,971 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:18,971 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,971 - Next token logits device: cuda:0
2025-02-11 07:55:18,971 - Entered do_sample
2025-02-11 07:55:18,971 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:18,974 - Probs max: 0.51220703125
2025-02-11 07:55:18,975 - Pre-cat
2025-02-11 07:55:18,975 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594]],
       device='cuda:0')
2025-02-11 07:55:18,976 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:18,976 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:18,976 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:18,976 - Step 3: Generated next token
2025-02-11 07:55:18,977 - Step 3: Updated current_ids
2025-02-11 07:55:18,977 - Step 3: Decoded token text:  speed
2025-02-11 07:55:18,977 - Step 3: Updated current_phrase
2025-02-11 07:55:18,977 - Step 3: Created step_acts
2025-02-11 07:55:18,977 - Step 3: Added to generation_acts
2025-02-11 07:55:18,977 - Step 3: Updated recent_tokens
2025-02-11 07:55:18,978 - Step 3: Decoded current text
2025-02-11 07:55:18,979 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:18,979 - 
Starting step 4
2025-02-11 07:55:18,979 - Current_ids device: cuda:0
2025-02-11 07:55:18,979 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,036 - Model output complete
2025-02-11 07:55:19,036 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:19,036 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,036 - Next token logits device: cuda:0
2025-02-11 07:55:19,036 - Entered do_sample
2025-02-11 07:55:19,036 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,038 - Probs max: 0.90283203125
2025-02-11 07:55:19,039 - Pre-cat
2025-02-11 07:55:19,039 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628]],
       device='cuda:0')
2025-02-11 07:55:19,040 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:19,041 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:19,041 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,041 - Step 4: Generated next token
2025-02-11 07:55:19,041 - Step 4: Updated current_ids
2025-02-11 07:55:19,041 - Step 4: Decoded token text:  is
2025-02-11 07:55:19,041 - Step 4: Updated current_phrase
2025-02-11 07:55:19,041 - Step 4: Created step_acts
2025-02-11 07:55:19,041 - Step 4: Added to generation_acts
2025-02-11 07:55:19,041 - Step 4: Updated recent_tokens
2025-02-11 07:55:19,043 - Step 4: Decoded current text
2025-02-11 07:55:19,043 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:19,043 - 
Starting step 5
2025-02-11 07:55:19,043 - Current_ids device: cuda:0
2025-02-11 07:55:19,043 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,065 - Model output complete
2025-02-11 07:55:19,066 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:19,066 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,066 - Next token logits device: cuda:0
2025-02-11 07:55:19,066 - Entered do_sample
2025-02-11 07:55:19,066 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,069 - Probs max: 0.29638671875
2025-02-11 07:55:19,070 - Pre-cat
2025-02-11 07:55:19,070 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374]],
       device='cuda:0')
2025-02-11 07:55:19,071 - Next token: tensor([[348]], device='cuda:0')
2025-02-11 07:55:19,071 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:19,071 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,071 - Step 5: Generated next token
2025-02-11 07:55:19,072 - Step 5: Updated current_ids
2025-02-11 07:55:19,072 - Step 5: Decoded token text:  v
2025-02-11 07:55:19,072 - Step 5: Updated current_phrase
2025-02-11 07:55:19,072 - Step 5: Created step_acts
2025-02-11 07:55:19,072 - Step 5: Added to generation_acts
2025-02-11 07:55:19,073 - Step 5: Updated generated_texts
2025-02-11 07:55:19,074 - Step 5: Updated recent_tokens
2025-02-11 07:55:19,074 - Step 5: Decoded current text
2025-02-11 07:55:19,074 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:19,074 - 
Starting step 6
2025-02-11 07:55:19,074 - Current_ids device: cuda:0
2025-02-11 07:55:19,074 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,100 - Model output complete
2025-02-11 07:55:19,100 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:19,100 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,100 - Next token logits device: cuda:0
2025-02-11 07:55:19,100 - Entered do_sample
2025-02-11 07:55:19,100 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,102 - Probs max: 0.90478515625
2025-02-11 07:55:19,103 - Pre-cat
2025-02-11 07:55:19,103 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348]], device='cuda:0')
2025-02-11 07:55:19,105 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:19,105 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:19,105 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,105 - Step 6: Generated next token
2025-02-11 07:55:19,105 - Step 6: Updated current_ids
2025-02-11 07:55:19,105 - Step 6: Decoded token text: ,
2025-02-11 07:55:19,105 - Step 6: Updated current_phrase
2025-02-11 07:55:19,106 - Step 6: Created step_acts
2025-02-11 07:55:19,106 - Step 6: Added to generation_acts
2025-02-11 07:55:19,106 - Step 6: Updated recent_tokens
2025-02-11 07:55:19,107 - Step 6: Found phrase end token
2025-02-11 07:55:19,107 - Step 6: Updated recent_phrases
2025-02-11 07:55:19,107 - Step 6: Decoded current text
2025-02-11 07:55:19,108 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:19,108 - 
Starting step 7
2025-02-11 07:55:19,108 - Current_ids device: cuda:0
2025-02-11 07:55:19,108 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,133 - Model output complete
2025-02-11 07:55:19,133 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:19,133 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,134 - Next token logits device: cuda:0
2025-02-11 07:55:19,134 - Entered do_sample
2025-02-11 07:55:19,134 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,136 - Probs max: 0.79296875
2025-02-11 07:55:19,137 - Pre-cat
2025-02-11 07:55:19,137 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11]], device='cuda:0')
2025-02-11 07:55:19,139 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:19,139 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:19,139 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,139 - Step 7: Generated next token
2025-02-11 07:55:19,139 - Step 7: Updated current_ids
2025-02-11 07:55:19,139 - Step 7: Decoded token text:  the
2025-02-11 07:55:19,139 - Step 7: Updated current_phrase
2025-02-11 07:55:19,140 - Step 7: Created step_acts
2025-02-11 07:55:19,140 - Step 7: Added to generation_acts
2025-02-11 07:55:19,140 - Step 7: Updated recent_tokens
2025-02-11 07:55:19,141 - Step 7: Decoded current text
2025-02-11 07:55:19,141 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:19,141 - 
Starting step 8
2025-02-11 07:55:19,141 - Current_ids device: cuda:0
2025-02-11 07:55:19,141 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,164 - Model output complete
2025-02-11 07:55:19,164 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:19,164 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,165 - Next token logits device: cuda:0
2025-02-11 07:55:19,165 - Entered do_sample
2025-02-11 07:55:19,165 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,167 - Probs max: 0.98046875
2025-02-11 07:55:19,168 - Pre-cat
2025-02-11 07:55:19,168 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279]], device='cuda:0')
2025-02-11 07:55:19,170 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:19,170 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:19,170 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,170 - Step 8: Generated next token
2025-02-11 07:55:19,170 - Step 8: Updated current_ids
2025-02-11 07:55:19,170 - Step 8: Decoded token text:  wall
2025-02-11 07:55:19,171 - Step 8: Updated current_phrase
2025-02-11 07:55:19,171 - Step 8: Created step_acts
2025-02-11 07:55:19,171 - Step 8: Added to generation_acts
2025-02-11 07:55:19,171 - Step 8: Updated recent_tokens
2025-02-11 07:55:19,172 - Step 8: Decoded current text
2025-02-11 07:55:19,172 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:19,172 - 
Starting step 9
2025-02-11 07:55:19,172 - Current_ids device: cuda:0
2025-02-11 07:55:19,172 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,201 - Model output complete
2025-02-11 07:55:19,201 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:19,201 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,201 - Next token logits device: cuda:0
2025-02-11 07:55:19,201 - Entered do_sample
2025-02-11 07:55:19,202 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,204 - Probs max: 0.67041015625
2025-02-11 07:55:19,206 - Pre-cat
2025-02-11 07:55:19,206 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002]], device='cuda:0')
2025-02-11 07:55:19,210 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:55:19,210 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:19,210 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,210 - Step 9: Generated next token
2025-02-11 07:55:19,210 - Step 9: Updated current_ids
2025-02-11 07:55:19,211 - Step 9: Decoded token text: 's
2025-02-11 07:55:19,211 - Step 9: Updated current_phrase
2025-02-11 07:55:19,211 - Step 9: Created step_acts
2025-02-11 07:55:19,211 - Step 9: Added to generation_acts
2025-02-11 07:55:19,212 - Step 9: Updated recent_tokens
2025-02-11 07:55:19,213 - Step 9: Decoded current text
2025-02-11 07:55:19,213 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:19,213 - 
Starting step 10
2025-02-11 07:55:19,213 - Current_ids device: cuda:0
2025-02-11 07:55:19,213 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,255 - Model output complete
2025-02-11 07:55:19,255 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:19,255 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,255 - Next token logits device: cuda:0
2025-02-11 07:55:19,255 - Entered do_sample
2025-02-11 07:55:19,256 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,258 - Probs max: 0.99951171875
2025-02-11 07:55:19,261 - Pre-cat
2025-02-11 07:55:19,261 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594]], device='cuda:0')
2025-02-11 07:55:19,264 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:19,265 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:19,265 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,265 - Step 10: Generated next token
2025-02-11 07:55:19,265 - Step 10: Updated current_ids
2025-02-11 07:55:19,265 - Step 10: Decoded token text:  speed
2025-02-11 07:55:19,265 - Step 10: Updated current_phrase
2025-02-11 07:55:19,266 - Step 10: Created step_acts
2025-02-11 07:55:19,266 - Step 10: Added to generation_acts
2025-02-11 07:55:19,267 - Step 10: Updated generated_texts
2025-02-11 07:55:19,268 - Step 10: Updated recent_tokens
2025-02-11 07:55:19,268 - Step 10: Decoded current text
2025-02-11 07:55:19,268 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:19,268 - 
Starting step 11
2025-02-11 07:55:19,268 - Current_ids device: cuda:0
2025-02-11 07:55:19,268 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,354 - Model output complete
2025-02-11 07:55:19,354 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:19,354 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,355 - Next token logits device: cuda:0
2025-02-11 07:55:19,355 - Entered do_sample
2025-02-11 07:55:19,355 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,357 - Probs max: 0.9990234375
2025-02-11 07:55:19,358 - Pre-cat
2025-02-11 07:55:19,358 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628]], device='cuda:0')
2025-02-11 07:55:19,360 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:19,361 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:19,361 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,361 - Step 11: Generated next token
2025-02-11 07:55:19,361 - Step 11: Updated current_ids
2025-02-11 07:55:19,361 - Step 11: Decoded token text:  is
2025-02-11 07:55:19,361 - Step 11: Updated current_phrase
2025-02-11 07:55:19,361 - Step 11: Created step_acts
2025-02-11 07:55:19,361 - Step 11: Added to generation_acts
2025-02-11 07:55:19,361 - Step 11: Updated recent_tokens
2025-02-11 07:55:19,363 - Step 11: Decoded current text
2025-02-11 07:55:19,363 - Step 11: Reset consecutive_fillers
2025-02-11 07:55:19,363 - 
Starting step 12
2025-02-11 07:55:19,363 - Current_ids device: cuda:0
2025-02-11 07:55:19,363 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,393 - Model output complete
2025-02-11 07:55:19,393 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:19,393 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,393 - Next token logits device: cuda:0
2025-02-11 07:55:19,393 - Entered do_sample
2025-02-11 07:55:19,393 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,395 - Probs max: 0.52734375
2025-02-11 07:55:19,396 - Pre-cat
2025-02-11 07:55:19,396 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374]],
       device='cuda:0')
2025-02-11 07:55:19,399 - Next token: tensor([[575]], device='cuda:0')
2025-02-11 07:55:19,400 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:19,400 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,400 - Step 12: Generated next token
2025-02-11 07:55:19,400 - Step 12: Updated current_ids
2025-02-11 07:55:19,401 - Step 12: Decoded token text:  u
2025-02-11 07:55:19,401 - Step 12: Updated current_phrase
2025-02-11 07:55:19,401 - Step 12: Created step_acts
2025-02-11 07:55:19,402 - Step 12: Added to generation_acts
2025-02-11 07:55:19,402 - Step 12: Updated recent_tokens
2025-02-11 07:55:19,404 - Step 12: Decoded current text
2025-02-11 07:55:19,404 - Step 12: Reset consecutive_fillers
2025-02-11 07:55:19,404 - 
Starting step 13
2025-02-11 07:55:19,404 - Current_ids device: cuda:0
2025-02-11 07:55:19,404 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,440 - Model output complete
2025-02-11 07:55:19,441 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:19,441 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,441 - Next token logits device: cuda:0
2025-02-11 07:55:19,441 - Entered do_sample
2025-02-11 07:55:19,441 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,443 - Probs max: 0.462646484375
2025-02-11 07:55:19,445 - Pre-cat
2025-02-11 07:55:19,446 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575]],
       device='cuda:0')
2025-02-11 07:55:19,450 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:19,451 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:19,451 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,451 - Step 13: Generated next token
2025-02-11 07:55:19,451 - Step 13: Updated current_ids
2025-02-11 07:55:19,451 - Step 13: Decoded token text: ,
2025-02-11 07:55:19,451 - Step 13: Updated current_phrase
2025-02-11 07:55:19,452 - Step 13: Created step_acts
2025-02-11 07:55:19,452 - Step 13: Added to generation_acts
2025-02-11 07:55:19,452 - Step 13: Updated recent_tokens
2025-02-11 07:55:19,453 - Step 13: Found phrase end token
2025-02-11 07:55:19,453 - Step 13: Updated recent_phrases
2025-02-11 07:55:19,453 - Step 13: Calculated similarity: 0.6
2025-02-11 07:55:19,454 - Step 13: Decoded current text
2025-02-11 07:55:19,454 - Step 13: Reset consecutive_fillers
2025-02-11 07:55:19,454 - 
Starting step 14
2025-02-11 07:55:19,454 - Current_ids device: cuda:0
2025-02-11 07:55:19,454 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,517 - Model output complete
2025-02-11 07:55:19,518 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:55:19,518 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,518 - Next token logits device: cuda:0
2025-02-11 07:55:19,518 - Entered do_sample
2025-02-11 07:55:19,518 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,520 - Probs max: 0.40771484375
2025-02-11 07:55:19,521 - Pre-cat
2025-02-11 07:55:19,521 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11]],
       device='cuda:0')
2025-02-11 07:55:19,523 - Next token: tensor([[714]], device='cuda:0')
2025-02-11 07:55:19,523 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:55:19,523 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,523 - Step 14: Generated next token
2025-02-11 07:55:19,523 - Step 14: Updated current_ids
2025-02-11 07:55:19,523 - Step 14: Decoded token text:  but
2025-02-11 07:55:19,523 - Step 14: Updated current_phrase
2025-02-11 07:55:19,524 - Step 14: Created step_acts
2025-02-11 07:55:19,524 - Step 14: Added to generation_acts
2025-02-11 07:55:19,524 - Step 14: Updated recent_tokens
2025-02-11 07:55:19,525 - Step 14: Decoded current text
2025-02-11 07:55:19,525 - Step 14: Reset consecutive_fillers
2025-02-11 07:55:19,525 - 
Starting step 15
2025-02-11 07:55:19,525 - Current_ids device: cuda:0
2025-02-11 07:55:19,525 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,547 - Model output complete
2025-02-11 07:55:19,547 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:55:19,548 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,548 - Next token logits device: cuda:0
2025-02-11 07:55:19,548 - Entered do_sample
2025-02-11 07:55:19,548 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,551 - Probs max: 0.294677734375
2025-02-11 07:55:19,552 - Pre-cat
2025-02-11 07:55:19,552 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714]], device='cuda:0')
2025-02-11 07:55:19,554 - Next token: tensor([[582]], device='cuda:0')
2025-02-11 07:55:19,554 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:55:19,554 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,554 - Step 15: Generated next token
2025-02-11 07:55:19,554 - Step 15: Updated current_ids
2025-02-11 07:55:19,554 - Step 15: Decoded token text:  we
2025-02-11 07:55:19,554 - Step 15: Updated current_phrase
2025-02-11 07:55:19,555 - Step 15: Created step_acts
2025-02-11 07:55:19,555 - Step 15: Added to generation_acts
2025-02-11 07:55:19,556 - Step 15: Updated generated_texts
2025-02-11 07:55:19,556 - Step 15: Updated recent_tokens
2025-02-11 07:55:19,556 - Step 15: Decoded current text
2025-02-11 07:55:19,556 - Step 15: Reset consecutive_fillers
2025-02-11 07:55:19,556 - 
Starting step 16
2025-02-11 07:55:19,557 - Current_ids device: cuda:0
2025-02-11 07:55:19,557 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,578 - Model output complete
2025-02-11 07:55:19,579 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:55:19,579 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,579 - Next token logits device: cuda:0
2025-02-11 07:55:19,579 - Entered do_sample
2025-02-11 07:55:19,579 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,582 - Probs max: 0.7177734375
2025-02-11 07:55:19,583 - Pre-cat
2025-02-11 07:55:19,583 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582]], device='cuda:0')
2025-02-11 07:55:19,586 - Next token: tensor([[646]], device='cuda:0')
2025-02-11 07:55:19,586 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:55:19,586 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,586 - Step 16: Generated next token
2025-02-11 07:55:19,587 - Step 16: Updated current_ids
2025-02-11 07:55:19,587 - Step 16: Decoded token text:  can
2025-02-11 07:55:19,587 - Step 16: Updated current_phrase
2025-02-11 07:55:19,587 - Step 16: Created step_acts
2025-02-11 07:55:19,587 - Step 16: Added to generation_acts
2025-02-11 07:55:19,587 - Step 16: Updated recent_tokens
2025-02-11 07:55:19,589 - Step 16: Decoded current text
2025-02-11 07:55:19,589 - Step 16: Reset consecutive_fillers
2025-02-11 07:55:19,589 - Step 16: Calculated unique_ratio: 0.75
2025-02-11 07:55:19,590 - 
Starting step 17
2025-02-11 07:55:19,590 - Current_ids device: cuda:0
2025-02-11 07:55:19,590 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,638 - Model output complete
2025-02-11 07:55:19,638 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:55:19,638 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,638 - Next token logits device: cuda:0
2025-02-11 07:55:19,638 - Entered do_sample
2025-02-11 07:55:19,638 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,640 - Probs max: 0.9013671875
2025-02-11 07:55:19,642 - Pre-cat
2025-02-11 07:55:19,642 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646]], device='cuda:0')
2025-02-11 07:55:19,646 - Next token: tensor([[944]], device='cuda:0')
2025-02-11 07:55:19,647 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:55:19,647 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,647 - Step 17: Generated next token
2025-02-11 07:55:19,647 - Step 17: Updated current_ids
2025-02-11 07:55:19,647 - Step 17: Decoded token text: 't
2025-02-11 07:55:19,647 - Step 17: Updated current_phrase
2025-02-11 07:55:19,648 - Step 17: Created step_acts
2025-02-11 07:55:19,648 - Step 17: Added to generation_acts
2025-02-11 07:55:19,648 - Step 17: Updated recent_tokens
2025-02-11 07:55:19,649 - Step 17: Decoded current text
2025-02-11 07:55:19,649 - Step 17: Reset consecutive_fillers
2025-02-11 07:55:19,650 - Step 17: Calculated unique_ratio: 0.75
2025-02-11 07:55:19,650 - 
Starting step 18
2025-02-11 07:55:19,650 - Current_ids device: cuda:0
2025-02-11 07:55:19,650 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,715 - Model output complete
2025-02-11 07:55:19,715 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:55:19,715 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,715 - Next token logits device: cuda:0
2025-02-11 07:55:19,715 - Entered do_sample
2025-02-11 07:55:19,715 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,718 - Probs max: 0.30029296875
2025-02-11 07:55:19,718 - Pre-cat
2025-02-11 07:55:19,718 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944]], device='cuda:0')
2025-02-11 07:55:19,720 - Next token: tensor([[912]], device='cuda:0')
2025-02-11 07:55:19,720 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:55:19,721 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,721 - Step 18: Generated next token
2025-02-11 07:55:19,721 - Step 18: Updated current_ids
2025-02-11 07:55:19,721 - Step 18: Decoded token text:  add
2025-02-11 07:55:19,721 - Step 18: Updated current_phrase
2025-02-11 07:55:19,721 - Step 18: Created step_acts
2025-02-11 07:55:19,721 - Step 18: Added to generation_acts
2025-02-11 07:55:19,722 - Step 18: Updated recent_tokens
2025-02-11 07:55:19,723 - Step 18: Decoded current text
2025-02-11 07:55:19,723 - Step 18: Reset consecutive_fillers
2025-02-11 07:55:19,723 - Step 18: Calculated unique_ratio: 0.8125
2025-02-11 07:55:19,723 - 
Starting step 19
2025-02-11 07:55:19,723 - Current_ids device: cuda:0
2025-02-11 07:55:19,723 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,746 - Model output complete
2025-02-11 07:55:19,746 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:55:19,746 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,746 - Next token logits device: cuda:0
2025-02-11 07:55:19,746 - Entered do_sample
2025-02-11 07:55:19,746 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,749 - Probs max: 0.476806640625
2025-02-11 07:55:19,750 - Pre-cat
2025-02-11 07:55:19,750 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912]], device='cuda:0')
2025-02-11 07:55:19,752 - Next token: tensor([[1105]], device='cuda:0')
2025-02-11 07:55:19,752 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:55:19,752 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,752 - Step 19: Generated next token
2025-02-11 07:55:19,752 - Step 19: Updated current_ids
2025-02-11 07:55:19,753 - Step 19: Decoded token text:  them
2025-02-11 07:55:19,753 - Step 19: Updated current_phrase
2025-02-11 07:55:19,753 - Step 19: Created step_acts
2025-02-11 07:55:19,753 - Step 19: Added to generation_acts
2025-02-11 07:55:19,753 - Step 19: Updated recent_tokens
2025-02-11 07:55:19,755 - Step 19: Decoded current text
2025-02-11 07:55:19,755 - Step 19: Reset consecutive_fillers
2025-02-11 07:55:19,755 - Step 19: Calculated unique_ratio: 0.875
2025-02-11 07:55:19,755 - 
Starting step 20
2025-02-11 07:55:19,755 - Current_ids device: cuda:0
2025-02-11 07:55:19,755 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,780 - Model output complete
2025-02-11 07:55:19,780 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:55:19,780 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,780 - Next token logits device: cuda:0
2025-02-11 07:55:19,780 - Entered do_sample
2025-02-11 07:55:19,780 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,783 - Probs max: 0.61279296875
2025-02-11 07:55:19,784 - Pre-cat
2025-02-11 07:55:19,784 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105]], device='cuda:0')
2025-02-11 07:55:19,788 - Next token: tensor([[1576]], device='cuda:0')
2025-02-11 07:55:19,788 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:55:19,788 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,788 - Step 20: Generated next token
2025-02-11 07:55:19,789 - Step 20: Updated current_ids
2025-02-11 07:55:19,789 - Step 20: Decoded token text:  because
2025-02-11 07:55:19,789 - Step 20: Updated current_phrase
2025-02-11 07:55:19,790 - Step 20: Created step_acts
2025-02-11 07:55:19,790 - Step 20: Added to generation_acts
2025-02-11 07:55:19,792 - Step 20: Updated generated_texts
2025-02-11 07:55:19,792 - Step 20: Updated recent_tokens
2025-02-11 07:55:19,792 - Step 20: Decoded current text
2025-02-11 07:55:19,793 - Step 20: Reset consecutive_fillers
2025-02-11 07:55:19,793 - Step 20: Calculated unique_ratio: 0.9375
2025-02-11 07:55:19,793 - 
Starting step 21
2025-02-11 07:55:19,793 - Current_ids device: cuda:0
2025-02-11 07:55:19,793 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,817 - Model output complete
2025-02-11 07:55:19,817 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:55:19,817 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,817 - Next token logits device: cuda:0
2025-02-11 07:55:19,817 - Entered do_sample
2025-02-11 07:55:19,817 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,820 - Probs max: 0.7548828125
2025-02-11 07:55:19,820 - Pre-cat
2025-02-11 07:55:19,820 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576]],
       device='cuda:0')
2025-02-11 07:55:19,823 - Next token: tensor([[807]], device='cuda:0')
2025-02-11 07:55:19,823 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:55:19,823 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,823 - Step 21: Generated next token
2025-02-11 07:55:19,823 - Step 21: Updated current_ids
2025-02-11 07:55:19,823 - Step 21: Decoded token text:  they
2025-02-11 07:55:19,823 - Step 21: Updated current_phrase
2025-02-11 07:55:19,824 - Step 21: Created step_acts
2025-02-11 07:55:19,824 - Step 21: Added to generation_acts
2025-02-11 07:55:19,824 - Step 21: Updated recent_tokens
2025-02-11 07:55:19,825 - Step 21: Decoded current text
2025-02-11 07:55:19,825 - Step 21: Reset consecutive_fillers
2025-02-11 07:55:19,825 - Step 21: Calculated unique_ratio: 0.9375
2025-02-11 07:55:19,826 - 
Starting step 22
2025-02-11 07:55:19,826 - Current_ids device: cuda:0
2025-02-11 07:55:19,826 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,847 - Model output complete
2025-02-11 07:55:19,847 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:55:19,847 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,847 - Next token logits device: cuda:0
2025-02-11 07:55:19,847 - Entered do_sample
2025-02-11 07:55:19,848 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,852 - Probs max: 0.52587890625
2025-02-11 07:55:19,852 - Pre-cat
2025-02-11 07:55:19,852 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807]],
       device='cuda:0')
2025-02-11 07:55:19,855 - Next token: tensor([[2299]], device='cuda:0')
2025-02-11 07:55:19,855 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:55:19,855 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,855 - Step 22: Generated next token
2025-02-11 07:55:19,855 - Step 22: Updated current_ids
2025-02-11 07:55:19,856 - Step 22: Decoded token text: 're
2025-02-11 07:55:19,856 - Step 22: Updated current_phrase
2025-02-11 07:55:19,856 - Step 22: Created step_acts
2025-02-11 07:55:19,856 - Step 22: Added to generation_acts
2025-02-11 07:55:19,856 - Step 22: Updated recent_tokens
2025-02-11 07:55:19,857 - Step 22: Decoded current text
2025-02-11 07:55:19,858 - Step 22: Reset consecutive_fillers
2025-02-11 07:55:19,858 - Step 22: Calculated unique_ratio: 1.0
2025-02-11 07:55:19,858 - 
Starting step 23
2025-02-11 07:55:19,858 - Current_ids device: cuda:0
2025-02-11 07:55:19,858 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,892 - Model output complete
2025-02-11 07:55:19,892 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:55:19,892 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,892 - Next token logits device: cuda:0
2025-02-11 07:55:19,892 - Entered do_sample
2025-02-11 07:55:19,892 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,894 - Probs max: 0.5517578125
2025-02-11 07:55:19,896 - Pre-cat
2025-02-11 07:55:19,896 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299]],
       device='cuda:0')
2025-02-11 07:55:19,900 - Next token: tensor([[304]], device='cuda:0')
2025-02-11 07:55:19,901 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:55:19,901 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,901 - Step 23: Generated next token
2025-02-11 07:55:19,901 - Step 23: Updated current_ids
2025-02-11 07:55:19,901 - Step 23: Decoded token text:  in
2025-02-11 07:55:19,901 - Step 23: Updated current_phrase
2025-02-11 07:55:19,902 - Step 23: Created step_acts
2025-02-11 07:55:19,902 - Step 23: Added to generation_acts
2025-02-11 07:55:19,902 - Step 23: Updated recent_tokens
2025-02-11 07:55:19,903 - Step 23: Decoded current text
2025-02-11 07:55:19,903 - Step 23: Reset consecutive_fillers
2025-02-11 07:55:19,903 - Step 23: Calculated unique_ratio: 1.0
2025-02-11 07:55:19,903 - 
Starting step 24
2025-02-11 07:55:19,904 - Current_ids device: cuda:0
2025-02-11 07:55:19,904 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,926 - Model output complete
2025-02-11 07:55:19,927 - Logits shape: torch.Size([1, 55, 151936])
2025-02-11 07:55:19,927 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,927 - Next token logits device: cuda:0
2025-02-11 07:55:19,927 - Entered do_sample
2025-02-11 07:55:19,927 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,930 - Probs max: 0.5625
2025-02-11 07:55:19,930 - Pre-cat
2025-02-11 07:55:19,930 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304]], device='cuda:0')
2025-02-11 07:55:19,932 - Next token: tensor([[14002]], device='cuda:0')
2025-02-11 07:55:19,932 - Current_ids shape: torch.Size([1, 55])
2025-02-11 07:55:19,933 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,933 - Step 24: Generated next token
2025-02-11 07:55:19,933 - Step 24: Updated current_ids
2025-02-11 07:55:19,933 - Step 24: Decoded token text:  opposite
2025-02-11 07:55:19,933 - Step 24: Updated current_phrase
2025-02-11 07:55:19,933 - Step 24: Created step_acts
2025-02-11 07:55:19,933 - Step 24: Added to generation_acts
2025-02-11 07:55:19,934 - Step 24: Updated recent_tokens
2025-02-11 07:55:19,935 - Step 24: Decoded current text
2025-02-11 07:55:19,935 - Step 24: Reset consecutive_fillers
2025-02-11 07:55:19,935 - Step 24: Calculated unique_ratio: 1.0
2025-02-11 07:55:19,935 - 
Starting step 25
2025-02-11 07:55:19,935 - Current_ids device: cuda:0
2025-02-11 07:55:19,935 - Current_ids dtype: torch.int64
2025-02-11 07:55:19,958 - Model output complete
2025-02-11 07:55:19,958 - Logits shape: torch.Size([1, 56, 151936])
2025-02-11 07:55:19,958 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,958 - Next token logits device: cuda:0
2025-02-11 07:55:19,959 - Entered do_sample
2025-02-11 07:55:19,959 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:19,962 - Probs max: 0.99853515625
2025-02-11 07:55:19,962 - Pre-cat
2025-02-11 07:55:19,962 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002]], device='cuda:0')
2025-02-11 07:55:19,966 - Next token: tensor([[17961]], device='cuda:0')
2025-02-11 07:55:19,967 - Current_ids shape: torch.Size([1, 56])
2025-02-11 07:55:19,967 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:19,967 - Step 25: Generated next token
2025-02-11 07:55:19,967 - Step 25: Updated current_ids
2025-02-11 07:55:19,967 - Step 25: Decoded token text:  directions
2025-02-11 07:55:19,967 - Step 25: Updated current_phrase
2025-02-11 07:55:19,968 - Step 25: Created step_acts
2025-02-11 07:55:19,968 - Step 25: Added to generation_acts
2025-02-11 07:55:19,969 - Step 25: Updated generated_texts
2025-02-11 07:55:19,969 - Step 25: Updated recent_tokens
2025-02-11 07:55:19,970 - Step 25: Decoded current text
2025-02-11 07:55:19,970 - Step 25: Reset consecutive_fillers
2025-02-11 07:55:19,970 - Step 25: Calculated unique_ratio: 1.0
2025-02-11 07:55:19,970 - 
Starting step 26
2025-02-11 07:55:19,970 - Current_ids device: cuda:0
2025-02-11 07:55:19,971 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,044 - Model output complete
2025-02-11 07:55:20,044 - Logits shape: torch.Size([1, 57, 151936])
2025-02-11 07:55:20,044 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,044 - Next token logits device: cuda:0
2025-02-11 07:55:20,044 - Entered do_sample
2025-02-11 07:55:20,044 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,046 - Probs max: 0.79150390625
2025-02-11 07:55:20,047 - Pre-cat
2025-02-11 07:55:20,047 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961]], device='cuda:0')
2025-02-11 07:55:20,049 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:55:20,050 - Current_ids shape: torch.Size([1, 57])
2025-02-11 07:55:20,050 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,050 - Step 26: Generated next token
2025-02-11 07:55:20,050 - Step 26: Updated current_ids
2025-02-11 07:55:20,050 - Step 26: Decoded token text: .
2025-02-11 07:55:20,050 - Step 26: Updated current_phrase
2025-02-11 07:55:20,050 - Step 26: Created step_acts
2025-02-11 07:55:20,050 - Step 26: Added to generation_acts
2025-02-11 07:55:20,051 - Step 26: Updated recent_tokens
2025-02-11 07:55:20,052 - Step 26: Found phrase end token
2025-02-11 07:55:20,052 - Step 26: Updated recent_phrases
2025-02-11 07:55:20,052 - Step 26: Calculated similarity: 0.0
2025-02-11 07:55:20,052 - Step 26: Decoded current text
2025-02-11 07:55:20,052 - Step 26: Reset consecutive_fillers
2025-02-11 07:55:20,052 - Step 26: Calculated unique_ratio: 1.0
2025-02-11 07:55:20,052 - 
Starting step 27
2025-02-11 07:55:20,052 - Current_ids device: cuda:0
2025-02-11 07:55:20,052 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,076 - Model output complete
2025-02-11 07:55:20,077 - Logits shape: torch.Size([1, 58, 151936])
2025-02-11 07:55:20,077 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,077 - Next token logits device: cuda:0
2025-02-11 07:55:20,077 - Entered do_sample
2025-02-11 07:55:20,077 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,079 - Probs max: 0.83251953125
2025-02-11 07:55:20,080 - Pre-cat
2025-02-11 07:55:20,080 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13]], device='cuda:0')
2025-02-11 07:55:20,083 - Next token: tensor([[2055]], device='cuda:0')
2025-02-11 07:55:20,084 - Current_ids shape: torch.Size([1, 58])
2025-02-11 07:55:20,084 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,084 - Step 27: Generated next token
2025-02-11 07:55:20,084 - Step 27: Updated current_ids
2025-02-11 07:55:20,084 - Step 27: Decoded token text:  So
2025-02-11 07:55:20,084 - Step 27: Updated current_phrase
2025-02-11 07:55:20,085 - Step 27: Created step_acts
2025-02-11 07:55:20,085 - Step 27: Added to generation_acts
2025-02-11 07:55:20,085 - Step 27: Updated recent_tokens
2025-02-11 07:55:20,086 - Step 27: Decoded current text
2025-02-11 07:55:20,086 - Step 27: Reset consecutive_fillers
2025-02-11 07:55:20,086 - Step 27: Calculated unique_ratio: 1.0
2025-02-11 07:55:20,086 - 
Starting step 28
2025-02-11 07:55:20,086 - Current_ids device: cuda:0
2025-02-11 07:55:20,087 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,146 - Model output complete
2025-02-11 07:55:20,146 - Logits shape: torch.Size([1, 59, 151936])
2025-02-11 07:55:20,146 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,146 - Next token logits device: cuda:0
2025-02-11 07:55:20,146 - Entered do_sample
2025-02-11 07:55:20,146 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,148 - Probs max: 0.82177734375
2025-02-11 07:55:20,150 - Pre-cat
2025-02-11 07:55:20,150 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055]], device='cuda:0')
2025-02-11 07:55:20,154 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:20,155 - Current_ids shape: torch.Size([1, 59])
2025-02-11 07:55:20,155 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,155 - Step 28: Generated next token
2025-02-11 07:55:20,155 - Step 28: Updated current_ids
2025-02-11 07:55:20,155 - Step 28: Decoded token text: ,
2025-02-11 07:55:20,155 - Step 28: Updated current_phrase
2025-02-11 07:55:20,156 - Step 28: Created step_acts
2025-02-11 07:55:20,156 - Step 28: Added to generation_acts
2025-02-11 07:55:20,156 - Step 28: Updated recent_tokens
2025-02-11 07:55:20,157 - Step 28: Found phrase end token
2025-02-11 07:55:20,157 - Step 28: Updated recent_phrases
2025-02-11 07:55:20,157 - Step 28: Calculated similarity: 0.0
2025-02-11 07:55:20,157 - Step 28: Decoded current text
2025-02-11 07:55:20,158 - Step 28: Reset consecutive_fillers
2025-02-11 07:55:20,158 - Step 28: Calculated unique_ratio: 0.9375
2025-02-11 07:55:20,158 - 
Starting step 29
2025-02-11 07:55:20,158 - Current_ids device: cuda:0
2025-02-11 07:55:20,158 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,215 - Model output complete
2025-02-11 07:55:20,215 - Logits shape: torch.Size([1, 60, 151936])
2025-02-11 07:55:20,215 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,215 - Next token logits device: cuda:0
2025-02-11 07:55:20,215 - Entered do_sample
2025-02-11 07:55:20,215 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,218 - Probs max: 0.9140625
2025-02-11 07:55:20,221 - Pre-cat
2025-02-11 07:55:20,221 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11]], device='cuda:0')
2025-02-11 07:55:20,225 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:20,226 - Current_ids shape: torch.Size([1, 60])
2025-02-11 07:55:20,226 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,226 - Step 29: Generated next token
2025-02-11 07:55:20,226 - Step 29: Updated current_ids
2025-02-11 07:55:20,227 - Step 29: Decoded token text:  the
2025-02-11 07:55:20,227 - Step 29: Updated current_phrase
2025-02-11 07:55:20,230 - Step 29: Created step_acts
2025-02-11 07:55:20,232 - Step 29: Added to generation_acts
2025-02-11 07:55:20,233 - Step 29: Updated recent_tokens
2025-02-11 07:55:20,235 - Step 29: Decoded current text
2025-02-11 07:55:20,235 - Step 29: Reset consecutive_fillers
2025-02-11 07:55:20,236 - Step 29: Calculated unique_ratio: 1.0
2025-02-11 07:55:20,236 - 
Starting step 30
2025-02-11 07:55:20,236 - Current_ids device: cuda:0
2025-02-11 07:55:20,237 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,274 - Model output complete
2025-02-11 07:55:20,274 - Logits shape: torch.Size([1, 61, 151936])
2025-02-11 07:55:20,274 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,274 - Next token logits device: cuda:0
2025-02-11 07:55:20,274 - Entered do_sample
2025-02-11 07:55:20,274 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,276 - Probs max: 0.7421875
2025-02-11 07:55:20,278 - Pre-cat
2025-02-11 07:55:20,278 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279]],
       device='cuda:0')
2025-02-11 07:55:20,284 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:20,284 - Current_ids shape: torch.Size([1, 61])
2025-02-11 07:55:20,284 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,284 - Step 30: Generated next token
2025-02-11 07:55:20,285 - Step 30: Updated current_ids
2025-02-11 07:55:20,285 - Step 30: Decoded token text:  ball
2025-02-11 07:55:20,285 - Step 30: Updated current_phrase
2025-02-11 07:55:20,286 - Step 30: Created step_acts
2025-02-11 07:55:20,286 - Step 30: Added to generation_acts
2025-02-11 07:55:20,287 - Step 30: Updated generated_texts
2025-02-11 07:55:20,288 - Step 30: Updated recent_tokens
2025-02-11 07:55:20,288 - Step 30: Decoded current text
2025-02-11 07:55:20,288 - Step 30: Reset consecutive_fillers
2025-02-11 07:55:20,289 - Step 30: Calculated unique_ratio: 1.0
2025-02-11 07:55:20,289 - 
Starting step 31
2025-02-11 07:55:20,289 - Current_ids device: cuda:0
2025-02-11 07:55:20,289 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,315 - Model output complete
2025-02-11 07:55:20,315 - Logits shape: torch.Size([1, 62, 151936])
2025-02-11 07:55:20,315 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,315 - Next token logits device: cuda:0
2025-02-11 07:55:20,316 - Entered do_sample
2025-02-11 07:55:20,316 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,318 - Probs max: 0.87841796875
2025-02-11 07:55:20,319 - Pre-cat
2025-02-11 07:55:20,319 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935]],
       device='cuda:0')
2025-02-11 07:55:20,321 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:55:20,321 - Current_ids shape: torch.Size([1, 62])
2025-02-11 07:55:20,321 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,321 - Step 31: Generated next token
2025-02-11 07:55:20,321 - Step 31: Updated current_ids
2025-02-11 07:55:20,322 - Step 31: Decoded token text: 's
2025-02-11 07:55:20,322 - Step 31: Updated current_phrase
2025-02-11 07:55:20,322 - Step 31: Created step_acts
2025-02-11 07:55:20,322 - Step 31: Added to generation_acts
2025-02-11 07:55:20,322 - Step 31: Updated recent_tokens
2025-02-11 07:55:20,324 - Step 31: Decoded current text
2025-02-11 07:55:20,324 - Step 31: Reset consecutive_fillers
2025-02-11 07:55:20,324 - Step 31: Calculated unique_ratio: 1.0
2025-02-11 07:55:20,324 - 
Starting step 32
2025-02-11 07:55:20,324 - Current_ids device: cuda:0
2025-02-11 07:55:20,324 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,347 - Model output complete
2025-02-11 07:55:20,347 - Logits shape: torch.Size([1, 63, 151936])
2025-02-11 07:55:20,347 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,347 - Next token logits device: cuda:0
2025-02-11 07:55:20,347 - Entered do_sample
2025-02-11 07:55:20,347 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,350 - Probs max: 0.96826171875
2025-02-11 07:55:20,351 - Pre-cat
2025-02-11 07:55:20,351 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594]],
       device='cuda:0')
2025-02-11 07:55:20,353 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:20,353 - Current_ids shape: torch.Size([1, 63])
2025-02-11 07:55:20,353 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,353 - Step 32: Generated next token
2025-02-11 07:55:20,354 - Step 32: Updated current_ids
2025-02-11 07:55:20,354 - Step 32: Decoded token text:  speed
2025-02-11 07:55:20,354 - Step 32: Updated current_phrase
2025-02-11 07:55:20,354 - Step 32: Created step_acts
2025-02-11 07:55:20,354 - Step 32: Added to generation_acts
2025-02-11 07:55:20,354 - Step 32: Updated recent_tokens
2025-02-11 07:55:20,356 - Step 32: Decoded current text
2025-02-11 07:55:20,356 - Step 32: Reset consecutive_fillers
2025-02-11 07:55:20,356 - Step 32: Calculated unique_ratio: 1.0
2025-02-11 07:55:20,356 - 
Starting step 33
2025-02-11 07:55:20,356 - Current_ids device: cuda:0
2025-02-11 07:55:20,356 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,405 - Model output complete
2025-02-11 07:55:20,405 - Logits shape: torch.Size([1, 64, 151936])
2025-02-11 07:55:20,405 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,405 - Next token logits device: cuda:0
2025-02-11 07:55:20,406 - Entered do_sample
2025-02-11 07:55:20,406 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,408 - Probs max: 0.8837890625
2025-02-11 07:55:20,410 - Pre-cat
2025-02-11 07:55:20,410 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628]], device='cuda:0')
2025-02-11 07:55:20,414 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:20,414 - Current_ids shape: torch.Size([1, 64])
2025-02-11 07:55:20,415 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,415 - Step 33: Generated next token
2025-02-11 07:55:20,415 - Step 33: Updated current_ids
2025-02-11 07:55:20,415 - Step 33: Decoded token text:  is
2025-02-11 07:55:20,415 - Step 33: Updated current_phrase
2025-02-11 07:55:20,416 - Step 33: Created step_acts
2025-02-11 07:55:20,416 - Step 33: Added to generation_acts
2025-02-11 07:55:20,416 - Step 33: Updated recent_tokens
2025-02-11 07:55:20,417 - Step 33: Decoded current text
2025-02-11 07:55:20,417 - Step 33: Reset consecutive_fillers
2025-02-11 07:55:20,417 - Step 33: Calculated unique_ratio: 1.0
2025-02-11 07:55:20,418 - 
Starting step 34
2025-02-11 07:55:20,418 - Current_ids device: cuda:0
2025-02-11 07:55:20,418 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,448 - Model output complete
2025-02-11 07:55:20,448 - Logits shape: torch.Size([1, 65, 151936])
2025-02-11 07:55:20,448 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,448 - Next token logits device: cuda:0
2025-02-11 07:55:20,448 - Entered do_sample
2025-02-11 07:55:20,448 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,451 - Probs max: 0.97216796875
2025-02-11 07:55:20,453 - Pre-cat
2025-02-11 07:55:20,453 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374]], device='cuda:0')
2025-02-11 07:55:20,457 - Next token: tensor([[348]], device='cuda:0')
2025-02-11 07:55:20,458 - Current_ids shape: torch.Size([1, 65])
2025-02-11 07:55:20,458 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,458 - Step 34: Generated next token
2025-02-11 07:55:20,458 - Step 34: Updated current_ids
2025-02-11 07:55:20,458 - Step 34: Decoded token text:  v
2025-02-11 07:55:20,458 - Step 34: Updated current_phrase
2025-02-11 07:55:20,459 - Step 34: Created step_acts
2025-02-11 07:55:20,459 - Step 34: Added to generation_acts
2025-02-11 07:55:20,459 - Step 34: Updated recent_tokens
2025-02-11 07:55:20,460 - Step 34: Decoded current text
2025-02-11 07:55:20,461 - Step 34: Reset consecutive_fillers
2025-02-11 07:55:20,461 - Step 34: Calculated unique_ratio: 1.0
2025-02-11 07:55:20,461 - 
Starting step 35
2025-02-11 07:55:20,461 - Current_ids device: cuda:0
2025-02-11 07:55:20,461 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,507 - Model output complete
2025-02-11 07:55:20,507 - Logits shape: torch.Size([1, 66, 151936])
2025-02-11 07:55:20,507 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,507 - Next token logits device: cuda:0
2025-02-11 07:55:20,507 - Entered do_sample
2025-02-11 07:55:20,507 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,510 - Probs max: 0.72314453125
2025-02-11 07:55:20,510 - Pre-cat
2025-02-11 07:55:20,510 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348]], device='cuda:0')
2025-02-11 07:55:20,513 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:20,513 - Current_ids shape: torch.Size([1, 66])
2025-02-11 07:55:20,513 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,513 - Step 35: Generated next token
2025-02-11 07:55:20,513 - Step 35: Updated current_ids
2025-02-11 07:55:20,513 - Step 35: Decoded token text: ,
2025-02-11 07:55:20,514 - Step 35: Updated current_phrase
2025-02-11 07:55:20,514 - Step 35: Created step_acts
2025-02-11 07:55:20,514 - Step 35: Added to generation_acts
2025-02-11 07:55:20,515 - Step 35: Updated generated_texts
2025-02-11 07:55:20,516 - Step 35: Updated recent_tokens
2025-02-11 07:55:20,516 - Step 35: Found phrase end token
2025-02-11 07:55:20,516 - Step 35: Updated recent_phrases
2025-02-11 07:55:20,516 - Step 35: Calculated similarity: 0.0
2025-02-11 07:55:20,516 - Step 35: Decoded current text
2025-02-11 07:55:20,516 - Step 35: Reset consecutive_fillers
2025-02-11 07:55:20,516 - Step 35: Calculated unique_ratio: 0.9375
2025-02-11 07:55:20,516 - 
Starting step 36
2025-02-11 07:55:20,516 - Current_ids device: cuda:0
2025-02-11 07:55:20,516 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,541 - Model output complete
2025-02-11 07:55:20,542 - Logits shape: torch.Size([1, 67, 151936])
2025-02-11 07:55:20,542 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,542 - Next token logits device: cuda:0
2025-02-11 07:55:20,542 - Entered do_sample
2025-02-11 07:55:20,542 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,545 - Probs max: 0.8642578125
2025-02-11 07:55:20,546 - Pre-cat
2025-02-11 07:55:20,546 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11]], device='cuda:0')
2025-02-11 07:55:20,548 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:20,548 - Current_ids shape: torch.Size([1, 67])
2025-02-11 07:55:20,548 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,548 - Step 36: Generated next token
2025-02-11 07:55:20,548 - Step 36: Updated current_ids
2025-02-11 07:55:20,549 - Step 36: Decoded token text:  the
2025-02-11 07:55:20,549 - Step 36: Updated current_phrase
2025-02-11 07:55:20,549 - Step 36: Created step_acts
2025-02-11 07:55:20,549 - Step 36: Added to generation_acts
2025-02-11 07:55:20,549 - Step 36: Updated recent_tokens
2025-02-11 07:55:20,551 - Step 36: Decoded current text
2025-02-11 07:55:20,551 - Step 36: Reset consecutive_fillers
2025-02-11 07:55:20,551 - Step 36: Calculated unique_ratio: 0.875
2025-02-11 07:55:20,551 - 
Starting step 37
2025-02-11 07:55:20,551 - Current_ids device: cuda:0
2025-02-11 07:55:20,551 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,579 - Model output complete
2025-02-11 07:55:20,580 - Logits shape: torch.Size([1, 68, 151936])
2025-02-11 07:55:20,580 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,580 - Next token logits device: cuda:0
2025-02-11 07:55:20,580 - Entered do_sample
2025-02-11 07:55:20,580 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,582 - Probs max: 0.99609375
2025-02-11 07:55:20,583 - Pre-cat
2025-02-11 07:55:20,583 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279]], device='cuda:0')
2025-02-11 07:55:20,585 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:20,586 - Current_ids shape: torch.Size([1, 68])
2025-02-11 07:55:20,586 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,586 - Step 37: Generated next token
2025-02-11 07:55:20,586 - Step 37: Updated current_ids
2025-02-11 07:55:20,586 - Step 37: Decoded token text:  wall
2025-02-11 07:55:20,586 - Step 37: Updated current_phrase
2025-02-11 07:55:20,586 - Step 37: Created step_acts
2025-02-11 07:55:20,586 - Step 37: Added to generation_acts
2025-02-11 07:55:20,587 - Step 37: Updated recent_tokens
2025-02-11 07:55:20,588 - Step 37: Decoded current text
2025-02-11 07:55:20,588 - Step 37: Reset consecutive_fillers
2025-02-11 07:55:20,588 - Step 37: Calculated unique_ratio: 0.875
2025-02-11 07:55:20,588 - 
Starting step 38
2025-02-11 07:55:20,589 - Current_ids device: cuda:0
2025-02-11 07:55:20,589 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,611 - Model output complete
2025-02-11 07:55:20,612 - Logits shape: torch.Size([1, 69, 151936])
2025-02-11 07:55:20,612 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,612 - Next token logits device: cuda:0
2025-02-11 07:55:20,612 - Entered do_sample
2025-02-11 07:55:20,612 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,616 - Probs max: 0.990234375
2025-02-11 07:55:20,616 - Pre-cat
2025-02-11 07:55:20,617 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002]], device='cuda:0')
2025-02-11 07:55:20,619 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:55:20,620 - Current_ids shape: torch.Size([1, 69])
2025-02-11 07:55:20,620 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,620 - Step 38: Generated next token
2025-02-11 07:55:20,620 - Step 38: Updated current_ids
2025-02-11 07:55:20,620 - Step 38: Decoded token text: 's
2025-02-11 07:55:20,620 - Step 38: Updated current_phrase
2025-02-11 07:55:20,620 - Step 38: Created step_acts
2025-02-11 07:55:20,620 - Step 38: Added to generation_acts
2025-02-11 07:55:20,620 - Step 38: Updated recent_tokens
2025-02-11 07:55:20,622 - Step 38: Decoded current text
2025-02-11 07:55:20,622 - Step 38: Reset consecutive_fillers
2025-02-11 07:55:20,622 - Step 38: Calculated unique_ratio: 0.8125
2025-02-11 07:55:20,622 - 
Starting step 39
2025-02-11 07:55:20,622 - Current_ids device: cuda:0
2025-02-11 07:55:20,622 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,650 - Model output complete
2025-02-11 07:55:20,650 - Logits shape: torch.Size([1, 70, 151936])
2025-02-11 07:55:20,650 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,650 - Next token logits device: cuda:0
2025-02-11 07:55:20,650 - Entered do_sample
2025-02-11 07:55:20,650 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,653 - Probs max: 0.99951171875
2025-02-11 07:55:20,654 - Pre-cat
2025-02-11 07:55:20,655 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594]],
       device='cuda:0')
2025-02-11 07:55:20,660 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:20,660 - Current_ids shape: torch.Size([1, 70])
2025-02-11 07:55:20,660 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,660 - Step 39: Generated next token
2025-02-11 07:55:20,660 - Step 39: Updated current_ids
2025-02-11 07:55:20,661 - Step 39: Decoded token text:  speed
2025-02-11 07:55:20,661 - Step 39: Updated current_phrase
2025-02-11 07:55:20,661 - Step 39: Created step_acts
2025-02-11 07:55:20,661 - Step 39: Added to generation_acts
2025-02-11 07:55:20,661 - Step 39: Updated recent_tokens
2025-02-11 07:55:20,663 - Step 39: Decoded current text
2025-02-11 07:55:20,663 - Step 39: Reset consecutive_fillers
2025-02-11 07:55:20,663 - Step 39: Calculated unique_ratio: 0.75
2025-02-11 07:55:20,663 - 
Starting step 40
2025-02-11 07:55:20,664 - Current_ids device: cuda:0
2025-02-11 07:55:20,664 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,704 - Model output complete
2025-02-11 07:55:20,704 - Logits shape: torch.Size([1, 71, 151936])
2025-02-11 07:55:20,704 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,704 - Next token logits device: cuda:0
2025-02-11 07:55:20,704 - Entered do_sample
2025-02-11 07:55:20,704 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,708 - Probs max: 0.99853515625
2025-02-11 07:55:20,708 - Pre-cat
2025-02-11 07:55:20,708 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628]],
       device='cuda:0')
2025-02-11 07:55:20,711 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:20,711 - Current_ids shape: torch.Size([1, 71])
2025-02-11 07:55:20,711 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,711 - Step 40: Generated next token
2025-02-11 07:55:20,711 - Step 40: Updated current_ids
2025-02-11 07:55:20,711 - Step 40: Decoded token text:  is
2025-02-11 07:55:20,712 - Step 40: Updated current_phrase
2025-02-11 07:55:20,712 - Step 40: Created step_acts
2025-02-11 07:55:20,712 - Step 40: Added to generation_acts
2025-02-11 07:55:20,714 - Step 40: Updated generated_texts
2025-02-11 07:55:20,714 - Step 40: Updated recent_tokens
2025-02-11 07:55:20,714 - Step 40: Decoded current text
2025-02-11 07:55:20,715 - Step 40: Reset consecutive_fillers
2025-02-11 07:55:20,715 - Step 40: Calculated unique_ratio: 0.6875
2025-02-11 07:55:20,715 - 
Starting step 41
2025-02-11 07:55:20,715 - Current_ids device: cuda:0
2025-02-11 07:55:20,715 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,745 - Model output complete
2025-02-11 07:55:20,745 - Logits shape: torch.Size([1, 72, 151936])
2025-02-11 07:55:20,745 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,745 - Next token logits device: cuda:0
2025-02-11 07:55:20,745 - Entered do_sample
2025-02-11 07:55:20,745 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,747 - Probs max: 0.98779296875
2025-02-11 07:55:20,749 - Pre-cat
2025-02-11 07:55:20,749 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374]],
       device='cuda:0')
2025-02-11 07:55:20,754 - Next token: tensor([[575]], device='cuda:0')
2025-02-11 07:55:20,755 - Current_ids shape: torch.Size([1, 72])
2025-02-11 07:55:20,755 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,755 - Step 41: Generated next token
2025-02-11 07:55:20,755 - Step 41: Updated current_ids
2025-02-11 07:55:20,755 - Step 41: Decoded token text:  u
2025-02-11 07:55:20,755 - Step 41: Updated current_phrase
2025-02-11 07:55:20,756 - Step 41: Created step_acts
2025-02-11 07:55:20,756 - Step 41: Added to generation_acts
2025-02-11 07:55:20,756 - Step 41: Updated recent_tokens
2025-02-11 07:55:20,758 - Step 41: Decoded current text
2025-02-11 07:55:20,758 - Step 41: Reset consecutive_fillers
2025-02-11 07:55:20,758 - Step 41: Calculated unique_ratio: 0.6875
2025-02-11 07:55:20,758 - 
Starting step 42
2025-02-11 07:55:20,758 - Current_ids device: cuda:0
2025-02-11 07:55:20,758 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,804 - Model output complete
2025-02-11 07:55:20,804 - Logits shape: torch.Size([1, 73, 151936])
2025-02-11 07:55:20,804 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,804 - Next token logits device: cuda:0
2025-02-11 07:55:20,804 - Entered do_sample
2025-02-11 07:55:20,804 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,807 - Probs max: 0.97119140625
2025-02-11 07:55:20,808 - Pre-cat
2025-02-11 07:55:20,808 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575]], device='cuda:0')
2025-02-11 07:55:20,812 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:20,812 - Current_ids shape: torch.Size([1, 73])
2025-02-11 07:55:20,812 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,812 - Step 42: Generated next token
2025-02-11 07:55:20,812 - Step 42: Updated current_ids
2025-02-11 07:55:20,813 - Step 42: Decoded token text: ,
2025-02-11 07:55:20,813 - Step 42: Updated current_phrase
2025-02-11 07:55:20,813 - Step 42: Created step_acts
2025-02-11 07:55:20,813 - Step 42: Added to generation_acts
2025-02-11 07:55:20,813 - Step 42: Updated recent_tokens
2025-02-11 07:55:20,815 - Step 42: Found phrase end token
2025-02-11 07:55:20,815 - Step 42: Updated recent_phrases
2025-02-11 07:55:20,815 - Step 42: Calculated similarity: 0.6
2025-02-11 07:55:20,815 - Step 42: Decoded current text
2025-02-11 07:55:20,815 - Step 42: Reset consecutive_fillers
2025-02-11 07:55:20,816 - Step 42: Calculated unique_ratio: 0.625
2025-02-11 07:55:20,816 - 
Starting step 43
2025-02-11 07:55:20,816 - Current_ids device: cuda:0
2025-02-11 07:55:20,816 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,839 - Model output complete
2025-02-11 07:55:20,839 - Logits shape: torch.Size([1, 74, 151936])
2025-02-11 07:55:20,839 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,839 - Next token logits device: cuda:0
2025-02-11 07:55:20,840 - Entered do_sample
2025-02-11 07:55:20,840 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,843 - Probs max: 0.89404296875
2025-02-11 07:55:20,844 - Pre-cat
2025-02-11 07:55:20,844 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11]], device='cuda:0')
2025-02-11 07:55:20,847 - Next token: tensor([[714]], device='cuda:0')
2025-02-11 07:55:20,847 - Current_ids shape: torch.Size([1, 74])
2025-02-11 07:55:20,847 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,847 - Step 43: Generated next token
2025-02-11 07:55:20,848 - Step 43: Updated current_ids
2025-02-11 07:55:20,848 - Step 43: Decoded token text:  but
2025-02-11 07:55:20,848 - Step 43: Updated current_phrase
2025-02-11 07:55:20,848 - Step 43: Created step_acts
2025-02-11 07:55:20,848 - Step 43: Added to generation_acts
2025-02-11 07:55:20,848 - Step 43: Updated recent_tokens
2025-02-11 07:55:20,850 - Step 43: Decoded current text
2025-02-11 07:55:20,850 - Step 43: Reset consecutive_fillers
2025-02-11 07:55:20,850 - Step 43: Calculated unique_ratio: 0.625
2025-02-11 07:55:20,850 - 
Starting step 44
2025-02-11 07:55:20,850 - Current_ids device: cuda:0
2025-02-11 07:55:20,850 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,873 - Model output complete
2025-02-11 07:55:20,873 - Logits shape: torch.Size([1, 75, 151936])
2025-02-11 07:55:20,873 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,873 - Next token logits device: cuda:0
2025-02-11 07:55:20,873 - Entered do_sample
2025-02-11 07:55:20,873 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,878 - Probs max: 0.89892578125
2025-02-11 07:55:20,879 - Pre-cat
2025-02-11 07:55:20,879 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714]], device='cuda:0')
2025-02-11 07:55:20,882 - Next token: tensor([[582]], device='cuda:0')
2025-02-11 07:55:20,882 - Current_ids shape: torch.Size([1, 75])
2025-02-11 07:55:20,883 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,883 - Step 44: Generated next token
2025-02-11 07:55:20,883 - Step 44: Updated current_ids
2025-02-11 07:55:20,883 - Step 44: Decoded token text:  we
2025-02-11 07:55:20,883 - Step 44: Updated current_phrase
2025-02-11 07:55:20,883 - Step 44: Created step_acts
2025-02-11 07:55:20,883 - Step 44: Added to generation_acts
2025-02-11 07:55:20,883 - Step 44: Updated recent_tokens
2025-02-11 07:55:20,885 - Step 44: Decoded current text
2025-02-11 07:55:20,885 - Step 44: Reset consecutive_fillers
2025-02-11 07:55:20,885 - Step 44: Calculated unique_ratio: 0.6875
2025-02-11 07:55:20,885 - 
Starting step 45
2025-02-11 07:55:20,885 - Current_ids device: cuda:0
2025-02-11 07:55:20,885 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,908 - Model output complete
2025-02-11 07:55:20,908 - Logits shape: torch.Size([1, 76, 151936])
2025-02-11 07:55:20,909 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,909 - Next token logits device: cuda:0
2025-02-11 07:55:20,909 - Entered do_sample
2025-02-11 07:55:20,910 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,913 - Probs max: 0.97021484375
2025-02-11 07:55:20,915 - Pre-cat
2025-02-11 07:55:20,915 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582]], device='cuda:0')
2025-02-11 07:55:20,921 - Next token: tensor([[646]], device='cuda:0')
2025-02-11 07:55:20,921 - Current_ids shape: torch.Size([1, 76])
2025-02-11 07:55:20,921 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,921 - Step 45: Generated next token
2025-02-11 07:55:20,921 - Step 45: Updated current_ids
2025-02-11 07:55:20,922 - Step 45: Decoded token text:  can
2025-02-11 07:55:20,922 - Step 45: Updated current_phrase
2025-02-11 07:55:20,922 - Step 45: Created step_acts
2025-02-11 07:55:20,922 - Step 45: Added to generation_acts
2025-02-11 07:55:20,924 - Step 45: Updated generated_texts
2025-02-11 07:55:20,924 - Step 45: Updated recent_tokens
2025-02-11 07:55:20,924 - Step 45: Decoded current text
2025-02-11 07:55:20,924 - Step 45: Reset consecutive_fillers
2025-02-11 07:55:20,925 - Step 45: Calculated unique_ratio: 0.75
2025-02-11 07:55:20,925 - 
Starting step 46
2025-02-11 07:55:20,925 - Current_ids device: cuda:0
2025-02-11 07:55:20,925 - Current_ids dtype: torch.int64
2025-02-11 07:55:20,973 - Model output complete
2025-02-11 07:55:20,973 - Logits shape: torch.Size([1, 77, 151936])
2025-02-11 07:55:20,973 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,973 - Next token logits device: cuda:0
2025-02-11 07:55:20,973 - Entered do_sample
2025-02-11 07:55:20,973 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:20,977 - Probs max: 0.9931640625
2025-02-11 07:55:20,977 - Pre-cat
2025-02-11 07:55:20,977 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646]], device='cuda:0')
2025-02-11 07:55:20,980 - Next token: tensor([[944]], device='cuda:0')
2025-02-11 07:55:20,980 - Current_ids shape: torch.Size([1, 77])
2025-02-11 07:55:20,980 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:20,980 - Step 46: Generated next token
2025-02-11 07:55:20,980 - Step 46: Updated current_ids
2025-02-11 07:55:20,980 - Step 46: Decoded token text: 't
2025-02-11 07:55:20,980 - Step 46: Updated current_phrase
2025-02-11 07:55:20,981 - Step 46: Created step_acts
2025-02-11 07:55:20,981 - Step 46: Added to generation_acts
2025-02-11 07:55:20,981 - Step 46: Updated recent_tokens
2025-02-11 07:55:20,982 - Step 46: Decoded current text
2025-02-11 07:55:20,983 - Step 46: Reset consecutive_fillers
2025-02-11 07:55:20,983 - Step 46: Calculated unique_ratio: 0.75
2025-02-11 07:55:20,983 - 
Starting step 47
2025-02-11 07:55:20,983 - Current_ids device: cuda:0
2025-02-11 07:55:20,983 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,006 - Model output complete
2025-02-11 07:55:21,007 - Logits shape: torch.Size([1, 78, 151936])
2025-02-11 07:55:21,007 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,007 - Next token logits device: cuda:0
2025-02-11 07:55:21,007 - Entered do_sample
2025-02-11 07:55:21,007 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,011 - Probs max: 0.99365234375
2025-02-11 07:55:21,011 - Pre-cat
2025-02-11 07:55:21,011 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944]], device='cuda:0')
2025-02-11 07:55:21,014 - Next token: tensor([[912]], device='cuda:0')
2025-02-11 07:55:21,014 - Current_ids shape: torch.Size([1, 78])
2025-02-11 07:55:21,014 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,014 - Step 47: Generated next token
2025-02-11 07:55:21,015 - Step 47: Updated current_ids
2025-02-11 07:55:21,015 - Step 47: Decoded token text:  add
2025-02-11 07:55:21,015 - Step 47: Updated current_phrase
2025-02-11 07:55:21,015 - Step 47: Created step_acts
2025-02-11 07:55:21,015 - Step 47: Added to generation_acts
2025-02-11 07:55:21,015 - Step 47: Updated recent_tokens
2025-02-11 07:55:21,017 - Step 47: Decoded current text
2025-02-11 07:55:21,017 - Step 47: Reset consecutive_fillers
2025-02-11 07:55:21,017 - Step 47: Calculated unique_ratio: 0.8125
2025-02-11 07:55:21,017 - 
Starting step 48
2025-02-11 07:55:21,017 - Current_ids device: cuda:0
2025-02-11 07:55:21,017 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,040 - Model output complete
2025-02-11 07:55:21,040 - Logits shape: torch.Size([1, 79, 151936])
2025-02-11 07:55:21,040 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,040 - Next token logits device: cuda:0
2025-02-11 07:55:21,040 - Entered do_sample
2025-02-11 07:55:21,041 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,045 - Probs max: 0.99462890625
2025-02-11 07:55:21,045 - Pre-cat
2025-02-11 07:55:21,045 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912]],
       device='cuda:0')
2025-02-11 07:55:21,050 - Next token: tensor([[1105]], device='cuda:0')
2025-02-11 07:55:21,050 - Current_ids shape: torch.Size([1, 79])
2025-02-11 07:55:21,050 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,050 - Step 48: Generated next token
2025-02-11 07:55:21,050 - Step 48: Updated current_ids
2025-02-11 07:55:21,050 - Step 48: Decoded token text:  them
2025-02-11 07:55:21,050 - Step 48: Updated current_phrase
2025-02-11 07:55:21,051 - Step 48: Created step_acts
2025-02-11 07:55:21,051 - Step 48: Added to generation_acts
2025-02-11 07:55:21,051 - Step 48: Updated recent_tokens
2025-02-11 07:55:21,052 - Step 48: Decoded current text
2025-02-11 07:55:21,052 - Step 48: Reset consecutive_fillers
2025-02-11 07:55:21,053 - Step 48: Calculated unique_ratio: 0.875
2025-02-11 07:55:21,053 - 
Starting step 49
2025-02-11 07:55:21,053 - Current_ids device: cuda:0
2025-02-11 07:55:21,053 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,080 - Model output complete
2025-02-11 07:55:21,080 - Logits shape: torch.Size([1, 80, 151936])
2025-02-11 07:55:21,080 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,080 - Next token logits device: cuda:0
2025-02-11 07:55:21,080 - Entered do_sample
2025-02-11 07:55:21,081 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,083 - Probs max: 0.951171875
2025-02-11 07:55:21,085 - Pre-cat
2025-02-11 07:55:21,085 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105]],
       device='cuda:0')
2025-02-11 07:55:21,090 - Next token: tensor([[1576]], device='cuda:0')
2025-02-11 07:55:21,091 - Current_ids shape: torch.Size([1, 80])
2025-02-11 07:55:21,091 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,091 - Step 49: Generated next token
2025-02-11 07:55:21,091 - Step 49: Updated current_ids
2025-02-11 07:55:21,091 - Step 49: Decoded token text:  because
2025-02-11 07:55:21,091 - Step 49: Updated current_phrase
2025-02-11 07:55:21,092 - Step 49: Created step_acts
2025-02-11 07:55:21,092 - Step 49: Added to generation_acts
2025-02-11 07:55:21,092 - Step 49: Updated recent_tokens
2025-02-11 07:55:21,094 - Step 49: Decoded current text
2025-02-11 07:55:21,094 - Step 49: Reset consecutive_fillers
2025-02-11 07:55:21,094 - Step 49: Calculated unique_ratio: 0.9375
2025-02-11 07:55:21,094 - 
Starting step 50
2025-02-11 07:55:21,094 - Current_ids device: cuda:0
2025-02-11 07:55:21,094 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,139 - Model output complete
2025-02-11 07:55:21,139 - Logits shape: torch.Size([1, 81, 151936])
2025-02-11 07:55:21,140 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,140 - Next token logits device: cuda:0
2025-02-11 07:55:21,140 - Entered do_sample
2025-02-11 07:55:21,140 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,143 - Probs max: 0.9931640625
2025-02-11 07:55:21,144 - Pre-cat
2025-02-11 07:55:21,144 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576]],
       device='cuda:0')
2025-02-11 07:55:21,147 - Next token: tensor([[807]], device='cuda:0')
2025-02-11 07:55:21,147 - Current_ids shape: torch.Size([1, 81])
2025-02-11 07:55:21,147 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,147 - Step 50: Generated next token
2025-02-11 07:55:21,147 - Step 50: Updated current_ids
2025-02-11 07:55:21,147 - Step 50: Decoded token text:  they
2025-02-11 07:55:21,147 - Step 50: Updated current_phrase
2025-02-11 07:55:21,148 - Step 50: Created step_acts
2025-02-11 07:55:21,148 - Step 50: Added to generation_acts
2025-02-11 07:55:21,149 - Step 50: Updated generated_texts
2025-02-11 07:55:21,149 - Step 50: Updated recent_tokens
2025-02-11 07:55:21,150 - Step 50: Decoded current text
2025-02-11 07:55:21,150 - Step 50: Reset consecutive_fillers
2025-02-11 07:55:21,150 - Step 50: Calculated unique_ratio: 0.9375
2025-02-11 07:55:21,150 - 
Starting step 51
2025-02-11 07:55:21,150 - Current_ids device: cuda:0
2025-02-11 07:55:21,150 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,173 - Model output complete
2025-02-11 07:55:21,173 - Logits shape: torch.Size([1, 82, 151936])
2025-02-11 07:55:21,173 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,173 - Next token logits device: cuda:0
2025-02-11 07:55:21,173 - Entered do_sample
2025-02-11 07:55:21,173 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,178 - Probs max: 0.93798828125
2025-02-11 07:55:21,178 - Pre-cat
2025-02-11 07:55:21,178 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807]], device='cuda:0')
2025-02-11 07:55:21,181 - Next token: tensor([[2299]], device='cuda:0')
2025-02-11 07:55:21,181 - Current_ids shape: torch.Size([1, 82])
2025-02-11 07:55:21,181 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,182 - Step 51: Generated next token
2025-02-11 07:55:21,182 - Step 51: Updated current_ids
2025-02-11 07:55:21,182 - Step 51: Decoded token text: 're
2025-02-11 07:55:21,182 - Step 51: Updated current_phrase
2025-02-11 07:55:21,182 - Step 51: Created step_acts
2025-02-11 07:55:21,182 - Step 51: Added to generation_acts
2025-02-11 07:55:21,182 - Step 51: Updated recent_tokens
2025-02-11 07:55:21,184 - Step 51: Decoded current text
2025-02-11 07:55:21,184 - Step 51: Reset consecutive_fillers
2025-02-11 07:55:21,184 - Step 51: Calculated unique_ratio: 1.0
2025-02-11 07:55:21,184 - 
Starting step 52
2025-02-11 07:55:21,184 - Current_ids device: cuda:0
2025-02-11 07:55:21,184 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,208 - Model output complete
2025-02-11 07:55:21,208 - Logits shape: torch.Size([1, 83, 151936])
2025-02-11 07:55:21,208 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,208 - Next token logits device: cuda:0
2025-02-11 07:55:21,208 - Entered do_sample
2025-02-11 07:55:21,208 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,213 - Probs max: 0.982421875
2025-02-11 07:55:21,213 - Pre-cat
2025-02-11 07:55:21,213 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299]], device='cuda:0')
2025-02-11 07:55:21,216 - Next token: tensor([[304]], device='cuda:0')
2025-02-11 07:55:21,217 - Current_ids shape: torch.Size([1, 83])
2025-02-11 07:55:21,217 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,217 - Step 52: Generated next token
2025-02-11 07:55:21,217 - Step 52: Updated current_ids
2025-02-11 07:55:21,217 - Step 52: Decoded token text:  in
2025-02-11 07:55:21,217 - Step 52: Updated current_phrase
2025-02-11 07:55:21,218 - Step 52: Created step_acts
2025-02-11 07:55:21,218 - Step 52: Added to generation_acts
2025-02-11 07:55:21,218 - Step 52: Updated recent_tokens
2025-02-11 07:55:21,219 - Step 52: Decoded current text
2025-02-11 07:55:21,219 - Step 52: Reset consecutive_fillers
2025-02-11 07:55:21,220 - Step 52: Calculated unique_ratio: 1.0
2025-02-11 07:55:21,220 - 
Starting step 53
2025-02-11 07:55:21,220 - Current_ids device: cuda:0
2025-02-11 07:55:21,220 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,250 - Model output complete
2025-02-11 07:55:21,250 - Logits shape: torch.Size([1, 84, 151936])
2025-02-11 07:55:21,250 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,250 - Next token logits device: cuda:0
2025-02-11 07:55:21,250 - Entered do_sample
2025-02-11 07:55:21,250 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,253 - Probs max: 0.99853515625
2025-02-11 07:55:21,254 - Pre-cat
2025-02-11 07:55:21,254 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304]], device='cuda:0')
2025-02-11 07:55:21,257 - Next token: tensor([[14002]], device='cuda:0')
2025-02-11 07:55:21,257 - Current_ids shape: torch.Size([1, 84])
2025-02-11 07:55:21,257 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,257 - Step 53: Generated next token
2025-02-11 07:55:21,257 - Step 53: Updated current_ids
2025-02-11 07:55:21,258 - Step 53: Decoded token text:  opposite
2025-02-11 07:55:21,258 - Step 53: Updated current_phrase
2025-02-11 07:55:21,258 - Step 53: Created step_acts
2025-02-11 07:55:21,258 - Step 53: Added to generation_acts
2025-02-11 07:55:21,258 - Step 53: Updated recent_tokens
2025-02-11 07:55:21,260 - Step 53: Decoded current text
2025-02-11 07:55:21,260 - Step 53: Reset consecutive_fillers
2025-02-11 07:55:21,260 - Step 53: Calculated unique_ratio: 1.0
2025-02-11 07:55:21,260 - 
Starting step 54
2025-02-11 07:55:21,260 - Current_ids device: cuda:0
2025-02-11 07:55:21,260 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,284 - Model output complete
2025-02-11 07:55:21,284 - Logits shape: torch.Size([1, 85, 151936])
2025-02-11 07:55:21,284 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,284 - Next token logits device: cuda:0
2025-02-11 07:55:21,284 - Entered do_sample
2025-02-11 07:55:21,284 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,288 - Probs max: 0.99853515625
2025-02-11 07:55:21,289 - Pre-cat
2025-02-11 07:55:21,289 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002]], device='cuda:0')
2025-02-11 07:55:21,293 - Next token: tensor([[17961]], device='cuda:0')
2025-02-11 07:55:21,294 - Current_ids shape: torch.Size([1, 85])
2025-02-11 07:55:21,294 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,294 - Step 54: Generated next token
2025-02-11 07:55:21,294 - Step 54: Updated current_ids
2025-02-11 07:55:21,294 - Step 54: Decoded token text:  directions
2025-02-11 07:55:21,294 - Step 54: Updated current_phrase
2025-02-11 07:55:21,294 - Step 54: Created step_acts
2025-02-11 07:55:21,295 - Step 54: Added to generation_acts
2025-02-11 07:55:21,295 - Step 54: Updated recent_tokens
2025-02-11 07:55:21,296 - Step 54: Decoded current text
2025-02-11 07:55:21,297 - Step 54: Reset consecutive_fillers
2025-02-11 07:55:21,297 - Step 54: Calculated unique_ratio: 1.0
2025-02-11 07:55:21,297 - 
Starting step 55
2025-02-11 07:55:21,297 - Current_ids device: cuda:0
2025-02-11 07:55:21,297 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,347 - Model output complete
2025-02-11 07:55:21,347 - Logits shape: torch.Size([1, 86, 151936])
2025-02-11 07:55:21,348 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,348 - Next token logits device: cuda:0
2025-02-11 07:55:21,348 - Entered do_sample
2025-02-11 07:55:21,348 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,352 - Probs max: 0.95947265625
2025-02-11 07:55:21,352 - Pre-cat
2025-02-11 07:55:21,353 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961]], device='cuda:0')
2025-02-11 07:55:21,356 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:55:21,356 - Current_ids shape: torch.Size([1, 86])
2025-02-11 07:55:21,356 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,356 - Step 55: Generated next token
2025-02-11 07:55:21,356 - Step 55: Updated current_ids
2025-02-11 07:55:21,356 - Step 55: Decoded token text: .
2025-02-11 07:55:21,356 - Step 55: Updated current_phrase
2025-02-11 07:55:21,357 - Step 55: Created step_acts
2025-02-11 07:55:21,357 - Step 55: Added to generation_acts
2025-02-11 07:55:21,358 - Step 55: Updated generated_texts
2025-02-11 07:55:21,358 - Step 55: Updated recent_tokens
2025-02-11 07:55:21,358 - Step 55: Found phrase end token
2025-02-11 07:55:21,358 - Step 55: Updated recent_phrases
2025-02-11 07:55:21,359 - Step 55: Calculated similarity: 0.0
2025-02-11 07:55:21,359 - Step 55: Decoded current text
2025-02-11 07:55:21,359 - Step 55: Reset consecutive_fillers
2025-02-11 07:55:21,359 - Step 55: Calculated unique_ratio: 1.0
2025-02-11 07:55:21,359 - 
Starting step 56
2025-02-11 07:55:21,359 - Current_ids device: cuda:0
2025-02-11 07:55:21,359 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,386 - Model output complete
2025-02-11 07:55:21,387 - Logits shape: torch.Size([1, 87, 151936])
2025-02-11 07:55:21,387 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,387 - Next token logits device: cuda:0
2025-02-11 07:55:21,387 - Entered do_sample
2025-02-11 07:55:21,387 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,389 - Probs max: 0.90576171875
2025-02-11 07:55:21,391 - Pre-cat
2025-02-11 07:55:21,391 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13]], device='cuda:0')
2025-02-11 07:55:21,397 - Next token: tensor([[2055]], device='cuda:0')
2025-02-11 07:55:21,398 - Current_ids shape: torch.Size([1, 87])
2025-02-11 07:55:21,398 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,398 - Step 56: Generated next token
2025-02-11 07:55:21,398 - Step 56: Updated current_ids
2025-02-11 07:55:21,398 - Step 56: Decoded token text:  So
2025-02-11 07:55:21,398 - Step 56: Updated current_phrase
2025-02-11 07:55:21,399 - Step 56: Created step_acts
2025-02-11 07:55:21,399 - Step 56: Added to generation_acts
2025-02-11 07:55:21,399 - Step 56: Updated recent_tokens
2025-02-11 07:55:21,400 - Step 56: Decoded current text
2025-02-11 07:55:21,400 - Step 56: Reset consecutive_fillers
2025-02-11 07:55:21,401 - Step 56: Calculated unique_ratio: 1.0
2025-02-11 07:55:21,401 - 
Starting step 57
2025-02-11 07:55:21,401 - Current_ids device: cuda:0
2025-02-11 07:55:21,401 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,426 - Model output complete
2025-02-11 07:55:21,427 - Logits shape: torch.Size([1, 88, 151936])
2025-02-11 07:55:21,427 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,427 - Next token logits device: cuda:0
2025-02-11 07:55:21,427 - Entered do_sample
2025-02-11 07:55:21,427 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,431 - Probs max: 0.92529296875
2025-02-11 07:55:21,433 - Pre-cat
2025-02-11 07:55:21,434 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055]],
       device='cuda:0')
2025-02-11 07:55:21,439 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:21,440 - Current_ids shape: torch.Size([1, 88])
2025-02-11 07:55:21,440 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,440 - Step 57: Generated next token
2025-02-11 07:55:21,440 - Step 57: Updated current_ids
2025-02-11 07:55:21,440 - Step 57: Decoded token text: ,
2025-02-11 07:55:21,440 - Step 57: Updated current_phrase
2025-02-11 07:55:21,441 - Step 57: Created step_acts
2025-02-11 07:55:21,441 - Step 57: Added to generation_acts
2025-02-11 07:55:21,441 - Step 57: Updated recent_tokens
2025-02-11 07:55:21,443 - Step 57: Found phrase end token
2025-02-11 07:55:21,443 - Step 57: Updated recent_phrases
2025-02-11 07:55:21,443 - Step 57: Calculated similarity: 0.0
2025-02-11 07:55:21,443 - Step 57: Decoded current text
2025-02-11 07:55:21,443 - Step 57: Reset consecutive_fillers
2025-02-11 07:55:21,444 - Step 57: Calculated unique_ratio: 0.9375
2025-02-11 07:55:21,444 - 
Starting step 58
2025-02-11 07:55:21,444 - Current_ids device: cuda:0
2025-02-11 07:55:21,444 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,468 - Model output complete
2025-02-11 07:55:21,468 - Logits shape: torch.Size([1, 89, 151936])
2025-02-11 07:55:21,468 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,468 - Next token logits device: cuda:0
2025-02-11 07:55:21,468 - Entered do_sample
2025-02-11 07:55:21,468 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,472 - Probs max: 0.98974609375
2025-02-11 07:55:21,472 - Pre-cat
2025-02-11 07:55:21,473 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11]],
       device='cuda:0')
2025-02-11 07:55:21,477 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:21,477 - Current_ids shape: torch.Size([1, 89])
2025-02-11 07:55:21,477 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,477 - Step 58: Generated next token
2025-02-11 07:55:21,477 - Step 58: Updated current_ids
2025-02-11 07:55:21,477 - Step 58: Decoded token text:  the
2025-02-11 07:55:21,477 - Step 58: Updated current_phrase
2025-02-11 07:55:21,478 - Step 58: Created step_acts
2025-02-11 07:55:21,478 - Step 58: Added to generation_acts
2025-02-11 07:55:21,478 - Step 58: Updated recent_tokens
2025-02-11 07:55:21,480 - Step 58: Decoded current text
2025-02-11 07:55:21,480 - Step 58: Reset consecutive_fillers
2025-02-11 07:55:21,480 - Step 58: Calculated unique_ratio: 1.0
2025-02-11 07:55:21,480 - 
Starting step 59
2025-02-11 07:55:21,480 - Current_ids device: cuda:0
2025-02-11 07:55:21,480 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,505 - Model output complete
2025-02-11 07:55:21,505 - Logits shape: torch.Size([1, 90, 151936])
2025-02-11 07:55:21,505 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,505 - Next token logits device: cuda:0
2025-02-11 07:55:21,505 - Entered do_sample
2025-02-11 07:55:21,506 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,509 - Probs max: 0.9814453125
2025-02-11 07:55:21,510 - Pre-cat
2025-02-11 07:55:21,510 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279]],
       device='cuda:0')
2025-02-11 07:55:21,513 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:21,513 - Current_ids shape: torch.Size([1, 90])
2025-02-11 07:55:21,513 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,513 - Step 59: Generated next token
2025-02-11 07:55:21,513 - Step 59: Updated current_ids
2025-02-11 07:55:21,513 - Step 59: Decoded token text:  ball
2025-02-11 07:55:21,513 - Step 59: Updated current_phrase
2025-02-11 07:55:21,514 - Step 59: Created step_acts
2025-02-11 07:55:21,514 - Step 59: Added to generation_acts
2025-02-11 07:55:21,514 - Step 59: Updated recent_tokens
2025-02-11 07:55:21,516 - Step 59: Decoded current text
2025-02-11 07:55:21,516 - Step 59: Reset consecutive_fillers
2025-02-11 07:55:21,516 - Step 59: Calculated unique_ratio: 1.0
2025-02-11 07:55:21,516 - 
Starting step 60
2025-02-11 07:55:21,516 - Current_ids device: cuda:0
2025-02-11 07:55:21,516 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,541 - Model output complete
2025-02-11 07:55:21,541 - Logits shape: torch.Size([1, 91, 151936])
2025-02-11 07:55:21,541 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,541 - Next token logits device: cuda:0
2025-02-11 07:55:21,541 - Entered do_sample
2025-02-11 07:55:21,541 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,545 - Probs max: 0.966796875
2025-02-11 07:55:21,546 - Pre-cat
2025-02-11 07:55:21,546 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935]], device='cuda:0')
2025-02-11 07:55:21,549 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:55:21,549 - Current_ids shape: torch.Size([1, 91])
2025-02-11 07:55:21,549 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,549 - Step 60: Generated next token
2025-02-11 07:55:21,549 - Step 60: Updated current_ids
2025-02-11 07:55:21,549 - Step 60: Decoded token text: 's
2025-02-11 07:55:21,549 - Step 60: Updated current_phrase
2025-02-11 07:55:21,550 - Step 60: Created step_acts
2025-02-11 07:55:21,550 - Step 60: Added to generation_acts
2025-02-11 07:55:21,552 - Step 60: Updated generated_texts
2025-02-11 07:55:21,552 - Step 60: Updated recent_tokens
2025-02-11 07:55:21,552 - Step 60: Decoded current text
2025-02-11 07:55:21,552 - Step 60: Reset consecutive_fillers
2025-02-11 07:55:21,552 - Step 60: Calculated unique_ratio: 1.0
2025-02-11 07:55:21,552 - 
Starting step 61
2025-02-11 07:55:21,552 - Current_ids device: cuda:0
2025-02-11 07:55:21,552 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,575 - Model output complete
2025-02-11 07:55:21,575 - Logits shape: torch.Size([1, 92, 151936])
2025-02-11 07:55:21,575 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,575 - Next token logits device: cuda:0
2025-02-11 07:55:21,575 - Entered do_sample
2025-02-11 07:55:21,575 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,580 - Probs max: 0.99755859375
2025-02-11 07:55:21,581 - Pre-cat
2025-02-11 07:55:21,581 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594]], device='cuda:0')
2025-02-11 07:55:21,584 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:21,584 - Current_ids shape: torch.Size([1, 92])
2025-02-11 07:55:21,584 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,584 - Step 61: Generated next token
2025-02-11 07:55:21,585 - Step 61: Updated current_ids
2025-02-11 07:55:21,585 - Step 61: Decoded token text:  speed
2025-02-11 07:55:21,585 - Step 61: Updated current_phrase
2025-02-11 07:55:21,585 - Step 61: Created step_acts
2025-02-11 07:55:21,585 - Step 61: Added to generation_acts
2025-02-11 07:55:21,585 - Step 61: Updated recent_tokens
2025-02-11 07:55:21,587 - Step 61: Decoded current text
2025-02-11 07:55:21,587 - Step 61: Reset consecutive_fillers
2025-02-11 07:55:21,587 - Step 61: Calculated unique_ratio: 1.0
2025-02-11 07:55:21,587 - 
Starting step 62
2025-02-11 07:55:21,587 - Current_ids device: cuda:0
2025-02-11 07:55:21,587 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,615 - Model output complete
2025-02-11 07:55:21,615 - Logits shape: torch.Size([1, 93, 151936])
2025-02-11 07:55:21,615 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,615 - Next token logits device: cuda:0
2025-02-11 07:55:21,615 - Entered do_sample
2025-02-11 07:55:21,615 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,617 - Probs max: 0.99658203125
2025-02-11 07:55:21,619 - Pre-cat
2025-02-11 07:55:21,619 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628]], device='cuda:0')
2025-02-11 07:55:21,625 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:21,626 - Current_ids shape: torch.Size([1, 93])
2025-02-11 07:55:21,626 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,626 - Step 62: Generated next token
2025-02-11 07:55:21,626 - Step 62: Updated current_ids
2025-02-11 07:55:21,626 - Step 62: Decoded token text:  is
2025-02-11 07:55:21,626 - Step 62: Updated current_phrase
2025-02-11 07:55:21,627 - Step 62: Created step_acts
2025-02-11 07:55:21,627 - Step 62: Added to generation_acts
2025-02-11 07:55:21,627 - Step 62: Updated recent_tokens
2025-02-11 07:55:21,628 - Step 62: Decoded current text
2025-02-11 07:55:21,629 - Step 62: Reset consecutive_fillers
2025-02-11 07:55:21,629 - Step 62: Calculated unique_ratio: 1.0
2025-02-11 07:55:21,629 - 
Starting step 63
2025-02-11 07:55:21,629 - Current_ids device: cuda:0
2025-02-11 07:55:21,629 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,654 - Model output complete
2025-02-11 07:55:21,654 - Logits shape: torch.Size([1, 94, 151936])
2025-02-11 07:55:21,655 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,655 - Next token logits device: cuda:0
2025-02-11 07:55:21,655 - Entered do_sample
2025-02-11 07:55:21,655 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,658 - Probs max: 0.99609375
2025-02-11 07:55:21,660 - Pre-cat
2025-02-11 07:55:21,660 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374]], device='cuda:0')
2025-02-11 07:55:21,666 - Next token: tensor([[348]], device='cuda:0')
2025-02-11 07:55:21,666 - Current_ids shape: torch.Size([1, 94])
2025-02-11 07:55:21,666 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,666 - Step 63: Generated next token
2025-02-11 07:55:21,667 - Step 63: Updated current_ids
2025-02-11 07:55:21,667 - Step 63: Decoded token text:  v
2025-02-11 07:55:21,667 - Step 63: Updated current_phrase
2025-02-11 07:55:21,668 - Step 63: Created step_acts
2025-02-11 07:55:21,668 - Step 63: Added to generation_acts
2025-02-11 07:55:21,668 - Step 63: Updated recent_tokens
2025-02-11 07:55:21,669 - Step 63: Decoded current text
2025-02-11 07:55:21,670 - Step 63: Reset consecutive_fillers
2025-02-11 07:55:21,670 - Step 63: Calculated unique_ratio: 1.0
2025-02-11 07:55:21,670 - 
Starting step 64
2025-02-11 07:55:21,670 - Current_ids device: cuda:0
2025-02-11 07:55:21,670 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,707 - Model output complete
2025-02-11 07:55:21,707 - Logits shape: torch.Size([1, 95, 151936])
2025-02-11 07:55:21,707 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,707 - Next token logits device: cuda:0
2025-02-11 07:55:21,707 - Entered do_sample
2025-02-11 07:55:21,707 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,710 - Probs max: 0.947265625
2025-02-11 07:55:21,710 - Pre-cat
2025-02-11 07:55:21,710 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348]], device='cuda:0')
2025-02-11 07:55:21,716 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:21,717 - Current_ids shape: torch.Size([1, 95])
2025-02-11 07:55:21,717 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,717 - Step 64: Generated next token
2025-02-11 07:55:21,717 - Step 64: Updated current_ids
2025-02-11 07:55:21,717 - Step 64: Decoded token text: ,
2025-02-11 07:55:21,717 - Step 64: Updated current_phrase
2025-02-11 07:55:21,718 - Step 64: Created step_acts
2025-02-11 07:55:21,718 - Step 64: Added to generation_acts
2025-02-11 07:55:21,718 - Step 64: Updated recent_tokens
2025-02-11 07:55:21,719 - Step 64: Found phrase end token
2025-02-11 07:55:21,719 - Step 64: Updated recent_phrases
2025-02-11 07:55:21,720 - Step 64: Calculated similarity: 0.0
2025-02-11 07:55:21,720 - Step 64: Decoded current text
2025-02-11 07:55:21,720 - Step 64: Reset consecutive_fillers
2025-02-11 07:55:21,720 - Step 64: Calculated unique_ratio: 0.9375
2025-02-11 07:55:21,720 - 
Starting step 65
2025-02-11 07:55:21,720 - Current_ids device: cuda:0
2025-02-11 07:55:21,720 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,750 - Model output complete
2025-02-11 07:55:21,750 - Logits shape: torch.Size([1, 96, 151936])
2025-02-11 07:55:21,750 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,750 - Next token logits device: cuda:0
2025-02-11 07:55:21,750 - Entered do_sample
2025-02-11 07:55:21,751 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,754 - Probs max: 0.96533203125
2025-02-11 07:55:21,755 - Pre-cat
2025-02-11 07:55:21,755 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11]], device='cuda:0')
2025-02-11 07:55:21,758 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:21,758 - Current_ids shape: torch.Size([1, 96])
2025-02-11 07:55:21,758 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,758 - Step 65: Generated next token
2025-02-11 07:55:21,758 - Step 65: Updated current_ids
2025-02-11 07:55:21,759 - Step 65: Decoded token text:  the
2025-02-11 07:55:21,759 - Step 65: Updated current_phrase
2025-02-11 07:55:21,759 - Step 65: Created step_acts
2025-02-11 07:55:21,759 - Step 65: Added to generation_acts
2025-02-11 07:55:21,761 - Step 65: Updated generated_texts
2025-02-11 07:55:21,761 - Step 65: Updated recent_tokens
2025-02-11 07:55:21,761 - Step 65: Decoded current text
2025-02-11 07:55:21,761 - Step 65: Reset consecutive_fillers
2025-02-11 07:55:21,762 - Step 65: Calculated unique_ratio: 0.875
2025-02-11 07:55:21,762 - 
Starting step 66
2025-02-11 07:55:21,762 - Current_ids device: cuda:0
2025-02-11 07:55:21,762 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,798 - Model output complete
2025-02-11 07:55:21,798 - Logits shape: torch.Size([1, 97, 151936])
2025-02-11 07:55:21,799 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,799 - Next token logits device: cuda:0
2025-02-11 07:55:21,799 - Entered do_sample
2025-02-11 07:55:21,799 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,801 - Probs max: 0.9970703125
2025-02-11 07:55:21,802 - Pre-cat
2025-02-11 07:55:21,802 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279]],
       device='cuda:0')
2025-02-11 07:55:21,807 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:21,807 - Current_ids shape: torch.Size([1, 97])
2025-02-11 07:55:21,807 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,808 - Step 66: Generated next token
2025-02-11 07:55:21,808 - Step 66: Updated current_ids
2025-02-11 07:55:21,808 - Step 66: Decoded token text:  wall
2025-02-11 07:55:21,808 - Step 66: Updated current_phrase
2025-02-11 07:55:21,808 - Step 66: Created step_acts
2025-02-11 07:55:21,808 - Step 66: Added to generation_acts
2025-02-11 07:55:21,808 - Step 66: Updated recent_tokens
2025-02-11 07:55:21,810 - Step 66: Decoded current text
2025-02-11 07:55:21,810 - Step 66: Reset consecutive_fillers
2025-02-11 07:55:21,810 - Step 66: Calculated unique_ratio: 0.875
2025-02-11 07:55:21,810 - 
Starting step 67
2025-02-11 07:55:21,810 - Current_ids device: cuda:0
2025-02-11 07:55:21,811 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,852 - Model output complete
2025-02-11 07:55:21,852 - Logits shape: torch.Size([1, 98, 151936])
2025-02-11 07:55:21,852 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,852 - Next token logits device: cuda:0
2025-02-11 07:55:21,852 - Entered do_sample
2025-02-11 07:55:21,852 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,854 - Probs max: 0.99951171875
2025-02-11 07:55:21,857 - Pre-cat
2025-02-11 07:55:21,857 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002]],
       device='cuda:0')
2025-02-11 07:55:21,865 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:55:21,865 - Current_ids shape: torch.Size([1, 98])
2025-02-11 07:55:21,865 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,865 - Step 67: Generated next token
2025-02-11 07:55:21,866 - Step 67: Updated current_ids
2025-02-11 07:55:21,866 - Step 67: Decoded token text: 's
2025-02-11 07:55:21,866 - Step 67: Updated current_phrase
2025-02-11 07:55:21,867 - Step 67: Created step_acts
2025-02-11 07:55:21,867 - Step 67: Added to generation_acts
2025-02-11 07:55:21,867 - Step 67: Updated recent_tokens
2025-02-11 07:55:21,869 - Step 67: Decoded current text
2025-02-11 07:55:21,869 - Step 67: Reset consecutive_fillers
2025-02-11 07:55:21,870 - Step 67: Calculated unique_ratio: 0.8125
2025-02-11 07:55:21,870 - 
Starting step 68
2025-02-11 07:55:21,870 - Current_ids device: cuda:0
2025-02-11 07:55:21,870 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,902 - Model output complete
2025-02-11 07:55:21,902 - Logits shape: torch.Size([1, 99, 151936])
2025-02-11 07:55:21,902 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,902 - Next token logits device: cuda:0
2025-02-11 07:55:21,902 - Entered do_sample
2025-02-11 07:55:21,902 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,906 - Probs max: 0.99951171875
2025-02-11 07:55:21,907 - Pre-cat
2025-02-11 07:55:21,907 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594]],
       device='cuda:0')
2025-02-11 07:55:21,910 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:21,911 - Current_ids shape: torch.Size([1, 99])
2025-02-11 07:55:21,911 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,911 - Step 68: Generated next token
2025-02-11 07:55:21,911 - Step 68: Updated current_ids
2025-02-11 07:55:21,911 - Step 68: Decoded token text:  speed
2025-02-11 07:55:21,911 - Step 68: Updated current_phrase
2025-02-11 07:55:21,912 - Step 68: Created step_acts
2025-02-11 07:55:21,912 - Step 68: Added to generation_acts
2025-02-11 07:55:21,912 - Step 68: Updated recent_tokens
2025-02-11 07:55:21,914 - Step 68: Decoded current text
2025-02-11 07:55:21,914 - Step 68: Reset consecutive_fillers
2025-02-11 07:55:21,914 - Step 68: Calculated unique_ratio: 0.75
2025-02-11 07:55:21,914 - 
Starting step 69
2025-02-11 07:55:21,914 - Current_ids device: cuda:0
2025-02-11 07:55:21,914 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,942 - Model output complete
2025-02-11 07:55:21,942 - Logits shape: torch.Size([1, 100, 151936])
2025-02-11 07:55:21,942 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,942 - Next token logits device: cuda:0
2025-02-11 07:55:21,942 - Entered do_sample
2025-02-11 07:55:21,942 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,945 - Probs max: 0.9990234375
2025-02-11 07:55:21,946 - Pre-cat
2025-02-11 07:55:21,946 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628]], device='cuda:0')
2025-02-11 07:55:21,949 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:21,950 - Current_ids shape: torch.Size([1, 100])
2025-02-11 07:55:21,950 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,950 - Step 69: Generated next token
2025-02-11 07:55:21,950 - Step 69: Updated current_ids
2025-02-11 07:55:21,950 - Step 69: Decoded token text:  is
2025-02-11 07:55:21,950 - Step 69: Updated current_phrase
2025-02-11 07:55:21,950 - Step 69: Created step_acts
2025-02-11 07:55:21,950 - Step 69: Added to generation_acts
2025-02-11 07:55:21,951 - Step 69: Updated recent_tokens
2025-02-11 07:55:21,952 - Step 69: Decoded current text
2025-02-11 07:55:21,952 - Step 69: Reset consecutive_fillers
2025-02-11 07:55:21,953 - Step 69: Calculated unique_ratio: 0.6875
2025-02-11 07:55:21,953 - 
Starting step 70
2025-02-11 07:55:21,953 - Current_ids device: cuda:0
2025-02-11 07:55:21,953 - Current_ids dtype: torch.int64
2025-02-11 07:55:21,978 - Model output complete
2025-02-11 07:55:21,978 - Logits shape: torch.Size([1, 101, 151936])
2025-02-11 07:55:21,978 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,978 - Next token logits device: cuda:0
2025-02-11 07:55:21,978 - Entered do_sample
2025-02-11 07:55:21,978 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:21,983 - Probs max: 0.99951171875
2025-02-11 07:55:21,983 - Pre-cat
2025-02-11 07:55:21,983 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374]], device='cuda:0')
2025-02-11 07:55:21,987 - Next token: tensor([[575]], device='cuda:0')
2025-02-11 07:55:21,987 - Current_ids shape: torch.Size([1, 101])
2025-02-11 07:55:21,987 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:21,987 - Step 70: Generated next token
2025-02-11 07:55:21,987 - Step 70: Updated current_ids
2025-02-11 07:55:21,988 - Step 70: Decoded token text:  u
2025-02-11 07:55:21,988 - Step 70: Updated current_phrase
2025-02-11 07:55:21,988 - Step 70: Created step_acts
2025-02-11 07:55:21,988 - Step 70: Added to generation_acts
2025-02-11 07:55:21,990 - Step 70: Updated generated_texts
2025-02-11 07:55:21,990 - Step 70: Updated recent_tokens
2025-02-11 07:55:21,990 - Step 70: Decoded current text
2025-02-11 07:55:21,990 - Step 70: Reset consecutive_fillers
2025-02-11 07:55:21,990 - Step 70: Calculated unique_ratio: 0.6875
2025-02-11 07:55:21,990 - 
Starting step 71
2025-02-11 07:55:21,990 - Current_ids device: cuda:0
2025-02-11 07:55:21,991 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,013 - Model output complete
2025-02-11 07:55:22,014 - Logits shape: torch.Size([1, 102, 151936])
2025-02-11 07:55:22,014 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,014 - Next token logits device: cuda:0
2025-02-11 07:55:22,014 - Entered do_sample
2025-02-11 07:55:22,014 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,019 - Probs max: 0.9794921875
2025-02-11 07:55:22,019 - Pre-cat
2025-02-11 07:55:22,019 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575]], device='cuda:0')
2025-02-11 07:55:22,023 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:22,023 - Current_ids shape: torch.Size([1, 102])
2025-02-11 07:55:22,023 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,023 - Step 71: Generated next token
2025-02-11 07:55:22,023 - Step 71: Updated current_ids
2025-02-11 07:55:22,024 - Step 71: Decoded token text: ,
2025-02-11 07:55:22,024 - Step 71: Updated current_phrase
2025-02-11 07:55:22,024 - Step 71: Created step_acts
2025-02-11 07:55:22,024 - Step 71: Added to generation_acts
2025-02-11 07:55:22,024 - Step 71: Updated recent_tokens
2025-02-11 07:55:22,026 - Step 71: Found phrase end token
2025-02-11 07:55:22,026 - Step 71: Updated recent_phrases
2025-02-11 07:55:22,026 - Step 71: Calculated similarity: 0.6
2025-02-11 07:55:22,027 - Step 71: Decoded current text
2025-02-11 07:55:22,027 - Step 71: Reset consecutive_fillers
2025-02-11 07:55:22,027 - Step 71: Calculated unique_ratio: 0.625
2025-02-11 07:55:22,027 - 
Starting step 72
2025-02-11 07:55:22,027 - Current_ids device: cuda:0
2025-02-11 07:55:22,027 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,069 - Model output complete
2025-02-11 07:55:22,069 - Logits shape: torch.Size([1, 103, 151936])
2025-02-11 07:55:22,069 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,069 - Next token logits device: cuda:0
2025-02-11 07:55:22,069 - Entered do_sample
2025-02-11 07:55:22,069 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,072 - Probs max: 0.99853515625
2025-02-11 07:55:22,074 - Pre-cat
2025-02-11 07:55:22,074 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11]], device='cuda:0')
2025-02-11 07:55:22,082 - Next token: tensor([[714]], device='cuda:0')
2025-02-11 07:55:22,082 - Current_ids shape: torch.Size([1, 103])
2025-02-11 07:55:22,082 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,082 - Step 72: Generated next token
2025-02-11 07:55:22,082 - Step 72: Updated current_ids
2025-02-11 07:55:22,083 - Step 72: Decoded token text:  but
2025-02-11 07:55:22,083 - Step 72: Updated current_phrase
2025-02-11 07:55:22,083 - Step 72: Created step_acts
2025-02-11 07:55:22,084 - Step 72: Added to generation_acts
2025-02-11 07:55:22,084 - Step 72: Updated recent_tokens
2025-02-11 07:55:22,085 - Step 72: Decoded current text
2025-02-11 07:55:22,085 - Step 72: Reset consecutive_fillers
2025-02-11 07:55:22,086 - Step 72: Calculated unique_ratio: 0.625
2025-02-11 07:55:22,086 - 
Starting step 73
2025-02-11 07:55:22,086 - Current_ids device: cuda:0
2025-02-11 07:55:22,086 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,133 - Model output complete
2025-02-11 07:55:22,134 - Logits shape: torch.Size([1, 104, 151936])
2025-02-11 07:55:22,134 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,134 - Next token logits device: cuda:0
2025-02-11 07:55:22,134 - Entered do_sample
2025-02-11 07:55:22,134 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,136 - Probs max: 0.99853515625
2025-02-11 07:55:22,138 - Pre-cat
2025-02-11 07:55:22,138 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714]], device='cuda:0')
2025-02-11 07:55:22,146 - Next token: tensor([[582]], device='cuda:0')
2025-02-11 07:55:22,147 - Current_ids shape: torch.Size([1, 104])
2025-02-11 07:55:22,147 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,147 - Step 73: Generated next token
2025-02-11 07:55:22,147 - Step 73: Updated current_ids
2025-02-11 07:55:22,147 - Step 73: Decoded token text:  we
2025-02-11 07:55:22,147 - Step 73: Updated current_phrase
2025-02-11 07:55:22,148 - Step 73: Created step_acts
2025-02-11 07:55:22,148 - Step 73: Added to generation_acts
2025-02-11 07:55:22,148 - Step 73: Updated recent_tokens
2025-02-11 07:55:22,150 - Step 73: Decoded current text
2025-02-11 07:55:22,150 - Step 73: Reset consecutive_fillers
2025-02-11 07:55:22,150 - Step 73: Calculated unique_ratio: 0.6875
2025-02-11 07:55:22,150 - 
Starting step 74
2025-02-11 07:55:22,150 - Current_ids device: cuda:0
2025-02-11 07:55:22,150 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,205 - Model output complete
2025-02-11 07:55:22,205 - Logits shape: torch.Size([1, 105, 151936])
2025-02-11 07:55:22,205 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,205 - Next token logits device: cuda:0
2025-02-11 07:55:22,205 - Entered do_sample
2025-02-11 07:55:22,205 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,208 - Probs max: 0.99560546875
2025-02-11 07:55:22,210 - Pre-cat
2025-02-11 07:55:22,211 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582]], device='cuda:0')
2025-02-11 07:55:22,218 - Next token: tensor([[646]], device='cuda:0')
2025-02-11 07:55:22,218 - Current_ids shape: torch.Size([1, 105])
2025-02-11 07:55:22,219 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,219 - Step 74: Generated next token
2025-02-11 07:55:22,219 - Step 74: Updated current_ids
2025-02-11 07:55:22,219 - Step 74: Decoded token text:  can
2025-02-11 07:55:22,219 - Step 74: Updated current_phrase
2025-02-11 07:55:22,220 - Step 74: Created step_acts
2025-02-11 07:55:22,220 - Step 74: Added to generation_acts
2025-02-11 07:55:22,220 - Step 74: Updated recent_tokens
2025-02-11 07:55:22,222 - Step 74: Decoded current text
2025-02-11 07:55:22,222 - Step 74: Reset consecutive_fillers
2025-02-11 07:55:22,223 - Step 74: Calculated unique_ratio: 0.75
2025-02-11 07:55:22,223 - 
Starting step 75
2025-02-11 07:55:22,223 - Current_ids device: cuda:0
2025-02-11 07:55:22,223 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,261 - Model output complete
2025-02-11 07:55:22,261 - Logits shape: torch.Size([1, 106, 151936])
2025-02-11 07:55:22,261 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,261 - Next token logits device: cuda:0
2025-02-11 07:55:22,262 - Entered do_sample
2025-02-11 07:55:22,262 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,266 - Probs max: 0.99853515625
2025-02-11 07:55:22,266 - Pre-cat
2025-02-11 07:55:22,266 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646]],
       device='cuda:0')
2025-02-11 07:55:22,271 - Next token: tensor([[944]], device='cuda:0')
2025-02-11 07:55:22,272 - Current_ids shape: torch.Size([1, 106])
2025-02-11 07:55:22,272 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,272 - Step 75: Generated next token
2025-02-11 07:55:22,272 - Step 75: Updated current_ids
2025-02-11 07:55:22,272 - Step 75: Decoded token text: 't
2025-02-11 07:55:22,272 - Step 75: Updated current_phrase
2025-02-11 07:55:22,273 - Step 75: Created step_acts
2025-02-11 07:55:22,273 - Step 75: Added to generation_acts
2025-02-11 07:55:22,275 - Step 75: Updated generated_texts
2025-02-11 07:55:22,275 - Step 75: Updated recent_tokens
2025-02-11 07:55:22,275 - Step 75: Decoded current text
2025-02-11 07:55:22,275 - Step 75: Reset consecutive_fillers
2025-02-11 07:55:22,275 - Step 75: Calculated unique_ratio: 0.75
2025-02-11 07:55:22,275 - 
Starting step 76
2025-02-11 07:55:22,275 - Current_ids device: cuda:0
2025-02-11 07:55:22,275 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,304 - Model output complete
2025-02-11 07:55:22,304 - Logits shape: torch.Size([1, 107, 151936])
2025-02-11 07:55:22,304 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,304 - Next token logits device: cuda:0
2025-02-11 07:55:22,305 - Entered do_sample
2025-02-11 07:55:22,305 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,307 - Probs max: 0.99755859375
2025-02-11 07:55:22,308 - Pre-cat
2025-02-11 07:55:22,308 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944]],
       device='cuda:0')
2025-02-11 07:55:22,314 - Next token: tensor([[912]], device='cuda:0')
2025-02-11 07:55:22,314 - Current_ids shape: torch.Size([1, 107])
2025-02-11 07:55:22,314 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,314 - Step 76: Generated next token
2025-02-11 07:55:22,314 - Step 76: Updated current_ids
2025-02-11 07:55:22,315 - Step 76: Decoded token text:  add
2025-02-11 07:55:22,315 - Step 76: Updated current_phrase
2025-02-11 07:55:22,315 - Step 76: Created step_acts
2025-02-11 07:55:22,315 - Step 76: Added to generation_acts
2025-02-11 07:55:22,315 - Step 76: Updated recent_tokens
2025-02-11 07:55:22,317 - Step 76: Decoded current text
2025-02-11 07:55:22,317 - Step 76: Reset consecutive_fillers
2025-02-11 07:55:22,317 - Step 76: Calculated unique_ratio: 0.8125
2025-02-11 07:55:22,317 - 
Starting step 77
2025-02-11 07:55:22,317 - Current_ids device: cuda:0
2025-02-11 07:55:22,318 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,348 - Model output complete
2025-02-11 07:55:22,349 - Logits shape: torch.Size([1, 108, 151936])
2025-02-11 07:55:22,349 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,349 - Next token logits device: cuda:0
2025-02-11 07:55:22,349 - Entered do_sample
2025-02-11 07:55:22,349 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,351 - Probs max: 0.9990234375
2025-02-11 07:55:22,353 - Pre-cat
2025-02-11 07:55:22,353 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912]],
       device='cuda:0')
2025-02-11 07:55:22,360 - Next token: tensor([[1105]], device='cuda:0')
2025-02-11 07:55:22,361 - Current_ids shape: torch.Size([1, 108])
2025-02-11 07:55:22,361 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,361 - Step 77: Generated next token
2025-02-11 07:55:22,361 - Step 77: Updated current_ids
2025-02-11 07:55:22,361 - Step 77: Decoded token text:  them
2025-02-11 07:55:22,361 - Step 77: Updated current_phrase
2025-02-11 07:55:22,362 - Step 77: Created step_acts
2025-02-11 07:55:22,362 - Step 77: Added to generation_acts
2025-02-11 07:55:22,362 - Step 77: Updated recent_tokens
2025-02-11 07:55:22,364 - Step 77: Decoded current text
2025-02-11 07:55:22,364 - Step 77: Reset consecutive_fillers
2025-02-11 07:55:22,364 - Step 77: Calculated unique_ratio: 0.875
2025-02-11 07:55:22,364 - 
Starting step 78
2025-02-11 07:55:22,364 - Current_ids device: cuda:0
2025-02-11 07:55:22,365 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,398 - Model output complete
2025-02-11 07:55:22,398 - Logits shape: torch.Size([1, 109, 151936])
2025-02-11 07:55:22,398 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,398 - Next token logits device: cuda:0
2025-02-11 07:55:22,398 - Entered do_sample
2025-02-11 07:55:22,398 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,403 - Probs max: 0.998046875
2025-02-11 07:55:22,403 - Pre-cat
2025-02-11 07:55:22,403 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105]], device='cuda:0')
2025-02-11 07:55:22,407 - Next token: tensor([[1576]], device='cuda:0')
2025-02-11 07:55:22,407 - Current_ids shape: torch.Size([1, 109])
2025-02-11 07:55:22,407 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,407 - Step 78: Generated next token
2025-02-11 07:55:22,407 - Step 78: Updated current_ids
2025-02-11 07:55:22,408 - Step 78: Decoded token text:  because
2025-02-11 07:55:22,408 - Step 78: Updated current_phrase
2025-02-11 07:55:22,408 - Step 78: Created step_acts
2025-02-11 07:55:22,408 - Step 78: Added to generation_acts
2025-02-11 07:55:22,408 - Step 78: Updated recent_tokens
2025-02-11 07:55:22,410 - Step 78: Decoded current text
2025-02-11 07:55:22,410 - Step 78: Reset consecutive_fillers
2025-02-11 07:55:22,410 - Step 78: Calculated unique_ratio: 0.9375
2025-02-11 07:55:22,410 - 
Starting step 79
2025-02-11 07:55:22,410 - Current_ids device: cuda:0
2025-02-11 07:55:22,410 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,436 - Model output complete
2025-02-11 07:55:22,436 - Logits shape: torch.Size([1, 110, 151936])
2025-02-11 07:55:22,436 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,436 - Next token logits device: cuda:0
2025-02-11 07:55:22,436 - Entered do_sample
2025-02-11 07:55:22,437 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,440 - Probs max: 0.99853515625
2025-02-11 07:55:22,441 - Pre-cat
2025-02-11 07:55:22,441 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576]], device='cuda:0')
2025-02-11 07:55:22,447 - Next token: tensor([[807]], device='cuda:0')
2025-02-11 07:55:22,448 - Current_ids shape: torch.Size([1, 110])
2025-02-11 07:55:22,448 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,448 - Step 79: Generated next token
2025-02-11 07:55:22,448 - Step 79: Updated current_ids
2025-02-11 07:55:22,448 - Step 79: Decoded token text:  they
2025-02-11 07:55:22,448 - Step 79: Updated current_phrase
2025-02-11 07:55:22,449 - Step 79: Created step_acts
2025-02-11 07:55:22,449 - Step 79: Added to generation_acts
2025-02-11 07:55:22,449 - Step 79: Updated recent_tokens
2025-02-11 07:55:22,451 - Step 79: Decoded current text
2025-02-11 07:55:22,451 - Step 79: Reset consecutive_fillers
2025-02-11 07:55:22,451 - Step 79: Calculated unique_ratio: 0.9375
2025-02-11 07:55:22,451 - 
Starting step 80
2025-02-11 07:55:22,451 - Current_ids device: cuda:0
2025-02-11 07:55:22,451 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,475 - Model output complete
2025-02-11 07:55:22,475 - Logits shape: torch.Size([1, 111, 151936])
2025-02-11 07:55:22,475 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,475 - Next token logits device: cuda:0
2025-02-11 07:55:22,475 - Entered do_sample
2025-02-11 07:55:22,475 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,481 - Probs max: 0.94091796875
2025-02-11 07:55:22,482 - Pre-cat
2025-02-11 07:55:22,482 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807]], device='cuda:0')
2025-02-11 07:55:22,487 - Next token: tensor([[2299]], device='cuda:0')
2025-02-11 07:55:22,488 - Current_ids shape: torch.Size([1, 111])
2025-02-11 07:55:22,488 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,488 - Step 80: Generated next token
2025-02-11 07:55:22,488 - Step 80: Updated current_ids
2025-02-11 07:55:22,488 - Step 80: Decoded token text: 're
2025-02-11 07:55:22,488 - Step 80: Updated current_phrase
2025-02-11 07:55:22,489 - Step 80: Created step_acts
2025-02-11 07:55:22,489 - Step 80: Added to generation_acts
2025-02-11 07:55:22,491 - Step 80: Updated generated_texts
2025-02-11 07:55:22,491 - Step 80: Updated recent_tokens
2025-02-11 07:55:22,491 - Step 80: Decoded current text
2025-02-11 07:55:22,491 - Step 80: Reset consecutive_fillers
2025-02-11 07:55:22,492 - Step 80: Calculated unique_ratio: 1.0
2025-02-11 07:55:22,492 - 
Starting step 81
2025-02-11 07:55:22,492 - Current_ids device: cuda:0
2025-02-11 07:55:22,492 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,532 - Model output complete
2025-02-11 07:55:22,532 - Logits shape: torch.Size([1, 112, 151936])
2025-02-11 07:55:22,532 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,532 - Next token logits device: cuda:0
2025-02-11 07:55:22,532 - Entered do_sample
2025-02-11 07:55:22,533 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,537 - Probs max: 0.9970703125
2025-02-11 07:55:22,538 - Pre-cat
2025-02-11 07:55:22,538 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299]], device='cuda:0')
2025-02-11 07:55:22,542 - Next token: tensor([[304]], device='cuda:0')
2025-02-11 07:55:22,542 - Current_ids shape: torch.Size([1, 112])
2025-02-11 07:55:22,542 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,542 - Step 81: Generated next token
2025-02-11 07:55:22,542 - Step 81: Updated current_ids
2025-02-11 07:55:22,542 - Step 81: Decoded token text:  in
2025-02-11 07:55:22,542 - Step 81: Updated current_phrase
2025-02-11 07:55:22,542 - Step 81: Created step_acts
2025-02-11 07:55:22,543 - Step 81: Added to generation_acts
2025-02-11 07:55:22,543 - Step 81: Updated recent_tokens
2025-02-11 07:55:22,545 - Step 81: Decoded current text
2025-02-11 07:55:22,545 - Step 81: Reset consecutive_fillers
2025-02-11 07:55:22,545 - Step 81: Calculated unique_ratio: 1.0
2025-02-11 07:55:22,545 - 
Starting step 82
2025-02-11 07:55:22,545 - Current_ids device: cuda:0
2025-02-11 07:55:22,545 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,567 - Model output complete
2025-02-11 07:55:22,568 - Logits shape: torch.Size([1, 113, 151936])
2025-02-11 07:55:22,568 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,568 - Next token logits device: cuda:0
2025-02-11 07:55:22,568 - Entered do_sample
2025-02-11 07:55:22,568 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,575 - Probs max: 0.99951171875
2025-02-11 07:55:22,576 - Pre-cat
2025-02-11 07:55:22,576 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304]], device='cuda:0')
2025-02-11 07:55:22,580 - Next token: tensor([[14002]], device='cuda:0')
2025-02-11 07:55:22,580 - Current_ids shape: torch.Size([1, 113])
2025-02-11 07:55:22,580 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,580 - Step 82: Generated next token
2025-02-11 07:55:22,580 - Step 82: Updated current_ids
2025-02-11 07:55:22,581 - Step 82: Decoded token text:  opposite
2025-02-11 07:55:22,581 - Step 82: Updated current_phrase
2025-02-11 07:55:22,581 - Step 82: Created step_acts
2025-02-11 07:55:22,581 - Step 82: Added to generation_acts
2025-02-11 07:55:22,581 - Step 82: Updated recent_tokens
2025-02-11 07:55:22,583 - Step 82: Decoded current text
2025-02-11 07:55:22,583 - Step 82: Reset consecutive_fillers
2025-02-11 07:55:22,583 - Step 82: Calculated unique_ratio: 1.0
2025-02-11 07:55:22,583 - 
Starting step 83
2025-02-11 07:55:22,583 - Current_ids device: cuda:0
2025-02-11 07:55:22,583 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,607 - Model output complete
2025-02-11 07:55:22,607 - Logits shape: torch.Size([1, 114, 151936])
2025-02-11 07:55:22,607 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,607 - Next token logits device: cuda:0
2025-02-11 07:55:22,607 - Entered do_sample
2025-02-11 07:55:22,607 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,613 - Probs max: 0.99951171875
2025-02-11 07:55:22,614 - Pre-cat
2025-02-11 07:55:22,614 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002]], device='cuda:0')
2025-02-11 07:55:22,618 - Next token: tensor([[17961]], device='cuda:0')
2025-02-11 07:55:22,618 - Current_ids shape: torch.Size([1, 114])
2025-02-11 07:55:22,618 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,618 - Step 83: Generated next token
2025-02-11 07:55:22,618 - Step 83: Updated current_ids
2025-02-11 07:55:22,618 - Step 83: Decoded token text:  directions
2025-02-11 07:55:22,618 - Step 83: Updated current_phrase
2025-02-11 07:55:22,619 - Step 83: Created step_acts
2025-02-11 07:55:22,619 - Step 83: Added to generation_acts
2025-02-11 07:55:22,619 - Step 83: Updated recent_tokens
2025-02-11 07:55:22,621 - Step 83: Decoded current text
2025-02-11 07:55:22,621 - Step 83: Reset consecutive_fillers
2025-02-11 07:55:22,621 - Step 83: Calculated unique_ratio: 1.0
2025-02-11 07:55:22,621 - 
Starting step 84
2025-02-11 07:55:22,621 - Current_ids device: cuda:0
2025-02-11 07:55:22,621 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,649 - Model output complete
2025-02-11 07:55:22,649 - Logits shape: torch.Size([1, 115, 151936])
2025-02-11 07:55:22,649 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,649 - Next token logits device: cuda:0
2025-02-11 07:55:22,650 - Entered do_sample
2025-02-11 07:55:22,650 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,652 - Probs max: 0.9462890625
2025-02-11 07:55:22,654 - Pre-cat
2025-02-11 07:55:22,654 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961]],
       device='cuda:0')
2025-02-11 07:55:22,663 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:55:22,663 - Current_ids shape: torch.Size([1, 115])
2025-02-11 07:55:22,663 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,663 - Step 84: Generated next token
2025-02-11 07:55:22,663 - Step 84: Updated current_ids
2025-02-11 07:55:22,664 - Step 84: Decoded token text: .
2025-02-11 07:55:22,664 - Step 84: Updated current_phrase
2025-02-11 07:55:22,665 - Step 84: Created step_acts
2025-02-11 07:55:22,665 - Step 84: Added to generation_acts
2025-02-11 07:55:22,665 - Step 84: Updated recent_tokens
2025-02-11 07:55:22,666 - Step 84: Found phrase end token
2025-02-11 07:55:22,666 - Step 84: Updated recent_phrases
2025-02-11 07:55:22,666 - Step 84: Calculated similarity: 0.0
2025-02-11 07:55:22,667 - Step 84: Decoded current text
2025-02-11 07:55:22,667 - Step 84: Reset consecutive_fillers
2025-02-11 07:55:22,667 - Step 84: Calculated unique_ratio: 1.0
2025-02-11 07:55:22,667 - 
Starting step 85
2025-02-11 07:55:22,667 - Current_ids device: cuda:0
2025-02-11 07:55:22,667 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,745 - Model output complete
2025-02-11 07:55:22,745 - Logits shape: torch.Size([1, 116, 151936])
2025-02-11 07:55:22,745 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,746 - Next token logits device: cuda:0
2025-02-11 07:55:22,746 - Entered do_sample
2025-02-11 07:55:22,746 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,749 - Probs max: 0.91748046875
2025-02-11 07:55:22,750 - Pre-cat
2025-02-11 07:55:22,750 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13]],
       device='cuda:0')
2025-02-11 07:55:22,754 - Next token: tensor([[2055]], device='cuda:0')
2025-02-11 07:55:22,754 - Current_ids shape: torch.Size([1, 116])
2025-02-11 07:55:22,754 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,754 - Step 85: Generated next token
2025-02-11 07:55:22,754 - Step 85: Updated current_ids
2025-02-11 07:55:22,755 - Step 85: Decoded token text:  So
2025-02-11 07:55:22,755 - Step 85: Updated current_phrase
2025-02-11 07:55:22,755 - Step 85: Created step_acts
2025-02-11 07:55:22,755 - Step 85: Added to generation_acts
2025-02-11 07:55:22,757 - Step 85: Updated generated_texts
2025-02-11 07:55:22,757 - Step 85: Updated recent_tokens
2025-02-11 07:55:22,758 - Step 85: Decoded current text
2025-02-11 07:55:22,758 - Step 85: Reset consecutive_fillers
2025-02-11 07:55:22,758 - Step 85: Calculated unique_ratio: 1.0
2025-02-11 07:55:22,758 - 
Starting step 86
2025-02-11 07:55:22,758 - Current_ids device: cuda:0
2025-02-11 07:55:22,758 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,786 - Model output complete
2025-02-11 07:55:22,786 - Logits shape: torch.Size([1, 117, 151936])
2025-02-11 07:55:22,786 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,786 - Next token logits device: cuda:0
2025-02-11 07:55:22,786 - Entered do_sample
2025-02-11 07:55:22,787 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,789 - Probs max: 0.955078125
2025-02-11 07:55:22,791 - Pre-cat
2025-02-11 07:55:22,791 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055]],
       device='cuda:0')
2025-02-11 07:55:22,800 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:22,801 - Current_ids shape: torch.Size([1, 117])
2025-02-11 07:55:22,801 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,801 - Step 86: Generated next token
2025-02-11 07:55:22,801 - Step 86: Updated current_ids
2025-02-11 07:55:22,802 - Step 86: Decoded token text: ,
2025-02-11 07:55:22,802 - Step 86: Updated current_phrase
2025-02-11 07:55:22,803 - Step 86: Created step_acts
2025-02-11 07:55:22,803 - Step 86: Added to generation_acts
2025-02-11 07:55:22,803 - Step 86: Updated recent_tokens
2025-02-11 07:55:22,805 - Step 86: Found phrase end token
2025-02-11 07:55:22,805 - Step 86: Updated recent_phrases
2025-02-11 07:55:22,805 - Step 86: Calculated similarity: 0.0
2025-02-11 07:55:22,806 - Step 86: Decoded current text
2025-02-11 07:55:22,806 - Step 86: Reset consecutive_fillers
2025-02-11 07:55:22,806 - Step 86: Calculated unique_ratio: 0.9375
2025-02-11 07:55:22,806 - 
Starting step 87
2025-02-11 07:55:22,806 - Current_ids device: cuda:0
2025-02-11 07:55:22,806 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,830 - Model output complete
2025-02-11 07:55:22,831 - Logits shape: torch.Size([1, 118, 151936])
2025-02-11 07:55:22,831 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,831 - Next token logits device: cuda:0
2025-02-11 07:55:22,831 - Entered do_sample
2025-02-11 07:55:22,831 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,836 - Probs max: 0.99609375
2025-02-11 07:55:22,837 - Pre-cat
2025-02-11 07:55:22,837 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055,
             11]], device='cuda:0')
2025-02-11 07:55:22,842 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:22,842 - Current_ids shape: torch.Size([1, 118])
2025-02-11 07:55:22,842 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,843 - Step 87: Generated next token
2025-02-11 07:55:22,843 - Step 87: Updated current_ids
2025-02-11 07:55:22,843 - Step 87: Decoded token text:  the
2025-02-11 07:55:22,843 - Step 87: Updated current_phrase
2025-02-11 07:55:22,843 - Step 87: Created step_acts
2025-02-11 07:55:22,843 - Step 87: Added to generation_acts
2025-02-11 07:55:22,843 - Step 87: Updated recent_tokens
2025-02-11 07:55:22,845 - Step 87: Decoded current text
2025-02-11 07:55:22,845 - Step 87: Reset consecutive_fillers
2025-02-11 07:55:22,846 - Step 87: Calculated unique_ratio: 1.0
2025-02-11 07:55:22,846 - 
Starting step 88
2025-02-11 07:55:22,846 - Current_ids device: cuda:0
2025-02-11 07:55:22,846 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,870 - Model output complete
2025-02-11 07:55:22,871 - Logits shape: torch.Size([1, 119, 151936])
2025-02-11 07:55:22,871 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,871 - Next token logits device: cuda:0
2025-02-11 07:55:22,871 - Entered do_sample
2025-02-11 07:55:22,871 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,875 - Probs max: 0.99853515625
2025-02-11 07:55:22,876 - Pre-cat
2025-02-11 07:55:22,876 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055,
             11,    279]], device='cuda:0')
2025-02-11 07:55:22,883 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:22,884 - Current_ids shape: torch.Size([1, 119])
2025-02-11 07:55:22,884 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,884 - Step 88: Generated next token
2025-02-11 07:55:22,884 - Step 88: Updated current_ids
2025-02-11 07:55:22,885 - Step 88: Decoded token text:  ball
2025-02-11 07:55:22,885 - Step 88: Updated current_phrase
2025-02-11 07:55:22,885 - Step 88: Created step_acts
2025-02-11 07:55:22,885 - Step 88: Added to generation_acts
2025-02-11 07:55:22,885 - Step 88: Updated recent_tokens
2025-02-11 07:55:22,887 - Step 88: Decoded current text
2025-02-11 07:55:22,887 - Step 88: Reset consecutive_fillers
2025-02-11 07:55:22,887 - Step 88: Calculated unique_ratio: 1.0
2025-02-11 07:55:22,887 - 
Starting step 89
2025-02-11 07:55:22,887 - Current_ids device: cuda:0
2025-02-11 07:55:22,888 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,912 - Model output complete
2025-02-11 07:55:22,912 - Logits shape: torch.Size([1, 120, 151936])
2025-02-11 07:55:22,912 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,912 - Next token logits device: cuda:0
2025-02-11 07:55:22,912 - Entered do_sample
2025-02-11 07:55:22,913 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,917 - Probs max: 0.99853515625
2025-02-11 07:55:22,918 - Pre-cat
2025-02-11 07:55:22,918 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055,
             11,    279,   4935]], device='cuda:0')
2025-02-11 07:55:22,922 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:55:22,922 - Current_ids shape: torch.Size([1, 120])
2025-02-11 07:55:22,922 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,923 - Step 89: Generated next token
2025-02-11 07:55:22,923 - Step 89: Updated current_ids
2025-02-11 07:55:22,923 - Step 89: Decoded token text: 's
2025-02-11 07:55:22,923 - Step 89: Updated current_phrase
2025-02-11 07:55:22,923 - Step 89: Created step_acts
2025-02-11 07:55:22,923 - Step 89: Added to generation_acts
2025-02-11 07:55:22,923 - Step 89: Updated recent_tokens
2025-02-11 07:55:22,925 - Step 89: Decoded current text
2025-02-11 07:55:22,925 - Step 89: Reset consecutive_fillers
2025-02-11 07:55:22,926 - Step 89: Calculated unique_ratio: 1.0
2025-02-11 07:55:22,926 - 
Starting step 90
2025-02-11 07:55:22,926 - Current_ids device: cuda:0
2025-02-11 07:55:22,926 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,950 - Model output complete
2025-02-11 07:55:22,950 - Logits shape: torch.Size([1, 121, 151936])
2025-02-11 07:55:22,950 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,950 - Next token logits device: cuda:0
2025-02-11 07:55:22,950 - Entered do_sample
2025-02-11 07:55:22,950 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,955 - Probs max: 0.9990234375
2025-02-11 07:55:22,956 - Pre-cat
2025-02-11 07:55:22,956 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055,
             11,    279,   4935,    594]], device='cuda:0')
2025-02-11 07:55:22,961 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:22,961 - Current_ids shape: torch.Size([1, 121])
2025-02-11 07:55:22,961 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:22,961 - Step 90: Generated next token
2025-02-11 07:55:22,961 - Step 90: Updated current_ids
2025-02-11 07:55:22,961 - Step 90: Decoded token text:  speed
2025-02-11 07:55:22,961 - Step 90: Updated current_phrase
2025-02-11 07:55:22,962 - Step 90: Created step_acts
2025-02-11 07:55:22,962 - Step 90: Added to generation_acts
2025-02-11 07:55:22,964 - Step 90: Updated generated_texts
2025-02-11 07:55:22,964 - Step 90: Updated recent_tokens
2025-02-11 07:55:22,964 - Step 90: Decoded current text
2025-02-11 07:55:22,964 - Step 90: Reset consecutive_fillers
2025-02-11 07:55:22,964 - Step 90: Calculated unique_ratio: 1.0
2025-02-11 07:55:22,964 - 
Starting step 91
2025-02-11 07:55:22,964 - Current_ids device: cuda:0
2025-02-11 07:55:22,964 - Current_ids dtype: torch.int64
2025-02-11 07:55:22,988 - Model output complete
2025-02-11 07:55:22,988 - Logits shape: torch.Size([1, 122, 151936])
2025-02-11 07:55:22,988 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,988 - Next token logits device: cuda:0
2025-02-11 07:55:22,988 - Entered do_sample
2025-02-11 07:55:22,988 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:22,994 - Probs max: 0.9990234375
2025-02-11 07:55:22,994 - Pre-cat
2025-02-11 07:55:22,994 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055,
             11,    279,   4935,    594,   4628]], device='cuda:0')
2025-02-11 07:55:23,000 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:23,000 - Current_ids shape: torch.Size([1, 122])
2025-02-11 07:55:23,000 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,000 - Step 91: Generated next token
2025-02-11 07:55:23,001 - Step 91: Updated current_ids
2025-02-11 07:55:23,001 - Step 91: Decoded token text:  is
2025-02-11 07:55:23,001 - Step 91: Updated current_phrase
2025-02-11 07:55:23,001 - Step 91: Created step_acts
2025-02-11 07:55:23,001 - Step 91: Added to generation_acts
2025-02-11 07:55:23,001 - Step 91: Updated recent_tokens
2025-02-11 07:55:23,003 - Step 91: Decoded current text
2025-02-11 07:55:23,003 - Step 91: Reset consecutive_fillers
2025-02-11 07:55:23,004 - Step 91: Calculated unique_ratio: 1.0
2025-02-11 07:55:23,004 - 
Starting step 92
2025-02-11 07:55:23,004 - Current_ids device: cuda:0
2025-02-11 07:55:23,004 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,045 - Model output complete
2025-02-11 07:55:23,045 - Logits shape: torch.Size([1, 123, 151936])
2025-02-11 07:55:23,045 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,045 - Next token logits device: cuda:0
2025-02-11 07:55:23,046 - Entered do_sample
2025-02-11 07:55:23,046 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,049 - Probs max: 0.99951171875
2025-02-11 07:55:23,050 - Pre-cat
2025-02-11 07:55:23,050 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055,
             11,    279,   4935,    594,   4628,    374]], device='cuda:0')
2025-02-11 07:55:23,054 - Next token: tensor([[348]], device='cuda:0')
2025-02-11 07:55:23,054 - Current_ids shape: torch.Size([1, 123])
2025-02-11 07:55:23,054 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,054 - Step 92: Generated next token
2025-02-11 07:55:23,054 - Step 92: Updated current_ids
2025-02-11 07:55:23,055 - Step 92: Decoded token text:  v
2025-02-11 07:55:23,055 - Step 92: Updated current_phrase
2025-02-11 07:55:23,055 - Step 92: Created step_acts
2025-02-11 07:55:23,055 - Step 92: Added to generation_acts
2025-02-11 07:55:23,055 - Step 92: Updated recent_tokens
2025-02-11 07:55:23,057 - Step 92: Decoded current text
2025-02-11 07:55:23,057 - Step 92: Reset consecutive_fillers
2025-02-11 07:55:23,057 - Step 92: Calculated unique_ratio: 1.0
2025-02-11 07:55:23,057 - 
Starting step 93
2025-02-11 07:55:23,057 - Current_ids device: cuda:0
2025-02-11 07:55:23,057 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,082 - Model output complete
2025-02-11 07:55:23,082 - Logits shape: torch.Size([1, 124, 151936])
2025-02-11 07:55:23,082 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,082 - Next token logits device: cuda:0
2025-02-11 07:55:23,083 - Entered do_sample
2025-02-11 07:55:23,083 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,087 - Probs max: 0.998046875
2025-02-11 07:55:23,089 - Pre-cat
2025-02-11 07:55:23,089 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055,
             11,    279,   4935,    594,   4628,    374,    348]],
       device='cuda:0')
2025-02-11 07:55:23,099 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:23,100 - Current_ids shape: torch.Size([1, 124])
2025-02-11 07:55:23,100 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,100 - Step 93: Generated next token
2025-02-11 07:55:23,100 - Step 93: Updated current_ids
2025-02-11 07:55:23,100 - Step 93: Decoded token text: ,
2025-02-11 07:55:23,100 - Step 93: Updated current_phrase
2025-02-11 07:55:23,101 - Step 93: Created step_acts
2025-02-11 07:55:23,101 - Step 93: Added to generation_acts
2025-02-11 07:55:23,101 - Step 93: Updated recent_tokens
2025-02-11 07:55:23,103 - Step 93: Found phrase end token
2025-02-11 07:55:23,103 - Step 93: Updated recent_phrases
2025-02-11 07:55:23,103 - Step 93: Calculated similarity: 0.0
2025-02-11 07:55:23,103 - Step 93: Decoded current text
2025-02-11 07:55:23,103 - Step 93: Reset consecutive_fillers
2025-02-11 07:55:23,103 - Step 93: Calculated unique_ratio: 0.9375
2025-02-11 07:55:23,103 - 
Starting step 94
2025-02-11 07:55:23,104 - Current_ids device: cuda:0
2025-02-11 07:55:23,104 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,139 - Model output complete
2025-02-11 07:55:23,139 - Logits shape: torch.Size([1, 125, 151936])
2025-02-11 07:55:23,139 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,139 - Next token logits device: cuda:0
2025-02-11 07:55:23,140 - Entered do_sample
2025-02-11 07:55:23,140 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,143 - Probs max: 0.99853515625
2025-02-11 07:55:23,143 - Pre-cat
2025-02-11 07:55:23,144 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055,
             11,    279,   4935,    594,   4628,    374,    348,     11]],
       device='cuda:0')
2025-02-11 07:55:23,150 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:23,150 - Current_ids shape: torch.Size([1, 125])
2025-02-11 07:55:23,150 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,150 - Step 94: Generated next token
2025-02-11 07:55:23,150 - Step 94: Updated current_ids
2025-02-11 07:55:23,151 - Step 94: Decoded token text:  the
2025-02-11 07:55:23,151 - Step 94: Updated current_phrase
2025-02-11 07:55:23,151 - Step 94: Created step_acts
2025-02-11 07:55:23,151 - Step 94: Added to generation_acts
2025-02-11 07:55:23,151 - Step 94: Updated recent_tokens
2025-02-11 07:55:23,153 - Step 94: Decoded current text
2025-02-11 07:55:23,153 - Step 94: Reset consecutive_fillers
2025-02-11 07:55:23,153 - Step 94: Calculated unique_ratio: 0.875
2025-02-11 07:55:23,153 - 
Starting step 95
2025-02-11 07:55:23,154 - Current_ids device: cuda:0
2025-02-11 07:55:23,154 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,177 - Model output complete
2025-02-11 07:55:23,177 - Logits shape: torch.Size([1, 126, 151936])
2025-02-11 07:55:23,177 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,177 - Next token logits device: cuda:0
2025-02-11 07:55:23,177 - Entered do_sample
2025-02-11 07:55:23,177 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,184 - Probs max: 0.9990234375
2025-02-11 07:55:23,184 - Pre-cat
2025-02-11 07:55:23,185 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055,
             11,    279,   4935,    594,   4628,    374,    348,     11,    279]],
       device='cuda:0')
2025-02-11 07:55:23,189 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:23,189 - Current_ids shape: torch.Size([1, 126])
2025-02-11 07:55:23,189 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,189 - Step 95: Generated next token
2025-02-11 07:55:23,189 - Step 95: Updated current_ids
2025-02-11 07:55:23,189 - Step 95: Decoded token text:  wall
2025-02-11 07:55:23,190 - Step 95: Updated current_phrase
2025-02-11 07:55:23,190 - Step 95: Created step_acts
2025-02-11 07:55:23,190 - Step 95: Added to generation_acts
2025-02-11 07:55:23,192 - Step 95: Updated generated_texts
2025-02-11 07:55:23,192 - Step 95: Updated recent_tokens
2025-02-11 07:55:23,193 - Step 95: Decoded current text
2025-02-11 07:55:23,193 - Step 95: Reset consecutive_fillers
2025-02-11 07:55:23,193 - Step 95: Calculated unique_ratio: 0.875
2025-02-11 07:55:23,193 - 
Starting step 96
2025-02-11 07:55:23,193 - Current_ids device: cuda:0
2025-02-11 07:55:23,193 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,226 - Model output complete
2025-02-11 07:55:23,226 - Logits shape: torch.Size([1, 127, 151936])
2025-02-11 07:55:23,226 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,226 - Next token logits device: cuda:0
2025-02-11 07:55:23,226 - Entered do_sample
2025-02-11 07:55:23,226 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,228 - Probs max: 0.99951171875
2025-02-11 07:55:23,230 - Pre-cat
2025-02-11 07:55:23,230 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055,
             11,    279,   4935,    594,   4628,    374,    348,     11,    279,
           7002]], device='cuda:0')
2025-02-11 07:55:23,239 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:55:23,240 - Current_ids shape: torch.Size([1, 127])
2025-02-11 07:55:23,240 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,240 - Step 96: Generated next token
2025-02-11 07:55:23,240 - Step 96: Updated current_ids
2025-02-11 07:55:23,240 - Step 96: Decoded token text: 's
2025-02-11 07:55:23,240 - Step 96: Updated current_phrase
2025-02-11 07:55:23,241 - Step 96: Created step_acts
2025-02-11 07:55:23,241 - Step 96: Added to generation_acts
2025-02-11 07:55:23,241 - Step 96: Updated recent_tokens
2025-02-11 07:55:23,244 - Step 96: Decoded current text
2025-02-11 07:55:23,244 - Step 96: Reset consecutive_fillers
2025-02-11 07:55:23,244 - Step 96: Calculated unique_ratio: 0.8125
2025-02-11 07:55:23,244 - 
Starting step 97
2025-02-11 07:55:23,244 - Current_ids device: cuda:0
2025-02-11 07:55:23,244 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,275 - Model output complete
2025-02-11 07:55:23,275 - Logits shape: torch.Size([1, 128, 151936])
2025-02-11 07:55:23,275 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,275 - Next token logits device: cuda:0
2025-02-11 07:55:23,275 - Entered do_sample
2025-02-11 07:55:23,275 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,281 - Probs max: 0.99951171875
2025-02-11 07:55:23,282 - Pre-cat
2025-02-11 07:55:23,282 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055,
             11,    279,   4935,    594,   4628,    374,    348,     11,    279,
           7002,    594]], device='cuda:0')
2025-02-11 07:55:23,286 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:23,287 - Current_ids shape: torch.Size([1, 128])
2025-02-11 07:55:23,287 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,287 - Step 97: Generated next token
2025-02-11 07:55:23,287 - Step 97: Updated current_ids
2025-02-11 07:55:23,287 - Step 97: Decoded token text:  speed
2025-02-11 07:55:23,287 - Step 97: Updated current_phrase
2025-02-11 07:55:23,288 - Step 97: Created step_acts
2025-02-11 07:55:23,288 - Step 97: Added to generation_acts
2025-02-11 07:55:23,288 - Step 97: Updated recent_tokens
2025-02-11 07:55:23,290 - Step 97: Decoded current text
2025-02-11 07:55:23,290 - Step 97: Reset consecutive_fillers
2025-02-11 07:55:23,290 - Step 97: Calculated unique_ratio: 0.75
2025-02-11 07:55:23,290 - 
Starting step 98
2025-02-11 07:55:23,290 - Current_ids device: cuda:0
2025-02-11 07:55:23,290 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,317 - Model output complete
2025-02-11 07:55:23,317 - Logits shape: torch.Size([1, 129, 151936])
2025-02-11 07:55:23,317 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,317 - Next token logits device: cuda:0
2025-02-11 07:55:23,317 - Entered do_sample
2025-02-11 07:55:23,317 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,328 - Probs max: 0.99951171875
2025-02-11 07:55:23,329 - Pre-cat
2025-02-11 07:55:23,329 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055,
             11,    279,   4935,    594,   4628,    374,    348,     11,    279,
           7002,    594,   4628]], device='cuda:0')
2025-02-11 07:55:23,333 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:23,334 - Current_ids shape: torch.Size([1, 129])
2025-02-11 07:55:23,334 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,334 - Step 98: Generated next token
2025-02-11 07:55:23,334 - Step 98: Updated current_ids
2025-02-11 07:55:23,334 - Step 98: Decoded token text:  is
2025-02-11 07:55:23,334 - Step 98: Updated current_phrase
2025-02-11 07:55:23,335 - Step 98: Created step_acts
2025-02-11 07:55:23,335 - Step 98: Added to generation_acts
2025-02-11 07:55:23,335 - Step 98: Updated recent_tokens
2025-02-11 07:55:23,337 - Step 98: Decoded current text
2025-02-11 07:55:23,337 - Step 98: Reset consecutive_fillers
2025-02-11 07:55:23,338 - Step 98: Calculated unique_ratio: 0.6875
2025-02-11 07:55:23,338 - 
Starting step 99
2025-02-11 07:55:23,338 - Current_ids device: cuda:0
2025-02-11 07:55:23,338 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,363 - Model output complete
2025-02-11 07:55:23,363 - Logits shape: torch.Size([1, 130, 151936])
2025-02-11 07:55:23,363 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,363 - Next token logits device: cuda:0
2025-02-11 07:55:23,363 - Entered do_sample
2025-02-11 07:55:23,363 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,378 - Probs max: 1.0
2025-02-11 07:55:23,379 - Pre-cat
2025-02-11 07:55:23,379 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    594,   4628,    374,
            348,     11,    279,   7002,    594,   4628,    374,    575,     11,
            714,    582,    646,    944,    912,   1105,   1576,    807,   2299,
            304,  14002,  17961,     13,   2055,     11,    279,   4935,    594,
           4628,    374,    348,     11,    279,   7002,    594,   4628,    374,
            575,     11,    714,    582,    646,    944,    912,   1105,   1576,
            807,   2299,    304,  14002,  17961,     13,   2055,     11,    279,
           4935,    594,   4628,    374,    348,     11,    279,   7002,    594,
           4628,    374,    575,     11,    714,    582,    646,    944,    912,
           1105,   1576,    807,   2299,    304,  14002,  17961,     13,   2055,
             11,    279,   4935,    594,   4628,    374,    348,     11,    279,
           7002,    594,   4628,    374]], device='cuda:0')
2025-02-11 07:55:23,383 - Next token: tensor([[575]], device='cuda:0')
2025-02-11 07:55:23,384 - Current_ids shape: torch.Size([1, 130])
2025-02-11 07:55:23,384 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,384 - Step 99: Generated next token
2025-02-11 07:55:23,384 - Step 99: Updated current_ids
2025-02-11 07:55:23,384 - Step 99: Decoded token text:  u
2025-02-11 07:55:23,384 - Step 99: Updated current_phrase
2025-02-11 07:55:23,384 - Step 99: Created step_acts
2025-02-11 07:55:23,385 - Step 99: Added to generation_acts
2025-02-11 07:55:23,387 - Step 99: Updated generated_texts
2025-02-11 07:55:23,387 - Step 99: Updated recent_tokens
2025-02-11 07:55:23,388 - Step 99: Decoded current text
2025-02-11 07:55:23,388 - Step 99: Reset consecutive_fillers
2025-02-11 07:55:23,388 - Step 99: Calculated unique_ratio: 0.6875
2025-02-11 07:55:23,496 - 
Starting step 0
2025-02-11 07:55:23,496 - Current_ids device: cuda:0
2025-02-11 07:55:23,496 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,540 - Model output complete
2025-02-11 07:55:23,540 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:23,541 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,541 - Next token logits device: cuda:0
2025-02-11 07:55:23,541 - Entered do_sample
2025-02-11 07:55:23,541 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,543 - Probs max: 0.50341796875
2025-02-11 07:55:23,544 - Pre-cat
2025-02-11 07:55:23,544 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:23,547 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:23,547 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:23,547 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,547 - Step 0: Generated next token
2025-02-11 07:55:23,547 - Step 0: Updated current_ids
2025-02-11 07:55:23,547 - Step 0: Decoded token text:  The
2025-02-11 07:55:23,548 - Step 0: Updated current_phrase
2025-02-11 07:55:23,548 - Step 0: Created step_acts
2025-02-11 07:55:23,548 - Step 0: Added to generation_acts
2025-02-11 07:55:23,549 - Step 0: Updated generated_texts
2025-02-11 07:55:23,550 - Step 0: Updated recent_tokens
2025-02-11 07:55:23,550 - Step 0: Decoded current text
2025-02-11 07:55:23,550 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:23,550 - 
Starting step 1
2025-02-11 07:55:23,550 - Current_ids device: cuda:0
2025-02-11 07:55:23,550 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,577 - Model output complete
2025-02-11 07:55:23,577 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:23,577 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,577 - Next token logits device: cuda:0
2025-02-11 07:55:23,577 - Entered do_sample
2025-02-11 07:55:23,577 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,580 - Probs max: 0.83837890625
2025-02-11 07:55:23,580 - Pre-cat
2025-02-11 07:55:23,580 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:55:23,582 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:23,582 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:23,582 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,583 - Step 1: Generated next token
2025-02-11 07:55:23,583 - Step 1: Updated current_ids
2025-02-11 07:55:23,583 - Step 1: Decoded token text:  ball
2025-02-11 07:55:23,583 - Step 1: Updated current_phrase
2025-02-11 07:55:23,583 - Step 1: Created step_acts
2025-02-11 07:55:23,583 - Step 1: Added to generation_acts
2025-02-11 07:55:23,583 - Step 1: Updated recent_tokens
2025-02-11 07:55:23,585 - Step 1: Decoded current text
2025-02-11 07:55:23,585 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:23,585 - 
Starting step 2
2025-02-11 07:55:23,585 - Current_ids device: cuda:0
2025-02-11 07:55:23,585 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,639 - Model output complete
2025-02-11 07:55:23,639 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:23,639 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,639 - Next token logits device: cuda:0
2025-02-11 07:55:23,639 - Entered do_sample
2025-02-11 07:55:23,639 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,642 - Probs max: 0.583984375
2025-02-11 07:55:23,642 - Pre-cat
2025-02-11 07:55:23,642 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:55:23,644 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:23,644 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:23,644 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,644 - Step 2: Generated next token
2025-02-11 07:55:23,644 - Step 2: Updated current_ids
2025-02-11 07:55:23,644 - Step 2: Decoded token text:  will
2025-02-11 07:55:23,644 - Step 2: Updated current_phrase
2025-02-11 07:55:23,645 - Step 2: Created step_acts
2025-02-11 07:55:23,645 - Step 2: Added to generation_acts
2025-02-11 07:55:23,645 - Step 2: Updated recent_tokens
2025-02-11 07:55:23,646 - Step 2: Decoded current text
2025-02-11 07:55:23,646 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:23,646 - 
Starting step 3
2025-02-11 07:55:23,646 - Current_ids device: cuda:0
2025-02-11 07:55:23,646 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,670 - Model output complete
2025-02-11 07:55:23,670 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:23,670 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,670 - Next token logits device: cuda:0
2025-02-11 07:55:23,670 - Entered do_sample
2025-02-11 07:55:23,670 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,673 - Probs max: 0.56884765625
2025-02-11 07:55:23,674 - Pre-cat
2025-02-11 07:55:23,674 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686]],
       device='cuda:0')
2025-02-11 07:55:23,675 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:23,676 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:23,676 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,676 - Step 3: Generated next token
2025-02-11 07:55:23,676 - Step 3: Updated current_ids
2025-02-11 07:55:23,676 - Step 3: Decoded token text:  very
2025-02-11 07:55:23,676 - Step 3: Updated current_phrase
2025-02-11 07:55:23,676 - Step 3: Created step_acts
2025-02-11 07:55:23,676 - Step 3: Added to generation_acts
2025-02-11 07:55:23,677 - Step 3: Updated recent_tokens
2025-02-11 07:55:23,678 - Step 3: Decoded current text
2025-02-11 07:55:23,678 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:23,678 - 
Starting step 4
2025-02-11 07:55:23,678 - Current_ids device: cuda:0
2025-02-11 07:55:23,678 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,712 - Model output complete
2025-02-11 07:55:23,712 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:23,712 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,712 - Next token logits device: cuda:0
2025-02-11 07:55:23,712 - Entered do_sample
2025-02-11 07:55:23,713 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,715 - Probs max: 0.61181640625
2025-02-11 07:55:23,716 - Pre-cat
2025-02-11 07:55:23,716 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   1602]],
       device='cuda:0')
2025-02-11 07:55:23,718 - Next token: tensor([[6157]], device='cuda:0')
2025-02-11 07:55:23,718 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:23,718 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,718 - Step 4: Generated next token
2025-02-11 07:55:23,718 - Step 4: Updated current_ids
2025-02-11 07:55:23,719 - Step 4: Decoded token text:  quickly
2025-02-11 07:55:23,719 - Step 4: Updated current_phrase
2025-02-11 07:55:23,719 - Step 4: Created step_acts
2025-02-11 07:55:23,719 - Step 4: Added to generation_acts
2025-02-11 07:55:23,719 - Step 4: Updated recent_tokens
2025-02-11 07:55:23,721 - Step 4: Decoded current text
2025-02-11 07:55:23,721 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:23,721 - 
Starting step 5
2025-02-11 07:55:23,721 - Current_ids device: cuda:0
2025-02-11 07:55:23,721 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,745 - Model output complete
2025-02-11 07:55:23,746 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:23,746 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,746 - Next token logits device: cuda:0
2025-02-11 07:55:23,746 - Entered do_sample
2025-02-11 07:55:23,746 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,748 - Probs max: 0.5478515625
2025-02-11 07:55:23,749 - Pre-cat
2025-02-11 07:55:23,749 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   1602,   6157]],
       device='cuda:0')
2025-02-11 07:55:23,750 - Next token: tensor([[5545]], device='cuda:0')
2025-02-11 07:55:23,751 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:23,751 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,751 - Step 5: Generated next token
2025-02-11 07:55:23,751 - Step 5: Updated current_ids
2025-02-11 07:55:23,751 - Step 5: Decoded token text:  reach
2025-02-11 07:55:23,751 - Step 5: Updated current_phrase
2025-02-11 07:55:23,751 - Step 5: Created step_acts
2025-02-11 07:55:23,751 - Step 5: Added to generation_acts
2025-02-11 07:55:23,753 - Step 5: Updated generated_texts
2025-02-11 07:55:23,753 - Step 5: Updated recent_tokens
2025-02-11 07:55:23,753 - Step 5: Decoded current text
2025-02-11 07:55:23,753 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:23,753 - 
Starting step 6
2025-02-11 07:55:23,753 - Current_ids device: cuda:0
2025-02-11 07:55:23,753 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,777 - Model output complete
2025-02-11 07:55:23,778 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:23,778 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,778 - Next token logits device: cuda:0
2025-02-11 07:55:23,778 - Entered do_sample
2025-02-11 07:55:23,778 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,780 - Probs max: 0.96435546875
2025-02-11 07:55:23,781 - Pre-cat
2025-02-11 07:55:23,781 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   1602,   6157,
           5545]], device='cuda:0')
2025-02-11 07:55:23,783 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:23,783 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:23,783 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,783 - Step 6: Generated next token
2025-02-11 07:55:23,783 - Step 6: Updated current_ids
2025-02-11 07:55:23,784 - Step 6: Decoded token text:  the
2025-02-11 07:55:23,784 - Step 6: Updated current_phrase
2025-02-11 07:55:23,784 - Step 6: Created step_acts
2025-02-11 07:55:23,784 - Step 6: Added to generation_acts
2025-02-11 07:55:23,784 - Step 6: Updated recent_tokens
2025-02-11 07:55:23,785 - Step 6: Decoded current text
2025-02-11 07:55:23,785 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:23,786 - 
Starting step 7
2025-02-11 07:55:23,786 - Current_ids device: cuda:0
2025-02-11 07:55:23,786 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,808 - Model output complete
2025-02-11 07:55:23,808 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:23,808 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,808 - Next token logits device: cuda:0
2025-02-11 07:55:23,808 - Entered do_sample
2025-02-11 07:55:23,809 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,811 - Probs max: 0.9970703125
2025-02-11 07:55:23,812 - Pre-cat
2025-02-11 07:55:23,812 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   1602,   6157,
           5545,    279]], device='cuda:0')
2025-02-11 07:55:23,814 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:23,814 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:23,814 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,814 - Step 7: Generated next token
2025-02-11 07:55:23,814 - Step 7: Updated current_ids
2025-02-11 07:55:23,814 - Step 7: Decoded token text:  wall
2025-02-11 07:55:23,814 - Step 7: Updated current_phrase
2025-02-11 07:55:23,815 - Step 7: Created step_acts
2025-02-11 07:55:23,815 - Step 7: Added to generation_acts
2025-02-11 07:55:23,815 - Step 7: Updated recent_tokens
2025-02-11 07:55:23,816 - Step 7: Decoded current text
2025-02-11 07:55:23,816 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:23,816 - 
Starting step 8
2025-02-11 07:55:23,816 - Current_ids device: cuda:0
2025-02-11 07:55:23,816 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,839 - Model output complete
2025-02-11 07:55:23,839 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:23,840 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,840 - Next token logits device: cuda:0
2025-02-11 07:55:23,840 - Entered do_sample
2025-02-11 07:55:23,840 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,842 - Probs max: 0.478515625
2025-02-11 07:55:23,843 - Pre-cat
2025-02-11 07:55:23,843 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   1602,   6157,
           5545,    279,   7002]], device='cuda:0')
2025-02-11 07:55:23,844 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:23,845 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:23,845 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,845 - Step 8: Generated next token
2025-02-11 07:55:23,845 - Step 8: Updated current_ids
2025-02-11 07:55:23,845 - Step 8: Decoded token text: ,
2025-02-11 07:55:23,845 - Step 8: Updated current_phrase
2025-02-11 07:55:23,845 - Step 8: Created step_acts
2025-02-11 07:55:23,845 - Step 8: Added to generation_acts
2025-02-11 07:55:23,846 - Step 8: Updated recent_tokens
2025-02-11 07:55:23,847 - Step 8: Found phrase end token
2025-02-11 07:55:23,847 - Step 8: Updated recent_phrases
2025-02-11 07:55:23,847 - Step 8: Decoded current text
2025-02-11 07:55:23,847 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:23,847 - 
Starting step 9
2025-02-11 07:55:23,847 - Current_ids device: cuda:0
2025-02-11 07:55:23,847 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,871 - Model output complete
2025-02-11 07:55:23,871 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:23,871 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,871 - Next token logits device: cuda:0
2025-02-11 07:55:23,871 - Entered do_sample
2025-02-11 07:55:23,872 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,874 - Probs max: 0.60302734375
2025-02-11 07:55:23,875 - Pre-cat
2025-02-11 07:55:23,875 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   1602,   6157,
           5545,    279,   7002,     11]], device='cuda:0')
2025-02-11 07:55:23,877 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:55:23,877 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:23,877 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,877 - Step 9: Generated next token
2025-02-11 07:55:23,877 - Step 9: Updated current_ids
2025-02-11 07:55:23,877 - Step 9: Decoded token text:  and
2025-02-11 07:55:23,877 - Step 9: Updated current_phrase
2025-02-11 07:55:23,878 - Step 9: Created step_acts
2025-02-11 07:55:23,878 - Step 9: Added to generation_acts
2025-02-11 07:55:23,878 - Step 9: Updated recent_tokens
2025-02-11 07:55:23,879 - Step 9: Decoded current text
2025-02-11 07:55:23,879 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:23,879 - 
Starting step 10
2025-02-11 07:55:23,879 - Current_ids device: cuda:0
2025-02-11 07:55:23,879 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,902 - Model output complete
2025-02-11 07:55:23,903 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:23,903 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,903 - Next token logits device: cuda:0
2025-02-11 07:55:23,903 - Entered do_sample
2025-02-11 07:55:23,903 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,905 - Probs max: 0.72900390625
2025-02-11 07:55:23,906 - Pre-cat
2025-02-11 07:55:23,906 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   1602,   6157,
           5545,    279,   7002,     11,    323]], device='cuda:0')
2025-02-11 07:55:23,908 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:23,908 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:23,908 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,908 - Step 10: Generated next token
2025-02-11 07:55:23,908 - Step 10: Updated current_ids
2025-02-11 07:55:23,908 - Step 10: Decoded token text:  the
2025-02-11 07:55:23,908 - Step 10: Updated current_phrase
2025-02-11 07:55:23,909 - Step 10: Created step_acts
2025-02-11 07:55:23,909 - Step 10: Added to generation_acts
2025-02-11 07:55:23,911 - Step 10: Updated generated_texts
2025-02-11 07:55:23,911 - Step 10: Updated recent_tokens
2025-02-11 07:55:23,911 - Step 10: Decoded current text
2025-02-11 07:55:23,911 - Step 10: Incremented consecutive_fillers to 1
2025-02-11 07:55:23,911 - 
Starting step 11
2025-02-11 07:55:23,911 - Current_ids device: cuda:0
2025-02-11 07:55:23,911 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,934 - Model output complete
2025-02-11 07:55:23,934 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:23,934 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,934 - Next token logits device: cuda:0
2025-02-11 07:55:23,934 - Entered do_sample
2025-02-11 07:55:23,934 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,937 - Probs max: 0.96630859375
2025-02-11 07:55:23,938 - Pre-cat
2025-02-11 07:55:23,938 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   1602,   6157,
           5545,    279,   7002,     11,    323,    279]], device='cuda:0')
2025-02-11 07:55:23,939 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:23,940 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:23,940 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,940 - Step 11: Generated next token
2025-02-11 07:55:23,940 - Step 11: Updated current_ids
2025-02-11 07:55:23,940 - Step 11: Decoded token text:  wall
2025-02-11 07:55:23,940 - Step 11: Updated current_phrase
2025-02-11 07:55:23,940 - Step 11: Created step_acts
2025-02-11 07:55:23,941 - Step 11: Added to generation_acts
2025-02-11 07:55:23,941 - Step 11: Updated recent_tokens
2025-02-11 07:55:23,942 - Step 11: Decoded current text
2025-02-11 07:55:23,942 - Step 11: Incremented consecutive_fillers to 2
2025-02-11 07:55:23,942 - 
Starting step 12
2025-02-11 07:55:23,942 - Current_ids device: cuda:0
2025-02-11 07:55:23,942 - Current_ids dtype: torch.int64
2025-02-11 07:55:23,964 - Model output complete
2025-02-11 07:55:23,965 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:23,965 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,965 - Next token logits device: cuda:0
2025-02-11 07:55:23,965 - Entered do_sample
2025-02-11 07:55:23,965 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:23,968 - Probs max: 0.99072265625
2025-02-11 07:55:23,971 - Pre-cat
2025-02-11 07:55:23,971 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   1602,   6157,
           5545,    279,   7002,     11,    323,    279,   7002]],
       device='cuda:0')
2025-02-11 07:55:23,974 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:23,974 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:23,974 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:23,974 - Step 12: Generated next token
2025-02-11 07:55:23,974 - Step 12: Updated current_ids
2025-02-11 07:55:23,974 - Step 12: Decoded token text:  will
2025-02-11 07:55:23,974 - Step 12: Updated current_phrase
2025-02-11 07:55:23,975 - Step 12: Created step_acts
2025-02-11 07:55:23,975 - Step 12: Added to generation_acts
2025-02-11 07:55:23,975 - Step 12: Updated recent_tokens
2025-02-11 07:55:23,976 - Step 12: Decoded current text
2025-02-11 07:55:23,976 - Step 12: Incremented consecutive_fillers to 3
2025-02-11 07:55:24,074 - 
Starting step 0
2025-02-11 07:55:24,075 - Current_ids device: cuda:0
2025-02-11 07:55:24,075 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,101 - Model output complete
2025-02-11 07:55:24,102 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:24,102 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,102 - Next token logits device: cuda:0
2025-02-11 07:55:24,102 - Entered do_sample
2025-02-11 07:55:24,102 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,105 - Probs max: 0.50341796875
2025-02-11 07:55:24,106 - Pre-cat
2025-02-11 07:55:24,106 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:24,108 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:55:24,109 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:24,109 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,109 - Step 0: Generated next token
2025-02-11 07:55:24,109 - Step 0: Updated current_ids
2025-02-11 07:55:24,109 - Step 0: Decoded token text:  It
2025-02-11 07:55:24,109 - Step 0: Updated current_phrase
2025-02-11 07:55:24,110 - Step 0: Created step_acts
2025-02-11 07:55:24,110 - Step 0: Added to generation_acts
2025-02-11 07:55:24,111 - Step 0: Updated generated_texts
2025-02-11 07:55:24,111 - Step 0: Updated recent_tokens
2025-02-11 07:55:24,112 - Step 0: Decoded current text
2025-02-11 07:55:24,112 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:24,112 - 
Starting step 1
2025-02-11 07:55:24,112 - Current_ids device: cuda:0
2025-02-11 07:55:24,112 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,138 - Model output complete
2025-02-11 07:55:24,138 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:24,138 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,138 - Next token logits device: cuda:0
2025-02-11 07:55:24,138 - Entered do_sample
2025-02-11 07:55:24,139 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,140 - Probs max: 0.9384765625
2025-02-11 07:55:24,141 - Pre-cat
2025-02-11 07:55:24,141 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084]], device='cuda:0')
2025-02-11 07:55:24,144 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:24,145 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:24,145 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,145 - Step 1: Generated next token
2025-02-11 07:55:24,145 - Step 1: Updated current_ids
2025-02-11 07:55:24,145 - Step 1: Decoded token text:  will
2025-02-11 07:55:24,145 - Step 1: Updated current_phrase
2025-02-11 07:55:24,145 - Step 1: Created step_acts
2025-02-11 07:55:24,146 - Step 1: Added to generation_acts
2025-02-11 07:55:24,146 - Step 1: Updated recent_tokens
2025-02-11 07:55:24,147 - Step 1: Decoded current text
2025-02-11 07:55:24,147 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:24,147 - 
Starting step 2
2025-02-11 07:55:24,147 - Current_ids device: cuda:0
2025-02-11 07:55:24,147 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,173 - Model output complete
2025-02-11 07:55:24,173 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:24,173 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,173 - Next token logits device: cuda:0
2025-02-11 07:55:24,173 - Entered do_sample
2025-02-11 07:55:24,173 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,176 - Probs max: 0.2442626953125
2025-02-11 07:55:24,176 - Pre-cat
2025-02-11 07:55:24,176 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686]], device='cuda:0')
2025-02-11 07:55:24,178 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:55:24,178 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:24,178 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,178 - Step 2: Generated next token
2025-02-11 07:55:24,178 - Step 2: Updated current_ids
2025-02-11 07:55:24,178 - Step 2: Decoded token text:  hit
2025-02-11 07:55:24,178 - Step 2: Updated current_phrase
2025-02-11 07:55:24,179 - Step 2: Created step_acts
2025-02-11 07:55:24,179 - Step 2: Added to generation_acts
2025-02-11 07:55:24,179 - Step 2: Updated recent_tokens
2025-02-11 07:55:24,180 - Step 2: Decoded current text
2025-02-11 07:55:24,180 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:24,180 - 
Starting step 3
2025-02-11 07:55:24,180 - Current_ids device: cuda:0
2025-02-11 07:55:24,180 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,203 - Model output complete
2025-02-11 07:55:24,203 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:24,203 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,203 - Next token logits device: cuda:0
2025-02-11 07:55:24,203 - Entered do_sample
2025-02-11 07:55:24,203 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,206 - Probs max: 0.99169921875
2025-02-11 07:55:24,207 - Pre-cat
2025-02-11 07:55:24,207 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201]],
       device='cuda:0')
2025-02-11 07:55:24,208 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:24,208 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:24,208 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,208 - Step 3: Generated next token
2025-02-11 07:55:24,208 - Step 3: Updated current_ids
2025-02-11 07:55:24,209 - Step 3: Decoded token text:  the
2025-02-11 07:55:24,209 - Step 3: Updated current_phrase
2025-02-11 07:55:24,209 - Step 3: Created step_acts
2025-02-11 07:55:24,209 - Step 3: Added to generation_acts
2025-02-11 07:55:24,209 - Step 3: Updated recent_tokens
2025-02-11 07:55:24,210 - Step 3: Decoded current text
2025-02-11 07:55:24,210 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:24,211 - 
Starting step 4
2025-02-11 07:55:24,211 - Current_ids device: cuda:0
2025-02-11 07:55:24,211 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,246 - Model output complete
2025-02-11 07:55:24,246 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:24,246 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,246 - Next token logits device: cuda:0
2025-02-11 07:55:24,246 - Entered do_sample
2025-02-11 07:55:24,246 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,248 - Probs max: 0.998046875
2025-02-11 07:55:24,250 - Pre-cat
2025-02-11 07:55:24,250 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279]],
       device='cuda:0')
2025-02-11 07:55:24,253 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:24,254 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:24,254 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,254 - Step 4: Generated next token
2025-02-11 07:55:24,254 - Step 4: Updated current_ids
2025-02-11 07:55:24,254 - Step 4: Decoded token text:  wall
2025-02-11 07:55:24,254 - Step 4: Updated current_phrase
2025-02-11 07:55:24,255 - Step 4: Created step_acts
2025-02-11 07:55:24,255 - Step 4: Added to generation_acts
2025-02-11 07:55:24,255 - Step 4: Updated recent_tokens
2025-02-11 07:55:24,256 - Step 4: Decoded current text
2025-02-11 07:55:24,256 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:24,256 - 
Starting step 5
2025-02-11 07:55:24,256 - Current_ids device: cuda:0
2025-02-11 07:55:24,256 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,291 - Model output complete
2025-02-11 07:55:24,291 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:24,291 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,291 - Next token logits device: cuda:0
2025-02-11 07:55:24,291 - Entered do_sample
2025-02-11 07:55:24,292 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,294 - Probs max: 0.41455078125
2025-02-11 07:55:24,295 - Pre-cat
2025-02-11 07:55:24,295 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002]],
       device='cuda:0')
2025-02-11 07:55:24,296 - Next token: tensor([[448]], device='cuda:0')
2025-02-11 07:55:24,297 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:24,297 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,297 - Step 5: Generated next token
2025-02-11 07:55:24,297 - Step 5: Updated current_ids
2025-02-11 07:55:24,297 - Step 5: Decoded token text:  with
2025-02-11 07:55:24,297 - Step 5: Updated current_phrase
2025-02-11 07:55:24,297 - Step 5: Created step_acts
2025-02-11 07:55:24,297 - Step 5: Added to generation_acts
2025-02-11 07:55:24,299 - Step 5: Updated generated_texts
2025-02-11 07:55:24,299 - Step 5: Updated recent_tokens
2025-02-11 07:55:24,299 - Step 5: Decoded current text
2025-02-11 07:55:24,299 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:24,299 - 
Starting step 6
2025-02-11 07:55:24,300 - Current_ids device: cuda:0
2025-02-11 07:55:24,300 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,322 - Model output complete
2025-02-11 07:55:24,323 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:24,323 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,323 - Next token logits device: cuda:0
2025-02-11 07:55:24,323 - Entered do_sample
2025-02-11 07:55:24,323 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,325 - Probs max: 0.5830078125
2025-02-11 07:55:24,327 - Pre-cat
2025-02-11 07:55:24,327 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448]], device='cuda:0')
2025-02-11 07:55:24,331 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:24,331 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:24,331 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,331 - Step 6: Generated next token
2025-02-11 07:55:24,331 - Step 6: Updated current_ids
2025-02-11 07:55:24,332 - Step 6: Decoded token text:  a
2025-02-11 07:55:24,332 - Step 6: Updated current_phrase
2025-02-11 07:55:24,332 - Step 6: Created step_acts
2025-02-11 07:55:24,332 - Step 6: Added to generation_acts
2025-02-11 07:55:24,332 - Step 6: Updated recent_tokens
2025-02-11 07:55:24,334 - Step 6: Decoded current text
2025-02-11 07:55:24,334 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:24,334 - 
Starting step 7
2025-02-11 07:55:24,334 - Current_ids device: cuda:0
2025-02-11 07:55:24,334 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,396 - Model output complete
2025-02-11 07:55:24,396 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:24,396 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,396 - Next token logits device: cuda:0
2025-02-11 07:55:24,396 - Entered do_sample
2025-02-11 07:55:24,396 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,400 - Probs max: 0.429443359375
2025-02-11 07:55:24,402 - Pre-cat
2025-02-11 07:55:24,402 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264]], device='cuda:0')
2025-02-11 07:55:24,405 - Next token: tensor([[4722]], device='cuda:0')
2025-02-11 07:55:24,405 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:24,405 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,405 - Step 7: Generated next token
2025-02-11 07:55:24,405 - Step 7: Updated current_ids
2025-02-11 07:55:24,406 - Step 7: Decoded token text:  lower
2025-02-11 07:55:24,406 - Step 7: Updated current_phrase
2025-02-11 07:55:24,406 - Step 7: Created step_acts
2025-02-11 07:55:24,406 - Step 7: Added to generation_acts
2025-02-11 07:55:24,406 - Step 7: Updated recent_tokens
2025-02-11 07:55:24,408 - Step 7: Decoded current text
2025-02-11 07:55:24,408 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:24,408 - 
Starting step 8
2025-02-11 07:55:24,408 - Current_ids device: cuda:0
2025-02-11 07:55:24,408 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,451 - Model output complete
2025-02-11 07:55:24,451 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:24,451 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,451 - Next token logits device: cuda:0
2025-02-11 07:55:24,452 - Entered do_sample
2025-02-11 07:55:24,452 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,454 - Probs max: 0.876953125
2025-02-11 07:55:24,455 - Pre-cat
2025-02-11 07:55:24,455 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722]], device='cuda:0')
2025-02-11 07:55:24,456 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:24,457 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:24,457 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,457 - Step 8: Generated next token
2025-02-11 07:55:24,457 - Step 8: Updated current_ids
2025-02-11 07:55:24,457 - Step 8: Decoded token text:  speed
2025-02-11 07:55:24,457 - Step 8: Updated current_phrase
2025-02-11 07:55:24,457 - Step 8: Created step_acts
2025-02-11 07:55:24,457 - Step 8: Added to generation_acts
2025-02-11 07:55:24,457 - Step 8: Updated recent_tokens
2025-02-11 07:55:24,459 - Step 8: Decoded current text
2025-02-11 07:55:24,459 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:24,459 - 
Starting step 9
2025-02-11 07:55:24,459 - Current_ids device: cuda:0
2025-02-11 07:55:24,459 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,520 - Model output complete
2025-02-11 07:55:24,520 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:24,520 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,520 - Next token logits device: cuda:0
2025-02-11 07:55:24,520 - Entered do_sample
2025-02-11 07:55:24,521 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,522 - Probs max: 0.65673828125
2025-02-11 07:55:24,524 - Pre-cat
2025-02-11 07:55:24,524 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628]], device='cuda:0')
2025-02-11 07:55:24,528 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:55:24,528 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:24,528 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,528 - Step 9: Generated next token
2025-02-11 07:55:24,528 - Step 9: Updated current_ids
2025-02-11 07:55:24,528 - Step 9: Decoded token text: .
2025-02-11 07:55:24,528 - Step 9: Updated current_phrase
2025-02-11 07:55:24,529 - Step 9: Created step_acts
2025-02-11 07:55:24,529 - Step 9: Added to generation_acts
2025-02-11 07:55:24,529 - Step 9: Updated recent_tokens
2025-02-11 07:55:24,530 - Step 9: Found phrase end token
2025-02-11 07:55:24,530 - Step 9: Updated recent_phrases
2025-02-11 07:55:24,531 - Step 9: Decoded current text
2025-02-11 07:55:24,531 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:24,531 - 
Starting step 10
2025-02-11 07:55:24,531 - Current_ids device: cuda:0
2025-02-11 07:55:24,531 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,579 - Model output complete
2025-02-11 07:55:24,579 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:24,579 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,579 - Next token logits device: cuda:0
2025-02-11 07:55:24,579 - Entered do_sample
2025-02-11 07:55:24,579 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,581 - Probs max: 0.391357421875
2025-02-11 07:55:24,582 - Pre-cat
2025-02-11 07:55:24,582 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13]], device='cuda:0')
2025-02-11 07:55:24,585 - Next token: tensor([[425]], device='cuda:0')
2025-02-11 07:55:24,585 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:24,585 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,585 - Step 10: Generated next token
2025-02-11 07:55:24,585 - Step 10: Updated current_ids
2025-02-11 07:55:24,586 - Step 10: Decoded token text:  B
2025-02-11 07:55:24,586 - Step 10: Updated current_phrase
2025-02-11 07:55:24,586 - Step 10: Created step_acts
2025-02-11 07:55:24,586 - Step 10: Added to generation_acts
2025-02-11 07:55:24,588 - Step 10: Updated generated_texts
2025-02-11 07:55:24,588 - Step 10: Updated recent_tokens
2025-02-11 07:55:24,588 - Step 10: Decoded current text
2025-02-11 07:55:24,588 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:24,588 - 
Starting step 11
2025-02-11 07:55:24,588 - Current_ids device: cuda:0
2025-02-11 07:55:24,588 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,617 - Model output complete
2025-02-11 07:55:24,617 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:24,617 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,617 - Next token logits device: cuda:0
2025-02-11 07:55:24,617 - Entered do_sample
2025-02-11 07:55:24,617 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,620 - Probs max: 1.0
2025-02-11 07:55:24,621 - Pre-cat
2025-02-11 07:55:24,621 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425]], device='cuda:0')
2025-02-11 07:55:24,622 - Next token: tensor([[25]], device='cuda:0')
2025-02-11 07:55:24,623 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:24,623 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,623 - Step 11: Generated next token
2025-02-11 07:55:24,623 - Step 11: Updated current_ids
2025-02-11 07:55:24,623 - Step 11: Decoded token text: :
2025-02-11 07:55:24,623 - Step 11: Updated current_phrase
2025-02-11 07:55:24,623 - Step 11: Created step_acts
2025-02-11 07:55:24,624 - Step 11: Added to generation_acts
2025-02-11 07:55:24,624 - Step 11: Updated recent_tokens
2025-02-11 07:55:24,625 - Step 11: Found phrase end token
2025-02-11 07:55:24,625 - Step 11: Updated recent_phrases
2025-02-11 07:55:24,625 - Step 11: Calculated similarity: 0.0
2025-02-11 07:55:24,625 - Step 11: Decoded current text
2025-02-11 07:55:24,625 - Step 11: Reset consecutive_fillers
2025-02-11 07:55:24,625 - 
Starting step 12
2025-02-11 07:55:24,625 - Current_ids device: cuda:0
2025-02-11 07:55:24,625 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,648 - Model output complete
2025-02-11 07:55:24,648 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:24,649 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,649 - Next token logits device: cuda:0
2025-02-11 07:55:24,649 - Entered do_sample
2025-02-11 07:55:24,649 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,652 - Probs max: 0.99609375
2025-02-11 07:55:24,652 - Pre-cat
2025-02-11 07:55:24,652 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25]],
       device='cuda:0')
2025-02-11 07:55:24,654 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:55:24,654 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:24,654 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,654 - Step 12: Generated next token
2025-02-11 07:55:24,654 - Step 12: Updated current_ids
2025-02-11 07:55:24,654 - Step 12: Decoded token text:  It
2025-02-11 07:55:24,654 - Step 12: Updated current_phrase
2025-02-11 07:55:24,655 - Step 12: Created step_acts
2025-02-11 07:55:24,655 - Step 12: Added to generation_acts
2025-02-11 07:55:24,655 - Step 12: Updated recent_tokens
2025-02-11 07:55:24,656 - Step 12: Decoded current text
2025-02-11 07:55:24,656 - Step 12: Reset consecutive_fillers
2025-02-11 07:55:24,656 - 
Starting step 13
2025-02-11 07:55:24,656 - Current_ids device: cuda:0
2025-02-11 07:55:24,656 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,680 - Model output complete
2025-02-11 07:55:24,680 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:24,680 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,680 - Next token logits device: cuda:0
2025-02-11 07:55:24,680 - Entered do_sample
2025-02-11 07:55:24,680 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,683 - Probs max: 0.9951171875
2025-02-11 07:55:24,684 - Pre-cat
2025-02-11 07:55:24,684 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084]],
       device='cuda:0')
2025-02-11 07:55:24,686 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:24,686 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:24,686 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,686 - Step 13: Generated next token
2025-02-11 07:55:24,686 - Step 13: Updated current_ids
2025-02-11 07:55:24,686 - Step 13: Decoded token text:  will
2025-02-11 07:55:24,686 - Step 13: Updated current_phrase
2025-02-11 07:55:24,686 - Step 13: Created step_acts
2025-02-11 07:55:24,687 - Step 13: Added to generation_acts
2025-02-11 07:55:24,687 - Step 13: Updated recent_tokens
2025-02-11 07:55:24,688 - Step 13: Decoded current text
2025-02-11 07:55:24,688 - Step 13: Reset consecutive_fillers
2025-02-11 07:55:24,688 - 
Starting step 14
2025-02-11 07:55:24,688 - Current_ids device: cuda:0
2025-02-11 07:55:24,688 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,711 - Model output complete
2025-02-11 07:55:24,711 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:55:24,711 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,711 - Next token logits device: cuda:0
2025-02-11 07:55:24,711 - Entered do_sample
2025-02-11 07:55:24,711 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,715 - Probs max: 0.95751953125
2025-02-11 07:55:24,715 - Pre-cat
2025-02-11 07:55:24,716 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686]],
       device='cuda:0')
2025-02-11 07:55:24,717 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:55:24,718 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:55:24,718 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,718 - Step 14: Generated next token
2025-02-11 07:55:24,718 - Step 14: Updated current_ids
2025-02-11 07:55:24,718 - Step 14: Decoded token text:  hit
2025-02-11 07:55:24,718 - Step 14: Updated current_phrase
2025-02-11 07:55:24,718 - Step 14: Created step_acts
2025-02-11 07:55:24,718 - Step 14: Added to generation_acts
2025-02-11 07:55:24,718 - Step 14: Updated recent_tokens
2025-02-11 07:55:24,720 - Step 14: Decoded current text
2025-02-11 07:55:24,720 - Step 14: Reset consecutive_fillers
2025-02-11 07:55:24,720 - 
Starting step 15
2025-02-11 07:55:24,720 - Current_ids device: cuda:0
2025-02-11 07:55:24,720 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,743 - Model output complete
2025-02-11 07:55:24,743 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:55:24,743 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,743 - Next token logits device: cuda:0
2025-02-11 07:55:24,743 - Entered do_sample
2025-02-11 07:55:24,743 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,747 - Probs max: 0.98779296875
2025-02-11 07:55:24,747 - Pre-cat
2025-02-11 07:55:24,748 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201]], device='cuda:0')
2025-02-11 07:55:24,750 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:24,750 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:55:24,750 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,750 - Step 15: Generated next token
2025-02-11 07:55:24,750 - Step 15: Updated current_ids
2025-02-11 07:55:24,750 - Step 15: Decoded token text:  the
2025-02-11 07:55:24,750 - Step 15: Updated current_phrase
2025-02-11 07:55:24,751 - Step 15: Created step_acts
2025-02-11 07:55:24,751 - Step 15: Added to generation_acts
2025-02-11 07:55:24,752 - Step 15: Updated generated_texts
2025-02-11 07:55:24,752 - Step 15: Updated recent_tokens
2025-02-11 07:55:24,752 - Step 15: Decoded current text
2025-02-11 07:55:24,753 - Step 15: Reset consecutive_fillers
2025-02-11 07:55:24,753 - 
Starting step 16
2025-02-11 07:55:24,753 - Current_ids device: cuda:0
2025-02-11 07:55:24,753 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,775 - Model output complete
2025-02-11 07:55:24,775 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:55:24,775 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,775 - Next token logits device: cuda:0
2025-02-11 07:55:24,775 - Entered do_sample
2025-02-11 07:55:24,775 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,778 - Probs max: 0.99951171875
2025-02-11 07:55:24,779 - Pre-cat
2025-02-11 07:55:24,779 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279]], device='cuda:0')
2025-02-11 07:55:24,783 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:24,783 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:55:24,783 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,783 - Step 16: Generated next token
2025-02-11 07:55:24,783 - Step 16: Updated current_ids
2025-02-11 07:55:24,784 - Step 16: Decoded token text:  wall
2025-02-11 07:55:24,784 - Step 16: Updated current_phrase
2025-02-11 07:55:24,784 - Step 16: Created step_acts
2025-02-11 07:55:24,784 - Step 16: Added to generation_acts
2025-02-11 07:55:24,784 - Step 16: Updated recent_tokens
2025-02-11 07:55:24,786 - Step 16: Decoded current text
2025-02-11 07:55:24,786 - Step 16: Reset consecutive_fillers
2025-02-11 07:55:24,786 - Step 16: Calculated unique_ratio: 0.75
2025-02-11 07:55:24,786 - 
Starting step 17
2025-02-11 07:55:24,786 - Current_ids device: cuda:0
2025-02-11 07:55:24,786 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,819 - Model output complete
2025-02-11 07:55:24,819 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:55:24,819 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,819 - Next token logits device: cuda:0
2025-02-11 07:55:24,819 - Entered do_sample
2025-02-11 07:55:24,820 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,822 - Probs max: 0.96484375
2025-02-11 07:55:24,822 - Pre-cat
2025-02-11 07:55:24,822 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002]], device='cuda:0')
2025-02-11 07:55:24,826 - Next token: tensor([[448]], device='cuda:0')
2025-02-11 07:55:24,826 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:55:24,826 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,827 - Step 17: Generated next token
2025-02-11 07:55:24,827 - Step 17: Updated current_ids
2025-02-11 07:55:24,827 - Step 17: Decoded token text:  with
2025-02-11 07:55:24,827 - Step 17: Updated current_phrase
2025-02-11 07:55:24,827 - Step 17: Created step_acts
2025-02-11 07:55:24,827 - Step 17: Added to generation_acts
2025-02-11 07:55:24,828 - Step 17: Updated recent_tokens
2025-02-11 07:55:24,829 - Step 17: Decoded current text
2025-02-11 07:55:24,829 - Step 17: Reset consecutive_fillers
2025-02-11 07:55:24,829 - Step 17: Calculated unique_ratio: 0.75
2025-02-11 07:55:24,829 - 
Starting step 18
2025-02-11 07:55:24,829 - Current_ids device: cuda:0
2025-02-11 07:55:24,829 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,865 - Model output complete
2025-02-11 07:55:24,865 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:55:24,865 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,865 - Next token logits device: cuda:0
2025-02-11 07:55:24,865 - Entered do_sample
2025-02-11 07:55:24,865 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,868 - Probs max: 0.96826171875
2025-02-11 07:55:24,870 - Pre-cat
2025-02-11 07:55:24,870 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448]], device='cuda:0')
2025-02-11 07:55:24,873 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:24,874 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:55:24,874 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,874 - Step 18: Generated next token
2025-02-11 07:55:24,874 - Step 18: Updated current_ids
2025-02-11 07:55:24,874 - Step 18: Decoded token text:  a
2025-02-11 07:55:24,874 - Step 18: Updated current_phrase
2025-02-11 07:55:24,875 - Step 18: Created step_acts
2025-02-11 07:55:24,875 - Step 18: Added to generation_acts
2025-02-11 07:55:24,875 - Step 18: Updated recent_tokens
2025-02-11 07:55:24,876 - Step 18: Decoded current text
2025-02-11 07:55:24,876 - Step 18: Reset consecutive_fillers
2025-02-11 07:55:24,877 - Step 18: Calculated unique_ratio: 0.75
2025-02-11 07:55:24,877 - 
Starting step 19
2025-02-11 07:55:24,877 - Current_ids device: cuda:0
2025-02-11 07:55:24,877 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,962 - Model output complete
2025-02-11 07:55:24,962 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:55:24,962 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,963 - Next token logits device: cuda:0
2025-02-11 07:55:24,963 - Entered do_sample
2025-02-11 07:55:24,963 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,965 - Probs max: 0.92626953125
2025-02-11 07:55:24,966 - Pre-cat
2025-02-11 07:55:24,966 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264]], device='cuda:0')
2025-02-11 07:55:24,968 - Next token: tensor([[5080]], device='cuda:0')
2025-02-11 07:55:24,968 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:55:24,968 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:24,968 - Step 19: Generated next token
2025-02-11 07:55:24,968 - Step 19: Updated current_ids
2025-02-11 07:55:24,968 - Step 19: Decoded token text:  higher
2025-02-11 07:55:24,968 - Step 19: Updated current_phrase
2025-02-11 07:55:24,969 - Step 19: Created step_acts
2025-02-11 07:55:24,969 - Step 19: Added to generation_acts
2025-02-11 07:55:24,969 - Step 19: Updated recent_tokens
2025-02-11 07:55:24,970 - Step 19: Decoded current text
2025-02-11 07:55:24,970 - Step 19: Reset consecutive_fillers
2025-02-11 07:55:24,970 - Step 19: Calculated unique_ratio: 0.8125
2025-02-11 07:55:24,971 - 
Starting step 20
2025-02-11 07:55:24,971 - Current_ids device: cuda:0
2025-02-11 07:55:24,971 - Current_ids dtype: torch.int64
2025-02-11 07:55:24,993 - Model output complete
2025-02-11 07:55:24,993 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:55:24,993 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,993 - Next token logits device: cuda:0
2025-02-11 07:55:24,994 - Entered do_sample
2025-02-11 07:55:24,994 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:24,997 - Probs max: 0.99951171875
2025-02-11 07:55:24,998 - Pre-cat
2025-02-11 07:55:24,998 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080]], device='cuda:0')
2025-02-11 07:55:25,000 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:25,000 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:55:25,000 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,000 - Step 20: Generated next token
2025-02-11 07:55:25,000 - Step 20: Updated current_ids
2025-02-11 07:55:25,000 - Step 20: Decoded token text:  speed
2025-02-11 07:55:25,000 - Step 20: Updated current_phrase
2025-02-11 07:55:25,001 - Step 20: Created step_acts
2025-02-11 07:55:25,001 - Step 20: Added to generation_acts
2025-02-11 07:55:25,002 - Step 20: Updated generated_texts
2025-02-11 07:55:25,002 - Step 20: Updated recent_tokens
2025-02-11 07:55:25,002 - Step 20: Decoded current text
2025-02-11 07:55:25,003 - Step 20: Reset consecutive_fillers
2025-02-11 07:55:25,003 - Step 20: Calculated unique_ratio: 0.8125
2025-02-11 07:55:25,003 - 
Starting step 21
2025-02-11 07:55:25,003 - Current_ids device: cuda:0
2025-02-11 07:55:25,003 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,034 - Model output complete
2025-02-11 07:55:25,034 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:55:25,034 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,034 - Next token logits device: cuda:0
2025-02-11 07:55:25,034 - Entered do_sample
2025-02-11 07:55:25,034 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,037 - Probs max: 0.9736328125
2025-02-11 07:55:25,038 - Pre-cat
2025-02-11 07:55:25,038 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628]],
       device='cuda:0')
2025-02-11 07:55:25,040 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:55:25,041 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:55:25,041 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,041 - Step 21: Generated next token
2025-02-11 07:55:25,041 - Step 21: Updated current_ids
2025-02-11 07:55:25,041 - Step 21: Decoded token text: .
2025-02-11 07:55:25,041 - Step 21: Updated current_phrase
2025-02-11 07:55:25,042 - Step 21: Created step_acts
2025-02-11 07:55:25,042 - Step 21: Added to generation_acts
2025-02-11 07:55:25,042 - Step 21: Updated recent_tokens
2025-02-11 07:55:25,043 - Step 21: Found phrase end token
2025-02-11 07:55:25,043 - Step 21: Updated recent_phrases
2025-02-11 07:55:25,043 - Step 21: Calculated similarity: 0.0
2025-02-11 07:55:25,043 - Step 21: Decoded current text
2025-02-11 07:55:25,043 - Step 21: Reset consecutive_fillers
2025-02-11 07:55:25,044 - Step 21: Calculated unique_ratio: 0.8125
2025-02-11 07:55:25,044 - 
Starting step 22
2025-02-11 07:55:25,044 - Current_ids device: cuda:0
2025-02-11 07:55:25,044 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,066 - Model output complete
2025-02-11 07:55:25,067 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:55:25,067 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,067 - Next token logits device: cuda:0
2025-02-11 07:55:25,067 - Entered do_sample
2025-02-11 07:55:25,067 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,070 - Probs max: 0.99853515625
2025-02-11 07:55:25,070 - Pre-cat
2025-02-11 07:55:25,071 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13]],
       device='cuda:0')
2025-02-11 07:55:25,073 - Next token: tensor([[356]], device='cuda:0')
2025-02-11 07:55:25,073 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:55:25,073 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,073 - Step 22: Generated next token
2025-02-11 07:55:25,073 - Step 22: Updated current_ids
2025-02-11 07:55:25,074 - Step 22: Decoded token text:  C
2025-02-11 07:55:25,074 - Step 22: Updated current_phrase
2025-02-11 07:55:25,074 - Step 22: Created step_acts
2025-02-11 07:55:25,074 - Step 22: Added to generation_acts
2025-02-11 07:55:25,074 - Step 22: Updated recent_tokens
2025-02-11 07:55:25,076 - Step 22: Decoded current text
2025-02-11 07:55:25,076 - Step 22: Reset consecutive_fillers
2025-02-11 07:55:25,076 - Step 22: Calculated unique_ratio: 0.875
2025-02-11 07:55:25,076 - 
Starting step 23
2025-02-11 07:55:25,076 - Current_ids device: cuda:0
2025-02-11 07:55:25,076 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,099 - Model output complete
2025-02-11 07:55:25,099 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:55:25,099 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,099 - Next token logits device: cuda:0
2025-02-11 07:55:25,099 - Entered do_sample
2025-02-11 07:55:25,100 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,103 - Probs max: 1.0
2025-02-11 07:55:25,104 - Pre-cat
2025-02-11 07:55:25,104 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356]],
       device='cuda:0')
2025-02-11 07:55:25,107 - Next token: tensor([[25]], device='cuda:0')
2025-02-11 07:55:25,107 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:55:25,107 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,107 - Step 23: Generated next token
2025-02-11 07:55:25,108 - Step 23: Updated current_ids
2025-02-11 07:55:25,108 - Step 23: Decoded token text: :
2025-02-11 07:55:25,108 - Step 23: Updated current_phrase
2025-02-11 07:55:25,108 - Step 23: Created step_acts
2025-02-11 07:55:25,108 - Step 23: Added to generation_acts
2025-02-11 07:55:25,108 - Step 23: Updated recent_tokens
2025-02-11 07:55:25,110 - Step 23: Found phrase end token
2025-02-11 07:55:25,110 - Step 23: Updated recent_phrases
2025-02-11 07:55:25,110 - Step 23: Calculated similarity: 0.0
2025-02-11 07:55:25,110 - Step 23: Decoded current text
2025-02-11 07:55:25,110 - Step 23: Reset consecutive_fillers
2025-02-11 07:55:25,110 - Step 23: Calculated unique_ratio: 0.8125
2025-02-11 07:55:25,110 - 
Starting step 24
2025-02-11 07:55:25,110 - Current_ids device: cuda:0
2025-02-11 07:55:25,110 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,134 - Model output complete
2025-02-11 07:55:25,134 - Logits shape: torch.Size([1, 55, 151936])
2025-02-11 07:55:25,134 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,134 - Next token logits device: cuda:0
2025-02-11 07:55:25,134 - Entered do_sample
2025-02-11 07:55:25,134 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,138 - Probs max: 0.998046875
2025-02-11 07:55:25,139 - Pre-cat
2025-02-11 07:55:25,139 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25]], device='cuda:0')
2025-02-11 07:55:25,141 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:55:25,141 - Current_ids shape: torch.Size([1, 55])
2025-02-11 07:55:25,141 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,141 - Step 24: Generated next token
2025-02-11 07:55:25,141 - Step 24: Updated current_ids
2025-02-11 07:55:25,141 - Step 24: Decoded token text:  It
2025-02-11 07:55:25,141 - Step 24: Updated current_phrase
2025-02-11 07:55:25,142 - Step 24: Created step_acts
2025-02-11 07:55:25,142 - Step 24: Added to generation_acts
2025-02-11 07:55:25,142 - Step 24: Updated recent_tokens
2025-02-11 07:55:25,143 - Step 24: Decoded current text
2025-02-11 07:55:25,143 - Step 24: Reset consecutive_fillers
2025-02-11 07:55:25,143 - Step 24: Calculated unique_ratio: 0.8125
2025-02-11 07:55:25,143 - 
Starting step 25
2025-02-11 07:55:25,144 - Current_ids device: cuda:0
2025-02-11 07:55:25,144 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,171 - Model output complete
2025-02-11 07:55:25,171 - Logits shape: torch.Size([1, 56, 151936])
2025-02-11 07:55:25,171 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,171 - Next token logits device: cuda:0
2025-02-11 07:55:25,171 - Entered do_sample
2025-02-11 07:55:25,171 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,174 - Probs max: 0.99658203125
2025-02-11 07:55:25,174 - Pre-cat
2025-02-11 07:55:25,174 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084]], device='cuda:0')
2025-02-11 07:55:25,176 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:25,176 - Current_ids shape: torch.Size([1, 56])
2025-02-11 07:55:25,176 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,177 - Step 25: Generated next token
2025-02-11 07:55:25,177 - Step 25: Updated current_ids
2025-02-11 07:55:25,177 - Step 25: Decoded token text:  will
2025-02-11 07:55:25,177 - Step 25: Updated current_phrase
2025-02-11 07:55:25,177 - Step 25: Created step_acts
2025-02-11 07:55:25,177 - Step 25: Added to generation_acts
2025-02-11 07:55:25,179 - Step 25: Updated generated_texts
2025-02-11 07:55:25,179 - Step 25: Updated recent_tokens
2025-02-11 07:55:25,180 - Step 25: Decoded current text
2025-02-11 07:55:25,180 - Step 25: Reset consecutive_fillers
2025-02-11 07:55:25,180 - Step 25: Calculated unique_ratio: 0.8125
2025-02-11 07:55:25,180 - 
Starting step 26
2025-02-11 07:55:25,180 - Current_ids device: cuda:0
2025-02-11 07:55:25,180 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,203 - Model output complete
2025-02-11 07:55:25,203 - Logits shape: torch.Size([1, 57, 151936])
2025-02-11 07:55:25,203 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,203 - Next token logits device: cuda:0
2025-02-11 07:55:25,203 - Entered do_sample
2025-02-11 07:55:25,203 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,207 - Probs max: 0.9501953125
2025-02-11 07:55:25,207 - Pre-cat
2025-02-11 07:55:25,207 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686]], device='cuda:0')
2025-02-11 07:55:25,210 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:55:25,211 - Current_ids shape: torch.Size([1, 57])
2025-02-11 07:55:25,211 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,211 - Step 26: Generated next token
2025-02-11 07:55:25,211 - Step 26: Updated current_ids
2025-02-11 07:55:25,211 - Step 26: Decoded token text:  hit
2025-02-11 07:55:25,211 - Step 26: Updated current_phrase
2025-02-11 07:55:25,212 - Step 26: Created step_acts
2025-02-11 07:55:25,212 - Step 26: Added to generation_acts
2025-02-11 07:55:25,212 - Step 26: Updated recent_tokens
2025-02-11 07:55:25,213 - Step 26: Decoded current text
2025-02-11 07:55:25,213 - Step 26: Reset consecutive_fillers
2025-02-11 07:55:25,213 - Step 26: Calculated unique_ratio: 0.75
2025-02-11 07:55:25,213 - 
Starting step 27
2025-02-11 07:55:25,213 - Current_ids device: cuda:0
2025-02-11 07:55:25,214 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,236 - Model output complete
2025-02-11 07:55:25,236 - Logits shape: torch.Size([1, 58, 151936])
2025-02-11 07:55:25,236 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,237 - Next token logits device: cuda:0
2025-02-11 07:55:25,237 - Entered do_sample
2025-02-11 07:55:25,237 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,240 - Probs max: 0.986328125
2025-02-11 07:55:25,241 - Pre-cat
2025-02-11 07:55:25,241 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201]], device='cuda:0')
2025-02-11 07:55:25,243 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:25,244 - Current_ids shape: torch.Size([1, 58])
2025-02-11 07:55:25,244 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,244 - Step 27: Generated next token
2025-02-11 07:55:25,244 - Step 27: Updated current_ids
2025-02-11 07:55:25,244 - Step 27: Decoded token text:  the
2025-02-11 07:55:25,244 - Step 27: Updated current_phrase
2025-02-11 07:55:25,244 - Step 27: Created step_acts
2025-02-11 07:55:25,245 - Step 27: Added to generation_acts
2025-02-11 07:55:25,245 - Step 27: Updated recent_tokens
2025-02-11 07:55:25,246 - Step 27: Decoded current text
2025-02-11 07:55:25,246 - Step 27: Reset consecutive_fillers
2025-02-11 07:55:25,246 - Step 27: Calculated unique_ratio: 0.75
2025-02-11 07:55:25,246 - 
Starting step 28
2025-02-11 07:55:25,246 - Current_ids device: cuda:0
2025-02-11 07:55:25,247 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,269 - Model output complete
2025-02-11 07:55:25,269 - Logits shape: torch.Size([1, 59, 151936])
2025-02-11 07:55:25,269 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,269 - Next token logits device: cuda:0
2025-02-11 07:55:25,269 - Entered do_sample
2025-02-11 07:55:25,270 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,273 - Probs max: 1.0
2025-02-11 07:55:25,273 - Pre-cat
2025-02-11 07:55:25,273 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279]], device='cuda:0')
2025-02-11 07:55:25,276 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:25,276 - Current_ids shape: torch.Size([1, 59])
2025-02-11 07:55:25,277 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,277 - Step 28: Generated next token
2025-02-11 07:55:25,277 - Step 28: Updated current_ids
2025-02-11 07:55:25,277 - Step 28: Decoded token text:  wall
2025-02-11 07:55:25,277 - Step 28: Updated current_phrase
2025-02-11 07:55:25,277 - Step 28: Created step_acts
2025-02-11 07:55:25,277 - Step 28: Added to generation_acts
2025-02-11 07:55:25,277 - Step 28: Updated recent_tokens
2025-02-11 07:55:25,279 - Step 28: Decoded current text
2025-02-11 07:55:25,279 - Step 28: Reset consecutive_fillers
2025-02-11 07:55:25,279 - Step 28: Calculated unique_ratio: 0.75
2025-02-11 07:55:25,279 - 
Starting step 29
2025-02-11 07:55:25,279 - Current_ids device: cuda:0
2025-02-11 07:55:25,279 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,301 - Model output complete
2025-02-11 07:55:25,301 - Logits shape: torch.Size([1, 60, 151936])
2025-02-11 07:55:25,301 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,301 - Next token logits device: cuda:0
2025-02-11 07:55:25,302 - Entered do_sample
2025-02-11 07:55:25,302 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,305 - Probs max: 0.9306640625
2025-02-11 07:55:25,306 - Pre-cat
2025-02-11 07:55:25,306 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002]], device='cuda:0')
2025-02-11 07:55:25,308 - Next token: tensor([[448]], device='cuda:0')
2025-02-11 07:55:25,308 - Current_ids shape: torch.Size([1, 60])
2025-02-11 07:55:25,308 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,309 - Step 29: Generated next token
2025-02-11 07:55:25,309 - Step 29: Updated current_ids
2025-02-11 07:55:25,309 - Step 29: Decoded token text:  with
2025-02-11 07:55:25,309 - Step 29: Updated current_phrase
2025-02-11 07:55:25,309 - Step 29: Created step_acts
2025-02-11 07:55:25,309 - Step 29: Added to generation_acts
2025-02-11 07:55:25,309 - Step 29: Updated recent_tokens
2025-02-11 07:55:25,311 - Step 29: Decoded current text
2025-02-11 07:55:25,311 - Step 29: Reset consecutive_fillers
2025-02-11 07:55:25,311 - Step 29: Calculated unique_ratio: 0.75
2025-02-11 07:55:25,311 - 
Starting step 30
2025-02-11 07:55:25,311 - Current_ids device: cuda:0
2025-02-11 07:55:25,311 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,335 - Model output complete
2025-02-11 07:55:25,335 - Logits shape: torch.Size([1, 61, 151936])
2025-02-11 07:55:25,335 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,335 - Next token logits device: cuda:0
2025-02-11 07:55:25,335 - Entered do_sample
2025-02-11 07:55:25,335 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,338 - Probs max: 0.646484375
2025-02-11 07:55:25,339 - Pre-cat
2025-02-11 07:55:25,339 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448]],
       device='cuda:0')
2025-02-11 07:55:25,341 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:25,342 - Current_ids shape: torch.Size([1, 61])
2025-02-11 07:55:25,342 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,342 - Step 30: Generated next token
2025-02-11 07:55:25,342 - Step 30: Updated current_ids
2025-02-11 07:55:25,342 - Step 30: Decoded token text:  the
2025-02-11 07:55:25,342 - Step 30: Updated current_phrase
2025-02-11 07:55:25,343 - Step 30: Created step_acts
2025-02-11 07:55:25,343 - Step 30: Added to generation_acts
2025-02-11 07:55:25,344 - Step 30: Updated generated_texts
2025-02-11 07:55:25,344 - Step 30: Updated recent_tokens
2025-02-11 07:55:25,345 - Step 30: Decoded current text
2025-02-11 07:55:25,345 - Step 30: Reset consecutive_fillers
2025-02-11 07:55:25,345 - Step 30: Calculated unique_ratio: 0.75
2025-02-11 07:55:25,345 - 
Starting step 31
2025-02-11 07:55:25,345 - Current_ids device: cuda:0
2025-02-11 07:55:25,345 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,368 - Model output complete
2025-02-11 07:55:25,368 - Logits shape: torch.Size([1, 62, 151936])
2025-02-11 07:55:25,368 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,368 - Next token logits device: cuda:0
2025-02-11 07:55:25,368 - Entered do_sample
2025-02-11 07:55:25,369 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,372 - Probs max: 1.0
2025-02-11 07:55:25,372 - Pre-cat
2025-02-11 07:55:25,372 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279]],
       device='cuda:0')
2025-02-11 07:55:25,376 - Next token: tensor([[1852]], device='cuda:0')
2025-02-11 07:55:25,376 - Current_ids shape: torch.Size([1, 62])
2025-02-11 07:55:25,376 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,376 - Step 31: Generated next token
2025-02-11 07:55:25,376 - Step 31: Updated current_ids
2025-02-11 07:55:25,377 - Step 31: Decoded token text:  same
2025-02-11 07:55:25,377 - Step 31: Updated current_phrase
2025-02-11 07:55:25,377 - Step 31: Created step_acts
2025-02-11 07:55:25,377 - Step 31: Added to generation_acts
2025-02-11 07:55:25,377 - Step 31: Updated recent_tokens
2025-02-11 07:55:25,379 - Step 31: Decoded current text
2025-02-11 07:55:25,379 - Step 31: Reset consecutive_fillers
2025-02-11 07:55:25,379 - Step 31: Calculated unique_ratio: 0.8125
2025-02-11 07:55:25,379 - 
Starting step 32
2025-02-11 07:55:25,379 - Current_ids device: cuda:0
2025-02-11 07:55:25,379 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,401 - Model output complete
2025-02-11 07:55:25,401 - Logits shape: torch.Size([1, 63, 151936])
2025-02-11 07:55:25,401 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,401 - Next token logits device: cuda:0
2025-02-11 07:55:25,402 - Entered do_sample
2025-02-11 07:55:25,402 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,405 - Probs max: 1.0
2025-02-11 07:55:25,406 - Pre-cat
2025-02-11 07:55:25,406 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852]],
       device='cuda:0')
2025-02-11 07:55:25,408 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:25,408 - Current_ids shape: torch.Size([1, 63])
2025-02-11 07:55:25,408 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,409 - Step 32: Generated next token
2025-02-11 07:55:25,409 - Step 32: Updated current_ids
2025-02-11 07:55:25,409 - Step 32: Decoded token text:  speed
2025-02-11 07:55:25,409 - Step 32: Updated current_phrase
2025-02-11 07:55:25,409 - Step 32: Created step_acts
2025-02-11 07:55:25,409 - Step 32: Added to generation_acts
2025-02-11 07:55:25,409 - Step 32: Updated recent_tokens
2025-02-11 07:55:25,411 - Step 32: Decoded current text
2025-02-11 07:55:25,411 - Step 32: Reset consecutive_fillers
2025-02-11 07:55:25,411 - Step 32: Calculated unique_ratio: 0.8125
2025-02-11 07:55:25,411 - 
Starting step 33
2025-02-11 07:55:25,411 - Current_ids device: cuda:0
2025-02-11 07:55:25,411 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,435 - Model output complete
2025-02-11 07:55:25,435 - Logits shape: torch.Size([1, 64, 151936])
2025-02-11 07:55:25,435 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,435 - Next token logits device: cuda:0
2025-02-11 07:55:25,435 - Entered do_sample
2025-02-11 07:55:25,435 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,439 - Probs max: 0.91064453125
2025-02-11 07:55:25,439 - Pre-cat
2025-02-11 07:55:25,440 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628]], device='cuda:0')
2025-02-11 07:55:25,442 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:55:25,442 - Current_ids shape: torch.Size([1, 64])
2025-02-11 07:55:25,442 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,442 - Step 33: Generated next token
2025-02-11 07:55:25,442 - Step 33: Updated current_ids
2025-02-11 07:55:25,442 - Step 33: Decoded token text: .
2025-02-11 07:55:25,442 - Step 33: Updated current_phrase
2025-02-11 07:55:25,443 - Step 33: Created step_acts
2025-02-11 07:55:25,443 - Step 33: Added to generation_acts
2025-02-11 07:55:25,443 - Step 33: Updated recent_tokens
2025-02-11 07:55:25,444 - Step 33: Found phrase end token
2025-02-11 07:55:25,444 - Step 33: Updated recent_phrases
2025-02-11 07:55:25,444 - Step 33: Calculated similarity: 0.0
2025-02-11 07:55:25,445 - Step 33: Decoded current text
2025-02-11 07:55:25,445 - Step 33: Reset consecutive_fillers
2025-02-11 07:55:25,445 - Step 33: Calculated unique_ratio: 0.8125
2025-02-11 07:55:25,445 - 
Starting step 34
2025-02-11 07:55:25,445 - Current_ids device: cuda:0
2025-02-11 07:55:25,445 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,491 - Model output complete
2025-02-11 07:55:25,491 - Logits shape: torch.Size([1, 65, 151936])
2025-02-11 07:55:25,491 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,491 - Next token logits device: cuda:0
2025-02-11 07:55:25,491 - Entered do_sample
2025-02-11 07:55:25,491 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,494 - Probs max: 0.99755859375
2025-02-11 07:55:25,495 - Pre-cat
2025-02-11 07:55:25,496 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13]], device='cuda:0')
2025-02-11 07:55:25,500 - Next token: tensor([[422]], device='cuda:0')
2025-02-11 07:55:25,501 - Current_ids shape: torch.Size([1, 65])
2025-02-11 07:55:25,501 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,501 - Step 34: Generated next token
2025-02-11 07:55:25,501 - Step 34: Updated current_ids
2025-02-11 07:55:25,501 - Step 34: Decoded token text:  D
2025-02-11 07:55:25,501 - Step 34: Updated current_phrase
2025-02-11 07:55:25,502 - Step 34: Created step_acts
2025-02-11 07:55:25,502 - Step 34: Added to generation_acts
2025-02-11 07:55:25,502 - Step 34: Updated recent_tokens
2025-02-11 07:55:25,503 - Step 34: Decoded current text
2025-02-11 07:55:25,503 - Step 34: Reset consecutive_fillers
2025-02-11 07:55:25,504 - Step 34: Calculated unique_ratio: 0.8125
2025-02-11 07:55:25,504 - 
Starting step 35
2025-02-11 07:55:25,504 - Current_ids device: cuda:0
2025-02-11 07:55:25,504 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,564 - Model output complete
2025-02-11 07:55:25,564 - Logits shape: torch.Size([1, 66, 151936])
2025-02-11 07:55:25,564 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,564 - Next token logits device: cuda:0
2025-02-11 07:55:25,564 - Entered do_sample
2025-02-11 07:55:25,564 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,567 - Probs max: 1.0
2025-02-11 07:55:25,569 - Pre-cat
2025-02-11 07:55:25,569 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422]], device='cuda:0')
2025-02-11 07:55:25,575 - Next token: tensor([[25]], device='cuda:0')
2025-02-11 07:55:25,575 - Current_ids shape: torch.Size([1, 66])
2025-02-11 07:55:25,575 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,575 - Step 35: Generated next token
2025-02-11 07:55:25,575 - Step 35: Updated current_ids
2025-02-11 07:55:25,576 - Step 35: Decoded token text: :
2025-02-11 07:55:25,576 - Step 35: Updated current_phrase
2025-02-11 07:55:25,576 - Step 35: Created step_acts
2025-02-11 07:55:25,576 - Step 35: Added to generation_acts
2025-02-11 07:55:25,578 - Step 35: Updated generated_texts
2025-02-11 07:55:25,578 - Step 35: Updated recent_tokens
2025-02-11 07:55:25,578 - Step 35: Found phrase end token
2025-02-11 07:55:25,578 - Step 35: Updated recent_phrases
2025-02-11 07:55:25,578 - Step 35: Calculated similarity: 0.0
2025-02-11 07:55:25,579 - Step 35: Decoded current text
2025-02-11 07:55:25,579 - Step 35: Reset consecutive_fillers
2025-02-11 07:55:25,579 - Step 35: Calculated unique_ratio: 0.75
2025-02-11 07:55:25,579 - 
Starting step 36
2025-02-11 07:55:25,579 - Current_ids device: cuda:0
2025-02-11 07:55:25,579 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,616 - Model output complete
2025-02-11 07:55:25,616 - Logits shape: torch.Size([1, 67, 151936])
2025-02-11 07:55:25,616 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,616 - Next token logits device: cuda:0
2025-02-11 07:55:25,616 - Entered do_sample
2025-02-11 07:55:25,616 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,619 - Probs max: 0.9951171875
2025-02-11 07:55:25,620 - Pre-cat
2025-02-11 07:55:25,620 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25]], device='cuda:0')
2025-02-11 07:55:25,623 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:55:25,624 - Current_ids shape: torch.Size([1, 67])
2025-02-11 07:55:25,624 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,624 - Step 36: Generated next token
2025-02-11 07:55:25,624 - Step 36: Updated current_ids
2025-02-11 07:55:25,624 - Step 36: Decoded token text:  It
2025-02-11 07:55:25,624 - Step 36: Updated current_phrase
2025-02-11 07:55:25,625 - Step 36: Created step_acts
2025-02-11 07:55:25,625 - Step 36: Added to generation_acts
2025-02-11 07:55:25,625 - Step 36: Updated recent_tokens
2025-02-11 07:55:25,626 - Step 36: Decoded current text
2025-02-11 07:55:25,626 - Step 36: Reset consecutive_fillers
2025-02-11 07:55:25,627 - Step 36: Calculated unique_ratio: 0.75
2025-02-11 07:55:25,627 - 
Starting step 37
2025-02-11 07:55:25,627 - Current_ids device: cuda:0
2025-02-11 07:55:25,627 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,656 - Model output complete
2025-02-11 07:55:25,656 - Logits shape: torch.Size([1, 68, 151936])
2025-02-11 07:55:25,656 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,656 - Next token logits device: cuda:0
2025-02-11 07:55:25,656 - Entered do_sample
2025-02-11 07:55:25,656 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,659 - Probs max: 0.99609375
2025-02-11 07:55:25,659 - Pre-cat
2025-02-11 07:55:25,660 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084]], device='cuda:0')
2025-02-11 07:55:25,663 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:25,663 - Current_ids shape: torch.Size([1, 68])
2025-02-11 07:55:25,663 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,663 - Step 37: Generated next token
2025-02-11 07:55:25,664 - Step 37: Updated current_ids
2025-02-11 07:55:25,664 - Step 37: Decoded token text:  will
2025-02-11 07:55:25,664 - Step 37: Updated current_phrase
2025-02-11 07:55:25,665 - Step 37: Created step_acts
2025-02-11 07:55:25,665 - Step 37: Added to generation_acts
2025-02-11 07:55:25,665 - Step 37: Updated recent_tokens
2025-02-11 07:55:25,666 - Step 37: Decoded current text
2025-02-11 07:55:25,667 - Step 37: Reset consecutive_fillers
2025-02-11 07:55:25,667 - Step 37: Calculated unique_ratio: 0.75
2025-02-11 07:55:25,667 - 
Starting step 38
2025-02-11 07:55:25,667 - Current_ids device: cuda:0
2025-02-11 07:55:25,667 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,694 - Model output complete
2025-02-11 07:55:25,694 - Logits shape: torch.Size([1, 69, 151936])
2025-02-11 07:55:25,694 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,694 - Next token logits device: cuda:0
2025-02-11 07:55:25,694 - Entered do_sample
2025-02-11 07:55:25,694 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,696 - Probs max: 0.9833984375
2025-02-11 07:55:25,697 - Pre-cat
2025-02-11 07:55:25,697 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686]], device='cuda:0')
2025-02-11 07:55:25,700 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:55:25,700 - Current_ids shape: torch.Size([1, 69])
2025-02-11 07:55:25,700 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,700 - Step 38: Generated next token
2025-02-11 07:55:25,701 - Step 38: Updated current_ids
2025-02-11 07:55:25,701 - Step 38: Decoded token text:  hit
2025-02-11 07:55:25,701 - Step 38: Updated current_phrase
2025-02-11 07:55:25,702 - Step 38: Created step_acts
2025-02-11 07:55:25,702 - Step 38: Added to generation_acts
2025-02-11 07:55:25,702 - Step 38: Updated recent_tokens
2025-02-11 07:55:25,703 - Step 38: Decoded current text
2025-02-11 07:55:25,703 - Step 38: Reset consecutive_fillers
2025-02-11 07:55:25,704 - Step 38: Calculated unique_ratio: 0.6875
2025-02-11 07:55:25,704 - 
Starting step 39
2025-02-11 07:55:25,704 - Current_ids device: cuda:0
2025-02-11 07:55:25,704 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,771 - Model output complete
2025-02-11 07:55:25,771 - Logits shape: torch.Size([1, 70, 151936])
2025-02-11 07:55:25,771 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,772 - Next token logits device: cuda:0
2025-02-11 07:55:25,772 - Entered do_sample
2025-02-11 07:55:25,772 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,775 - Probs max: 0.99365234375
2025-02-11 07:55:25,776 - Pre-cat
2025-02-11 07:55:25,776 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201]],
       device='cuda:0')
2025-02-11 07:55:25,780 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:25,780 - Current_ids shape: torch.Size([1, 70])
2025-02-11 07:55:25,780 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,780 - Step 39: Generated next token
2025-02-11 07:55:25,780 - Step 39: Updated current_ids
2025-02-11 07:55:25,781 - Step 39: Decoded token text:  the
2025-02-11 07:55:25,781 - Step 39: Updated current_phrase
2025-02-11 07:55:25,781 - Step 39: Created step_acts
2025-02-11 07:55:25,781 - Step 39: Added to generation_acts
2025-02-11 07:55:25,781 - Step 39: Updated recent_tokens
2025-02-11 07:55:25,783 - Step 39: Decoded current text
2025-02-11 07:55:25,783 - Step 39: Reset consecutive_fillers
2025-02-11 07:55:25,783 - Step 39: Calculated unique_ratio: 0.6875
2025-02-11 07:55:25,783 - 
Starting step 40
2025-02-11 07:55:25,783 - Current_ids device: cuda:0
2025-02-11 07:55:25,783 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,821 - Model output complete
2025-02-11 07:55:25,822 - Logits shape: torch.Size([1, 71, 151936])
2025-02-11 07:55:25,822 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,822 - Next token logits device: cuda:0
2025-02-11 07:55:25,822 - Entered do_sample
2025-02-11 07:55:25,822 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,824 - Probs max: 1.0
2025-02-11 07:55:25,825 - Pre-cat
2025-02-11 07:55:25,826 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201,    279]],
       device='cuda:0')
2025-02-11 07:55:25,830 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:25,830 - Current_ids shape: torch.Size([1, 71])
2025-02-11 07:55:25,830 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,831 - Step 40: Generated next token
2025-02-11 07:55:25,831 - Step 40: Updated current_ids
2025-02-11 07:55:25,831 - Step 40: Decoded token text:  wall
2025-02-11 07:55:25,831 - Step 40: Updated current_phrase
2025-02-11 07:55:25,831 - Step 40: Created step_acts
2025-02-11 07:55:25,831 - Step 40: Added to generation_acts
2025-02-11 07:55:25,833 - Step 40: Updated generated_texts
2025-02-11 07:55:25,833 - Step 40: Updated recent_tokens
2025-02-11 07:55:25,833 - Step 40: Decoded current text
2025-02-11 07:55:25,833 - Step 40: Reset consecutive_fillers
2025-02-11 07:55:25,834 - Step 40: Calculated unique_ratio: 0.6875
2025-02-11 07:55:25,834 - 
Starting step 41
2025-02-11 07:55:25,834 - Current_ids device: cuda:0
2025-02-11 07:55:25,834 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,861 - Model output complete
2025-02-11 07:55:25,861 - Logits shape: torch.Size([1, 72, 151936])
2025-02-11 07:55:25,861 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,861 - Next token logits device: cuda:0
2025-02-11 07:55:25,861 - Entered do_sample
2025-02-11 07:55:25,861 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,865 - Probs max: 0.9892578125
2025-02-11 07:55:25,866 - Pre-cat
2025-02-11 07:55:25,866 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201,    279,   7002]],
       device='cuda:0')
2025-02-11 07:55:25,870 - Next token: tensor([[448]], device='cuda:0')
2025-02-11 07:55:25,870 - Current_ids shape: torch.Size([1, 72])
2025-02-11 07:55:25,870 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,870 - Step 41: Generated next token
2025-02-11 07:55:25,870 - Step 41: Updated current_ids
2025-02-11 07:55:25,871 - Step 41: Decoded token text:  with
2025-02-11 07:55:25,871 - Step 41: Updated current_phrase
2025-02-11 07:55:25,871 - Step 41: Created step_acts
2025-02-11 07:55:25,871 - Step 41: Added to generation_acts
2025-02-11 07:55:25,871 - Step 41: Updated recent_tokens
2025-02-11 07:55:25,873 - Step 41: Decoded current text
2025-02-11 07:55:25,873 - Step 41: Reset consecutive_fillers
2025-02-11 07:55:25,873 - Step 41: Calculated unique_ratio: 0.6875
2025-02-11 07:55:25,873 - 
Starting step 42
2025-02-11 07:55:25,873 - Current_ids device: cuda:0
2025-02-11 07:55:25,873 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,897 - Model output complete
2025-02-11 07:55:25,897 - Logits shape: torch.Size([1, 73, 151936])
2025-02-11 07:55:25,897 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,897 - Next token logits device: cuda:0
2025-02-11 07:55:25,897 - Entered do_sample
2025-02-11 07:55:25,897 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,901 - Probs max: 0.845703125
2025-02-11 07:55:25,902 - Pre-cat
2025-02-11 07:55:25,902 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201,    279,   7002,
            448]], device='cuda:0')
2025-02-11 07:55:25,905 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:25,905 - Current_ids shape: torch.Size([1, 73])
2025-02-11 07:55:25,905 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,905 - Step 42: Generated next token
2025-02-11 07:55:25,905 - Step 42: Updated current_ids
2025-02-11 07:55:25,906 - Step 42: Decoded token text:  a
2025-02-11 07:55:25,906 - Step 42: Updated current_phrase
2025-02-11 07:55:25,906 - Step 42: Created step_acts
2025-02-11 07:55:25,906 - Step 42: Added to generation_acts
2025-02-11 07:55:25,906 - Step 42: Updated recent_tokens
2025-02-11 07:55:25,908 - Step 42: Decoded current text
2025-02-11 07:55:25,908 - Step 42: Reset consecutive_fillers
2025-02-11 07:55:25,908 - Step 42: Calculated unique_ratio: 0.75
2025-02-11 07:55:25,908 - 
Starting step 43
2025-02-11 07:55:25,908 - Current_ids device: cuda:0
2025-02-11 07:55:25,908 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,931 - Model output complete
2025-02-11 07:55:25,931 - Logits shape: torch.Size([1, 74, 151936])
2025-02-11 07:55:25,931 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,931 - Next token logits device: cuda:0
2025-02-11 07:55:25,932 - Entered do_sample
2025-02-11 07:55:25,932 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,936 - Probs max: 0.63330078125
2025-02-11 07:55:25,937 - Pre-cat
2025-02-11 07:55:25,937 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201,    279,   7002,
            448,    264]], device='cuda:0')
2025-02-11 07:55:25,940 - Next token: tensor([[5080]], device='cuda:0')
2025-02-11 07:55:25,940 - Current_ids shape: torch.Size([1, 74])
2025-02-11 07:55:25,940 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,940 - Step 43: Generated next token
2025-02-11 07:55:25,940 - Step 43: Updated current_ids
2025-02-11 07:55:25,940 - Step 43: Decoded token text:  higher
2025-02-11 07:55:25,940 - Step 43: Updated current_phrase
2025-02-11 07:55:25,941 - Step 43: Created step_acts
2025-02-11 07:55:25,941 - Step 43: Added to generation_acts
2025-02-11 07:55:25,941 - Step 43: Updated recent_tokens
2025-02-11 07:55:25,942 - Step 43: Decoded current text
2025-02-11 07:55:25,943 - Step 43: Reset consecutive_fillers
2025-02-11 07:55:25,943 - Step 43: Calculated unique_ratio: 0.8125
2025-02-11 07:55:25,943 - 
Starting step 44
2025-02-11 07:55:25,943 - Current_ids device: cuda:0
2025-02-11 07:55:25,943 - Current_ids dtype: torch.int64
2025-02-11 07:55:25,966 - Model output complete
2025-02-11 07:55:25,966 - Logits shape: torch.Size([1, 75, 151936])
2025-02-11 07:55:25,966 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,966 - Next token logits device: cuda:0
2025-02-11 07:55:25,966 - Entered do_sample
2025-02-11 07:55:25,966 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:25,972 - Probs max: 0.99365234375
2025-02-11 07:55:25,972 - Pre-cat
2025-02-11 07:55:25,972 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   5080]], device='cuda:0')
2025-02-11 07:55:25,976 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:25,976 - Current_ids shape: torch.Size([1, 75])
2025-02-11 07:55:25,976 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:25,976 - Step 44: Generated next token
2025-02-11 07:55:25,976 - Step 44: Updated current_ids
2025-02-11 07:55:25,976 - Step 44: Decoded token text:  speed
2025-02-11 07:55:25,976 - Step 44: Updated current_phrase
2025-02-11 07:55:25,977 - Step 44: Created step_acts
2025-02-11 07:55:25,977 - Step 44: Added to generation_acts
2025-02-11 07:55:25,977 - Step 44: Updated recent_tokens
2025-02-11 07:55:25,978 - Step 44: Decoded current text
2025-02-11 07:55:25,979 - Step 44: Reset consecutive_fillers
2025-02-11 07:55:25,979 - Step 44: Calculated unique_ratio: 0.8125
2025-02-11 07:55:25,979 - 
Starting step 45
2025-02-11 07:55:25,979 - Current_ids device: cuda:0
2025-02-11 07:55:25,979 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,002 - Model output complete
2025-02-11 07:55:26,002 - Logits shape: torch.Size([1, 76, 151936])
2025-02-11 07:55:26,002 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,002 - Next token logits device: cuda:0
2025-02-11 07:55:26,002 - Entered do_sample
2025-02-11 07:55:26,002 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,006 - Probs max: 0.450439453125
2025-02-11 07:55:26,008 - Pre-cat
2025-02-11 07:55:26,008 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   5080,   4628]], device='cuda:0')
2025-02-11 07:55:26,015 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:26,016 - Current_ids shape: torch.Size([1, 76])
2025-02-11 07:55:26,016 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,016 - Step 45: Generated next token
2025-02-11 07:55:26,016 - Step 45: Updated current_ids
2025-02-11 07:55:26,016 - Step 45: Decoded token text: ,
2025-02-11 07:55:26,016 - Step 45: Updated current_phrase
2025-02-11 07:55:26,017 - Step 45: Created step_acts
2025-02-11 07:55:26,017 - Step 45: Added to generation_acts
2025-02-11 07:55:26,019 - Step 45: Updated generated_texts
2025-02-11 07:55:26,019 - Step 45: Updated recent_tokens
2025-02-11 07:55:26,019 - Step 45: Found phrase end token
2025-02-11 07:55:26,019 - Step 45: Updated recent_phrases
2025-02-11 07:55:26,019 - Step 45: Calculated similarity: 0.0
2025-02-11 07:55:26,019 - Step 45: Decoded current text
2025-02-11 07:55:26,019 - Step 45: Reset consecutive_fillers
2025-02-11 07:55:26,020 - Step 45: Calculated unique_ratio: 0.875
2025-02-11 07:55:26,020 - 
Starting step 46
2025-02-11 07:55:26,020 - Current_ids device: cuda:0
2025-02-11 07:55:26,020 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,077 - Model output complete
2025-02-11 07:55:26,078 - Logits shape: torch.Size([1, 77, 151936])
2025-02-11 07:55:26,078 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,078 - Next token logits device: cuda:0
2025-02-11 07:55:26,078 - Entered do_sample
2025-02-11 07:55:26,078 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,081 - Probs max: 0.9541015625
2025-02-11 07:55:26,082 - Pre-cat
2025-02-11 07:55:26,082 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   5080,   4628,     11]], device='cuda:0')
2025-02-11 07:55:26,085 - Next token: tensor([[714]], device='cuda:0')
2025-02-11 07:55:26,085 - Current_ids shape: torch.Size([1, 77])
2025-02-11 07:55:26,085 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,085 - Step 46: Generated next token
2025-02-11 07:55:26,085 - Step 46: Updated current_ids
2025-02-11 07:55:26,085 - Step 46: Decoded token text:  but
2025-02-11 07:55:26,086 - Step 46: Updated current_phrase
2025-02-11 07:55:26,086 - Step 46: Created step_acts
2025-02-11 07:55:26,086 - Step 46: Added to generation_acts
2025-02-11 07:55:26,086 - Step 46: Updated recent_tokens
2025-02-11 07:55:26,088 - Step 46: Decoded current text
2025-02-11 07:55:26,088 - Step 46: Reset consecutive_fillers
2025-02-11 07:55:26,088 - Step 46: Calculated unique_ratio: 0.9375
2025-02-11 07:55:26,088 - 
Starting step 47
2025-02-11 07:55:26,088 - Current_ids device: cuda:0
2025-02-11 07:55:26,088 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,110 - Model output complete
2025-02-11 07:55:26,111 - Logits shape: torch.Size([1, 78, 151936])
2025-02-11 07:55:26,111 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,111 - Next token logits device: cuda:0
2025-02-11 07:55:26,111 - Entered do_sample
2025-02-11 07:55:26,111 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,116 - Probs max: 0.410400390625
2025-02-11 07:55:26,117 - Pre-cat
2025-02-11 07:55:26,117 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   5080,   4628,     11,    714]], device='cuda:0')
2025-02-11 07:55:26,120 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:26,120 - Current_ids shape: torch.Size([1, 78])
2025-02-11 07:55:26,120 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,120 - Step 47: Generated next token
2025-02-11 07:55:26,120 - Step 47: Updated current_ids
2025-02-11 07:55:26,120 - Step 47: Decoded token text:  the
2025-02-11 07:55:26,120 - Step 47: Updated current_phrase
2025-02-11 07:55:26,121 - Step 47: Created step_acts
2025-02-11 07:55:26,121 - Step 47: Added to generation_acts
2025-02-11 07:55:26,121 - Step 47: Updated recent_tokens
2025-02-11 07:55:26,123 - Step 47: Decoded current text
2025-02-11 07:55:26,123 - Step 47: Reset consecutive_fillers
2025-02-11 07:55:26,123 - Step 47: Calculated unique_ratio: 0.875
2025-02-11 07:55:26,123 - 
Starting step 48
2025-02-11 07:55:26,123 - Current_ids device: cuda:0
2025-02-11 07:55:26,123 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,148 - Model output complete
2025-02-11 07:55:26,148 - Logits shape: torch.Size([1, 79, 151936])
2025-02-11 07:55:26,149 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,149 - Next token logits device: cuda:0
2025-02-11 07:55:26,149 - Entered do_sample
2025-02-11 07:55:26,149 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,151 - Probs max: 0.298583984375
2025-02-11 07:55:26,153 - Pre-cat
2025-02-11 07:55:26,154 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   5080,   4628,     11,    714,    279]],
       device='cuda:0')
2025-02-11 07:55:26,161 - Next token: tensor([[9210]], device='cuda:0')
2025-02-11 07:55:26,162 - Current_ids shape: torch.Size([1, 79])
2025-02-11 07:55:26,162 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,162 - Step 48: Generated next token
2025-02-11 07:55:26,162 - Step 48: Updated current_ids
2025-02-11 07:55:26,162 - Step 48: Decoded token text:  angle
2025-02-11 07:55:26,163 - Step 48: Updated current_phrase
2025-02-11 07:55:26,163 - Step 48: Created step_acts
2025-02-11 07:55:26,163 - Step 48: Added to generation_acts
2025-02-11 07:55:26,163 - Step 48: Updated recent_tokens
2025-02-11 07:55:26,165 - Step 48: Decoded current text
2025-02-11 07:55:26,165 - Step 48: Reset consecutive_fillers
2025-02-11 07:55:26,165 - Step 48: Calculated unique_ratio: 0.9375
2025-02-11 07:55:26,165 - 
Starting step 49
2025-02-11 07:55:26,165 - Current_ids device: cuda:0
2025-02-11 07:55:26,165 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,189 - Model output complete
2025-02-11 07:55:26,189 - Logits shape: torch.Size([1, 80, 151936])
2025-02-11 07:55:26,189 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,189 - Next token logits device: cuda:0
2025-02-11 07:55:26,189 - Entered do_sample
2025-02-11 07:55:26,189 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,193 - Probs max: 0.7802734375
2025-02-11 07:55:26,193 - Pre-cat
2025-02-11 07:55:26,193 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   5080,   4628,     11,    714,    279,   9210]],
       device='cuda:0')
2025-02-11 07:55:26,196 - Next token: tensor([[315]], device='cuda:0')
2025-02-11 07:55:26,196 - Current_ids shape: torch.Size([1, 80])
2025-02-11 07:55:26,196 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,197 - Step 49: Generated next token
2025-02-11 07:55:26,197 - Step 49: Updated current_ids
2025-02-11 07:55:26,197 - Step 49: Decoded token text:  of
2025-02-11 07:55:26,197 - Step 49: Updated current_phrase
2025-02-11 07:55:26,197 - Step 49: Created step_acts
2025-02-11 07:55:26,197 - Step 49: Added to generation_acts
2025-02-11 07:55:26,197 - Step 49: Updated recent_tokens
2025-02-11 07:55:26,199 - Step 49: Decoded current text
2025-02-11 07:55:26,199 - Step 49: Reset consecutive_fillers
2025-02-11 07:55:26,199 - Step 49: Calculated unique_ratio: 0.9375
2025-02-11 07:55:26,199 - 
Starting step 50
2025-02-11 07:55:26,199 - Current_ids device: cuda:0
2025-02-11 07:55:26,199 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,226 - Model output complete
2025-02-11 07:55:26,226 - Logits shape: torch.Size([1, 81, 151936])
2025-02-11 07:55:26,226 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,226 - Next token logits device: cuda:0
2025-02-11 07:55:26,226 - Entered do_sample
2025-02-11 07:55:26,226 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,228 - Probs max: 0.83837890625
2025-02-11 07:55:26,230 - Pre-cat
2025-02-11 07:55:26,230 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   5080,   4628,     11,    714,    279,   9210,    315]],
       device='cuda:0')
2025-02-11 07:55:26,237 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:26,237 - Current_ids shape: torch.Size([1, 81])
2025-02-11 07:55:26,237 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,237 - Step 50: Generated next token
2025-02-11 07:55:26,237 - Step 50: Updated current_ids
2025-02-11 07:55:26,237 - Step 50: Decoded token text:  the
2025-02-11 07:55:26,237 - Step 50: Updated current_phrase
2025-02-11 07:55:26,238 - Step 50: Created step_acts
2025-02-11 07:55:26,238 - Step 50: Added to generation_acts
2025-02-11 07:55:26,240 - Step 50: Updated generated_texts
2025-02-11 07:55:26,240 - Step 50: Updated recent_tokens
2025-02-11 07:55:26,240 - Step 50: Decoded current text
2025-02-11 07:55:26,240 - Step 50: Incremented consecutive_fillers to 1
2025-02-11 07:55:26,240 - Step 50: Calculated unique_ratio: 0.875
2025-02-11 07:55:26,241 - 
Starting step 51
2025-02-11 07:55:26,241 - Current_ids device: cuda:0
2025-02-11 07:55:26,241 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,267 - Model output complete
2025-02-11 07:55:26,267 - Logits shape: torch.Size([1, 82, 151936])
2025-02-11 07:55:26,267 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,267 - Next token logits device: cuda:0
2025-02-11 07:55:26,267 - Entered do_sample
2025-02-11 07:55:26,267 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,271 - Probs max: 0.94580078125
2025-02-11 07:55:26,272 - Pre-cat
2025-02-11 07:55:26,272 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   5080,   4628,     11,    714,    279,   9210,    315,
            279]], device='cuda:0')
2025-02-11 07:55:26,275 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:26,276 - Current_ids shape: torch.Size([1, 82])
2025-02-11 07:55:26,276 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,276 - Step 51: Generated next token
2025-02-11 07:55:26,276 - Step 51: Updated current_ids
2025-02-11 07:55:26,276 - Step 51: Decoded token text:  wall
2025-02-11 07:55:26,276 - Step 51: Updated current_phrase
2025-02-11 07:55:26,276 - Step 51: Created step_acts
2025-02-11 07:55:26,277 - Step 51: Added to generation_acts
2025-02-11 07:55:26,277 - Step 51: Updated recent_tokens
2025-02-11 07:55:26,278 - Step 51: Decoded current text
2025-02-11 07:55:26,278 - Step 51: Incremented consecutive_fillers to 2
2025-02-11 07:55:26,279 - Step 51: Calculated unique_ratio: 0.8125
2025-02-11 07:55:26,279 - 
Starting step 52
2025-02-11 07:55:26,279 - Current_ids device: cuda:0
2025-02-11 07:55:26,279 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,302 - Model output complete
2025-02-11 07:55:26,302 - Logits shape: torch.Size([1, 83, 151936])
2025-02-11 07:55:26,302 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,302 - Next token logits device: cuda:0
2025-02-11 07:55:26,302 - Entered do_sample
2025-02-11 07:55:26,302 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,307 - Probs max: 0.53759765625
2025-02-11 07:55:26,308 - Pre-cat
2025-02-11 07:55:26,308 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   4722,   4628,     13,    425,     25,   1084,    686,
           4201,    279,   7002,    448,    264,   5080,   4628,     13,    356,
             25,   1084,    686,   4201,    279,   7002,    448,    279,   1852,
           4628,     13,    422,     25,   1084,    686,   4201,    279,   7002,
            448,    264,   5080,   4628,     11,    714,    279,   9210,    315,
            279,   7002]], device='cuda:0')
2025-02-11 07:55:26,311 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:26,312 - Current_ids shape: torch.Size([1, 83])
2025-02-11 07:55:26,312 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,312 - Step 52: Generated next token
2025-02-11 07:55:26,312 - Step 52: Updated current_ids
2025-02-11 07:55:26,312 - Step 52: Decoded token text:  will
2025-02-11 07:55:26,312 - Step 52: Updated current_phrase
2025-02-11 07:55:26,313 - Step 52: Created step_acts
2025-02-11 07:55:26,313 - Step 52: Added to generation_acts
2025-02-11 07:55:26,313 - Step 52: Updated recent_tokens
2025-02-11 07:55:26,314 - Step 52: Decoded current text
2025-02-11 07:55:26,315 - Step 52: Incremented consecutive_fillers to 3
2025-02-11 07:55:26,417 - 
Starting step 0
2025-02-11 07:55:26,417 - Current_ids device: cuda:0
2025-02-11 07:55:26,417 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,441 - Model output complete
2025-02-11 07:55:26,441 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:26,441 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,441 - Next token logits device: cuda:0
2025-02-11 07:55:26,441 - Entered do_sample
2025-02-11 07:55:26,441 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,445 - Probs max: 0.50341796875
2025-02-11 07:55:26,445 - Pre-cat
2025-02-11 07:55:26,445 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:26,448 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:55:26,449 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:26,449 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,449 - Step 0: Generated next token
2025-02-11 07:55:26,449 - Step 0: Updated current_ids
2025-02-11 07:55:26,449 - Step 0: Decoded token text:  It
2025-02-11 07:55:26,449 - Step 0: Updated current_phrase
2025-02-11 07:55:26,450 - Step 0: Created step_acts
2025-02-11 07:55:26,450 - Step 0: Added to generation_acts
2025-02-11 07:55:26,451 - Step 0: Updated generated_texts
2025-02-11 07:55:26,451 - Step 0: Updated recent_tokens
2025-02-11 07:55:26,451 - Step 0: Decoded current text
2025-02-11 07:55:26,451 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:26,451 - 
Starting step 1
2025-02-11 07:55:26,451 - Current_ids device: cuda:0
2025-02-11 07:55:26,451 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,475 - Model output complete
2025-02-11 07:55:26,475 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:26,475 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,475 - Next token logits device: cuda:0
2025-02-11 07:55:26,475 - Entered do_sample
2025-02-11 07:55:26,476 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,478 - Probs max: 0.9384765625
2025-02-11 07:55:26,480 - Pre-cat
2025-02-11 07:55:26,481 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084]], device='cuda:0')
2025-02-11 07:55:26,483 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:26,484 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:26,484 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,484 - Step 1: Generated next token
2025-02-11 07:55:26,484 - Step 1: Updated current_ids
2025-02-11 07:55:26,484 - Step 1: Decoded token text:  will
2025-02-11 07:55:26,484 - Step 1: Updated current_phrase
2025-02-11 07:55:26,485 - Step 1: Created step_acts
2025-02-11 07:55:26,485 - Step 1: Added to generation_acts
2025-02-11 07:55:26,485 - Step 1: Updated recent_tokens
2025-02-11 07:55:26,486 - Step 1: Decoded current text
2025-02-11 07:55:26,486 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:26,486 - 
Starting step 2
2025-02-11 07:55:26,486 - Current_ids device: cuda:0
2025-02-11 07:55:26,487 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,514 - Model output complete
2025-02-11 07:55:26,514 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:26,515 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,515 - Next token logits device: cuda:0
2025-02-11 07:55:26,515 - Entered do_sample
2025-02-11 07:55:26,515 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,518 - Probs max: 0.2442626953125
2025-02-11 07:55:26,519 - Pre-cat
2025-02-11 07:55:26,519 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686]], device='cuda:0')
2025-02-11 07:55:26,521 - Next token: tensor([[1191]], device='cuda:0')
2025-02-11 07:55:26,521 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:26,521 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,522 - Step 2: Generated next token
2025-02-11 07:55:26,522 - Step 2: Updated current_ids
2025-02-11 07:55:26,522 - Step 2: Decoded token text:  start
2025-02-11 07:55:26,522 - Step 2: Updated current_phrase
2025-02-11 07:55:26,522 - Step 2: Created step_acts
2025-02-11 07:55:26,522 - Step 2: Added to generation_acts
2025-02-11 07:55:26,522 - Step 2: Updated recent_tokens
2025-02-11 07:55:26,524 - Step 2: Decoded current text
2025-02-11 07:55:26,524 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:26,524 - 
Starting step 3
2025-02-11 07:55:26,524 - Current_ids device: cuda:0
2025-02-11 07:55:26,524 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,548 - Model output complete
2025-02-11 07:55:26,548 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:26,548 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,548 - Next token logits device: cuda:0
2025-02-11 07:55:26,548 - Entered do_sample
2025-02-11 07:55:26,548 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,551 - Probs max: 0.67041015625
2025-02-11 07:55:26,552 - Pre-cat
2025-02-11 07:55:26,552 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1191]],
       device='cuda:0')
2025-02-11 07:55:26,553 - Next token: tensor([[311]], device='cuda:0')
2025-02-11 07:55:26,553 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:26,553 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,553 - Step 3: Generated next token
2025-02-11 07:55:26,553 - Step 3: Updated current_ids
2025-02-11 07:55:26,554 - Step 3: Decoded token text:  to
2025-02-11 07:55:26,554 - Step 3: Updated current_phrase
2025-02-11 07:55:26,554 - Step 3: Created step_acts
2025-02-11 07:55:26,554 - Step 3: Added to generation_acts
2025-02-11 07:55:26,554 - Step 3: Updated recent_tokens
2025-02-11 07:55:26,556 - Step 3: Decoded current text
2025-02-11 07:55:26,556 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:26,556 - 
Starting step 4
2025-02-11 07:55:26,556 - Current_ids device: cuda:0
2025-02-11 07:55:26,556 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,578 - Model output complete
2025-02-11 07:55:26,578 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:26,578 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,578 - Next token logits device: cuda:0
2025-02-11 07:55:26,578 - Entered do_sample
2025-02-11 07:55:26,579 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,581 - Probs max: 0.364990234375
2025-02-11 07:55:26,582 - Pre-cat
2025-02-11 07:55:26,582 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1191,    311]],
       device='cuda:0')
2025-02-11 07:55:26,583 - Next token: tensor([[3271]], device='cuda:0')
2025-02-11 07:55:26,583 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:26,583 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,584 - Step 4: Generated next token
2025-02-11 07:55:26,584 - Step 4: Updated current_ids
2025-02-11 07:55:26,584 - Step 4: Decoded token text:  move
2025-02-11 07:55:26,584 - Step 4: Updated current_phrase
2025-02-11 07:55:26,584 - Step 4: Created step_acts
2025-02-11 07:55:26,584 - Step 4: Added to generation_acts
2025-02-11 07:55:26,584 - Step 4: Updated recent_tokens
2025-02-11 07:55:26,586 - Step 4: Decoded current text
2025-02-11 07:55:26,586 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:26,586 - 
Starting step 5
2025-02-11 07:55:26,586 - Current_ids device: cuda:0
2025-02-11 07:55:26,586 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,608 - Model output complete
2025-02-11 07:55:26,608 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:26,608 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,608 - Next token logits device: cuda:0
2025-02-11 07:55:26,608 - Entered do_sample
2025-02-11 07:55:26,609 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,612 - Probs max: 0.2666015625
2025-02-11 07:55:26,612 - Pre-cat
2025-02-11 07:55:26,612 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1191,    311,   3271]],
       device='cuda:0')
2025-02-11 07:55:26,616 - Next token: tensor([[6974]], device='cuda:0')
2025-02-11 07:55:26,616 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:26,616 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,616 - Step 5: Generated next token
2025-02-11 07:55:26,616 - Step 5: Updated current_ids
2025-02-11 07:55:26,616 - Step 5: Decoded token text:  towards
2025-02-11 07:55:26,617 - Step 5: Updated current_phrase
2025-02-11 07:55:26,617 - Step 5: Created step_acts
2025-02-11 07:55:26,617 - Step 5: Added to generation_acts
2025-02-11 07:55:26,618 - Step 5: Updated generated_texts
2025-02-11 07:55:26,618 - Step 5: Updated recent_tokens
2025-02-11 07:55:26,619 - Step 5: Decoded current text
2025-02-11 07:55:26,619 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:26,619 - 
Starting step 6
2025-02-11 07:55:26,619 - Current_ids device: cuda:0
2025-02-11 07:55:26,619 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,642 - Model output complete
2025-02-11 07:55:26,643 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:26,643 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,643 - Next token logits device: cuda:0
2025-02-11 07:55:26,643 - Entered do_sample
2025-02-11 07:55:26,643 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,646 - Probs max: 0.95068359375
2025-02-11 07:55:26,647 - Pre-cat
2025-02-11 07:55:26,647 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1191,    311,   3271,
           6974]], device='cuda:0')
2025-02-11 07:55:26,648 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:26,648 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:26,648 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,648 - Step 6: Generated next token
2025-02-11 07:55:26,649 - Step 6: Updated current_ids
2025-02-11 07:55:26,649 - Step 6: Decoded token text:  the
2025-02-11 07:55:26,649 - Step 6: Updated current_phrase
2025-02-11 07:55:26,649 - Step 6: Created step_acts
2025-02-11 07:55:26,649 - Step 6: Added to generation_acts
2025-02-11 07:55:26,649 - Step 6: Updated recent_tokens
2025-02-11 07:55:26,651 - Step 6: Decoded current text
2025-02-11 07:55:26,651 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:26,651 - 
Starting step 7
2025-02-11 07:55:26,651 - Current_ids device: cuda:0
2025-02-11 07:55:26,651 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,673 - Model output complete
2025-02-11 07:55:26,673 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:26,673 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,673 - Next token logits device: cuda:0
2025-02-11 07:55:26,673 - Entered do_sample
2025-02-11 07:55:26,673 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,676 - Probs max: 0.95458984375
2025-02-11 07:55:26,677 - Pre-cat
2025-02-11 07:55:26,677 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1191,    311,   3271,
           6974,    279]], device='cuda:0')
2025-02-11 07:55:26,678 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:26,679 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:26,679 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,679 - Step 7: Generated next token
2025-02-11 07:55:26,679 - Step 7: Updated current_ids
2025-02-11 07:55:26,679 - Step 7: Decoded token text:  wall
2025-02-11 07:55:26,679 - Step 7: Updated current_phrase
2025-02-11 07:55:26,679 - Step 7: Created step_acts
2025-02-11 07:55:26,679 - Step 7: Added to generation_acts
2025-02-11 07:55:26,679 - Step 7: Updated recent_tokens
2025-02-11 07:55:26,681 - Step 7: Decoded current text
2025-02-11 07:55:26,681 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:26,681 - 
Starting step 8
2025-02-11 07:55:26,681 - Current_ids device: cuda:0
2025-02-11 07:55:26,681 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,704 - Model output complete
2025-02-11 07:55:26,704 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:26,704 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,704 - Next token logits device: cuda:0
2025-02-11 07:55:26,704 - Entered do_sample
2025-02-11 07:55:26,704 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,707 - Probs max: 0.3955078125
2025-02-11 07:55:26,708 - Pre-cat
2025-02-11 07:55:26,708 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1191,    311,   3271,
           6974,    279,   7002]], device='cuda:0')
2025-02-11 07:55:26,710 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:26,711 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:26,711 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,711 - Step 8: Generated next token
2025-02-11 07:55:26,711 - Step 8: Updated current_ids
2025-02-11 07:55:26,711 - Step 8: Decoded token text: ,
2025-02-11 07:55:26,711 - Step 8: Updated current_phrase
2025-02-11 07:55:26,711 - Step 8: Created step_acts
2025-02-11 07:55:26,711 - Step 8: Added to generation_acts
2025-02-11 07:55:26,711 - Step 8: Updated recent_tokens
2025-02-11 07:55:26,713 - Step 8: Found phrase end token
2025-02-11 07:55:26,713 - Step 8: Updated recent_phrases
2025-02-11 07:55:26,713 - Step 8: Decoded current text
2025-02-11 07:55:26,713 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:26,713 - 
Starting step 9
2025-02-11 07:55:26,713 - Current_ids device: cuda:0
2025-02-11 07:55:26,713 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,736 - Model output complete
2025-02-11 07:55:26,736 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:26,736 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,736 - Next token logits device: cuda:0
2025-02-11 07:55:26,736 - Entered do_sample
2025-02-11 07:55:26,737 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,739 - Probs max: 0.4453125
2025-02-11 07:55:26,740 - Pre-cat
2025-02-11 07:55:26,740 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1191,    311,   3271,
           6974,    279,   7002,     11]], device='cuda:0')
2025-02-11 07:55:26,743 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:55:26,743 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:26,743 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,743 - Step 9: Generated next token
2025-02-11 07:55:26,744 - Step 9: Updated current_ids
2025-02-11 07:55:26,744 - Step 9: Decoded token text:  and
2025-02-11 07:55:26,744 - Step 9: Updated current_phrase
2025-02-11 07:55:26,744 - Step 9: Created step_acts
2025-02-11 07:55:26,744 - Step 9: Added to generation_acts
2025-02-11 07:55:26,744 - Step 9: Updated recent_tokens
2025-02-11 07:55:26,746 - Step 9: Decoded current text
2025-02-11 07:55:26,746 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:26,746 - 
Starting step 10
2025-02-11 07:55:26,746 - Current_ids device: cuda:0
2025-02-11 07:55:26,746 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,769 - Model output complete
2025-02-11 07:55:26,769 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:26,769 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,769 - Next token logits device: cuda:0
2025-02-11 07:55:26,769 - Entered do_sample
2025-02-11 07:55:26,769 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,771 - Probs max: 0.81005859375
2025-02-11 07:55:26,772 - Pre-cat
2025-02-11 07:55:26,772 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1191,    311,   3271,
           6974,    279,   7002,     11,    323]], device='cuda:0')
2025-02-11 07:55:26,774 - Next token: tensor([[1221]], device='cuda:0')
2025-02-11 07:55:26,774 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:26,774 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,774 - Step 10: Generated next token
2025-02-11 07:55:26,774 - Step 10: Updated current_ids
2025-02-11 07:55:26,775 - Step 10: Decoded token text:  then
2025-02-11 07:55:26,775 - Step 10: Updated current_phrase
2025-02-11 07:55:26,775 - Step 10: Created step_acts
2025-02-11 07:55:26,775 - Step 10: Added to generation_acts
2025-02-11 07:55:26,776 - Step 10: Updated generated_texts
2025-02-11 07:55:26,777 - Step 10: Updated recent_tokens
2025-02-11 07:55:26,777 - Step 10: Decoded current text
2025-02-11 07:55:26,777 - Step 10: Incremented consecutive_fillers to 1
2025-02-11 07:55:26,777 - 
Starting step 11
2025-02-11 07:55:26,777 - Current_ids device: cuda:0
2025-02-11 07:55:26,777 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,800 - Model output complete
2025-02-11 07:55:26,801 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:26,801 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,801 - Next token logits device: cuda:0
2025-02-11 07:55:26,801 - Entered do_sample
2025-02-11 07:55:26,801 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,804 - Probs max: 0.30126953125
2025-02-11 07:55:26,805 - Pre-cat
2025-02-11 07:55:26,805 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1191,    311,   3271,
           6974,    279,   7002,     11,    323,   1221]], device='cuda:0')
2025-02-11 07:55:26,807 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:26,807 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:26,807 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,807 - Step 11: Generated next token
2025-02-11 07:55:26,807 - Step 11: Updated current_ids
2025-02-11 07:55:26,808 - Step 11: Decoded token text:  the
2025-02-11 07:55:26,808 - Step 11: Updated current_phrase
2025-02-11 07:55:26,808 - Step 11: Created step_acts
2025-02-11 07:55:26,808 - Step 11: Added to generation_acts
2025-02-11 07:55:26,808 - Step 11: Updated recent_tokens
2025-02-11 07:55:26,809 - Step 11: Decoded current text
2025-02-11 07:55:26,809 - Step 11: Incremented consecutive_fillers to 2
2025-02-11 07:55:26,810 - 
Starting step 12
2025-02-11 07:55:26,810 - Current_ids device: cuda:0
2025-02-11 07:55:26,810 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,832 - Model output complete
2025-02-11 07:55:26,832 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:26,832 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,832 - Next token logits device: cuda:0
2025-02-11 07:55:26,832 - Entered do_sample
2025-02-11 07:55:26,832 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,835 - Probs max: 0.91650390625
2025-02-11 07:55:26,836 - Pre-cat
2025-02-11 07:55:26,836 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   1191,    311,   3271,
           6974,    279,   7002,     11,    323,   1221,    279]],
       device='cuda:0')
2025-02-11 07:55:26,838 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:26,838 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:26,838 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,839 - Step 12: Generated next token
2025-02-11 07:55:26,839 - Step 12: Updated current_ids
2025-02-11 07:55:26,839 - Step 12: Decoded token text:  wall
2025-02-11 07:55:26,839 - Step 12: Updated current_phrase
2025-02-11 07:55:26,839 - Step 12: Created step_acts
2025-02-11 07:55:26,839 - Step 12: Added to generation_acts
2025-02-11 07:55:26,840 - Step 12: Updated recent_tokens
2025-02-11 07:55:26,841 - Step 12: Decoded current text
2025-02-11 07:55:26,841 - Step 12: Incremented consecutive_fillers to 3
2025-02-11 07:55:26,939 - 
Starting step 0
2025-02-11 07:55:26,939 - Current_ids device: cuda:0
2025-02-11 07:55:26,939 - Current_ids dtype: torch.int64
2025-02-11 07:55:26,963 - Model output complete
2025-02-11 07:55:26,963 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:26,963 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,963 - Next token logits device: cuda:0
2025-02-11 07:55:26,963 - Entered do_sample
2025-02-11 07:55:26,964 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:26,967 - Probs max: 0.50341796875
2025-02-11 07:55:26,967 - Pre-cat
2025-02-11 07:55:26,968 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:26,969 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:26,969 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:26,969 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:26,969 - Step 0: Generated next token
2025-02-11 07:55:26,970 - Step 0: Updated current_ids
2025-02-11 07:55:26,970 - Step 0: Decoded token text:  The
2025-02-11 07:55:26,970 - Step 0: Updated current_phrase
2025-02-11 07:55:26,970 - Step 0: Created step_acts
2025-02-11 07:55:26,970 - Step 0: Added to generation_acts
2025-02-11 07:55:26,971 - Step 0: Updated generated_texts
2025-02-11 07:55:26,972 - Step 0: Updated recent_tokens
2025-02-11 07:55:26,972 - Step 0: Decoded current text
2025-02-11 07:55:26,972 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:26,972 - 
Starting step 1
2025-02-11 07:55:26,972 - Current_ids device: cuda:0
2025-02-11 07:55:26,972 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,001 - Model output complete
2025-02-11 07:55:27,001 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:27,001 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,001 - Next token logits device: cuda:0
2025-02-11 07:55:27,001 - Entered do_sample
2025-02-11 07:55:27,002 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,004 - Probs max: 0.83837890625
2025-02-11 07:55:27,005 - Pre-cat
2025-02-11 07:55:27,005 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:55:27,007 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:27,008 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:27,008 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,008 - Step 1: Generated next token
2025-02-11 07:55:27,008 - Step 1: Updated current_ids
2025-02-11 07:55:27,008 - Step 1: Decoded token text:  ball
2025-02-11 07:55:27,008 - Step 1: Updated current_phrase
2025-02-11 07:55:27,009 - Step 1: Created step_acts
2025-02-11 07:55:27,009 - Step 1: Added to generation_acts
2025-02-11 07:55:27,009 - Step 1: Updated recent_tokens
2025-02-11 07:55:27,011 - Step 1: Decoded current text
2025-02-11 07:55:27,012 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:27,012 - 
Starting step 2
2025-02-11 07:55:27,012 - Current_ids device: cuda:0
2025-02-11 07:55:27,012 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,058 - Model output complete
2025-02-11 07:55:27,058 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:27,058 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,058 - Next token logits device: cuda:0
2025-02-11 07:55:27,058 - Entered do_sample
2025-02-11 07:55:27,058 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,060 - Probs max: 0.583984375
2025-02-11 07:55:27,061 - Pre-cat
2025-02-11 07:55:27,061 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:55:27,063 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:27,063 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:27,063 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,064 - Step 2: Generated next token
2025-02-11 07:55:27,064 - Step 2: Updated current_ids
2025-02-11 07:55:27,064 - Step 2: Decoded token text:  will
2025-02-11 07:55:27,064 - Step 2: Updated current_phrase
2025-02-11 07:55:27,065 - Step 2: Created step_acts
2025-02-11 07:55:27,065 - Step 2: Added to generation_acts
2025-02-11 07:55:27,065 - Step 2: Updated recent_tokens
2025-02-11 07:55:27,066 - Step 2: Decoded current text
2025-02-11 07:55:27,066 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:27,067 - 
Starting step 3
2025-02-11 07:55:27,067 - Current_ids device: cuda:0
2025-02-11 07:55:27,067 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,093 - Model output complete
2025-02-11 07:55:27,093 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:27,093 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,093 - Next token logits device: cuda:0
2025-02-11 07:55:27,093 - Entered do_sample
2025-02-11 07:55:27,093 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,096 - Probs max: 0.56884765625
2025-02-11 07:55:27,097 - Pre-cat
2025-02-11 07:55:27,097 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686]],
       device='cuda:0')
2025-02-11 07:55:27,098 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:55:27,099 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:27,099 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,099 - Step 3: Generated next token
2025-02-11 07:55:27,099 - Step 3: Updated current_ids
2025-02-11 07:55:27,099 - Step 3: Decoded token text:  hit
2025-02-11 07:55:27,099 - Step 3: Updated current_phrase
2025-02-11 07:55:27,099 - Step 3: Created step_acts
2025-02-11 07:55:27,099 - Step 3: Added to generation_acts
2025-02-11 07:55:27,100 - Step 3: Updated recent_tokens
2025-02-11 07:55:27,101 - Step 3: Decoded current text
2025-02-11 07:55:27,101 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:27,101 - 
Starting step 4
2025-02-11 07:55:27,101 - Current_ids device: cuda:0
2025-02-11 07:55:27,101 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,123 - Model output complete
2025-02-11 07:55:27,123 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:27,123 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,123 - Next token logits device: cuda:0
2025-02-11 07:55:27,123 - Entered do_sample
2025-02-11 07:55:27,123 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,126 - Probs max: 0.99609375
2025-02-11 07:55:27,127 - Pre-cat
2025-02-11 07:55:27,127 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201]],
       device='cuda:0')
2025-02-11 07:55:27,128 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:27,128 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:27,128 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,129 - Step 4: Generated next token
2025-02-11 07:55:27,129 - Step 4: Updated current_ids
2025-02-11 07:55:27,129 - Step 4: Decoded token text:  the
2025-02-11 07:55:27,129 - Step 4: Updated current_phrase
2025-02-11 07:55:27,129 - Step 4: Created step_acts
2025-02-11 07:55:27,129 - Step 4: Added to generation_acts
2025-02-11 07:55:27,129 - Step 4: Updated recent_tokens
2025-02-11 07:55:27,131 - Step 4: Decoded current text
2025-02-11 07:55:27,131 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:27,131 - 
Starting step 5
2025-02-11 07:55:27,131 - Current_ids device: cuda:0
2025-02-11 07:55:27,131 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,163 - Model output complete
2025-02-11 07:55:27,163 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:27,163 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,163 - Next token logits device: cuda:0
2025-02-11 07:55:27,163 - Entered do_sample
2025-02-11 07:55:27,164 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,166 - Probs max: 0.99853515625
2025-02-11 07:55:27,166 - Pre-cat
2025-02-11 07:55:27,166 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279]],
       device='cuda:0')
2025-02-11 07:55:27,168 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:27,168 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:27,168 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,168 - Step 5: Generated next token
2025-02-11 07:55:27,168 - Step 5: Updated current_ids
2025-02-11 07:55:27,168 - Step 5: Decoded token text:  wall
2025-02-11 07:55:27,169 - Step 5: Updated current_phrase
2025-02-11 07:55:27,169 - Step 5: Created step_acts
2025-02-11 07:55:27,169 - Step 5: Added to generation_acts
2025-02-11 07:55:27,170 - Step 5: Updated generated_texts
2025-02-11 07:55:27,170 - Step 5: Updated recent_tokens
2025-02-11 07:55:27,170 - Step 5: Decoded current text
2025-02-11 07:55:27,171 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:27,171 - 
Starting step 6
2025-02-11 07:55:27,171 - Current_ids device: cuda:0
2025-02-11 07:55:27,171 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,193 - Model output complete
2025-02-11 07:55:27,193 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:27,193 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,193 - Next token logits device: cuda:0
2025-02-11 07:55:27,193 - Entered do_sample
2025-02-11 07:55:27,193 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,197 - Probs max: 0.376708984375
2025-02-11 07:55:27,197 - Pre-cat
2025-02-11 07:55:27,197 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002]], device='cuda:0')
2025-02-11 07:55:27,199 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:55:27,199 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:27,199 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,199 - Step 6: Generated next token
2025-02-11 07:55:27,199 - Step 6: Updated current_ids
2025-02-11 07:55:27,199 - Step 6: Decoded token text:  and
2025-02-11 07:55:27,199 - Step 6: Updated current_phrase
2025-02-11 07:55:27,200 - Step 6: Created step_acts
2025-02-11 07:55:27,200 - Step 6: Added to generation_acts
2025-02-11 07:55:27,200 - Step 6: Updated recent_tokens
2025-02-11 07:55:27,201 - Step 6: Decoded current text
2025-02-11 07:55:27,201 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:27,201 - 
Starting step 7
2025-02-11 07:55:27,201 - Current_ids device: cuda:0
2025-02-11 07:55:27,201 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,223 - Model output complete
2025-02-11 07:55:27,223 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:27,223 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,223 - Next token logits device: cuda:0
2025-02-11 07:55:27,223 - Entered do_sample
2025-02-11 07:55:27,224 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,226 - Probs max: 0.416748046875
2025-02-11 07:55:27,227 - Pre-cat
2025-02-11 07:55:27,227 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323]], device='cuda:0')
2025-02-11 07:55:27,232 - Next token: tensor([[20524]], device='cuda:0')
2025-02-11 07:55:27,232 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:27,232 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,232 - Step 7: Generated next token
2025-02-11 07:55:27,232 - Step 7: Updated current_ids
2025-02-11 07:55:27,233 - Step 7: Decoded token text:  burst
2025-02-11 07:55:27,233 - Step 7: Updated current_phrase
2025-02-11 07:55:27,233 - Step 7: Created step_acts
2025-02-11 07:55:27,233 - Step 7: Added to generation_acts
2025-02-11 07:55:27,233 - Step 7: Updated recent_tokens
2025-02-11 07:55:27,234 - Step 7: Decoded current text
2025-02-11 07:55:27,235 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:27,235 - 
Starting step 8
2025-02-11 07:55:27,235 - Current_ids device: cuda:0
2025-02-11 07:55:27,235 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,258 - Model output complete
2025-02-11 07:55:27,258 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:27,258 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,258 - Next token logits device: cuda:0
2025-02-11 07:55:27,258 - Entered do_sample
2025-02-11 07:55:27,258 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,261 - Probs max: 0.76611328125
2025-02-11 07:55:27,261 - Pre-cat
2025-02-11 07:55:27,261 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,  20524]], device='cuda:0')
2025-02-11 07:55:27,263 - Next token: tensor([[1119]], device='cuda:0')
2025-02-11 07:55:27,263 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:27,263 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,263 - Step 8: Generated next token
2025-02-11 07:55:27,263 - Step 8: Updated current_ids
2025-02-11 07:55:27,263 - Step 8: Decoded token text:  into
2025-02-11 07:55:27,264 - Step 8: Updated current_phrase
2025-02-11 07:55:27,264 - Step 8: Created step_acts
2025-02-11 07:55:27,264 - Step 8: Added to generation_acts
2025-02-11 07:55:27,264 - Step 8: Updated recent_tokens
2025-02-11 07:55:27,265 - Step 8: Decoded current text
2025-02-11 07:55:27,265 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:27,265 - 
Starting step 9
2025-02-11 07:55:27,265 - Current_ids device: cuda:0
2025-02-11 07:55:27,265 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,291 - Model output complete
2025-02-11 07:55:27,291 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:27,291 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,291 - Next token logits device: cuda:0
2025-02-11 07:55:27,291 - Entered do_sample
2025-02-11 07:55:27,292 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,294 - Probs max: 0.56396484375
2025-02-11 07:55:27,294 - Pre-cat
2025-02-11 07:55:27,294 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,  20524,   1119]], device='cuda:0')
2025-02-11 07:55:27,296 - Next token: tensor([[1378]], device='cuda:0')
2025-02-11 07:55:27,296 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:27,296 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,296 - Step 9: Generated next token
2025-02-11 07:55:27,297 - Step 9: Updated current_ids
2025-02-11 07:55:27,297 - Step 9: Decoded token text:  two
2025-02-11 07:55:27,297 - Step 9: Updated current_phrase
2025-02-11 07:55:27,297 - Step 9: Created step_acts
2025-02-11 07:55:27,297 - Step 9: Added to generation_acts
2025-02-11 07:55:27,297 - Step 9: Updated recent_tokens
2025-02-11 07:55:27,299 - Step 9: Decoded current text
2025-02-11 07:55:27,299 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:27,299 - 
Starting step 10
2025-02-11 07:55:27,299 - Current_ids device: cuda:0
2025-02-11 07:55:27,299 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,328 - Model output complete
2025-02-11 07:55:27,328 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:27,328 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,328 - Next token logits device: cuda:0
2025-02-11 07:55:27,328 - Entered do_sample
2025-02-11 07:55:27,328 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,331 - Probs max: 0.8623046875
2025-02-11 07:55:27,332 - Pre-cat
2025-02-11 07:55:27,332 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,  20524,   1119,   1378]], device='cuda:0')
2025-02-11 07:55:27,334 - Next token: tensor([[5479]], device='cuda:0')
2025-02-11 07:55:27,334 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:27,334 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,334 - Step 10: Generated next token
2025-02-11 07:55:27,334 - Step 10: Updated current_ids
2025-02-11 07:55:27,334 - Step 10: Decoded token text:  parts
2025-02-11 07:55:27,334 - Step 10: Updated current_phrase
2025-02-11 07:55:27,335 - Step 10: Created step_acts
2025-02-11 07:55:27,335 - Step 10: Added to generation_acts
2025-02-11 07:55:27,336 - Step 10: Updated generated_texts
2025-02-11 07:55:27,336 - Step 10: Updated recent_tokens
2025-02-11 07:55:27,336 - Step 10: Decoded current text
2025-02-11 07:55:27,336 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:27,336 - 
Starting step 11
2025-02-11 07:55:27,336 - Current_ids device: cuda:0
2025-02-11 07:55:27,337 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,358 - Model output complete
2025-02-11 07:55:27,359 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:27,359 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,359 - Next token logits device: cuda:0
2025-02-11 07:55:27,359 - Entered do_sample
2025-02-11 07:55:27,359 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,362 - Probs max: 0.60791015625
2025-02-11 07:55:27,363 - Pre-cat
2025-02-11 07:55:27,363 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,  20524,   1119,   1378,   5479]], device='cuda:0')
2025-02-11 07:55:27,364 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:55:27,365 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:27,365 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,365 - Step 11: Generated next token
2025-02-11 07:55:27,365 - Step 11: Updated current_ids
2025-02-11 07:55:27,365 - Step 11: Decoded token text: .
2025-02-11 07:55:27,365 - Step 11: Updated current_phrase
2025-02-11 07:55:27,365 - Step 11: Created step_acts
2025-02-11 07:55:27,365 - Step 11: Added to generation_acts
2025-02-11 07:55:27,365 - Step 11: Updated recent_tokens
2025-02-11 07:55:27,367 - Step 11: Found phrase end token
2025-02-11 07:55:27,367 - Step 11: Updated recent_phrases
2025-02-11 07:55:27,367 - Step 11: Decoded current text
2025-02-11 07:55:27,367 - Step 11: Reset consecutive_fillers
2025-02-11 07:55:27,367 - 
Starting step 12
2025-02-11 07:55:27,367 - Current_ids device: cuda:0
2025-02-11 07:55:27,367 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,390 - Model output complete
2025-02-11 07:55:27,390 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:27,390 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,390 - Next token logits device: cuda:0
2025-02-11 07:55:27,390 - Entered do_sample
2025-02-11 07:55:27,390 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,394 - Probs max: 0.431640625
2025-02-11 07:55:27,394 - Pre-cat
2025-02-11 07:55:27,394 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,  20524,   1119,   1378,   5479,     13]],
       device='cuda:0')
2025-02-11 07:55:27,396 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:27,397 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:27,397 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,397 - Step 12: Generated next token
2025-02-11 07:55:27,397 - Step 12: Updated current_ids
2025-02-11 07:55:27,397 - Step 12: Decoded token text:  The
2025-02-11 07:55:27,397 - Step 12: Updated current_phrase
2025-02-11 07:55:27,397 - Step 12: Created step_acts
2025-02-11 07:55:27,397 - Step 12: Added to generation_acts
2025-02-11 07:55:27,397 - Step 12: Updated recent_tokens
2025-02-11 07:55:27,399 - Step 12: Decoded current text
2025-02-11 07:55:27,399 - Step 12: Reset consecutive_fillers
2025-02-11 07:55:27,399 - 
Starting step 13
2025-02-11 07:55:27,399 - Current_ids device: cuda:0
2025-02-11 07:55:27,399 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,421 - Model output complete
2025-02-11 07:55:27,421 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:27,421 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,421 - Next token logits device: cuda:0
2025-02-11 07:55:27,421 - Entered do_sample
2025-02-11 07:55:27,421 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,425 - Probs max: 0.476806640625
2025-02-11 07:55:27,426 - Pre-cat
2025-02-11 07:55:27,426 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,  20524,   1119,   1378,   5479,     13,    576]],
       device='cuda:0')
2025-02-11 07:55:27,428 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:27,428 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:27,428 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,428 - Step 13: Generated next token
2025-02-11 07:55:27,428 - Step 13: Updated current_ids
2025-02-11 07:55:27,429 - Step 13: Decoded token text:  wall
2025-02-11 07:55:27,429 - Step 13: Updated current_phrase
2025-02-11 07:55:27,429 - Step 13: Created step_acts
2025-02-11 07:55:27,429 - Step 13: Added to generation_acts
2025-02-11 07:55:27,429 - Step 13: Updated recent_tokens
2025-02-11 07:55:27,430 - Step 13: Decoded current text
2025-02-11 07:55:27,431 - Step 13: Reset consecutive_fillers
2025-02-11 07:55:27,431 - 
Starting step 14
2025-02-11 07:55:27,431 - Current_ids device: cuda:0
2025-02-11 07:55:27,431 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,453 - Model output complete
2025-02-11 07:55:27,454 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:55:27,454 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,454 - Next token logits device: cuda:0
2025-02-11 07:55:27,454 - Entered do_sample
2025-02-11 07:55:27,454 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,456 - Probs max: 0.59619140625
2025-02-11 07:55:27,457 - Pre-cat
2025-02-11 07:55:27,457 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,    323,  20524,   1119,   1378,   5479,     13,    576,   7002]],
       device='cuda:0')
2025-02-11 07:55:27,459 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:27,460 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:55:27,460 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,460 - Step 14: Generated next token
2025-02-11 07:55:27,460 - Step 14: Updated current_ids
2025-02-11 07:55:27,460 - Step 14: Decoded token text:  will
2025-02-11 07:55:27,460 - Step 14: Updated current_phrase
2025-02-11 07:55:27,461 - Step 14: Created step_acts
2025-02-11 07:55:27,461 - Step 14: Added to generation_acts
2025-02-11 07:55:27,461 - Step 14: Updated recent_tokens
2025-02-11 07:55:27,462 - Step 14: Decoded current text
2025-02-11 07:55:27,462 - Step 14: Reset consecutive_fillers
2025-02-11 07:55:27,462 - 
Starting step 15
2025-02-11 07:55:27,462 - Current_ids device: cuda:0
2025-02-11 07:55:27,462 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,490 - Model output complete
2025-02-11 07:55:27,491 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:55:27,491 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,491 - Next token logits device: cuda:0
2025-02-11 07:55:27,491 - Entered do_sample
2025-02-11 07:55:27,491 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,493 - Probs max: 0.1669921875
2025-02-11 07:55:27,598 - 
Starting step 0
2025-02-11 07:55:27,598 - Current_ids device: cuda:0
2025-02-11 07:55:27,598 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,653 - Model output complete
2025-02-11 07:55:27,653 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:27,653 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,653 - Next token logits device: cuda:0
2025-02-11 07:55:27,653 - Entered do_sample
2025-02-11 07:55:27,654 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,656 - Probs max: 0.50341796875
2025-02-11 07:55:27,657 - Pre-cat
2025-02-11 07:55:27,657 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:27,660 - Next token: tensor([[1084]], device='cuda:0')
2025-02-11 07:55:27,660 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:27,660 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,660 - Step 0: Generated next token
2025-02-11 07:55:27,660 - Step 0: Updated current_ids
2025-02-11 07:55:27,660 - Step 0: Decoded token text:  It
2025-02-11 07:55:27,660 - Step 0: Updated current_phrase
2025-02-11 07:55:27,661 - Step 0: Created step_acts
2025-02-11 07:55:27,661 - Step 0: Added to generation_acts
2025-02-11 07:55:27,662 - Step 0: Updated generated_texts
2025-02-11 07:55:27,662 - Step 0: Updated recent_tokens
2025-02-11 07:55:27,663 - Step 0: Decoded current text
2025-02-11 07:55:27,663 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:27,663 - 
Starting step 1
2025-02-11 07:55:27,663 - Current_ids device: cuda:0
2025-02-11 07:55:27,663 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,725 - Model output complete
2025-02-11 07:55:27,725 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:27,726 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,726 - Next token logits device: cuda:0
2025-02-11 07:55:27,726 - Entered do_sample
2025-02-11 07:55:27,726 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,728 - Probs max: 0.9384765625
2025-02-11 07:55:27,729 - Pre-cat
2025-02-11 07:55:27,729 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084]], device='cuda:0')
2025-02-11 07:55:27,731 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:27,732 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:27,732 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,732 - Step 1: Generated next token
2025-02-11 07:55:27,732 - Step 1: Updated current_ids
2025-02-11 07:55:27,732 - Step 1: Decoded token text:  will
2025-02-11 07:55:27,732 - Step 1: Updated current_phrase
2025-02-11 07:55:27,734 - Step 1: Created step_acts
2025-02-11 07:55:27,734 - Step 1: Added to generation_acts
2025-02-11 07:55:27,734 - Step 1: Updated recent_tokens
2025-02-11 07:55:27,735 - Step 1: Decoded current text
2025-02-11 07:55:27,735 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:27,735 - 
Starting step 2
2025-02-11 07:55:27,735 - Current_ids device: cuda:0
2025-02-11 07:55:27,735 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,783 - Model output complete
2025-02-11 07:55:27,783 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:27,783 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,783 - Next token logits device: cuda:0
2025-02-11 07:55:27,783 - Entered do_sample
2025-02-11 07:55:27,784 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,786 - Probs max: 0.2442626953125
2025-02-11 07:55:27,787 - Pre-cat
2025-02-11 07:55:27,787 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686]], device='cuda:0')
2025-02-11 07:55:27,790 - Next token: tensor([[2506]], device='cuda:0')
2025-02-11 07:55:27,791 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:27,791 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,791 - Step 2: Generated next token
2025-02-11 07:55:27,791 - Step 2: Updated current_ids
2025-02-11 07:55:27,791 - Step 2: Decoded token text:  keep
2025-02-11 07:55:27,791 - Step 2: Updated current_phrase
2025-02-11 07:55:27,792 - Step 2: Created step_acts
2025-02-11 07:55:27,792 - Step 2: Added to generation_acts
2025-02-11 07:55:27,792 - Step 2: Updated recent_tokens
2025-02-11 07:55:27,793 - Step 2: Decoded current text
2025-02-11 07:55:27,793 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:27,793 - 
Starting step 3
2025-02-11 07:55:27,793 - Current_ids device: cuda:0
2025-02-11 07:55:27,793 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,820 - Model output complete
2025-02-11 07:55:27,821 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:27,821 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,821 - Next token logits device: cuda:0
2025-02-11 07:55:27,821 - Entered do_sample
2025-02-11 07:55:27,821 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,823 - Probs max: 0.6640625
2025-02-11 07:55:27,824 - Pre-cat
2025-02-11 07:55:27,824 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506]],
       device='cuda:0')
2025-02-11 07:55:27,825 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:27,825 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:27,826 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,826 - Step 3: Generated next token
2025-02-11 07:55:27,826 - Step 3: Updated current_ids
2025-02-11 07:55:27,826 - Step 3: Decoded token text:  the
2025-02-11 07:55:27,826 - Step 3: Updated current_phrase
2025-02-11 07:55:27,826 - Step 3: Created step_acts
2025-02-11 07:55:27,826 - Step 3: Added to generation_acts
2025-02-11 07:55:27,826 - Step 3: Updated recent_tokens
2025-02-11 07:55:27,828 - Step 3: Decoded current text
2025-02-11 07:55:27,828 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:27,828 - 
Starting step 4
2025-02-11 07:55:27,828 - Current_ids device: cuda:0
2025-02-11 07:55:27,828 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,851 - Model output complete
2025-02-11 07:55:27,852 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:27,852 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,852 - Next token logits device: cuda:0
2025-02-11 07:55:27,852 - Entered do_sample
2025-02-11 07:55:27,852 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,854 - Probs max: 0.59814453125
2025-02-11 07:55:27,855 - Pre-cat
2025-02-11 07:55:27,855 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279]],
       device='cuda:0')
2025-02-11 07:55:27,856 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:27,857 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:27,857 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,857 - Step 4: Generated next token
2025-02-11 07:55:27,857 - Step 4: Updated current_ids
2025-02-11 07:55:27,857 - Step 4: Decoded token text:  wall
2025-02-11 07:55:27,857 - Step 4: Updated current_phrase
2025-02-11 07:55:27,857 - Step 4: Created step_acts
2025-02-11 07:55:27,857 - Step 4: Added to generation_acts
2025-02-11 07:55:27,857 - Step 4: Updated recent_tokens
2025-02-11 07:55:27,859 - Step 4: Decoded current text
2025-02-11 07:55:27,859 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:27,859 - 
Starting step 5
2025-02-11 07:55:27,859 - Current_ids device: cuda:0
2025-02-11 07:55:27,859 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,886 - Model output complete
2025-02-11 07:55:27,886 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:27,887 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,887 - Next token logits device: cuda:0
2025-02-11 07:55:27,887 - Entered do_sample
2025-02-11 07:55:27,887 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,889 - Probs max: 0.5810546875
2025-02-11 07:55:27,891 - Pre-cat
2025-02-11 07:55:27,891 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002]],
       device='cuda:0')
2025-02-11 07:55:27,894 - Next token: tensor([[7218]], device='cuda:0')
2025-02-11 07:55:27,894 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:27,894 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,894 - Step 5: Generated next token
2025-02-11 07:55:27,894 - Step 5: Updated current_ids
2025-02-11 07:55:27,895 - Step 5: Decoded token text:  moving
2025-02-11 07:55:27,895 - Step 5: Updated current_phrase
2025-02-11 07:55:27,895 - Step 5: Created step_acts
2025-02-11 07:55:27,895 - Step 5: Added to generation_acts
2025-02-11 07:55:27,897 - Step 5: Updated generated_texts
2025-02-11 07:55:27,897 - Step 5: Updated recent_tokens
2025-02-11 07:55:27,897 - Step 5: Decoded current text
2025-02-11 07:55:27,897 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:27,897 - 
Starting step 6
2025-02-11 07:55:27,897 - Current_ids device: cuda:0
2025-02-11 07:55:27,897 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,931 - Model output complete
2025-02-11 07:55:27,931 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:27,931 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,931 - Next token logits device: cuda:0
2025-02-11 07:55:27,931 - Entered do_sample
2025-02-11 07:55:27,932 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,934 - Probs max: 0.2666015625
2025-02-11 07:55:27,934 - Pre-cat
2025-02-11 07:55:27,934 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218]], device='cuda:0')
2025-02-11 07:55:27,936 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:27,936 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:27,936 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,936 - Step 6: Generated next token
2025-02-11 07:55:27,936 - Step 6: Updated current_ids
2025-02-11 07:55:27,937 - Step 6: Decoded token text: ,
2025-02-11 07:55:27,937 - Step 6: Updated current_phrase
2025-02-11 07:55:27,937 - Step 6: Created step_acts
2025-02-11 07:55:27,937 - Step 6: Added to generation_acts
2025-02-11 07:55:27,937 - Step 6: Updated recent_tokens
2025-02-11 07:55:27,938 - Step 6: Found phrase end token
2025-02-11 07:55:27,938 - Step 6: Updated recent_phrases
2025-02-11 07:55:27,939 - Step 6: Decoded current text
2025-02-11 07:55:27,939 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:27,939 - 
Starting step 7
2025-02-11 07:55:27,939 - Current_ids device: cuda:0
2025-02-11 07:55:27,939 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,964 - Model output complete
2025-02-11 07:55:27,964 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:27,964 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,964 - Next token logits device: cuda:0
2025-02-11 07:55:27,964 - Entered do_sample
2025-02-11 07:55:27,964 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,967 - Probs max: 0.398193359375
2025-02-11 07:55:27,968 - Pre-cat
2025-02-11 07:55:27,968 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11]], device='cuda:0')
2025-02-11 07:55:27,970 - Next token: tensor([[476]], device='cuda:0')
2025-02-11 07:55:27,970 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:27,970 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:27,970 - Step 7: Generated next token
2025-02-11 07:55:27,970 - Step 7: Updated current_ids
2025-02-11 07:55:27,970 - Step 7: Decoded token text:  or
2025-02-11 07:55:27,971 - Step 7: Updated current_phrase
2025-02-11 07:55:27,971 - Step 7: Created step_acts
2025-02-11 07:55:27,971 - Step 7: Added to generation_acts
2025-02-11 07:55:27,971 - Step 7: Updated recent_tokens
2025-02-11 07:55:27,972 - Step 7: Decoded current text
2025-02-11 07:55:27,972 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:27,972 - 
Starting step 8
2025-02-11 07:55:27,973 - Current_ids device: cuda:0
2025-02-11 07:55:27,973 - Current_ids dtype: torch.int64
2025-02-11 07:55:27,995 - Model output complete
2025-02-11 07:55:27,996 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:27,996 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,996 - Next token logits device: cuda:0
2025-02-11 07:55:27,996 - Entered do_sample
2025-02-11 07:55:27,996 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:27,998 - Probs max: 0.474609375
2025-02-11 07:55:27,999 - Pre-cat
2025-02-11 07:55:27,999 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476]], device='cuda:0')
2025-02-11 07:55:28,000 - Next token: tensor([[432]], device='cuda:0')
2025-02-11 07:55:28,001 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:28,001 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,001 - Step 8: Generated next token
2025-02-11 07:55:28,001 - Step 8: Updated current_ids
2025-02-11 07:55:28,001 - Step 8: Decoded token text:  it
2025-02-11 07:55:28,001 - Step 8: Updated current_phrase
2025-02-11 07:55:28,001 - Step 8: Created step_acts
2025-02-11 07:55:28,002 - Step 8: Added to generation_acts
2025-02-11 07:55:28,002 - Step 8: Updated recent_tokens
2025-02-11 07:55:28,003 - Step 8: Decoded current text
2025-02-11 07:55:28,003 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:28,003 - 
Starting step 9
2025-02-11 07:55:28,003 - Current_ids device: cuda:0
2025-02-11 07:55:28,003 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,026 - Model output complete
2025-02-11 07:55:28,026 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:28,026 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,026 - Next token logits device: cuda:0
2025-02-11 07:55:28,026 - Entered do_sample
2025-02-11 07:55:28,027 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,029 - Probs max: 0.98583984375
2025-02-11 07:55:28,030 - Pre-cat
2025-02-11 07:55:28,030 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432]], device='cuda:0')
2025-02-11 07:55:28,032 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:28,032 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:28,032 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,032 - Step 9: Generated next token
2025-02-11 07:55:28,033 - Step 9: Updated current_ids
2025-02-11 07:55:28,033 - Step 9: Decoded token text:  will
2025-02-11 07:55:28,033 - Step 9: Updated current_phrase
2025-02-11 07:55:28,033 - Step 9: Created step_acts
2025-02-11 07:55:28,033 - Step 9: Added to generation_acts
2025-02-11 07:55:28,033 - Step 9: Updated recent_tokens
2025-02-11 07:55:28,035 - Step 9: Decoded current text
2025-02-11 07:55:28,035 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:28,035 - 
Starting step 10
2025-02-11 07:55:28,035 - Current_ids device: cuda:0
2025-02-11 07:55:28,035 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,060 - Model output complete
2025-02-11 07:55:28,060 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:28,060 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,060 - Next token logits device: cuda:0
2025-02-11 07:55:28,060 - Entered do_sample
2025-02-11 07:55:28,060 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,064 - Probs max: 0.322265625
2025-02-11 07:55:28,065 - Pre-cat
2025-02-11 07:55:28,065 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686]], device='cuda:0')
2025-02-11 07:55:28,066 - Next token: tensor([[2287]], device='cuda:0')
2025-02-11 07:55:28,067 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:28,067 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,067 - Step 10: Generated next token
2025-02-11 07:55:28,067 - Step 10: Updated current_ids
2025-02-11 07:55:28,067 - Step 10: Decoded token text:  catch
2025-02-11 07:55:28,067 - Step 10: Updated current_phrase
2025-02-11 07:55:28,067 - Step 10: Created step_acts
2025-02-11 07:55:28,067 - Step 10: Added to generation_acts
2025-02-11 07:55:28,069 - Step 10: Updated generated_texts
2025-02-11 07:55:28,069 - Step 10: Updated recent_tokens
2025-02-11 07:55:28,069 - Step 10: Decoded current text
2025-02-11 07:55:28,069 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:28,069 - 
Starting step 11
2025-02-11 07:55:28,069 - Current_ids device: cuda:0
2025-02-11 07:55:28,069 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,099 - Model output complete
2025-02-11 07:55:28,099 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:28,099 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,099 - Next token logits device: cuda:0
2025-02-11 07:55:28,099 - Entered do_sample
2025-02-11 07:55:28,099 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,102 - Probs max: 0.95263671875
2025-02-11 07:55:28,102 - Pre-cat
2025-02-11 07:55:28,102 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686,   2287]], device='cuda:0')
2025-02-11 07:55:28,104 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:28,104 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:28,104 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,104 - Step 11: Generated next token
2025-02-11 07:55:28,104 - Step 11: Updated current_ids
2025-02-11 07:55:28,104 - Step 11: Decoded token text:  the
2025-02-11 07:55:28,104 - Step 11: Updated current_phrase
2025-02-11 07:55:28,105 - Step 11: Created step_acts
2025-02-11 07:55:28,105 - Step 11: Added to generation_acts
2025-02-11 07:55:28,105 - Step 11: Updated recent_tokens
2025-02-11 07:55:28,106 - Step 11: Decoded current text
2025-02-11 07:55:28,106 - Step 11: Reset consecutive_fillers
2025-02-11 07:55:28,106 - 
Starting step 12
2025-02-11 07:55:28,106 - Current_ids device: cuda:0
2025-02-11 07:55:28,106 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,129 - Model output complete
2025-02-11 07:55:28,129 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:28,129 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,129 - Next token logits device: cuda:0
2025-02-11 07:55:28,130 - Entered do_sample
2025-02-11 07:55:28,130 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,132 - Probs max: 0.99462890625
2025-02-11 07:55:28,133 - Pre-cat
2025-02-11 07:55:28,133 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686,   2287,    279]],
       device='cuda:0')
2025-02-11 07:55:28,134 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:28,135 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:28,135 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,135 - Step 12: Generated next token
2025-02-11 07:55:28,135 - Step 12: Updated current_ids
2025-02-11 07:55:28,135 - Step 12: Decoded token text:  wall
2025-02-11 07:55:28,135 - Step 12: Updated current_phrase
2025-02-11 07:55:28,135 - Step 12: Created step_acts
2025-02-11 07:55:28,135 - Step 12: Added to generation_acts
2025-02-11 07:55:28,136 - Step 12: Updated recent_tokens
2025-02-11 07:55:28,137 - Step 12: Decoded current text
2025-02-11 07:55:28,137 - Step 12: Reset consecutive_fillers
2025-02-11 07:55:28,137 - 
Starting step 13
2025-02-11 07:55:28,137 - Current_ids device: cuda:0
2025-02-11 07:55:28,137 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,160 - Model output complete
2025-02-11 07:55:28,160 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:28,160 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,160 - Next token logits device: cuda:0
2025-02-11 07:55:28,160 - Entered do_sample
2025-02-11 07:55:28,160 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,163 - Probs max: 0.27490234375
2025-02-11 07:55:28,164 - Pre-cat
2025-02-11 07:55:28,164 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686,   2287,    279,   7002]],
       device='cuda:0')
2025-02-11 07:55:28,166 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:55:28,166 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:28,166 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,167 - Step 13: Generated next token
2025-02-11 07:55:28,167 - Step 13: Updated current_ids
2025-02-11 07:55:28,167 - Step 13: Decoded token text:  and
2025-02-11 07:55:28,167 - Step 13: Updated current_phrase
2025-02-11 07:55:28,167 - Step 13: Created step_acts
2025-02-11 07:55:28,167 - Step 13: Added to generation_acts
2025-02-11 07:55:28,167 - Step 13: Updated recent_tokens
2025-02-11 07:55:28,169 - Step 13: Decoded current text
2025-02-11 07:55:28,169 - Step 13: Reset consecutive_fillers
2025-02-11 07:55:28,169 - 
Starting step 14
2025-02-11 07:55:28,169 - Current_ids device: cuda:0
2025-02-11 07:55:28,169 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,191 - Model output complete
2025-02-11 07:55:28,191 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:55:28,191 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,191 - Next token logits device: cuda:0
2025-02-11 07:55:28,191 - Entered do_sample
2025-02-11 07:55:28,191 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,195 - Probs max: 0.261474609375
2025-02-11 07:55:28,195 - Pre-cat
2025-02-11 07:55:28,195 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686,   2287,    279,   7002,    323]],
       device='cuda:0')
2025-02-11 07:55:28,197 - Next token: tensor([[6301]], device='cuda:0')
2025-02-11 07:55:28,197 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:55:28,197 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,197 - Step 14: Generated next token
2025-02-11 07:55:28,197 - Step 14: Updated current_ids
2025-02-11 07:55:28,197 - Step 14: Decoded token text:  slow
2025-02-11 07:55:28,198 - Step 14: Updated current_phrase
2025-02-11 07:55:28,198 - Step 14: Created step_acts
2025-02-11 07:55:28,198 - Step 14: Added to generation_acts
2025-02-11 07:55:28,198 - Step 14: Updated recent_tokens
2025-02-11 07:55:28,199 - Step 14: Decoded current text
2025-02-11 07:55:28,199 - Step 14: Reset consecutive_fillers
2025-02-11 07:55:28,199 - 
Starting step 15
2025-02-11 07:55:28,199 - Current_ids device: cuda:0
2025-02-11 07:55:28,200 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,226 - Model output complete
2025-02-11 07:55:28,226 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:55:28,226 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,226 - Next token logits device: cuda:0
2025-02-11 07:55:28,227 - Entered do_sample
2025-02-11 07:55:28,227 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,229 - Probs max: 0.77880859375
2025-02-11 07:55:28,230 - Pre-cat
2025-02-11 07:55:28,230 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686,   2287,    279,   7002,    323,
           6301]], device='cuda:0')
2025-02-11 07:55:28,232 - Next token: tensor([[1495]], device='cuda:0')
2025-02-11 07:55:28,232 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:55:28,232 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,232 - Step 15: Generated next token
2025-02-11 07:55:28,232 - Step 15: Updated current_ids
2025-02-11 07:55:28,232 - Step 15: Decoded token text:  down
2025-02-11 07:55:28,232 - Step 15: Updated current_phrase
2025-02-11 07:55:28,233 - Step 15: Created step_acts
2025-02-11 07:55:28,233 - Step 15: Added to generation_acts
2025-02-11 07:55:28,234 - Step 15: Updated generated_texts
2025-02-11 07:55:28,234 - Step 15: Updated recent_tokens
2025-02-11 07:55:28,235 - Step 15: Decoded current text
2025-02-11 07:55:28,235 - Step 15: Reset consecutive_fillers
2025-02-11 07:55:28,235 - 
Starting step 16
2025-02-11 07:55:28,235 - Current_ids device: cuda:0
2025-02-11 07:55:28,235 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,292 - Model output complete
2025-02-11 07:55:28,292 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:55:28,293 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,293 - Next token logits device: cuda:0
2025-02-11 07:55:28,293 - Entered do_sample
2025-02-11 07:55:28,293 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,295 - Probs max: 0.560546875
2025-02-11 07:55:28,297 - Pre-cat
2025-02-11 07:55:28,297 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686,   2287,    279,   7002,    323,
           6301,   1495]], device='cuda:0')
2025-02-11 07:55:28,300 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:28,301 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:55:28,301 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,301 - Step 16: Generated next token
2025-02-11 07:55:28,301 - Step 16: Updated current_ids
2025-02-11 07:55:28,301 - Step 16: Decoded token text:  the
2025-02-11 07:55:28,301 - Step 16: Updated current_phrase
2025-02-11 07:55:28,302 - Step 16: Created step_acts
2025-02-11 07:55:28,302 - Step 16: Added to generation_acts
2025-02-11 07:55:28,302 - Step 16: Updated recent_tokens
2025-02-11 07:55:28,304 - Step 16: Decoded current text
2025-02-11 07:55:28,304 - Step 16: Reset consecutive_fillers
2025-02-11 07:55:28,305 - Step 16: Calculated unique_ratio: 0.75
2025-02-11 07:55:28,305 - 
Starting step 17
2025-02-11 07:55:28,305 - Current_ids device: cuda:0
2025-02-11 07:55:28,305 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,332 - Model output complete
2025-02-11 07:55:28,332 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:55:28,332 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,332 - Next token logits device: cuda:0
2025-02-11 07:55:28,332 - Entered do_sample
2025-02-11 07:55:28,332 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,335 - Probs max: 0.99462890625
2025-02-11 07:55:28,336 - Pre-cat
2025-02-11 07:55:28,336 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686,   2287,    279,   7002,    323,
           6301,   1495,    279]], device='cuda:0')
2025-02-11 07:55:28,340 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:28,340 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:55:28,340 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,340 - Step 17: Generated next token
2025-02-11 07:55:28,340 - Step 17: Updated current_ids
2025-02-11 07:55:28,340 - Step 17: Decoded token text:  wall
2025-02-11 07:55:28,340 - Step 17: Updated current_phrase
2025-02-11 07:55:28,341 - Step 17: Created step_acts
2025-02-11 07:55:28,341 - Step 17: Added to generation_acts
2025-02-11 07:55:28,341 - Step 17: Updated recent_tokens
2025-02-11 07:55:28,342 - Step 17: Decoded current text
2025-02-11 07:55:28,342 - Step 17: Reset consecutive_fillers
2025-02-11 07:55:28,343 - Step 17: Calculated unique_ratio: 0.75
2025-02-11 07:55:28,343 - 
Starting step 18
2025-02-11 07:55:28,343 - Current_ids device: cuda:0
2025-02-11 07:55:28,343 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,368 - Model output complete
2025-02-11 07:55:28,368 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:55:28,368 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,368 - Next token logits device: cuda:0
2025-02-11 07:55:28,368 - Entered do_sample
2025-02-11 07:55:28,368 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,370 - Probs max: 0.65478515625
2025-02-11 07:55:28,371 - Pre-cat
2025-02-11 07:55:28,371 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686,   2287,    279,   7002,    323,
           6301,   1495,    279,   7002]], device='cuda:0')
2025-02-11 07:55:28,373 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:55:28,373 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:55:28,373 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,373 - Step 18: Generated next token
2025-02-11 07:55:28,373 - Step 18: Updated current_ids
2025-02-11 07:55:28,373 - Step 18: Decoded token text: .
2025-02-11 07:55:28,373 - Step 18: Updated current_phrase
2025-02-11 07:55:28,374 - Step 18: Created step_acts
2025-02-11 07:55:28,374 - Step 18: Added to generation_acts
2025-02-11 07:55:28,374 - Step 18: Updated recent_tokens
2025-02-11 07:55:28,375 - Step 18: Found phrase end token
2025-02-11 07:55:28,375 - Step 18: Updated recent_phrases
2025-02-11 07:55:28,375 - Step 18: Calculated similarity: 0.6666666666666666
2025-02-11 07:55:28,376 - Step 18: Decoded current text
2025-02-11 07:55:28,376 - Step 18: Reset consecutive_fillers
2025-02-11 07:55:28,376 - Step 18: Calculated unique_ratio: 0.75
2025-02-11 07:55:28,376 - 
Starting step 19
2025-02-11 07:55:28,376 - Current_ids device: cuda:0
2025-02-11 07:55:28,376 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,408 - Model output complete
2025-02-11 07:55:28,408 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:55:28,408 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,408 - Next token logits device: cuda:0
2025-02-11 07:55:28,408 - Entered do_sample
2025-02-11 07:55:28,408 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,410 - Probs max: 0.430908203125
2025-02-11 07:55:28,412 - Pre-cat
2025-02-11 07:55:28,412 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686,   2287,    279,   7002,    323,
           6301,   1495,    279,   7002,     13]], device='cuda:0')
2025-02-11 07:55:28,417 - Next token: tensor([[3085]], device='cuda:0')
2025-02-11 07:55:28,417 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:55:28,417 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,417 - Step 19: Generated next token
2025-02-11 07:55:28,417 - Step 19: Updated current_ids
2025-02-11 07:55:28,417 - Step 19: Decoded token text:  With
2025-02-11 07:55:28,417 - Step 19: Updated current_phrase
2025-02-11 07:55:28,418 - Step 19: Created step_acts
2025-02-11 07:55:28,418 - Step 19: Added to generation_acts
2025-02-11 07:55:28,418 - Step 19: Updated recent_tokens
2025-02-11 07:55:28,420 - Step 19: Decoded current text
2025-02-11 07:55:28,420 - Step 19: Reset consecutive_fillers
2025-02-11 07:55:28,420 - Step 19: Calculated unique_ratio: 0.8125
2025-02-11 07:55:28,420 - 
Starting step 20
2025-02-11 07:55:28,420 - Current_ids device: cuda:0
2025-02-11 07:55:28,420 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,459 - Model output complete
2025-02-11 07:55:28,459 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:55:28,459 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,459 - Next token logits device: cuda:0
2025-02-11 07:55:28,459 - Entered do_sample
2025-02-11 07:55:28,460 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,463 - Probs max: 0.6806640625
2025-02-11 07:55:28,463 - Pre-cat
2025-02-11 07:55:28,463 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686,   2287,    279,   7002,    323,
           6301,   1495,    279,   7002,     13,   3085]], device='cuda:0')
2025-02-11 07:55:28,465 - Next token: tensor([[419]], device='cuda:0')
2025-02-11 07:55:28,466 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:55:28,466 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,466 - Step 20: Generated next token
2025-02-11 07:55:28,466 - Step 20: Updated current_ids
2025-02-11 07:55:28,466 - Step 20: Decoded token text:  this
2025-02-11 07:55:28,466 - Step 20: Updated current_phrase
2025-02-11 07:55:28,466 - Step 20: Created step_acts
2025-02-11 07:55:28,467 - Step 20: Added to generation_acts
2025-02-11 07:55:28,468 - Step 20: Updated generated_texts
2025-02-11 07:55:28,468 - Step 20: Updated recent_tokens
2025-02-11 07:55:28,468 - Step 20: Decoded current text
2025-02-11 07:55:28,468 - Step 20: Reset consecutive_fillers
2025-02-11 07:55:28,468 - Step 20: Calculated unique_ratio: 0.875
2025-02-11 07:55:28,469 - 
Starting step 21
2025-02-11 07:55:28,469 - Current_ids device: cuda:0
2025-02-11 07:55:28,469 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,530 - Model output complete
2025-02-11 07:55:28,530 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:55:28,530 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,531 - Next token logits device: cuda:0
2025-02-11 07:55:28,531 - Entered do_sample
2025-02-11 07:55:28,531 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,533 - Probs max: 0.54345703125
2025-02-11 07:55:28,534 - Pre-cat
2025-02-11 07:55:28,534 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686,   2287,    279,   7002,    323,
           6301,   1495,    279,   7002,     13,   3085,    419]],
       device='cuda:0')
2025-02-11 07:55:28,537 - Next token: tensor([[362]], device='cuda:0')
2025-02-11 07:55:28,537 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:55:28,537 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,537 - Step 21: Generated next token
2025-02-11 07:55:28,538 - Step 21: Updated current_ids
2025-02-11 07:55:28,538 - Step 21: Decoded token text:  A
2025-02-11 07:55:28,538 - Step 21: Updated current_phrase
2025-02-11 07:55:28,538 - Step 21: Created step_acts
2025-02-11 07:55:28,538 - Step 21: Added to generation_acts
2025-02-11 07:55:28,538 - Step 21: Updated recent_tokens
2025-02-11 07:55:28,540 - Step 21: Decoded current text
2025-02-11 07:55:28,540 - Step 21: Incremented consecutive_fillers to 1
2025-02-11 07:55:28,540 - Step 21: Calculated unique_ratio: 0.875
2025-02-11 07:55:28,540 - 
Starting step 22
2025-02-11 07:55:28,540 - Current_ids device: cuda:0
2025-02-11 07:55:28,540 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,563 - Model output complete
2025-02-11 07:55:28,563 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:55:28,564 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,564 - Next token logits device: cuda:0
2025-02-11 07:55:28,564 - Entered do_sample
2025-02-11 07:55:28,564 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,567 - Probs max: 0.90966796875
2025-02-11 07:55:28,567 - Pre-cat
2025-02-11 07:55:28,567 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686,   2287,    279,   7002,    323,
           6301,   1495,    279,   7002,     13,   3085,    419,    362]],
       device='cuda:0')
2025-02-11 07:55:28,569 - Next token: tensor([[25]], device='cuda:0')
2025-02-11 07:55:28,570 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:55:28,570 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,570 - Step 22: Generated next token
2025-02-11 07:55:28,570 - Step 22: Updated current_ids
2025-02-11 07:55:28,570 - Step 22: Decoded token text: :
2025-02-11 07:55:28,570 - Step 22: Updated current_phrase
2025-02-11 07:55:28,570 - Step 22: Created step_acts
2025-02-11 07:55:28,570 - Step 22: Added to generation_acts
2025-02-11 07:55:28,570 - Step 22: Updated recent_tokens
2025-02-11 07:55:28,572 - Step 22: Found phrase end token
2025-02-11 07:55:28,572 - Step 22: Updated recent_phrases
2025-02-11 07:55:28,572 - Step 22: Calculated similarity: 0.0
2025-02-11 07:55:28,572 - Step 22: Decoded current text
2025-02-11 07:55:28,572 - Step 22: Incremented consecutive_fillers to 2
2025-02-11 07:55:28,572 - Step 22: Calculated unique_ratio: 0.875
2025-02-11 07:55:28,572 - 
Starting step 23
2025-02-11 07:55:28,572 - Current_ids device: cuda:0
2025-02-11 07:55:28,572 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,595 - Model output complete
2025-02-11 07:55:28,595 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:55:28,595 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,595 - Next token logits device: cuda:0
2025-02-11 07:55:28,595 - Entered do_sample
2025-02-11 07:55:28,595 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,599 - Probs max: 0.22216796875
2025-02-11 07:55:28,600 - Pre-cat
2025-02-11 07:55:28,600 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1084,    686,   2506,    279,   7002,
           7218,     11,    476,    432,    686,   2287,    279,   7002,    323,
           6301,   1495,    279,   7002,     13,   3085,    419,    362,     25]],
       device='cuda:0')
2025-02-11 07:55:28,603 - Next token: tensor([[498]], device='cuda:0')
2025-02-11 07:55:28,604 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:55:28,604 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,604 - Step 23: Generated next token
2025-02-11 07:55:28,604 - Step 23: Updated current_ids
2025-02-11 07:55:28,605 - Step 23: Decoded token text:  you
2025-02-11 07:55:28,605 - Step 23: Updated current_phrase
2025-02-11 07:55:28,606 - Step 23: Created step_acts
2025-02-11 07:55:28,606 - Step 23: Added to generation_acts
2025-02-11 07:55:28,606 - Step 23: Updated recent_tokens
2025-02-11 07:55:28,607 - Step 23: Decoded current text
2025-02-11 07:55:28,608 - Step 23: Incremented consecutive_fillers to 3
2025-02-11 07:55:28,712 - 
Starting step 0
2025-02-11 07:55:28,712 - Current_ids device: cuda:0
2025-02-11 07:55:28,712 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,741 - Model output complete
2025-02-11 07:55:28,742 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:28,742 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,742 - Next token logits device: cuda:0
2025-02-11 07:55:28,742 - Entered do_sample
2025-02-11 07:55:28,742 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,744 - Probs max: 0.50341796875
2025-02-11 07:55:28,746 - Pre-cat
2025-02-11 07:55:28,746 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:28,749 - Next token: tensor([[1416]], device='cuda:0')
2025-02-11 07:55:28,750 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:28,750 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,750 - Step 0: Generated next token
2025-02-11 07:55:28,750 - Step 0: Updated current_ids
2025-02-11 07:55:28,751 - Step 0: Decoded token text:  If
2025-02-11 07:55:28,751 - Step 0: Updated current_phrase
2025-02-11 07:55:28,751 - Step 0: Created step_acts
2025-02-11 07:55:28,752 - Step 0: Added to generation_acts
2025-02-11 07:55:28,753 - Step 0: Updated generated_texts
2025-02-11 07:55:28,753 - Step 0: Updated recent_tokens
2025-02-11 07:55:28,753 - Step 0: Decoded current text
2025-02-11 07:55:28,753 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:28,753 - 
Starting step 1
2025-02-11 07:55:28,754 - Current_ids device: cuda:0
2025-02-11 07:55:28,754 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,778 - Model output complete
2025-02-11 07:55:28,779 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:28,779 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,779 - Next token logits device: cuda:0
2025-02-11 07:55:28,779 - Entered do_sample
2025-02-11 07:55:28,779 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,781 - Probs max: 0.7099609375
2025-02-11 07:55:28,782 - Pre-cat
2025-02-11 07:55:28,782 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416]], device='cuda:0')
2025-02-11 07:55:28,783 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:28,783 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:28,783 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,783 - Step 1: Generated next token
2025-02-11 07:55:28,784 - Step 1: Updated current_ids
2025-02-11 07:55:28,784 - Step 1: Decoded token text:  the
2025-02-11 07:55:28,784 - Step 1: Updated current_phrase
2025-02-11 07:55:28,784 - Step 1: Created step_acts
2025-02-11 07:55:28,784 - Step 1: Added to generation_acts
2025-02-11 07:55:28,784 - Step 1: Updated recent_tokens
2025-02-11 07:55:28,785 - Step 1: Decoded current text
2025-02-11 07:55:28,785 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:28,786 - 
Starting step 2
2025-02-11 07:55:28,786 - Current_ids device: cuda:0
2025-02-11 07:55:28,786 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,808 - Model output complete
2025-02-11 07:55:28,808 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:28,808 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,808 - Next token logits device: cuda:0
2025-02-11 07:55:28,809 - Entered do_sample
2025-02-11 07:55:28,809 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,811 - Probs max: 0.76123046875
2025-02-11 07:55:28,812 - Pre-cat
2025-02-11 07:55:28,812 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279]], device='cuda:0')
2025-02-11 07:55:28,813 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:28,814 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:28,814 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,814 - Step 2: Generated next token
2025-02-11 07:55:28,814 - Step 2: Updated current_ids
2025-02-11 07:55:28,814 - Step 2: Decoded token text:  ball
2025-02-11 07:55:28,814 - Step 2: Updated current_phrase
2025-02-11 07:55:28,814 - Step 2: Created step_acts
2025-02-11 07:55:28,814 - Step 2: Added to generation_acts
2025-02-11 07:55:28,814 - Step 2: Updated recent_tokens
2025-02-11 07:55:28,816 - Step 2: Decoded current text
2025-02-11 07:55:28,816 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:28,816 - 
Starting step 3
2025-02-11 07:55:28,816 - Current_ids device: cuda:0
2025-02-11 07:55:28,816 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,838 - Model output complete
2025-02-11 07:55:28,838 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:28,838 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,838 - Next token logits device: cuda:0
2025-02-11 07:55:28,838 - Entered do_sample
2025-02-11 07:55:28,838 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,842 - Probs max: 0.9921875
2025-02-11 07:55:28,843 - Pre-cat
2025-02-11 07:55:28,843 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935]],
       device='cuda:0')
2025-02-11 07:55:28,844 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:28,844 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:28,845 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,845 - Step 3: Generated next token
2025-02-11 07:55:28,845 - Step 3: Updated current_ids
2025-02-11 07:55:28,845 - Step 3: Decoded token text:  is
2025-02-11 07:55:28,845 - Step 3: Updated current_phrase
2025-02-11 07:55:28,845 - Step 3: Created step_acts
2025-02-11 07:55:28,845 - Step 3: Added to generation_acts
2025-02-11 07:55:28,845 - Step 3: Updated recent_tokens
2025-02-11 07:55:28,847 - Step 3: Decoded current text
2025-02-11 07:55:28,847 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:28,847 - 
Starting step 4
2025-02-11 07:55:28,847 - Current_ids device: cuda:0
2025-02-11 07:55:28,847 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,870 - Model output complete
2025-02-11 07:55:28,870 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:28,870 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,870 - Next token logits device: cuda:0
2025-02-11 07:55:28,870 - Entered do_sample
2025-02-11 07:55:28,870 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,872 - Probs max: 0.94921875
2025-02-11 07:55:28,873 - Pre-cat
2025-02-11 07:55:28,873 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:28,875 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:28,875 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:28,875 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,875 - Step 4: Generated next token
2025-02-11 07:55:28,875 - Step 4: Updated current_ids
2025-02-11 07:55:28,875 - Step 4: Decoded token text:  thrown
2025-02-11 07:55:28,875 - Step 4: Updated current_phrase
2025-02-11 07:55:28,876 - Step 4: Created step_acts
2025-02-11 07:55:28,876 - Step 4: Added to generation_acts
2025-02-11 07:55:28,876 - Step 4: Updated recent_tokens
2025-02-11 07:55:28,877 - Step 4: Decoded current text
2025-02-11 07:55:28,877 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:28,877 - 
Starting step 5
2025-02-11 07:55:28,877 - Current_ids device: cuda:0
2025-02-11 07:55:28,877 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,901 - Model output complete
2025-02-11 07:55:28,901 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:28,901 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,901 - Next token logits device: cuda:0
2025-02-11 07:55:28,901 - Entered do_sample
2025-02-11 07:55:28,901 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,904 - Probs max: 0.7861328125
2025-02-11 07:55:28,904 - Pre-cat
2025-02-11 07:55:28,904 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:55:28,906 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:28,906 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:28,906 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,906 - Step 5: Generated next token
2025-02-11 07:55:28,907 - Step 5: Updated current_ids
2025-02-11 07:55:28,907 - Step 5: Decoded token text:  very
2025-02-11 07:55:28,907 - Step 5: Updated current_phrase
2025-02-11 07:55:28,907 - Step 5: Created step_acts
2025-02-11 07:55:28,907 - Step 5: Added to generation_acts
2025-02-11 07:55:28,908 - Step 5: Updated generated_texts
2025-02-11 07:55:28,909 - Step 5: Updated recent_tokens
2025-02-11 07:55:28,909 - Step 5: Decoded current text
2025-02-11 07:55:28,909 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:28,909 - 
Starting step 6
2025-02-11 07:55:28,909 - Current_ids device: cuda:0
2025-02-11 07:55:28,909 - Current_ids dtype: torch.int64
2025-02-11 07:55:28,974 - Model output complete
2025-02-11 07:55:28,974 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:28,974 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,974 - Next token logits device: cuda:0
2025-02-11 07:55:28,974 - Entered do_sample
2025-02-11 07:55:28,974 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:28,976 - Probs max: 0.98388671875
2025-02-11 07:55:28,977 - Pre-cat
2025-02-11 07:55:28,977 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602]], device='cuda:0')
2025-02-11 07:55:28,978 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:55:28,979 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:28,979 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:28,979 - Step 6: Generated next token
2025-02-11 07:55:28,979 - Step 6: Updated current_ids
2025-02-11 07:55:28,979 - Step 6: Decoded token text:  fast
2025-02-11 07:55:28,979 - Step 6: Updated current_phrase
2025-02-11 07:55:28,979 - Step 6: Created step_acts
2025-02-11 07:55:28,979 - Step 6: Added to generation_acts
2025-02-11 07:55:28,979 - Step 6: Updated recent_tokens
2025-02-11 07:55:28,981 - Step 6: Decoded current text
2025-02-11 07:55:28,981 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:28,981 - 
Starting step 7
2025-02-11 07:55:28,981 - Current_ids device: cuda:0
2025-02-11 07:55:28,981 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,003 - Model output complete
2025-02-11 07:55:29,003 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:29,003 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,003 - Next token logits device: cuda:0
2025-02-11 07:55:29,003 - Entered do_sample
2025-02-11 07:55:29,003 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,007 - Probs max: 0.97607421875
2025-02-11 07:55:29,007 - Pre-cat
2025-02-11 07:55:29,008 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937]], device='cuda:0')
2025-02-11 07:55:29,010 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:29,010 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:29,010 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,010 - Step 7: Generated next token
2025-02-11 07:55:29,010 - Step 7: Updated current_ids
2025-02-11 07:55:29,010 - Step 7: Decoded token text: ,
2025-02-11 07:55:29,010 - Step 7: Updated current_phrase
2025-02-11 07:55:29,010 - Step 7: Created step_acts
2025-02-11 07:55:29,011 - Step 7: Added to generation_acts
2025-02-11 07:55:29,011 - Step 7: Updated recent_tokens
2025-02-11 07:55:29,012 - Step 7: Found phrase end token
2025-02-11 07:55:29,012 - Step 7: Updated recent_phrases
2025-02-11 07:55:29,012 - Step 7: Decoded current text
2025-02-11 07:55:29,012 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:29,012 - 
Starting step 8
2025-02-11 07:55:29,012 - Current_ids device: cuda:0
2025-02-11 07:55:29,012 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,034 - Model output complete
2025-02-11 07:55:29,034 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:29,034 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,034 - Next token logits device: cuda:0
2025-02-11 07:55:29,034 - Entered do_sample
2025-02-11 07:55:29,034 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,038 - Probs max: 0.57958984375
2025-02-11 07:55:29,038 - Pre-cat
2025-02-11 07:55:29,038 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11]], device='cuda:0')
2025-02-11 07:55:29,040 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:29,041 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:29,041 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,041 - Step 8: Generated next token
2025-02-11 07:55:29,041 - Step 8: Updated current_ids
2025-02-11 07:55:29,042 - Step 8: Decoded token text:  the
2025-02-11 07:55:29,042 - Step 8: Updated current_phrase
2025-02-11 07:55:29,042 - Step 8: Created step_acts
2025-02-11 07:55:29,042 - Step 8: Added to generation_acts
2025-02-11 07:55:29,043 - Step 8: Updated recent_tokens
2025-02-11 07:55:29,044 - Step 8: Decoded current text
2025-02-11 07:55:29,044 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:29,044 - 
Starting step 9
2025-02-11 07:55:29,044 - Current_ids device: cuda:0
2025-02-11 07:55:29,044 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,071 - Model output complete
2025-02-11 07:55:29,071 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:29,071 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,072 - Next token logits device: cuda:0
2025-02-11 07:55:29,072 - Entered do_sample
2025-02-11 07:55:29,072 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,074 - Probs max: 0.603515625
2025-02-11 07:55:29,075 - Pre-cat
2025-02-11 07:55:29,075 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279]], device='cuda:0')
2025-02-11 07:55:29,077 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:29,078 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:29,078 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,078 - Step 9: Generated next token
2025-02-11 07:55:29,078 - Step 9: Updated current_ids
2025-02-11 07:55:29,078 - Step 9: Decoded token text:  wall
2025-02-11 07:55:29,078 - Step 9: Updated current_phrase
2025-02-11 07:55:29,079 - Step 9: Created step_acts
2025-02-11 07:55:29,079 - Step 9: Added to generation_acts
2025-02-11 07:55:29,079 - Step 9: Updated recent_tokens
2025-02-11 07:55:29,080 - Step 9: Decoded current text
2025-02-11 07:55:29,080 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:29,080 - 
Starting step 10
2025-02-11 07:55:29,080 - Current_ids device: cuda:0
2025-02-11 07:55:29,080 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,104 - Model output complete
2025-02-11 07:55:29,104 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:29,104 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,104 - Next token logits device: cuda:0
2025-02-11 07:55:29,105 - Entered do_sample
2025-02-11 07:55:29,105 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,107 - Probs max: 0.716796875
2025-02-11 07:55:29,108 - Pre-cat
2025-02-11 07:55:29,108 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002]], device='cuda:0')
2025-02-11 07:55:29,109 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:29,110 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:29,110 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,110 - Step 10: Generated next token
2025-02-11 07:55:29,110 - Step 10: Updated current_ids
2025-02-11 07:55:29,110 - Step 10: Decoded token text:  will
2025-02-11 07:55:29,110 - Step 10: Updated current_phrase
2025-02-11 07:55:29,110 - Step 10: Created step_acts
2025-02-11 07:55:29,110 - Step 10: Added to generation_acts
2025-02-11 07:55:29,112 - Step 10: Updated generated_texts
2025-02-11 07:55:29,112 - Step 10: Updated recent_tokens
2025-02-11 07:55:29,112 - Step 10: Decoded current text
2025-02-11 07:55:29,112 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:29,112 - 
Starting step 11
2025-02-11 07:55:29,112 - Current_ids device: cuda:0
2025-02-11 07:55:29,112 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,135 - Model output complete
2025-02-11 07:55:29,135 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:29,135 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,135 - Next token logits device: cuda:0
2025-02-11 07:55:29,135 - Entered do_sample
2025-02-11 07:55:29,135 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,139 - Probs max: 0.2247314453125
2025-02-11 07:55:29,139 - Pre-cat
2025-02-11 07:55:29,139 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686]], device='cuda:0')
2025-02-11 07:55:29,141 - Next token: tensor([[387]], device='cuda:0')
2025-02-11 07:55:29,141 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:29,141 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,141 - Step 11: Generated next token
2025-02-11 07:55:29,141 - Step 11: Updated current_ids
2025-02-11 07:55:29,142 - Step 11: Decoded token text:  be
2025-02-11 07:55:29,142 - Step 11: Updated current_phrase
2025-02-11 07:55:29,142 - Step 11: Created step_acts
2025-02-11 07:55:29,142 - Step 11: Added to generation_acts
2025-02-11 07:55:29,142 - Step 11: Updated recent_tokens
2025-02-11 07:55:29,143 - Step 11: Decoded current text
2025-02-11 07:55:29,144 - Step 11: Reset consecutive_fillers
2025-02-11 07:55:29,144 - 
Starting step 12
2025-02-11 07:55:29,144 - Current_ids device: cuda:0
2025-02-11 07:55:29,144 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,165 - Model output complete
2025-02-11 07:55:29,166 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:29,166 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,166 - Next token logits device: cuda:0
2025-02-11 07:55:29,166 - Entered do_sample
2025-02-11 07:55:29,166 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,169 - Probs max: 0.6279296875
2025-02-11 07:55:29,170 - Pre-cat
2025-02-11 07:55:29,170 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387]],
       device='cuda:0')
2025-02-11 07:55:29,171 - Next token: tensor([[1112]], device='cuda:0')
2025-02-11 07:55:29,172 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:29,172 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,172 - Step 12: Generated next token
2025-02-11 07:55:29,172 - Step 12: Updated current_ids
2025-02-11 07:55:29,172 - Step 12: Decoded token text: ...
2025-02-11 07:55:29,172 - Step 12: Updated current_phrase
2025-02-11 07:55:29,172 - Step 12: Created step_acts
2025-02-11 07:55:29,172 - Step 12: Added to generation_acts
2025-02-11 07:55:29,172 - Step 12: Updated recent_tokens
2025-02-11 07:55:29,174 - Step 12: Found phrase end token
2025-02-11 07:55:29,174 - Step 12: Updated recent_phrases
2025-02-11 07:55:29,174 - Step 12: Calculated similarity: 0.25
2025-02-11 07:55:29,174 - Step 12: Decoded current text
2025-02-11 07:55:29,174 - Step 12: Reset consecutive_fillers
2025-02-11 07:55:29,174 - 
Starting step 13
2025-02-11 07:55:29,174 - Current_ids device: cuda:0
2025-02-11 07:55:29,174 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,197 - Model output complete
2025-02-11 07:55:29,197 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:29,197 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,197 - Next token logits device: cuda:0
2025-02-11 07:55:29,197 - Entered do_sample
2025-02-11 07:55:29,197 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,200 - Probs max: 0.2156982421875
2025-02-11 07:55:29,201 - Pre-cat
2025-02-11 07:55:29,201 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112]],
       device='cuda:0')
2025-02-11 07:55:29,203 - Next token: tensor([[362]], device='cuda:0')
2025-02-11 07:55:29,203 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:29,203 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,203 - Step 13: Generated next token
2025-02-11 07:55:29,203 - Step 13: Updated current_ids
2025-02-11 07:55:29,204 - Step 13: Decoded token text:  A
2025-02-11 07:55:29,204 - Step 13: Updated current_phrase
2025-02-11 07:55:29,204 - Step 13: Created step_acts
2025-02-11 07:55:29,204 - Step 13: Added to generation_acts
2025-02-11 07:55:29,204 - Step 13: Updated recent_tokens
2025-02-11 07:55:29,205 - Step 13: Decoded current text
2025-02-11 07:55:29,205 - Step 13: Reset consecutive_fillers
2025-02-11 07:55:29,206 - 
Starting step 14
2025-02-11 07:55:29,206 - Current_ids device: cuda:0
2025-02-11 07:55:29,206 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,229 - Model output complete
2025-02-11 07:55:29,229 - Logits shape: torch.Size([1, 45, 151936])
2025-02-11 07:55:29,229 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,229 - Next token logits device: cuda:0
2025-02-11 07:55:29,229 - Entered do_sample
2025-02-11 07:55:29,229 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,233 - Probs max: 0.791015625
2025-02-11 07:55:29,233 - Pre-cat
2025-02-11 07:55:29,234 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362]],
       device='cuda:0')
2025-02-11 07:55:29,235 - Next token: tensor([[8]], device='cuda:0')
2025-02-11 07:55:29,236 - Current_ids shape: torch.Size([1, 45])
2025-02-11 07:55:29,236 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,236 - Step 14: Generated next token
2025-02-11 07:55:29,236 - Step 14: Updated current_ids
2025-02-11 07:55:29,236 - Step 14: Decoded token text: )
2025-02-11 07:55:29,236 - Step 14: Updated current_phrase
2025-02-11 07:55:29,236 - Step 14: Created step_acts
2025-02-11 07:55:29,237 - Step 14: Added to generation_acts
2025-02-11 07:55:29,237 - Step 14: Updated recent_tokens
2025-02-11 07:55:29,238 - Step 14: Decoded current text
2025-02-11 07:55:29,238 - Step 14: Reset consecutive_fillers
2025-02-11 07:55:29,238 - 
Starting step 15
2025-02-11 07:55:29,238 - Current_ids device: cuda:0
2025-02-11 07:55:29,238 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,261 - Model output complete
2025-02-11 07:55:29,261 - Logits shape: torch.Size([1, 46, 151936])
2025-02-11 07:55:29,261 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,261 - Next token logits device: cuda:0
2025-02-11 07:55:29,261 - Entered do_sample
2025-02-11 07:55:29,261 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,264 - Probs max: 0.31298828125
2025-02-11 07:55:29,264 - Pre-cat
2025-02-11 07:55:29,264 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8]], device='cuda:0')
2025-02-11 07:55:29,267 - Next token: tensor([[12236]], device='cuda:0')
2025-02-11 07:55:29,267 - Current_ids shape: torch.Size([1, 46])
2025-02-11 07:55:29,267 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,267 - Step 15: Generated next token
2025-02-11 07:55:29,267 - Step 15: Updated current_ids
2025-02-11 07:55:29,267 - Step 15: Decoded token text:  ...

2025-02-11 07:55:29,267 - Step 15: Updated current_phrase
2025-02-11 07:55:29,268 - Step 15: Created step_acts
2025-02-11 07:55:29,268 - Step 15: Added to generation_acts
2025-02-11 07:55:29,269 - Step 15: Updated generated_texts
2025-02-11 07:55:29,269 - Step 15: Updated recent_tokens
2025-02-11 07:55:29,269 - Step 15: Decoded current text
2025-02-11 07:55:29,269 - Step 15: Reset consecutive_fillers
2025-02-11 07:55:29,270 - 
Starting step 16
2025-02-11 07:55:29,270 - Current_ids device: cuda:0
2025-02-11 07:55:29,270 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,293 - Model output complete
2025-02-11 07:55:29,294 - Logits shape: torch.Size([1, 47, 151936])
2025-02-11 07:55:29,294 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,294 - Next token logits device: cuda:0
2025-02-11 07:55:29,294 - Entered do_sample
2025-02-11 07:55:29,294 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,296 - Probs max: 0.5888671875
2025-02-11 07:55:29,297 - Pre-cat
2025-02-11 07:55:29,297 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236]], device='cuda:0')
2025-02-11 07:55:29,299 - Next token: tensor([[785]], device='cuda:0')
2025-02-11 07:55:29,299 - Current_ids shape: torch.Size([1, 47])
2025-02-11 07:55:29,299 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,299 - Step 16: Generated next token
2025-02-11 07:55:29,299 - Step 16: Updated current_ids
2025-02-11 07:55:29,299 - Step 16: Decoded token text: The
2025-02-11 07:55:29,299 - Step 16: Updated current_phrase
2025-02-11 07:55:29,300 - Step 16: Created step_acts
2025-02-11 07:55:29,300 - Step 16: Added to generation_acts
2025-02-11 07:55:29,300 - Step 16: Updated recent_tokens
2025-02-11 07:55:29,301 - Step 16: Decoded current text
2025-02-11 07:55:29,301 - Step 16: Reset consecutive_fillers
2025-02-11 07:55:29,302 - Step 16: Calculated unique_ratio: 0.9375
2025-02-11 07:55:29,302 - 
Starting step 17
2025-02-11 07:55:29,302 - Current_ids device: cuda:0
2025-02-11 07:55:29,302 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,326 - Model output complete
2025-02-11 07:55:29,326 - Logits shape: torch.Size([1, 48, 151936])
2025-02-11 07:55:29,326 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,326 - Next token logits device: cuda:0
2025-02-11 07:55:29,326 - Entered do_sample
2025-02-11 07:55:29,327 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,329 - Probs max: 0.6875
2025-02-11 07:55:29,331 - Pre-cat
2025-02-11 07:55:29,332 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785]], device='cuda:0')
2025-02-11 07:55:29,335 - Next token: tensor([[2606]], device='cuda:0')
2025-02-11 07:55:29,336 - Current_ids shape: torch.Size([1, 48])
2025-02-11 07:55:29,336 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,336 - Step 17: Generated next token
2025-02-11 07:55:29,336 - Step 17: Updated current_ids
2025-02-11 07:55:29,336 - Step 17: Decoded token text:  options
2025-02-11 07:55:29,336 - Step 17: Updated current_phrase
2025-02-11 07:55:29,337 - Step 17: Created step_acts
2025-02-11 07:55:29,337 - Step 17: Added to generation_acts
2025-02-11 07:55:29,337 - Step 17: Updated recent_tokens
2025-02-11 07:55:29,338 - Step 17: Decoded current text
2025-02-11 07:55:29,338 - Step 17: Reset consecutive_fillers
2025-02-11 07:55:29,338 - Step 17: Calculated unique_ratio: 1.0
2025-02-11 07:55:29,338 - 
Starting step 18
2025-02-11 07:55:29,338 - Current_ids device: cuda:0
2025-02-11 07:55:29,338 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,360 - Model output complete
2025-02-11 07:55:29,360 - Logits shape: torch.Size([1, 49, 151936])
2025-02-11 07:55:29,360 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,360 - Next token logits device: cuda:0
2025-02-11 07:55:29,360 - Entered do_sample
2025-02-11 07:55:29,361 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,364 - Probs max: 0.83544921875
2025-02-11 07:55:29,365 - Pre-cat
2025-02-11 07:55:29,365 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606]], device='cuda:0')
2025-02-11 07:55:29,367 - Next token: tensor([[525]], device='cuda:0')
2025-02-11 07:55:29,367 - Current_ids shape: torch.Size([1, 49])
2025-02-11 07:55:29,367 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,367 - Step 18: Generated next token
2025-02-11 07:55:29,367 - Step 18: Updated current_ids
2025-02-11 07:55:29,367 - Step 18: Decoded token text:  are
2025-02-11 07:55:29,367 - Step 18: Updated current_phrase
2025-02-11 07:55:29,368 - Step 18: Created step_acts
2025-02-11 07:55:29,368 - Step 18: Added to generation_acts
2025-02-11 07:55:29,368 - Step 18: Updated recent_tokens
2025-02-11 07:55:29,369 - Step 18: Decoded current text
2025-02-11 07:55:29,369 - Step 18: Reset consecutive_fillers
2025-02-11 07:55:29,369 - Step 18: Calculated unique_ratio: 1.0
2025-02-11 07:55:29,370 - 
Starting step 19
2025-02-11 07:55:29,370 - Current_ids device: cuda:0
2025-02-11 07:55:29,370 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,391 - Model output complete
2025-02-11 07:55:29,391 - Logits shape: torch.Size([1, 50, 151936])
2025-02-11 07:55:29,392 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,392 - Next token logits device: cuda:0
2025-02-11 07:55:29,392 - Entered do_sample
2025-02-11 07:55:29,392 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,396 - Probs max: 0.85693359375
2025-02-11 07:55:29,397 - Pre-cat
2025-02-11 07:55:29,397 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525]], device='cuda:0')
2025-02-11 07:55:29,399 - Next token: tensor([[362]], device='cuda:0')
2025-02-11 07:55:29,399 - Current_ids shape: torch.Size([1, 50])
2025-02-11 07:55:29,399 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,399 - Step 19: Generated next token
2025-02-11 07:55:29,399 - Step 19: Updated current_ids
2025-02-11 07:55:29,399 - Step 19: Decoded token text:  A
2025-02-11 07:55:29,399 - Step 19: Updated current_phrase
2025-02-11 07:55:29,399 - Step 19: Created step_acts
2025-02-11 07:55:29,400 - Step 19: Added to generation_acts
2025-02-11 07:55:29,400 - Step 19: Updated recent_tokens
2025-02-11 07:55:29,401 - Step 19: Decoded current text
2025-02-11 07:55:29,401 - Step 19: Reset consecutive_fillers
2025-02-11 07:55:29,401 - Step 19: Calculated unique_ratio: 0.9375
2025-02-11 07:55:29,401 - 
Starting step 20
2025-02-11 07:55:29,401 - Current_ids device: cuda:0
2025-02-11 07:55:29,401 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,429 - Model output complete
2025-02-11 07:55:29,429 - Logits shape: torch.Size([1, 51, 151936])
2025-02-11 07:55:29,429 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,429 - Next token logits device: cuda:0
2025-02-11 07:55:29,429 - Entered do_sample
2025-02-11 07:55:29,429 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,432 - Probs max: 0.9423828125
2025-02-11 07:55:29,432 - Pre-cat
2025-02-11 07:55:29,432 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362]], device='cuda:0')
2025-02-11 07:55:29,434 - Next token: tensor([[8]], device='cuda:0')
2025-02-11 07:55:29,434 - Current_ids shape: torch.Size([1, 51])
2025-02-11 07:55:29,434 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,434 - Step 20: Generated next token
2025-02-11 07:55:29,434 - Step 20: Updated current_ids
2025-02-11 07:55:29,435 - Step 20: Decoded token text: )
2025-02-11 07:55:29,435 - Step 20: Updated current_phrase
2025-02-11 07:55:29,435 - Step 20: Created step_acts
2025-02-11 07:55:29,435 - Step 20: Added to generation_acts
2025-02-11 07:55:29,436 - Step 20: Updated generated_texts
2025-02-11 07:55:29,437 - Step 20: Updated recent_tokens
2025-02-11 07:55:29,437 - Step 20: Decoded current text
2025-02-11 07:55:29,437 - Step 20: Reset consecutive_fillers
2025-02-11 07:55:29,437 - Step 20: Calculated unique_ratio: 0.875
2025-02-11 07:55:29,437 - 
Starting step 21
2025-02-11 07:55:29,437 - Current_ids device: cuda:0
2025-02-11 07:55:29,437 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,460 - Model output complete
2025-02-11 07:55:29,460 - Logits shape: torch.Size([1, 52, 151936])
2025-02-11 07:55:29,460 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,460 - Next token logits device: cuda:0
2025-02-11 07:55:29,460 - Entered do_sample
2025-02-11 07:55:29,460 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,463 - Probs max: 0.24072265625
2025-02-11 07:55:29,463 - Pre-cat
2025-02-11 07:55:29,463 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8]],
       device='cuda:0')
2025-02-11 07:55:29,465 - Next token: tensor([[60353]], device='cuda:0')
2025-02-11 07:55:29,465 - Current_ids shape: torch.Size([1, 52])
2025-02-11 07:55:29,466 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,466 - Step 21: Generated next token
2025-02-11 07:55:29,466 - Step 21: Updated current_ids
2025-02-11 07:55:29,466 - Step 21: Decoded token text:  ...,
2025-02-11 07:55:29,466 - Step 21: Updated current_phrase
2025-02-11 07:55:29,466 - Step 21: Created step_acts
2025-02-11 07:55:29,466 - Step 21: Added to generation_acts
2025-02-11 07:55:29,466 - Step 21: Updated recent_tokens
2025-02-11 07:55:29,468 - Step 21: Found phrase end token
2025-02-11 07:55:29,468 - Step 21: Updated recent_phrases
2025-02-11 07:55:29,468 - Step 21: Calculated similarity: 0.25
2025-02-11 07:55:29,468 - Step 21: Decoded current text
2025-02-11 07:55:29,468 - Step 21: Reset consecutive_fillers
2025-02-11 07:55:29,468 - Step 21: Calculated unique_ratio: 0.875
2025-02-11 07:55:29,468 - 
Starting step 22
2025-02-11 07:55:29,468 - Current_ids device: cuda:0
2025-02-11 07:55:29,468 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,505 - Model output complete
2025-02-11 07:55:29,505 - Logits shape: torch.Size([1, 53, 151936])
2025-02-11 07:55:29,505 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,505 - Next token logits device: cuda:0
2025-02-11 07:55:29,505 - Entered do_sample
2025-02-11 07:55:29,505 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,508 - Probs max: 0.9755859375
2025-02-11 07:55:29,509 - Pre-cat
2025-02-11 07:55:29,509 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353]],
       device='cuda:0')
2025-02-11 07:55:29,511 - Next token: tensor([[425]], device='cuda:0')
2025-02-11 07:55:29,511 - Current_ids shape: torch.Size([1, 53])
2025-02-11 07:55:29,511 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,511 - Step 22: Generated next token
2025-02-11 07:55:29,512 - Step 22: Updated current_ids
2025-02-11 07:55:29,512 - Step 22: Decoded token text:  B
2025-02-11 07:55:29,512 - Step 22: Updated current_phrase
2025-02-11 07:55:29,512 - Step 22: Created step_acts
2025-02-11 07:55:29,512 - Step 22: Added to generation_acts
2025-02-11 07:55:29,512 - Step 22: Updated recent_tokens
2025-02-11 07:55:29,514 - Step 22: Decoded current text
2025-02-11 07:55:29,514 - Step 22: Reset consecutive_fillers
2025-02-11 07:55:29,515 - Step 22: Calculated unique_ratio: 0.875
2025-02-11 07:55:29,515 - 
Starting step 23
2025-02-11 07:55:29,515 - Current_ids device: cuda:0
2025-02-11 07:55:29,515 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,539 - Model output complete
2025-02-11 07:55:29,539 - Logits shape: torch.Size([1, 54, 151936])
2025-02-11 07:55:29,539 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,539 - Next token logits device: cuda:0
2025-02-11 07:55:29,540 - Entered do_sample
2025-02-11 07:55:29,540 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,542 - Probs max: 0.9287109375
2025-02-11 07:55:29,543 - Pre-cat
2025-02-11 07:55:29,544 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425]],
       device='cuda:0')
2025-02-11 07:55:29,549 - Next token: tensor([[8]], device='cuda:0')
2025-02-11 07:55:29,550 - Current_ids shape: torch.Size([1, 54])
2025-02-11 07:55:29,550 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,550 - Step 23: Generated next token
2025-02-11 07:55:29,550 - Step 23: Updated current_ids
2025-02-11 07:55:29,550 - Step 23: Decoded token text: )
2025-02-11 07:55:29,550 - Step 23: Updated current_phrase
2025-02-11 07:55:29,551 - Step 23: Created step_acts
2025-02-11 07:55:29,551 - Step 23: Added to generation_acts
2025-02-11 07:55:29,551 - Step 23: Updated recent_tokens
2025-02-11 07:55:29,553 - Step 23: Decoded current text
2025-02-11 07:55:29,553 - Step 23: Reset consecutive_fillers
2025-02-11 07:55:29,553 - Step 23: Calculated unique_ratio: 0.8125
2025-02-11 07:55:29,553 - 
Starting step 24
2025-02-11 07:55:29,553 - Current_ids device: cuda:0
2025-02-11 07:55:29,553 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,577 - Model output complete
2025-02-11 07:55:29,577 - Logits shape: torch.Size([1, 55, 151936])
2025-02-11 07:55:29,577 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,577 - Next token logits device: cuda:0
2025-02-11 07:55:29,578 - Entered do_sample
2025-02-11 07:55:29,578 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,581 - Probs max: 0.9658203125
2025-02-11 07:55:29,582 - Pre-cat
2025-02-11 07:55:29,582 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8]], device='cuda:0')
2025-02-11 07:55:29,585 - Next token: tensor([[60353]], device='cuda:0')
2025-02-11 07:55:29,585 - Current_ids shape: torch.Size([1, 55])
2025-02-11 07:55:29,585 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,585 - Step 24: Generated next token
2025-02-11 07:55:29,585 - Step 24: Updated current_ids
2025-02-11 07:55:29,586 - Step 24: Decoded token text:  ...,
2025-02-11 07:55:29,586 - Step 24: Updated current_phrase
2025-02-11 07:55:29,586 - Step 24: Created step_acts
2025-02-11 07:55:29,586 - Step 24: Added to generation_acts
2025-02-11 07:55:29,586 - Step 24: Updated recent_tokens
2025-02-11 07:55:29,588 - Step 24: Found phrase end token
2025-02-11 07:55:29,588 - Step 24: Updated recent_phrases
2025-02-11 07:55:29,588 - Step 24: Calculated similarity: 0.5
2025-02-11 07:55:29,588 - Step 24: Decoded current text
2025-02-11 07:55:29,588 - Step 24: Reset consecutive_fillers
2025-02-11 07:55:29,588 - Step 24: Calculated unique_ratio: 0.75
2025-02-11 07:55:29,588 - 
Starting step 25
2025-02-11 07:55:29,588 - Current_ids device: cuda:0
2025-02-11 07:55:29,588 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,617 - Model output complete
2025-02-11 07:55:29,617 - Logits shape: torch.Size([1, 56, 151936])
2025-02-11 07:55:29,617 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,617 - Next token logits device: cuda:0
2025-02-11 07:55:29,617 - Entered do_sample
2025-02-11 07:55:29,617 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,619 - Probs max: 0.85302734375
2025-02-11 07:55:29,621 - Pre-cat
2025-02-11 07:55:29,622 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353]], device='cuda:0')
2025-02-11 07:55:29,627 - Next token: tensor([[356]], device='cuda:0')
2025-02-11 07:55:29,627 - Current_ids shape: torch.Size([1, 56])
2025-02-11 07:55:29,627 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,627 - Step 25: Generated next token
2025-02-11 07:55:29,627 - Step 25: Updated current_ids
2025-02-11 07:55:29,628 - Step 25: Decoded token text:  C
2025-02-11 07:55:29,628 - Step 25: Updated current_phrase
2025-02-11 07:55:29,629 - Step 25: Created step_acts
2025-02-11 07:55:29,629 - Step 25: Added to generation_acts
2025-02-11 07:55:29,630 - Step 25: Updated generated_texts
2025-02-11 07:55:29,630 - Step 25: Updated recent_tokens
2025-02-11 07:55:29,631 - Step 25: Decoded current text
2025-02-11 07:55:29,631 - Step 25: Reset consecutive_fillers
2025-02-11 07:55:29,631 - Step 25: Calculated unique_ratio: 0.75
2025-02-11 07:55:29,631 - 
Starting step 26
2025-02-11 07:55:29,631 - Current_ids device: cuda:0
2025-02-11 07:55:29,631 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,661 - Model output complete
2025-02-11 07:55:29,661 - Logits shape: torch.Size([1, 57, 151936])
2025-02-11 07:55:29,661 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,661 - Next token logits device: cuda:0
2025-02-11 07:55:29,661 - Entered do_sample
2025-02-11 07:55:29,662 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,664 - Probs max: 0.7294921875
2025-02-11 07:55:29,665 - Pre-cat
2025-02-11 07:55:29,665 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356]], device='cuda:0')
2025-02-11 07:55:29,667 - Next token: tensor([[8]], device='cuda:0')
2025-02-11 07:55:29,667 - Current_ids shape: torch.Size([1, 57])
2025-02-11 07:55:29,667 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,667 - Step 26: Generated next token
2025-02-11 07:55:29,667 - Step 26: Updated current_ids
2025-02-11 07:55:29,667 - Step 26: Decoded token text: )
2025-02-11 07:55:29,667 - Step 26: Updated current_phrase
2025-02-11 07:55:29,668 - Step 26: Created step_acts
2025-02-11 07:55:29,668 - Step 26: Added to generation_acts
2025-02-11 07:55:29,668 - Step 26: Updated recent_tokens
2025-02-11 07:55:29,669 - Step 26: Decoded current text
2025-02-11 07:55:29,670 - Step 26: Reset consecutive_fillers
2025-02-11 07:55:29,670 - Step 26: Calculated unique_ratio: 0.6875
2025-02-11 07:55:29,670 - 
Starting step 27
2025-02-11 07:55:29,670 - Current_ids device: cuda:0
2025-02-11 07:55:29,670 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,693 - Model output complete
2025-02-11 07:55:29,693 - Logits shape: torch.Size([1, 58, 151936])
2025-02-11 07:55:29,693 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,693 - Next token logits device: cuda:0
2025-02-11 07:55:29,693 - Entered do_sample
2025-02-11 07:55:29,693 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,697 - Probs max: 0.98095703125
2025-02-11 07:55:29,697 - Pre-cat
2025-02-11 07:55:29,697 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8]], device='cuda:0')
2025-02-11 07:55:29,699 - Next token: tensor([[60353]], device='cuda:0')
2025-02-11 07:55:29,700 - Current_ids shape: torch.Size([1, 58])
2025-02-11 07:55:29,700 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,700 - Step 27: Generated next token
2025-02-11 07:55:29,700 - Step 27: Updated current_ids
2025-02-11 07:55:29,700 - Step 27: Decoded token text:  ...,
2025-02-11 07:55:29,700 - Step 27: Updated current_phrase
2025-02-11 07:55:29,700 - Step 27: Created step_acts
2025-02-11 07:55:29,700 - Step 27: Added to generation_acts
2025-02-11 07:55:29,700 - Step 27: Updated recent_tokens
2025-02-11 07:55:29,702 - Step 27: Found phrase end token
2025-02-11 07:55:29,702 - Step 27: Updated recent_phrases
2025-02-11 07:55:29,702 - Step 27: Calculated similarity: 0.5
2025-02-11 07:55:29,702 - Step 27: Decoded current text
2025-02-11 07:55:29,702 - Step 27: Reset consecutive_fillers
2025-02-11 07:55:29,702 - Step 27: Calculated unique_ratio: 0.625
2025-02-11 07:55:29,703 - 
Starting step 28
2025-02-11 07:55:29,703 - Current_ids device: cuda:0
2025-02-11 07:55:29,703 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,764 - Model output complete
2025-02-11 07:55:29,764 - Logits shape: torch.Size([1, 59, 151936])
2025-02-11 07:55:29,764 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,764 - Next token logits device: cuda:0
2025-02-11 07:55:29,764 - Entered do_sample
2025-02-11 07:55:29,765 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,766 - Probs max: 0.986328125
2025-02-11 07:55:29,768 - Pre-cat
2025-02-11 07:55:29,769 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353]], device='cuda:0')
2025-02-11 07:55:29,775 - Next token: tensor([[422]], device='cuda:0')
2025-02-11 07:55:29,776 - Current_ids shape: torch.Size([1, 59])
2025-02-11 07:55:29,776 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,776 - Step 28: Generated next token
2025-02-11 07:55:29,776 - Step 28: Updated current_ids
2025-02-11 07:55:29,776 - Step 28: Decoded token text:  D
2025-02-11 07:55:29,776 - Step 28: Updated current_phrase
2025-02-11 07:55:29,777 - Step 28: Created step_acts
2025-02-11 07:55:29,777 - Step 28: Added to generation_acts
2025-02-11 07:55:29,777 - Step 28: Updated recent_tokens
2025-02-11 07:55:29,778 - Step 28: Decoded current text
2025-02-11 07:55:29,779 - Step 28: Reset consecutive_fillers
2025-02-11 07:55:29,779 - Step 28: Calculated unique_ratio: 0.625
2025-02-11 07:55:29,779 - 
Starting step 29
2025-02-11 07:55:29,779 - Current_ids device: cuda:0
2025-02-11 07:55:29,779 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,814 - Model output complete
2025-02-11 07:55:29,814 - Logits shape: torch.Size([1, 60, 151936])
2025-02-11 07:55:29,814 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,814 - Next token logits device: cuda:0
2025-02-11 07:55:29,814 - Entered do_sample
2025-02-11 07:55:29,814 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,817 - Probs max: 0.52294921875
2025-02-11 07:55:29,820 - Pre-cat
2025-02-11 07:55:29,820 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422]], device='cuda:0')
2025-02-11 07:55:29,823 - Next token: tensor([[39025]], device='cuda:0')
2025-02-11 07:55:29,824 - Current_ids shape: torch.Size([1, 60])
2025-02-11 07:55:29,824 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,824 - Step 29: Generated next token
2025-02-11 07:55:29,824 - Step 29: Updated current_ids
2025-02-11 07:55:29,824 - Step 29: Decoded token text: )...
2025-02-11 07:55:29,824 - Step 29: Updated current_phrase
2025-02-11 07:55:29,824 - Step 29: Created step_acts
2025-02-11 07:55:29,825 - Step 29: Added to generation_acts
2025-02-11 07:55:29,825 - Step 29: Updated recent_tokens
2025-02-11 07:55:29,826 - Step 29: Found phrase end token
2025-02-11 07:55:29,826 - Step 29: Updated recent_phrases
2025-02-11 07:55:29,826 - Step 29: Calculated similarity: 0.0
2025-02-11 07:55:29,826 - Step 29: Decoded current text
2025-02-11 07:55:29,826 - Step 29: Reset consecutive_fillers
2025-02-11 07:55:29,826 - Step 29: Calculated unique_ratio: 0.6875
2025-02-11 07:55:29,827 - 
Starting step 30
2025-02-11 07:55:29,827 - Current_ids device: cuda:0
2025-02-11 07:55:29,827 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,861 - Model output complete
2025-02-11 07:55:29,861 - Logits shape: torch.Size([1, 61, 151936])
2025-02-11 07:55:29,862 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,862 - Next token logits device: cuda:0
2025-02-11 07:55:29,862 - Entered do_sample
2025-02-11 07:55:29,862 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,864 - Probs max: 0.35107421875
2025-02-11 07:55:29,865 - Pre-cat
2025-02-11 07:55:29,865 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025]],
       device='cuda:0')
2025-02-11 07:55:29,868 - Next token: tensor([[1939]], device='cuda:0')
2025-02-11 07:55:29,869 - Current_ids shape: torch.Size([1, 61])
2025-02-11 07:55:29,869 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,869 - Step 30: Generated next token
2025-02-11 07:55:29,869 - Step 30: Updated current_ids
2025-02-11 07:55:29,869 - Step 30: Decoded token text: ?


2025-02-11 07:55:29,869 - Step 30: Updated current_phrase
2025-02-11 07:55:29,869 - Step 30: Created step_acts
2025-02-11 07:55:29,870 - Step 30: Added to generation_acts
2025-02-11 07:55:29,871 - Step 30: Updated generated_texts
2025-02-11 07:55:29,871 - Step 30: Updated recent_tokens
2025-02-11 07:55:29,871 - Step 30: Decoded current text
2025-02-11 07:55:29,871 - Step 30: Reset consecutive_fillers
2025-02-11 07:55:29,872 - Step 30: Calculated unique_ratio: 0.75
2025-02-11 07:55:29,872 - 
Starting step 31
2025-02-11 07:55:29,872 - Current_ids device: cuda:0
2025-02-11 07:55:29,872 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,896 - Model output complete
2025-02-11 07:55:29,897 - Logits shape: torch.Size([1, 62, 151936])
2025-02-11 07:55:29,897 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,897 - Next token logits device: cuda:0
2025-02-11 07:55:29,897 - Entered do_sample
2025-02-11 07:55:29,897 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,900 - Probs max: 0.70751953125
2025-02-11 07:55:29,901 - Pre-cat
2025-02-11 07:55:29,901 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939]],
       device='cuda:0')
2025-02-11 07:55:29,903 - Next token: tensor([[14190]], device='cuda:0')
2025-02-11 07:55:29,904 - Current_ids shape: torch.Size([1, 62])
2025-02-11 07:55:29,904 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,904 - Step 31: Generated next token
2025-02-11 07:55:29,904 - Step 31: Updated current_ids
2025-02-11 07:55:29,904 - Step 31: Decoded token text: Wait
2025-02-11 07:55:29,904 - Step 31: Updated current_phrase
2025-02-11 07:55:29,905 - Step 31: Created step_acts
2025-02-11 07:55:29,905 - Step 31: Added to generation_acts
2025-02-11 07:55:29,905 - Step 31: Updated recent_tokens
2025-02-11 07:55:29,906 - Step 31: Decoded current text
2025-02-11 07:55:29,906 - Step 31: Reset consecutive_fillers
2025-02-11 07:55:29,907 - Step 31: Calculated unique_ratio: 0.75
2025-02-11 07:55:29,907 - 
Starting step 32
2025-02-11 07:55:29,907 - Current_ids device: cuda:0
2025-02-11 07:55:29,907 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,932 - Model output complete
2025-02-11 07:55:29,933 - Logits shape: torch.Size([1, 63, 151936])
2025-02-11 07:55:29,933 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,933 - Next token logits device: cuda:0
2025-02-11 07:55:29,933 - Entered do_sample
2025-02-11 07:55:29,933 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,936 - Probs max: 0.9951171875
2025-02-11 07:55:29,937 - Pre-cat
2025-02-11 07:55:29,937 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190]],
       device='cuda:0')
2025-02-11 07:55:29,940 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:29,940 - Current_ids shape: torch.Size([1, 63])
2025-02-11 07:55:29,940 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,940 - Step 32: Generated next token
2025-02-11 07:55:29,940 - Step 32: Updated current_ids
2025-02-11 07:55:29,940 - Step 32: Decoded token text: ,
2025-02-11 07:55:29,941 - Step 32: Updated current_phrase
2025-02-11 07:55:29,941 - Step 32: Created step_acts
2025-02-11 07:55:29,941 - Step 32: Added to generation_acts
2025-02-11 07:55:29,941 - Step 32: Updated recent_tokens
2025-02-11 07:55:29,942 - Step 32: Found phrase end token
2025-02-11 07:55:29,943 - Step 32: Updated recent_phrases
2025-02-11 07:55:29,943 - Step 32: Calculated similarity: 0.0
2025-02-11 07:55:29,943 - Step 32: Decoded current text
2025-02-11 07:55:29,943 - Step 32: Reset consecutive_fillers
2025-02-11 07:55:29,943 - Step 32: Calculated unique_ratio: 0.75
2025-02-11 07:55:29,943 - 
Starting step 33
2025-02-11 07:55:29,943 - Current_ids device: cuda:0
2025-02-11 07:55:29,943 - Current_ids dtype: torch.int64
2025-02-11 07:55:29,971 - Model output complete
2025-02-11 07:55:29,971 - Logits shape: torch.Size([1, 64, 151936])
2025-02-11 07:55:29,971 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,971 - Next token logits device: cuda:0
2025-02-11 07:55:29,971 - Entered do_sample
2025-02-11 07:55:29,971 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:29,974 - Probs max: 0.59765625
2025-02-11 07:55:29,975 - Pre-cat
2025-02-11 07:55:29,975 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11]], device='cuda:0')
2025-02-11 07:55:29,977 - Next token: tensor([[358]], device='cuda:0')
2025-02-11 07:55:29,977 - Current_ids shape: torch.Size([1, 64])
2025-02-11 07:55:29,977 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:29,977 - Step 33: Generated next token
2025-02-11 07:55:29,978 - Step 33: Updated current_ids
2025-02-11 07:55:29,978 - Step 33: Decoded token text:  I
2025-02-11 07:55:29,978 - Step 33: Updated current_phrase
2025-02-11 07:55:29,978 - Step 33: Created step_acts
2025-02-11 07:55:29,978 - Step 33: Added to generation_acts
2025-02-11 07:55:29,978 - Step 33: Updated recent_tokens
2025-02-11 07:55:29,980 - Step 33: Decoded current text
2025-02-11 07:55:29,981 - Step 33: Reset consecutive_fillers
2025-02-11 07:55:29,981 - Step 33: Calculated unique_ratio: 0.75
2025-02-11 07:55:29,981 - 
Starting step 34
2025-02-11 07:55:29,981 - Current_ids device: cuda:0
2025-02-11 07:55:29,981 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,004 - Model output complete
2025-02-11 07:55:30,004 - Logits shape: torch.Size([1, 65, 151936])
2025-02-11 07:55:30,004 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,004 - Next token logits device: cuda:0
2025-02-11 07:55:30,005 - Entered do_sample
2025-02-11 07:55:30,005 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,009 - Probs max: 0.27294921875
2025-02-11 07:55:30,010 - Pre-cat
2025-02-11 07:55:30,010 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358]], device='cuda:0')
2025-02-11 07:55:30,013 - Next token: tensor([[1184]], device='cuda:0')
2025-02-11 07:55:30,014 - Current_ids shape: torch.Size([1, 65])
2025-02-11 07:55:30,014 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,014 - Step 34: Generated next token
2025-02-11 07:55:30,014 - Step 34: Updated current_ids
2025-02-11 07:55:30,014 - Step 34: Decoded token text:  need
2025-02-11 07:55:30,014 - Step 34: Updated current_phrase
2025-02-11 07:55:30,014 - Step 34: Created step_acts
2025-02-11 07:55:30,015 - Step 34: Added to generation_acts
2025-02-11 07:55:30,015 - Step 34: Updated recent_tokens
2025-02-11 07:55:30,016 - Step 34: Decoded current text
2025-02-11 07:55:30,016 - Step 34: Reset consecutive_fillers
2025-02-11 07:55:30,017 - Step 34: Calculated unique_ratio: 0.75
2025-02-11 07:55:30,017 - 
Starting step 35
2025-02-11 07:55:30,017 - Current_ids device: cuda:0
2025-02-11 07:55:30,017 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,045 - Model output complete
2025-02-11 07:55:30,045 - Logits shape: torch.Size([1, 66, 151936])
2025-02-11 07:55:30,045 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,045 - Next token logits device: cuda:0
2025-02-11 07:55:30,045 - Entered do_sample
2025-02-11 07:55:30,046 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,048 - Probs max: 0.9921875
2025-02-11 07:55:30,049 - Pre-cat
2025-02-11 07:55:30,049 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184]], device='cuda:0')
2025-02-11 07:55:30,054 - Next token: tensor([[311]], device='cuda:0')
2025-02-11 07:55:30,055 - Current_ids shape: torch.Size([1, 66])
2025-02-11 07:55:30,055 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,055 - Step 35: Generated next token
2025-02-11 07:55:30,055 - Step 35: Updated current_ids
2025-02-11 07:55:30,055 - Step 35: Decoded token text:  to
2025-02-11 07:55:30,055 - Step 35: Updated current_phrase
2025-02-11 07:55:30,056 - Step 35: Created step_acts
2025-02-11 07:55:30,056 - Step 35: Added to generation_acts
2025-02-11 07:55:30,057 - Step 35: Updated generated_texts
2025-02-11 07:55:30,057 - Step 35: Updated recent_tokens
2025-02-11 07:55:30,058 - Step 35: Decoded current text
2025-02-11 07:55:30,058 - Step 35: Reset consecutive_fillers
2025-02-11 07:55:30,058 - Step 35: Calculated unique_ratio: 0.75
2025-02-11 07:55:30,058 - 
Starting step 36
2025-02-11 07:55:30,058 - Current_ids device: cuda:0
2025-02-11 07:55:30,058 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,134 - Model output complete
2025-02-11 07:55:30,134 - Logits shape: torch.Size([1, 67, 151936])
2025-02-11 07:55:30,134 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,134 - Next token logits device: cuda:0
2025-02-11 07:55:30,134 - Entered do_sample
2025-02-11 07:55:30,135 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,137 - Probs max: 0.7763671875
2025-02-11 07:55:30,139 - Pre-cat
2025-02-11 07:55:30,139 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311]], device='cuda:0')
2025-02-11 07:55:30,144 - Next token: tensor([[7071]], device='cuda:0')
2025-02-11 07:55:30,145 - Current_ids shape: torch.Size([1, 67])
2025-02-11 07:55:30,145 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,145 - Step 36: Generated next token
2025-02-11 07:55:30,145 - Step 36: Updated current_ids
2025-02-11 07:55:30,145 - Step 36: Decoded token text:  figure
2025-02-11 07:55:30,145 - Step 36: Updated current_phrase
2025-02-11 07:55:30,146 - Step 36: Created step_acts
2025-02-11 07:55:30,146 - Step 36: Added to generation_acts
2025-02-11 07:55:30,146 - Step 36: Updated recent_tokens
2025-02-11 07:55:30,148 - Step 36: Decoded current text
2025-02-11 07:55:30,148 - Step 36: Reset consecutive_fillers
2025-02-11 07:55:30,148 - Step 36: Calculated unique_ratio: 0.8125
2025-02-11 07:55:30,148 - 
Starting step 37
2025-02-11 07:55:30,148 - Current_ids device: cuda:0
2025-02-11 07:55:30,148 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,179 - Model output complete
2025-02-11 07:55:30,179 - Logits shape: torch.Size([1, 68, 151936])
2025-02-11 07:55:30,180 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,180 - Next token logits device: cuda:0
2025-02-11 07:55:30,180 - Entered do_sample
2025-02-11 07:55:30,180 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,184 - Probs max: 0.91357421875
2025-02-11 07:55:30,184 - Pre-cat
2025-02-11 07:55:30,185 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071]], device='cuda:0')
2025-02-11 07:55:30,188 - Next token: tensor([[700]], device='cuda:0')
2025-02-11 07:55:30,188 - Current_ids shape: torch.Size([1, 68])
2025-02-11 07:55:30,188 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,188 - Step 37: Generated next token
2025-02-11 07:55:30,188 - Step 37: Updated current_ids
2025-02-11 07:55:30,189 - Step 37: Decoded token text:  out
2025-02-11 07:55:30,189 - Step 37: Updated current_phrase
2025-02-11 07:55:30,189 - Step 37: Created step_acts
2025-02-11 07:55:30,189 - Step 37: Added to generation_acts
2025-02-11 07:55:30,189 - Step 37: Updated recent_tokens
2025-02-11 07:55:30,192 - Step 37: Decoded current text
2025-02-11 07:55:30,192 - Step 37: Reset consecutive_fillers
2025-02-11 07:55:30,193 - Step 37: Calculated unique_ratio: 0.875
2025-02-11 07:55:30,193 - 
Starting step 38
2025-02-11 07:55:30,193 - Current_ids device: cuda:0
2025-02-11 07:55:30,193 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,298 - Model output complete
2025-02-11 07:55:30,298 - Logits shape: torch.Size([1, 69, 151936])
2025-02-11 07:55:30,298 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,298 - Next token logits device: cuda:0
2025-02-11 07:55:30,298 - Entered do_sample
2025-02-11 07:55:30,299 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,301 - Probs max: 0.75634765625
2025-02-11 07:55:30,303 - Pre-cat
2025-02-11 07:55:30,303 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700]], device='cuda:0')
2025-02-11 07:55:30,309 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:30,309 - Current_ids shape: torch.Size([1, 69])
2025-02-11 07:55:30,309 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,309 - Step 38: Generated next token
2025-02-11 07:55:30,310 - Step 38: Updated current_ids
2025-02-11 07:55:30,310 - Step 38: Decoded token text:  the
2025-02-11 07:55:30,310 - Step 38: Updated current_phrase
2025-02-11 07:55:30,311 - Step 38: Created step_acts
2025-02-11 07:55:30,311 - Step 38: Added to generation_acts
2025-02-11 07:55:30,311 - Step 38: Updated recent_tokens
2025-02-11 07:55:30,312 - Step 38: Decoded current text
2025-02-11 07:55:30,312 - Step 38: Reset consecutive_fillers
2025-02-11 07:55:30,313 - Step 38: Calculated unique_ratio: 0.875
2025-02-11 07:55:30,313 - 
Starting step 39
2025-02-11 07:55:30,313 - Current_ids device: cuda:0
2025-02-11 07:55:30,313 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,356 - Model output complete
2025-02-11 07:55:30,356 - Logits shape: torch.Size([1, 70, 151936])
2025-02-11 07:55:30,356 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,356 - Next token logits device: cuda:0
2025-02-11 07:55:30,356 - Entered do_sample
2025-02-11 07:55:30,356 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,359 - Probs max: 0.8916015625
2025-02-11 07:55:30,360 - Pre-cat
2025-02-11 07:55:30,360 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279]],
       device='cuda:0')
2025-02-11 07:55:30,363 - Next token: tensor([[4396]], device='cuda:0')
2025-02-11 07:55:30,364 - Current_ids shape: torch.Size([1, 70])
2025-02-11 07:55:30,364 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,364 - Step 39: Generated next token
2025-02-11 07:55:30,364 - Step 39: Updated current_ids
2025-02-11 07:55:30,364 - Step 39: Decoded token text:  correct
2025-02-11 07:55:30,364 - Step 39: Updated current_phrase
2025-02-11 07:55:30,365 - Step 39: Created step_acts
2025-02-11 07:55:30,365 - Step 39: Added to generation_acts
2025-02-11 07:55:30,365 - Step 39: Updated recent_tokens
2025-02-11 07:55:30,367 - Step 39: Decoded current text
2025-02-11 07:55:30,367 - Step 39: Reset consecutive_fillers
2025-02-11 07:55:30,367 - Step 39: Calculated unique_ratio: 0.9375
2025-02-11 07:55:30,367 - 
Starting step 40
2025-02-11 07:55:30,367 - Current_ids device: cuda:0
2025-02-11 07:55:30,367 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,391 - Model output complete
2025-02-11 07:55:30,391 - Logits shape: torch.Size([1, 71, 151936])
2025-02-11 07:55:30,391 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,391 - Next token logits device: cuda:0
2025-02-11 07:55:30,391 - Entered do_sample
2025-02-11 07:55:30,391 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,396 - Probs max: 0.91943359375
2025-02-11 07:55:30,396 - Pre-cat
2025-02-11 07:55:30,396 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396]],
       device='cuda:0')
2025-02-11 07:55:30,399 - Next token: tensor([[4226]], device='cuda:0')
2025-02-11 07:55:30,399 - Current_ids shape: torch.Size([1, 71])
2025-02-11 07:55:30,399 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,399 - Step 40: Generated next token
2025-02-11 07:55:30,400 - Step 40: Updated current_ids
2025-02-11 07:55:30,400 - Step 40: Decoded token text:  answer
2025-02-11 07:55:30,400 - Step 40: Updated current_phrase
2025-02-11 07:55:30,400 - Step 40: Created step_acts
2025-02-11 07:55:30,400 - Step 40: Added to generation_acts
2025-02-11 07:55:30,402 - Step 40: Updated generated_texts
2025-02-11 07:55:30,402 - Step 40: Updated recent_tokens
2025-02-11 07:55:30,403 - Step 40: Decoded current text
2025-02-11 07:55:30,403 - Step 40: Reset consecutive_fillers
2025-02-11 07:55:30,404 - Step 40: Calculated unique_ratio: 1.0
2025-02-11 07:55:30,404 - 
Starting step 41
2025-02-11 07:55:30,404 - Current_ids device: cuda:0
2025-02-11 07:55:30,404 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,429 - Model output complete
2025-02-11 07:55:30,429 - Logits shape: torch.Size([1, 72, 151936])
2025-02-11 07:55:30,429 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,429 - Next token logits device: cuda:0
2025-02-11 07:55:30,429 - Entered do_sample
2025-02-11 07:55:30,429 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,432 - Probs max: 0.47021484375
2025-02-11 07:55:30,433 - Pre-cat
2025-02-11 07:55:30,433 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226]],
       device='cuda:0')
2025-02-11 07:55:30,436 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:55:30,437 - Current_ids shape: torch.Size([1, 72])
2025-02-11 07:55:30,437 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,437 - Step 41: Generated next token
2025-02-11 07:55:30,437 - Step 41: Updated current_ids
2025-02-11 07:55:30,437 - Step 41: Decoded token text: .
2025-02-11 07:55:30,437 - Step 41: Updated current_phrase
2025-02-11 07:55:30,438 - Step 41: Created step_acts
2025-02-11 07:55:30,438 - Step 41: Added to generation_acts
2025-02-11 07:55:30,438 - Step 41: Updated recent_tokens
2025-02-11 07:55:30,439 - Step 41: Found phrase end token
2025-02-11 07:55:30,439 - Step 41: Updated recent_phrases
2025-02-11 07:55:30,439 - Step 41: Calculated similarity: 0.0
2025-02-11 07:55:30,440 - Step 41: Decoded current text
2025-02-11 07:55:30,440 - Step 41: Reset consecutive_fillers
2025-02-11 07:55:30,440 - Step 41: Calculated unique_ratio: 1.0
2025-02-11 07:55:30,440 - 
Starting step 42
2025-02-11 07:55:30,440 - Current_ids device: cuda:0
2025-02-11 07:55:30,440 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,465 - Model output complete
2025-02-11 07:55:30,465 - Logits shape: torch.Size([1, 73, 151936])
2025-02-11 07:55:30,465 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,465 - Next token logits device: cuda:0
2025-02-11 07:55:30,465 - Entered do_sample
2025-02-11 07:55:30,465 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,469 - Probs max: 0.50830078125
2025-02-11 07:55:30,470 - Pre-cat
2025-02-11 07:55:30,470 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13]], device='cuda:0')
2025-02-11 07:55:30,473 - Next token: tensor([[2980]], device='cuda:0')
2025-02-11 07:55:30,473 - Current_ids shape: torch.Size([1, 73])
2025-02-11 07:55:30,473 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,473 - Step 42: Generated next token
2025-02-11 07:55:30,473 - Step 42: Updated current_ids
2025-02-11 07:55:30,474 - Step 42: Decoded token text:  Can
2025-02-11 07:55:30,474 - Step 42: Updated current_phrase
2025-02-11 07:55:30,474 - Step 42: Created step_acts
2025-02-11 07:55:30,474 - Step 42: Added to generation_acts
2025-02-11 07:55:30,474 - Step 42: Updated recent_tokens
2025-02-11 07:55:30,476 - Step 42: Decoded current text
2025-02-11 07:55:30,476 - Step 42: Reset consecutive_fillers
2025-02-11 07:55:30,476 - Step 42: Calculated unique_ratio: 1.0
2025-02-11 07:55:30,476 - 
Starting step 43
2025-02-11 07:55:30,476 - Current_ids device: cuda:0
2025-02-11 07:55:30,476 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,501 - Model output complete
2025-02-11 07:55:30,502 - Logits shape: torch.Size([1, 74, 151936])
2025-02-11 07:55:30,502 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,502 - Next token logits device: cuda:0
2025-02-11 07:55:30,502 - Entered do_sample
2025-02-11 07:55:30,502 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,505 - Probs max: 0.79541015625
2025-02-11 07:55:30,506 - Pre-cat
2025-02-11 07:55:30,506 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980]], device='cuda:0')
2025-02-11 07:55:30,509 - Next token: tensor([[498]], device='cuda:0')
2025-02-11 07:55:30,509 - Current_ids shape: torch.Size([1, 74])
2025-02-11 07:55:30,509 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,510 - Step 43: Generated next token
2025-02-11 07:55:30,510 - Step 43: Updated current_ids
2025-02-11 07:55:30,510 - Step 43: Decoded token text:  you
2025-02-11 07:55:30,510 - Step 43: Updated current_phrase
2025-02-11 07:55:30,510 - Step 43: Created step_acts
2025-02-11 07:55:30,510 - Step 43: Added to generation_acts
2025-02-11 07:55:30,510 - Step 43: Updated recent_tokens
2025-02-11 07:55:30,512 - Step 43: Decoded current text
2025-02-11 07:55:30,512 - Step 43: Reset consecutive_fillers
2025-02-11 07:55:30,513 - Step 43: Calculated unique_ratio: 1.0
2025-02-11 07:55:30,513 - 
Starting step 44
2025-02-11 07:55:30,513 - Current_ids device: cuda:0
2025-02-11 07:55:30,513 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,566 - Model output complete
2025-02-11 07:55:30,567 - Logits shape: torch.Size([1, 75, 151936])
2025-02-11 07:55:30,567 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,567 - Next token logits device: cuda:0
2025-02-11 07:55:30,568 - Entered do_sample
2025-02-11 07:55:30,568 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,570 - Probs max: 0.9091796875
2025-02-11 07:55:30,573 - Pre-cat
2025-02-11 07:55:30,573 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498]], device='cuda:0')
2025-02-11 07:55:30,579 - Next token: tensor([[10339]], device='cuda:0')
2025-02-11 07:55:30,580 - Current_ids shape: torch.Size([1, 75])
2025-02-11 07:55:30,580 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,580 - Step 44: Generated next token
2025-02-11 07:55:30,580 - Step 44: Updated current_ids
2025-02-11 07:55:30,581 - Step 44: Decoded token text:  explain
2025-02-11 07:55:30,581 - Step 44: Updated current_phrase
2025-02-11 07:55:30,582 - Step 44: Created step_acts
2025-02-11 07:55:30,582 - Step 44: Added to generation_acts
2025-02-11 07:55:30,582 - Step 44: Updated recent_tokens
2025-02-11 07:55:30,584 - Step 44: Decoded current text
2025-02-11 07:55:30,584 - Step 44: Reset consecutive_fillers
2025-02-11 07:55:30,585 - Step 44: Calculated unique_ratio: 1.0
2025-02-11 07:55:30,585 - 
Starting step 45
2025-02-11 07:55:30,585 - Current_ids device: cuda:0
2025-02-11 07:55:30,585 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,642 - Model output complete
2025-02-11 07:55:30,643 - Logits shape: torch.Size([1, 76, 151936])
2025-02-11 07:55:30,643 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,643 - Next token logits device: cuda:0
2025-02-11 07:55:30,643 - Entered do_sample
2025-02-11 07:55:30,643 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,645 - Probs max: 0.34814453125
2025-02-11 07:55:30,647 - Pre-cat
2025-02-11 07:55:30,647 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339]], device='cuda:0')
2025-02-11 07:55:30,653 - Next token: tensor([[3170]], device='cuda:0')
2025-02-11 07:55:30,653 - Current_ids shape: torch.Size([1, 76])
2025-02-11 07:55:30,653 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,654 - Step 45: Generated next token
2025-02-11 07:55:30,654 - Step 45: Updated current_ids
2025-02-11 07:55:30,654 - Step 45: Decoded token text:  why
2025-02-11 07:55:30,654 - Step 45: Updated current_phrase
2025-02-11 07:55:30,655 - Step 45: Created step_acts
2025-02-11 07:55:30,655 - Step 45: Added to generation_acts
2025-02-11 07:55:30,657 - Step 45: Updated generated_texts
2025-02-11 07:55:30,657 - Step 45: Updated recent_tokens
2025-02-11 07:55:30,657 - Step 45: Decoded current text
2025-02-11 07:55:30,658 - Step 45: Reset consecutive_fillers
2025-02-11 07:55:30,658 - Step 45: Calculated unique_ratio: 1.0
2025-02-11 07:55:30,658 - 
Starting step 46
2025-02-11 07:55:30,658 - Current_ids device: cuda:0
2025-02-11 07:55:30,658 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,719 - Model output complete
2025-02-11 07:55:30,720 - Logits shape: torch.Size([1, 77, 151936])
2025-02-11 07:55:30,720 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,720 - Next token logits device: cuda:0
2025-02-11 07:55:30,720 - Entered do_sample
2025-02-11 07:55:30,720 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,722 - Probs max: 0.39306640625
2025-02-11 07:55:30,723 - Pre-cat
2025-02-11 07:55:30,723 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170]], device='cuda:0')
2025-02-11 07:55:30,726 - Next token: tensor([[304]], device='cuda:0')
2025-02-11 07:55:30,726 - Current_ids shape: torch.Size([1, 77])
2025-02-11 07:55:30,726 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,726 - Step 46: Generated next token
2025-02-11 07:55:30,726 - Step 46: Updated current_ids
2025-02-11 07:55:30,726 - Step 46: Decoded token text:  in
2025-02-11 07:55:30,726 - Step 46: Updated current_phrase
2025-02-11 07:55:30,727 - Step 46: Created step_acts
2025-02-11 07:55:30,727 - Step 46: Added to generation_acts
2025-02-11 07:55:30,727 - Step 46: Updated recent_tokens
2025-02-11 07:55:30,729 - Step 46: Decoded current text
2025-02-11 07:55:30,729 - Step 46: Reset consecutive_fillers
2025-02-11 07:55:30,729 - Step 46: Calculated unique_ratio: 1.0
2025-02-11 07:55:30,729 - 
Starting step 47
2025-02-11 07:55:30,729 - Current_ids device: cuda:0
2025-02-11 07:55:30,729 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,753 - Model output complete
2025-02-11 07:55:30,753 - Logits shape: torch.Size([1, 78, 151936])
2025-02-11 07:55:30,753 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,753 - Next token logits device: cuda:0
2025-02-11 07:55:30,753 - Entered do_sample
2025-02-11 07:55:30,753 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,758 - Probs max: 0.40087890625
2025-02-11 07:55:30,758 - Pre-cat
2025-02-11 07:55:30,758 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304]], device='cuda:0')
2025-02-11 07:55:30,763 - Next token: tensor([[1741]], device='cuda:0')
2025-02-11 07:55:30,763 - Current_ids shape: torch.Size([1, 78])
2025-02-11 07:55:30,763 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,763 - Step 47: Generated next token
2025-02-11 07:55:30,763 - Step 47: Updated current_ids
2025-02-11 07:55:30,764 - Step 47: Decoded token text:  such
2025-02-11 07:55:30,764 - Step 47: Updated current_phrase
2025-02-11 07:55:30,764 - Step 47: Created step_acts
2025-02-11 07:55:30,764 - Step 47: Added to generation_acts
2025-02-11 07:55:30,764 - Step 47: Updated recent_tokens
2025-02-11 07:55:30,766 - Step 47: Decoded current text
2025-02-11 07:55:30,766 - Step 47: Reset consecutive_fillers
2025-02-11 07:55:30,766 - Step 47: Calculated unique_ratio: 1.0
2025-02-11 07:55:30,766 - 
Starting step 48
2025-02-11 07:55:30,766 - Current_ids device: cuda:0
2025-02-11 07:55:30,766 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,790 - Model output complete
2025-02-11 07:55:30,790 - Logits shape: torch.Size([1, 79, 151936])
2025-02-11 07:55:30,790 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,790 - Next token logits device: cuda:0
2025-02-11 07:55:30,790 - Entered do_sample
2025-02-11 07:55:30,790 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,794 - Probs max: 0.978515625
2025-02-11 07:55:30,796 - Pre-cat
2025-02-11 07:55:30,796 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741]],
       device='cuda:0')
2025-02-11 07:55:30,800 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:30,801 - Current_ids shape: torch.Size([1, 79])
2025-02-11 07:55:30,801 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,801 - Step 48: Generated next token
2025-02-11 07:55:30,801 - Step 48: Updated current_ids
2025-02-11 07:55:30,801 - Step 48: Decoded token text:  a
2025-02-11 07:55:30,802 - Step 48: Updated current_phrase
2025-02-11 07:55:30,802 - Step 48: Created step_acts
2025-02-11 07:55:30,802 - Step 48: Added to generation_acts
2025-02-11 07:55:30,802 - Step 48: Updated recent_tokens
2025-02-11 07:55:30,804 - Step 48: Decoded current text
2025-02-11 07:55:30,804 - Step 48: Reset consecutive_fillers
2025-02-11 07:55:30,804 - Step 48: Calculated unique_ratio: 1.0
2025-02-11 07:55:30,804 - 
Starting step 49
2025-02-11 07:55:30,804 - Current_ids device: cuda:0
2025-02-11 07:55:30,804 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,868 - Model output complete
2025-02-11 07:55:30,868 - Logits shape: torch.Size([1, 80, 151936])
2025-02-11 07:55:30,868 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,868 - Next token logits device: cuda:0
2025-02-11 07:55:30,868 - Entered do_sample
2025-02-11 07:55:30,868 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,870 - Probs max: 0.748046875
2025-02-11 07:55:30,872 - Pre-cat
2025-02-11 07:55:30,872 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264]],
       device='cuda:0')
2025-02-11 07:55:30,879 - Next token: tensor([[15048]], device='cuda:0')
2025-02-11 07:55:30,879 - Current_ids shape: torch.Size([1, 80])
2025-02-11 07:55:30,879 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,879 - Step 49: Generated next token
2025-02-11 07:55:30,879 - Step 49: Updated current_ids
2025-02-11 07:55:30,880 - Step 49: Decoded token text:  scenario
2025-02-11 07:55:30,880 - Step 49: Updated current_phrase
2025-02-11 07:55:30,880 - Step 49: Created step_acts
2025-02-11 07:55:30,880 - Step 49: Added to generation_acts
2025-02-11 07:55:30,881 - Step 49: Updated recent_tokens
2025-02-11 07:55:30,882 - Step 49: Decoded current text
2025-02-11 07:55:30,882 - Step 49: Reset consecutive_fillers
2025-02-11 07:55:30,882 - Step 49: Calculated unique_ratio: 1.0
2025-02-11 07:55:30,883 - 
Starting step 50
2025-02-11 07:55:30,883 - Current_ids device: cuda:0
2025-02-11 07:55:30,883 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,910 - Model output complete
2025-02-11 07:55:30,910 - Logits shape: torch.Size([1, 81, 151936])
2025-02-11 07:55:30,910 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,911 - Next token logits device: cuda:0
2025-02-11 07:55:30,911 - Entered do_sample
2025-02-11 07:55:30,911 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,915 - Probs max: 0.6142578125
2025-02-11 07:55:30,916 - Pre-cat
2025-02-11 07:55:30,916 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048]],
       device='cuda:0')
2025-02-11 07:55:30,919 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:30,919 - Current_ids shape: torch.Size([1, 81])
2025-02-11 07:55:30,919 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,919 - Step 50: Generated next token
2025-02-11 07:55:30,920 - Step 50: Updated current_ids
2025-02-11 07:55:30,920 - Step 50: Decoded token text: ,
2025-02-11 07:55:30,920 - Step 50: Updated current_phrase
2025-02-11 07:55:30,920 - Step 50: Created step_acts
2025-02-11 07:55:30,920 - Step 50: Added to generation_acts
2025-02-11 07:55:30,922 - Step 50: Updated generated_texts
2025-02-11 07:55:30,922 - Step 50: Updated recent_tokens
2025-02-11 07:55:30,922 - Step 50: Found phrase end token
2025-02-11 07:55:30,922 - Step 50: Updated recent_phrases
2025-02-11 07:55:30,922 - Step 50: Calculated similarity: 0.0
2025-02-11 07:55:30,922 - Step 50: Decoded current text
2025-02-11 07:55:30,922 - Step 50: Reset consecutive_fillers
2025-02-11 07:55:30,923 - Step 50: Calculated unique_ratio: 1.0
2025-02-11 07:55:30,923 - 
Starting step 51
2025-02-11 07:55:30,923 - Current_ids device: cuda:0
2025-02-11 07:55:30,923 - Current_ids dtype: torch.int64
2025-02-11 07:55:30,951 - Model output complete
2025-02-11 07:55:30,952 - Logits shape: torch.Size([1, 82, 151936])
2025-02-11 07:55:30,952 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,952 - Next token logits device: cuda:0
2025-02-11 07:55:30,952 - Entered do_sample
2025-02-11 07:55:30,952 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:30,954 - Probs max: 0.9736328125
2025-02-11 07:55:30,955 - Pre-cat
2025-02-11 07:55:30,955 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11]], device='cuda:0')
2025-02-11 07:55:30,960 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:30,960 - Current_ids shape: torch.Size([1, 82])
2025-02-11 07:55:30,960 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:30,960 - Step 51: Generated next token
2025-02-11 07:55:30,960 - Step 51: Updated current_ids
2025-02-11 07:55:30,960 - Step 51: Decoded token text:  the
2025-02-11 07:55:30,960 - Step 51: Updated current_phrase
2025-02-11 07:55:30,961 - Step 51: Created step_acts
2025-02-11 07:55:30,961 - Step 51: Added to generation_acts
2025-02-11 07:55:30,961 - Step 51: Updated recent_tokens
2025-02-11 07:55:30,963 - Step 51: Decoded current text
2025-02-11 07:55:30,963 - Step 51: Reset consecutive_fillers
2025-02-11 07:55:30,963 - Step 51: Calculated unique_ratio: 0.9375
2025-02-11 07:55:30,963 - 
Starting step 52
2025-02-11 07:55:30,963 - Current_ids device: cuda:0
2025-02-11 07:55:30,963 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,156 - Model output complete
2025-02-11 07:55:32,157 - Logits shape: torch.Size([1, 83, 151936])
2025-02-11 07:55:32,157 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,157 - Next token logits device: cuda:0
2025-02-11 07:55:32,157 - Entered do_sample
2025-02-11 07:55:32,157 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,159 - Probs max: 0.70947265625
2025-02-11 07:55:32,161 - Pre-cat
2025-02-11 07:55:32,161 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279]], device='cuda:0')
2025-02-11 07:55:32,167 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:32,167 - Current_ids shape: torch.Size([1, 83])
2025-02-11 07:55:32,167 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,167 - Step 52: Generated next token
2025-02-11 07:55:32,168 - Step 52: Updated current_ids
2025-02-11 07:55:32,168 - Step 52: Decoded token text:  wall
2025-02-11 07:55:32,168 - Step 52: Updated current_phrase
2025-02-11 07:55:32,169 - Step 52: Created step_acts
2025-02-11 07:55:32,169 - Step 52: Added to generation_acts
2025-02-11 07:55:32,169 - Step 52: Updated recent_tokens
2025-02-11 07:55:32,171 - Step 52: Decoded current text
2025-02-11 07:55:32,171 - Step 52: Reset consecutive_fillers
2025-02-11 07:55:32,171 - Step 52: Calculated unique_ratio: 0.9375
2025-02-11 07:55:32,171 - 
Starting step 53
2025-02-11 07:55:32,171 - Current_ids device: cuda:0
2025-02-11 07:55:32,171 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,222 - Model output complete
2025-02-11 07:55:32,222 - Logits shape: torch.Size([1, 84, 151936])
2025-02-11 07:55:32,222 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,222 - Next token logits device: cuda:0
2025-02-11 07:55:32,222 - Entered do_sample
2025-02-11 07:55:32,222 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,224 - Probs max: 0.31591796875
2025-02-11 07:55:32,226 - Pre-cat
2025-02-11 07:55:32,226 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002]], device='cuda:0')
2025-02-11 07:55:32,233 - Next token: tensor([[68845]], device='cuda:0')
2025-02-11 07:55:32,233 - Current_ids shape: torch.Size([1, 84])
2025-02-11 07:55:32,233 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,233 - Step 53: Generated next token
2025-02-11 07:55:32,234 - Step 53: Updated current_ids
2025-02-11 07:55:32,234 - Step 53: Decoded token text:  reacts
2025-02-11 07:55:32,234 - Step 53: Updated current_phrase
2025-02-11 07:55:32,235 - Step 53: Created step_acts
2025-02-11 07:55:32,235 - Step 53: Added to generation_acts
2025-02-11 07:55:32,235 - Step 53: Updated recent_tokens
2025-02-11 07:55:32,238 - Step 53: Decoded current text
2025-02-11 07:55:32,238 - Step 53: Reset consecutive_fillers
2025-02-11 07:55:32,239 - Step 53: Calculated unique_ratio: 0.9375
2025-02-11 07:55:32,239 - 
Starting step 54
2025-02-11 07:55:32,239 - Current_ids device: cuda:0
2025-02-11 07:55:32,239 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,307 - Model output complete
2025-02-11 07:55:32,307 - Logits shape: torch.Size([1, 85, 151936])
2025-02-11 07:55:32,307 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,307 - Next token logits device: cuda:0
2025-02-11 07:55:32,307 - Entered do_sample
2025-02-11 07:55:32,307 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,310 - Probs max: 0.46435546875
2025-02-11 07:55:32,312 - Pre-cat
2025-02-11 07:55:32,312 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845]], device='cuda:0')
2025-02-11 07:55:32,318 - Next token: tensor([[21303]], device='cuda:0')
2025-02-11 07:55:32,319 - Current_ids shape: torch.Size([1, 85])
2025-02-11 07:55:32,319 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,319 - Step 54: Generated next token
2025-02-11 07:55:32,319 - Step 54: Updated current_ids
2025-02-11 07:55:32,319 - Step 54: Decoded token text:  differently
2025-02-11 07:55:32,319 - Step 54: Updated current_phrase
2025-02-11 07:55:32,320 - Step 54: Created step_acts
2025-02-11 07:55:32,320 - Step 54: Added to generation_acts
2025-02-11 07:55:32,320 - Step 54: Updated recent_tokens
2025-02-11 07:55:32,322 - Step 54: Decoded current text
2025-02-11 07:55:32,322 - Step 54: Reset consecutive_fillers
2025-02-11 07:55:32,322 - Step 54: Calculated unique_ratio: 1.0
2025-02-11 07:55:32,322 - 
Starting step 55
2025-02-11 07:55:32,322 - Current_ids device: cuda:0
2025-02-11 07:55:32,322 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,366 - Model output complete
2025-02-11 07:55:32,367 - Logits shape: torch.Size([1, 86, 151936])
2025-02-11 07:55:32,367 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,367 - Next token logits device: cuda:0
2025-02-11 07:55:32,367 - Entered do_sample
2025-02-11 07:55:32,367 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,370 - Probs max: 0.2568359375
2025-02-11 07:55:32,372 - Pre-cat
2025-02-11 07:55:32,372 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303]], device='cuda:0')
2025-02-11 07:55:32,379 - Next token: tensor([[1091]], device='cuda:0')
2025-02-11 07:55:32,380 - Current_ids shape: torch.Size([1, 86])
2025-02-11 07:55:32,380 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,380 - Step 55: Generated next token
2025-02-11 07:55:32,380 - Step 55: Updated current_ids
2025-02-11 07:55:32,380 - Step 55: Decoded token text:  than
2025-02-11 07:55:32,380 - Step 55: Updated current_phrase
2025-02-11 07:55:32,381 - Step 55: Created step_acts
2025-02-11 07:55:32,381 - Step 55: Added to generation_acts
2025-02-11 07:55:32,382 - Step 55: Updated generated_texts
2025-02-11 07:55:32,383 - Step 55: Updated recent_tokens
2025-02-11 07:55:32,383 - Step 55: Decoded current text
2025-02-11 07:55:32,383 - Step 55: Reset consecutive_fillers
2025-02-11 07:55:32,384 - Step 55: Calculated unique_ratio: 1.0
2025-02-11 07:55:32,384 - 
Starting step 56
2025-02-11 07:55:32,384 - Current_ids device: cuda:0
2025-02-11 07:55:32,384 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,436 - Model output complete
2025-02-11 07:55:32,437 - Logits shape: torch.Size([1, 87, 151936])
2025-02-11 07:55:32,437 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,437 - Next token logits device: cuda:0
2025-02-11 07:55:32,437 - Entered do_sample
2025-02-11 07:55:32,437 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,439 - Probs max: 0.37060546875
2025-02-11 07:55:32,441 - Pre-cat
2025-02-11 07:55:32,441 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091]], device='cuda:0')
2025-02-11 07:55:32,450 - Next token: tensor([[304]], device='cuda:0')
2025-02-11 07:55:32,450 - Current_ids shape: torch.Size([1, 87])
2025-02-11 07:55:32,450 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,451 - Step 56: Generated next token
2025-02-11 07:55:32,451 - Step 56: Updated current_ids
2025-02-11 07:55:32,451 - Step 56: Decoded token text:  in
2025-02-11 07:55:32,451 - Step 56: Updated current_phrase
2025-02-11 07:55:32,452 - Step 56: Created step_acts
2025-02-11 07:55:32,452 - Step 56: Added to generation_acts
2025-02-11 07:55:32,452 - Step 56: Updated recent_tokens
2025-02-11 07:55:32,454 - Step 56: Decoded current text
2025-02-11 07:55:32,454 - Step 56: Reset consecutive_fillers
2025-02-11 07:55:32,455 - Step 56: Calculated unique_ratio: 0.9375
2025-02-11 07:55:32,455 - 
Starting step 57
2025-02-11 07:55:32,455 - Current_ids device: cuda:0
2025-02-11 07:55:32,455 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,509 - Model output complete
2025-02-11 07:55:32,509 - Logits shape: torch.Size([1, 88, 151936])
2025-02-11 07:55:32,509 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,509 - Next token logits device: cuda:0
2025-02-11 07:55:32,510 - Entered do_sample
2025-02-11 07:55:32,510 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,512 - Probs max: 0.43408203125
2025-02-11 07:55:32,513 - Pre-cat
2025-02-11 07:55:32,513 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304]],
       device='cuda:0')
2025-02-11 07:55:32,521 - Next token: tensor([[4622]], device='cuda:0')
2025-02-11 07:55:32,522 - Current_ids shape: torch.Size([1, 88])
2025-02-11 07:55:32,522 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,522 - Step 57: Generated next token
2025-02-11 07:55:32,522 - Step 57: Updated current_ids
2025-02-11 07:55:32,522 - Step 57: Decoded token text:  normal
2025-02-11 07:55:32,523 - Step 57: Updated current_phrase
2025-02-11 07:55:32,523 - Step 57: Created step_acts
2025-02-11 07:55:32,523 - Step 57: Added to generation_acts
2025-02-11 07:55:32,523 - Step 57: Updated recent_tokens
2025-02-11 07:55:32,525 - Step 57: Decoded current text
2025-02-11 07:55:32,525 - Step 57: Reset consecutive_fillers
2025-02-11 07:55:32,526 - Step 57: Calculated unique_ratio: 0.9375
2025-02-11 07:55:32,526 - 
Starting step 58
2025-02-11 07:55:32,526 - Current_ids device: cuda:0
2025-02-11 07:55:32,526 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,563 - Model output complete
2025-02-11 07:55:32,563 - Logits shape: torch.Size([1, 89, 151936])
2025-02-11 07:55:32,563 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,563 - Next token logits device: cuda:0
2025-02-11 07:55:32,563 - Entered do_sample
2025-02-11 07:55:32,564 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,566 - Probs max: 0.272216796875
2025-02-11 07:55:32,568 - Pre-cat
2025-02-11 07:55:32,568 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622]],
       device='cuda:0')
2025-02-11 07:55:32,571 - Next token: tensor([[4682]], device='cuda:0')
2025-02-11 07:55:32,571 - Current_ids shape: torch.Size([1, 89])
2025-02-11 07:55:32,571 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,571 - Step 58: Generated next token
2025-02-11 07:55:32,572 - Step 58: Updated current_ids
2025-02-11 07:55:32,572 - Step 58: Decoded token text:  conditions
2025-02-11 07:55:32,572 - Step 58: Updated current_phrase
2025-02-11 07:55:32,572 - Step 58: Created step_acts
2025-02-11 07:55:32,572 - Step 58: Added to generation_acts
2025-02-11 07:55:32,572 - Step 58: Updated recent_tokens
2025-02-11 07:55:32,574 - Step 58: Decoded current text
2025-02-11 07:55:32,574 - Step 58: Reset consecutive_fillers
2025-02-11 07:55:32,574 - Step 58: Calculated unique_ratio: 0.9375
2025-02-11 07:55:32,574 - 
Starting step 59
2025-02-11 07:55:32,574 - Current_ids device: cuda:0
2025-02-11 07:55:32,574 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,598 - Model output complete
2025-02-11 07:55:32,598 - Logits shape: torch.Size([1, 90, 151936])
2025-02-11 07:55:32,598 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,598 - Next token logits device: cuda:0
2025-02-11 07:55:32,598 - Entered do_sample
2025-02-11 07:55:32,598 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,603 - Probs max: 0.58251953125
2025-02-11 07:55:32,604 - Pre-cat
2025-02-11 07:55:32,604 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682]],
       device='cuda:0')
2025-02-11 07:55:32,607 - Next token: tensor([[1939]], device='cuda:0')
2025-02-11 07:55:32,607 - Current_ids shape: torch.Size([1, 90])
2025-02-11 07:55:32,607 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,607 - Step 59: Generated next token
2025-02-11 07:55:32,608 - Step 59: Updated current_ids
2025-02-11 07:55:32,608 - Step 59: Decoded token text: ?


2025-02-11 07:55:32,608 - Step 59: Updated current_phrase
2025-02-11 07:55:32,608 - Step 59: Created step_acts
2025-02-11 07:55:32,608 - Step 59: Added to generation_acts
2025-02-11 07:55:32,608 - Step 59: Updated recent_tokens
2025-02-11 07:55:32,610 - Step 59: Decoded current text
2025-02-11 07:55:32,610 - Step 59: Reset consecutive_fillers
2025-02-11 07:55:32,610 - Step 59: Calculated unique_ratio: 0.9375
2025-02-11 07:55:32,610 - 
Starting step 60
2025-02-11 07:55:32,610 - Current_ids device: cuda:0
2025-02-11 07:55:32,610 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,635 - Model output complete
2025-02-11 07:55:32,636 - Logits shape: torch.Size([1, 91, 151936])
2025-02-11 07:55:32,636 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,636 - Next token logits device: cuda:0
2025-02-11 07:55:32,636 - Entered do_sample
2025-02-11 07:55:32,636 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,639 - Probs max: 0.24853515625
2025-02-11 07:55:32,640 - Pre-cat
2025-02-11 07:55:32,640 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939]], device='cuda:0')
2025-02-11 07:55:32,646 - Next token: tensor([[40]], device='cuda:0')
2025-02-11 07:55:32,646 - Current_ids shape: torch.Size([1, 91])
2025-02-11 07:55:32,646 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,647 - Step 60: Generated next token
2025-02-11 07:55:32,647 - Step 60: Updated current_ids
2025-02-11 07:55:32,647 - Step 60: Decoded token text: I
2025-02-11 07:55:32,647 - Step 60: Updated current_phrase
2025-02-11 07:55:32,647 - Step 60: Created step_acts
2025-02-11 07:55:32,647 - Step 60: Added to generation_acts
2025-02-11 07:55:32,649 - Step 60: Updated generated_texts
2025-02-11 07:55:32,649 - Step 60: Updated recent_tokens
2025-02-11 07:55:32,650 - Step 60: Decoded current text
2025-02-11 07:55:32,650 - Step 60: Reset consecutive_fillers
2025-02-11 07:55:32,650 - Step 60: Calculated unique_ratio: 0.9375
2025-02-11 07:55:32,650 - 
Starting step 61
2025-02-11 07:55:32,650 - Current_ids device: cuda:0
2025-02-11 07:55:32,650 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,686 - Model output complete
2025-02-11 07:55:32,686 - Logits shape: torch.Size([1, 92, 151936])
2025-02-11 07:55:32,686 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,686 - Next token logits device: cuda:0
2025-02-11 07:55:32,686 - Entered do_sample
2025-02-11 07:55:32,687 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,689 - Probs max: 0.76611328125
2025-02-11 07:55:32,691 - Pre-cat
2025-02-11 07:55:32,691 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40]], device='cuda:0')
2025-02-11 07:55:32,695 - Next token: tensor([[6099]], device='cuda:0')
2025-02-11 07:55:32,695 - Current_ids shape: torch.Size([1, 92])
2025-02-11 07:55:32,695 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,696 - Step 61: Generated next token
2025-02-11 07:55:32,696 - Step 61: Updated current_ids
2025-02-11 07:55:32,696 - Step 61: Decoded token text:  remember
2025-02-11 07:55:32,696 - Step 61: Updated current_phrase
2025-02-11 07:55:32,696 - Step 61: Created step_acts
2025-02-11 07:55:32,696 - Step 61: Added to generation_acts
2025-02-11 07:55:32,696 - Step 61: Updated recent_tokens
2025-02-11 07:55:32,698 - Step 61: Decoded current text
2025-02-11 07:55:32,698 - Step 61: Reset consecutive_fillers
2025-02-11 07:55:32,699 - Step 61: Calculated unique_ratio: 0.9375
2025-02-11 07:55:32,699 - 
Starting step 62
2025-02-11 07:55:32,699 - Current_ids device: cuda:0
2025-02-11 07:55:32,699 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,722 - Model output complete
2025-02-11 07:55:32,722 - Logits shape: torch.Size([1, 93, 151936])
2025-02-11 07:55:32,722 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,722 - Next token logits device: cuda:0
2025-02-11 07:55:32,722 - Entered do_sample
2025-02-11 07:55:32,722 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,727 - Probs max: 0.505859375
2025-02-11 07:55:32,728 - Pre-cat
2025-02-11 07:55:32,728 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099]], device='cuda:0')
2025-02-11 07:55:32,731 - Next token: tensor([[429]], device='cuda:0')
2025-02-11 07:55:32,731 - Current_ids shape: torch.Size([1, 93])
2025-02-11 07:55:32,732 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,732 - Step 62: Generated next token
2025-02-11 07:55:32,732 - Step 62: Updated current_ids
2025-02-11 07:55:32,732 - Step 62: Decoded token text:  that
2025-02-11 07:55:32,732 - Step 62: Updated current_phrase
2025-02-11 07:55:32,732 - Step 62: Created step_acts
2025-02-11 07:55:32,732 - Step 62: Added to generation_acts
2025-02-11 07:55:32,732 - Step 62: Updated recent_tokens
2025-02-11 07:55:32,734 - Step 62: Decoded current text
2025-02-11 07:55:32,734 - Step 62: Reset consecutive_fillers
2025-02-11 07:55:32,734 - Step 62: Calculated unique_ratio: 1.0
2025-02-11 07:55:32,734 - 
Starting step 63
2025-02-11 07:55:32,734 - Current_ids device: cuda:0
2025-02-11 07:55:32,735 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,758 - Model output complete
2025-02-11 07:55:32,758 - Logits shape: torch.Size([1, 94, 151936])
2025-02-11 07:55:32,758 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,758 - Next token logits device: cuda:0
2025-02-11 07:55:32,758 - Entered do_sample
2025-02-11 07:55:32,758 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,763 - Probs max: 0.56982421875
2025-02-11 07:55:32,764 - Pre-cat
2025-02-11 07:55:32,764 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429]], device='cuda:0')
2025-02-11 07:55:32,768 - Next token: tensor([[979]], device='cuda:0')
2025-02-11 07:55:32,768 - Current_ids shape: torch.Size([1, 94])
2025-02-11 07:55:32,768 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,768 - Step 63: Generated next token
2025-02-11 07:55:32,769 - Step 63: Updated current_ids
2025-02-11 07:55:32,769 - Step 63: Decoded token text:  when
2025-02-11 07:55:32,769 - Step 63: Updated current_phrase
2025-02-11 07:55:32,769 - Step 63: Created step_acts
2025-02-11 07:55:32,769 - Step 63: Added to generation_acts
2025-02-11 07:55:32,769 - Step 63: Updated recent_tokens
2025-02-11 07:55:32,771 - Step 63: Decoded current text
2025-02-11 07:55:32,771 - Step 63: Reset consecutive_fillers
2025-02-11 07:55:32,771 - Step 63: Calculated unique_ratio: 1.0
2025-02-11 07:55:32,771 - 
Starting step 64
2025-02-11 07:55:32,771 - Current_ids device: cuda:0
2025-02-11 07:55:32,772 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,798 - Model output complete
2025-02-11 07:55:32,798 - Logits shape: torch.Size([1, 95, 151936])
2025-02-11 07:55:32,798 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,798 - Next token logits device: cuda:0
2025-02-11 07:55:32,798 - Entered do_sample
2025-02-11 07:55:32,798 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,801 - Probs max: 0.461181640625
2025-02-11 07:55:32,802 - Pre-cat
2025-02-11 07:55:32,802 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979]], device='cuda:0')
2025-02-11 07:55:32,808 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:32,809 - Current_ids shape: torch.Size([1, 95])
2025-02-11 07:55:32,809 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,809 - Step 64: Generated next token
2025-02-11 07:55:32,809 - Step 64: Updated current_ids
2025-02-11 07:55:32,809 - Step 64: Decoded token text:  a
2025-02-11 07:55:32,809 - Step 64: Updated current_phrase
2025-02-11 07:55:32,810 - Step 64: Created step_acts
2025-02-11 07:55:32,810 - Step 64: Added to generation_acts
2025-02-11 07:55:32,810 - Step 64: Updated recent_tokens
2025-02-11 07:55:32,812 - Step 64: Decoded current text
2025-02-11 07:55:32,812 - Step 64: Reset consecutive_fillers
2025-02-11 07:55:32,812 - Step 64: Calculated unique_ratio: 1.0
2025-02-11 07:55:32,812 - 
Starting step 65
2025-02-11 07:55:32,812 - Current_ids device: cuda:0
2025-02-11 07:55:32,812 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,843 - Model output complete
2025-02-11 07:55:32,843 - Logits shape: torch.Size([1, 96, 151936])
2025-02-11 07:55:32,843 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,843 - Next token logits device: cuda:0
2025-02-11 07:55:32,843 - Entered do_sample
2025-02-11 07:55:32,843 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,847 - Probs max: 0.9599609375
2025-02-11 07:55:32,848 - Pre-cat
2025-02-11 07:55:32,848 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264]], device='cuda:0')
2025-02-11 07:55:32,852 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:32,853 - Current_ids shape: torch.Size([1, 96])
2025-02-11 07:55:32,853 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,853 - Step 65: Generated next token
2025-02-11 07:55:32,853 - Step 65: Updated current_ids
2025-02-11 07:55:32,853 - Step 65: Decoded token text:  ball
2025-02-11 07:55:32,853 - Step 65: Updated current_phrase
2025-02-11 07:55:32,853 - Step 65: Created step_acts
2025-02-11 07:55:32,853 - Step 65: Added to generation_acts
2025-02-11 07:55:32,855 - Step 65: Updated generated_texts
2025-02-11 07:55:32,855 - Step 65: Updated recent_tokens
2025-02-11 07:55:32,856 - Step 65: Decoded current text
2025-02-11 07:55:32,856 - Step 65: Reset consecutive_fillers
2025-02-11 07:55:32,856 - Step 65: Calculated unique_ratio: 1.0
2025-02-11 07:55:32,856 - 
Starting step 66
2025-02-11 07:55:32,856 - Current_ids device: cuda:0
2025-02-11 07:55:32,856 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,880 - Model output complete
2025-02-11 07:55:32,880 - Logits shape: torch.Size([1, 97, 151936])
2025-02-11 07:55:32,881 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,881 - Next token logits device: cuda:0
2025-02-11 07:55:32,881 - Entered do_sample
2025-02-11 07:55:32,881 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,885 - Probs max: 0.96875
2025-02-11 07:55:32,885 - Pre-cat
2025-02-11 07:55:32,885 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935]],
       device='cuda:0')
2025-02-11 07:55:32,889 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:32,889 - Current_ids shape: torch.Size([1, 97])
2025-02-11 07:55:32,889 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,889 - Step 66: Generated next token
2025-02-11 07:55:32,889 - Step 66: Updated current_ids
2025-02-11 07:55:32,890 - Step 66: Decoded token text:  is
2025-02-11 07:55:32,890 - Step 66: Updated current_phrase
2025-02-11 07:55:32,890 - Step 66: Created step_acts
2025-02-11 07:55:32,890 - Step 66: Added to generation_acts
2025-02-11 07:55:32,890 - Step 66: Updated recent_tokens
2025-02-11 07:55:32,892 - Step 66: Decoded current text
2025-02-11 07:55:32,893 - Step 66: Reset consecutive_fillers
2025-02-11 07:55:32,893 - Step 66: Calculated unique_ratio: 1.0
2025-02-11 07:55:32,893 - 
Starting step 67
2025-02-11 07:55:32,893 - Current_ids device: cuda:0
2025-02-11 07:55:32,893 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,921 - Model output complete
2025-02-11 07:55:32,921 - Logits shape: torch.Size([1, 98, 151936])
2025-02-11 07:55:32,921 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,921 - Next token logits device: cuda:0
2025-02-11 07:55:32,921 - Entered do_sample
2025-02-11 07:55:32,922 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,924 - Probs max: 0.99267578125
2025-02-11 07:55:32,926 - Pre-cat
2025-02-11 07:55:32,926 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:32,933 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:32,933 - Current_ids shape: torch.Size([1, 98])
2025-02-11 07:55:32,933 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:32,933 - Step 67: Generated next token
2025-02-11 07:55:32,933 - Step 67: Updated current_ids
2025-02-11 07:55:32,934 - Step 67: Decoded token text:  thrown
2025-02-11 07:55:32,934 - Step 67: Updated current_phrase
2025-02-11 07:55:32,934 - Step 67: Created step_acts
2025-02-11 07:55:32,934 - Step 67: Added to generation_acts
2025-02-11 07:55:32,934 - Step 67: Updated recent_tokens
2025-02-11 07:55:32,936 - Step 67: Decoded current text
2025-02-11 07:55:32,936 - Step 67: Reset consecutive_fillers
2025-02-11 07:55:32,937 - Step 67: Calculated unique_ratio: 1.0
2025-02-11 07:55:32,937 - 
Starting step 68
2025-02-11 07:55:32,937 - Current_ids device: cuda:0
2025-02-11 07:55:32,937 - Current_ids dtype: torch.int64
2025-02-11 07:55:32,997 - Model output complete
2025-02-11 07:55:32,997 - Logits shape: torch.Size([1, 99, 151936])
2025-02-11 07:55:32,997 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:32,997 - Next token logits device: cuda:0
2025-02-11 07:55:32,997 - Entered do_sample
2025-02-11 07:55:32,997 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,000 - Probs max: 0.51416015625
2025-02-11 07:55:33,002 - Pre-cat
2025-02-11 07:55:33,002 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:55:33,009 - Next token: tensor([[6974]], device='cuda:0')
2025-02-11 07:55:33,009 - Current_ids shape: torch.Size([1, 99])
2025-02-11 07:55:33,009 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,009 - Step 68: Generated next token
2025-02-11 07:55:33,009 - Step 68: Updated current_ids
2025-02-11 07:55:33,010 - Step 68: Decoded token text:  towards
2025-02-11 07:55:33,010 - Step 68: Updated current_phrase
2025-02-11 07:55:33,010 - Step 68: Created step_acts
2025-02-11 07:55:33,010 - Step 68: Added to generation_acts
2025-02-11 07:55:33,011 - Step 68: Updated recent_tokens
2025-02-11 07:55:33,012 - Step 68: Decoded current text
2025-02-11 07:55:33,012 - Step 68: Reset consecutive_fillers
2025-02-11 07:55:33,013 - Step 68: Calculated unique_ratio: 1.0
2025-02-11 07:55:33,013 - 
Starting step 69
2025-02-11 07:55:33,013 - Current_ids device: cuda:0
2025-02-11 07:55:33,013 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,052 - Model output complete
2025-02-11 07:55:33,052 - Logits shape: torch.Size([1, 100, 151936])
2025-02-11 07:55:33,052 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,053 - Next token logits device: cuda:0
2025-02-11 07:55:33,053 - Entered do_sample
2025-02-11 07:55:33,053 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,056 - Probs max: 0.98828125
2025-02-11 07:55:33,057 - Pre-cat
2025-02-11 07:55:33,057 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974]], device='cuda:0')
2025-02-11 07:55:33,061 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:33,061 - Current_ids shape: torch.Size([1, 100])
2025-02-11 07:55:33,061 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,061 - Step 69: Generated next token
2025-02-11 07:55:33,061 - Step 69: Updated current_ids
2025-02-11 07:55:33,061 - Step 69: Decoded token text:  a
2025-02-11 07:55:33,061 - Step 69: Updated current_phrase
2025-02-11 07:55:33,062 - Step 69: Created step_acts
2025-02-11 07:55:33,062 - Step 69: Added to generation_acts
2025-02-11 07:55:33,062 - Step 69: Updated recent_tokens
2025-02-11 07:55:33,064 - Step 69: Decoded current text
2025-02-11 07:55:33,064 - Step 69: Reset consecutive_fillers
2025-02-11 07:55:33,064 - Step 69: Calculated unique_ratio: 0.9375
2025-02-11 07:55:33,064 - 
Starting step 70
2025-02-11 07:55:33,064 - Current_ids device: cuda:0
2025-02-11 07:55:33,064 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,093 - Model output complete
2025-02-11 07:55:33,094 - Logits shape: torch.Size([1, 101, 151936])
2025-02-11 07:55:33,094 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,094 - Next token logits device: cuda:0
2025-02-11 07:55:33,094 - Entered do_sample
2025-02-11 07:55:33,094 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,097 - Probs max: 0.99951171875
2025-02-11 07:55:33,098 - Pre-cat
2025-02-11 07:55:33,098 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264]], device='cuda:0')
2025-02-11 07:55:33,102 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:33,102 - Current_ids shape: torch.Size([1, 101])
2025-02-11 07:55:33,102 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,102 - Step 70: Generated next token
2025-02-11 07:55:33,102 - Step 70: Updated current_ids
2025-02-11 07:55:33,103 - Step 70: Decoded token text:  wall
2025-02-11 07:55:33,103 - Step 70: Updated current_phrase
2025-02-11 07:55:33,103 - Step 70: Created step_acts
2025-02-11 07:55:33,103 - Step 70: Added to generation_acts
2025-02-11 07:55:33,105 - Step 70: Updated generated_texts
2025-02-11 07:55:33,105 - Step 70: Updated recent_tokens
2025-02-11 07:55:33,105 - Step 70: Decoded current text
2025-02-11 07:55:33,105 - Step 70: Reset consecutive_fillers
2025-02-11 07:55:33,106 - Step 70: Calculated unique_ratio: 0.9375
2025-02-11 07:55:33,106 - 
Starting step 71
2025-02-11 07:55:33,106 - Current_ids device: cuda:0
2025-02-11 07:55:33,106 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,130 - Model output complete
2025-02-11 07:55:33,130 - Logits shape: torch.Size([1, 102, 151936])
2025-02-11 07:55:33,130 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,130 - Next token logits device: cuda:0
2025-02-11 07:55:33,130 - Entered do_sample
2025-02-11 07:55:33,130 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,135 - Probs max: 0.873046875
2025-02-11 07:55:33,136 - Pre-cat
2025-02-11 07:55:33,136 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002]], device='cuda:0')
2025-02-11 07:55:33,140 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:33,141 - Current_ids shape: torch.Size([1, 102])
2025-02-11 07:55:33,141 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,141 - Step 71: Generated next token
2025-02-11 07:55:33,141 - Step 71: Updated current_ids
2025-02-11 07:55:33,141 - Step 71: Decoded token text: ,
2025-02-11 07:55:33,141 - Step 71: Updated current_phrase
2025-02-11 07:55:33,142 - Step 71: Created step_acts
2025-02-11 07:55:33,142 - Step 71: Added to generation_acts
2025-02-11 07:55:33,142 - Step 71: Updated recent_tokens
2025-02-11 07:55:33,144 - Step 71: Found phrase end token
2025-02-11 07:55:33,144 - Step 71: Updated recent_phrases
2025-02-11 07:55:33,144 - Step 71: Calculated similarity: 0.25
2025-02-11 07:55:33,144 - Step 71: Decoded current text
2025-02-11 07:55:33,144 - Step 71: Reset consecutive_fillers
2025-02-11 07:55:33,144 - Step 71: Calculated unique_ratio: 0.9375
2025-02-11 07:55:33,144 - 
Starting step 72
2025-02-11 07:55:33,144 - Current_ids device: cuda:0
2025-02-11 07:55:33,144 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,172 - Model output complete
2025-02-11 07:55:33,172 - Logits shape: torch.Size([1, 103, 151936])
2025-02-11 07:55:33,172 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,172 - Next token logits device: cuda:0
2025-02-11 07:55:33,172 - Entered do_sample
2025-02-11 07:55:33,173 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,175 - Probs max: 0.74658203125
2025-02-11 07:55:33,177 - Pre-cat
2025-02-11 07:55:33,177 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11]], device='cuda:0')
2025-02-11 07:55:33,185 - Next token: tensor([[432]], device='cuda:0')
2025-02-11 07:55:33,185 - Current_ids shape: torch.Size([1, 103])
2025-02-11 07:55:33,185 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,185 - Step 72: Generated next token
2025-02-11 07:55:33,185 - Step 72: Updated current_ids
2025-02-11 07:55:33,186 - Step 72: Decoded token text:  it
2025-02-11 07:55:33,186 - Step 72: Updated current_phrase
2025-02-11 07:55:33,186 - Step 72: Created step_acts
2025-02-11 07:55:33,187 - Step 72: Added to generation_acts
2025-02-11 07:55:33,187 - Step 72: Updated recent_tokens
2025-02-11 07:55:33,189 - Step 72: Decoded current text
2025-02-11 07:55:33,189 - Step 72: Reset consecutive_fillers
2025-02-11 07:55:33,189 - Step 72: Calculated unique_ratio: 0.9375
2025-02-11 07:55:33,190 - 
Starting step 73
2025-02-11 07:55:33,190 - Current_ids device: cuda:0
2025-02-11 07:55:33,190 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,218 - Model output complete
2025-02-11 07:55:33,218 - Logits shape: torch.Size([1, 104, 151936])
2025-02-11 07:55:33,218 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,218 - Next token logits device: cuda:0
2025-02-11 07:55:33,218 - Entered do_sample
2025-02-11 07:55:33,218 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,222 - Probs max: 0.51025390625
2025-02-11 07:55:33,223 - Pre-cat
2025-02-11 07:55:33,223 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432]], device='cuda:0')
2025-02-11 07:55:33,226 - Next token: tensor([[293]], device='cuda:0')
2025-02-11 07:55:33,227 - Current_ids shape: torch.Size([1, 104])
2025-02-11 07:55:33,227 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,227 - Step 73: Generated next token
2025-02-11 07:55:33,227 - Step 73: Updated current_ids
2025-02-11 07:55:33,227 - Step 73: Decoded token text:  b
2025-02-11 07:55:33,227 - Step 73: Updated current_phrase
2025-02-11 07:55:33,227 - Step 73: Created step_acts
2025-02-11 07:55:33,227 - Step 73: Added to generation_acts
2025-02-11 07:55:33,228 - Step 73: Updated recent_tokens
2025-02-11 07:55:33,229 - Step 73: Decoded current text
2025-02-11 07:55:33,229 - Step 73: Reset consecutive_fillers
2025-02-11 07:55:33,230 - Step 73: Calculated unique_ratio: 0.9375
2025-02-11 07:55:33,230 - 
Starting step 74
2025-02-11 07:55:33,230 - Current_ids device: cuda:0
2025-02-11 07:55:33,230 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,260 - Model output complete
2025-02-11 07:55:33,260 - Logits shape: torch.Size([1, 105, 151936])
2025-02-11 07:55:33,260 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,260 - Next token logits device: cuda:0
2025-02-11 07:55:33,260 - Entered do_sample
2025-02-11 07:55:33,260 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,263 - Probs max: 1.0
2025-02-11 07:55:33,265 - Pre-cat
2025-02-11 07:55:33,265 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293]], device='cuda:0')
2025-02-11 07:55:33,273 - Next token: tensor([[29944]], device='cuda:0')
2025-02-11 07:55:33,273 - Current_ids shape: torch.Size([1, 105])
2025-02-11 07:55:33,273 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,274 - Step 74: Generated next token
2025-02-11 07:55:33,274 - Step 74: Updated current_ids
2025-02-11 07:55:33,274 - Step 74: Decoded token text: ounces
2025-02-11 07:55:33,274 - Step 74: Updated current_phrase
2025-02-11 07:55:33,275 - Step 74: Created step_acts
2025-02-11 07:55:33,275 - Step 74: Added to generation_acts
2025-02-11 07:55:33,275 - Step 74: Updated recent_tokens
2025-02-11 07:55:33,277 - Step 74: Decoded current text
2025-02-11 07:55:33,277 - Step 74: Reset consecutive_fillers
2025-02-11 07:55:33,277 - Step 74: Calculated unique_ratio: 0.9375
2025-02-11 07:55:33,277 - 
Starting step 75
2025-02-11 07:55:33,277 - Current_ids device: cuda:0
2025-02-11 07:55:33,277 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,341 - Model output complete
2025-02-11 07:55:33,341 - Logits shape: torch.Size([1, 106, 151936])
2025-02-11 07:55:33,341 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,341 - Next token logits device: cuda:0
2025-02-11 07:55:33,341 - Entered do_sample
2025-02-11 07:55:33,341 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,344 - Probs max: 0.71533203125
2025-02-11 07:55:33,346 - Pre-cat
2025-02-11 07:55:33,346 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944]],
       device='cuda:0')
2025-02-11 07:55:33,353 - Next token: tensor([[1007]], device='cuda:0')
2025-02-11 07:55:33,354 - Current_ids shape: torch.Size([1, 106])
2025-02-11 07:55:33,354 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,354 - Step 75: Generated next token
2025-02-11 07:55:33,354 - Step 75: Updated current_ids
2025-02-11 07:55:33,354 - Step 75: Decoded token text:  off
2025-02-11 07:55:33,355 - Step 75: Updated current_phrase
2025-02-11 07:55:33,355 - Step 75: Created step_acts
2025-02-11 07:55:33,355 - Step 75: Added to generation_acts
2025-02-11 07:55:33,357 - Step 75: Updated generated_texts
2025-02-11 07:55:33,357 - Step 75: Updated recent_tokens
2025-02-11 07:55:33,357 - Step 75: Decoded current text
2025-02-11 07:55:33,357 - Step 75: Reset consecutive_fillers
2025-02-11 07:55:33,358 - Step 75: Calculated unique_ratio: 0.9375
2025-02-11 07:55:33,358 - 
Starting step 76
2025-02-11 07:55:33,358 - Current_ids device: cuda:0
2025-02-11 07:55:33,358 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,398 - Model output complete
2025-02-11 07:55:33,398 - Logits shape: torch.Size([1, 107, 151936])
2025-02-11 07:55:33,398 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,398 - Next token logits device: cuda:0
2025-02-11 07:55:33,398 - Entered do_sample
2025-02-11 07:55:33,398 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,403 - Probs max: 0.38623046875
2025-02-11 07:55:33,404 - Pre-cat
2025-02-11 07:55:33,404 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007]],
       device='cuda:0')
2025-02-11 07:55:33,411 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:33,412 - Current_ids shape: torch.Size([1, 107])
2025-02-11 07:55:33,412 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,412 - Step 76: Generated next token
2025-02-11 07:55:33,412 - Step 76: Updated current_ids
2025-02-11 07:55:33,413 - Step 76: Decoded token text:  the
2025-02-11 07:55:33,413 - Step 76: Updated current_phrase
2025-02-11 07:55:33,413 - Step 76: Created step_acts
2025-02-11 07:55:33,413 - Step 76: Added to generation_acts
2025-02-11 07:55:33,414 - Step 76: Updated recent_tokens
2025-02-11 07:55:33,416 - Step 76: Decoded current text
2025-02-11 07:55:33,416 - Step 76: Reset consecutive_fillers
2025-02-11 07:55:33,417 - Step 76: Calculated unique_ratio: 0.9375
2025-02-11 07:55:33,417 - 
Starting step 77
2025-02-11 07:55:33,417 - Current_ids device: cuda:0
2025-02-11 07:55:33,417 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,441 - Model output complete
2025-02-11 07:55:33,442 - Logits shape: torch.Size([1, 108, 151936])
2025-02-11 07:55:33,442 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,442 - Next token logits device: cuda:0
2025-02-11 07:55:33,442 - Entered do_sample
2025-02-11 07:55:33,442 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,447 - Probs max: 1.0
2025-02-11 07:55:33,448 - Pre-cat
2025-02-11 07:55:33,448 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279]],
       device='cuda:0')
2025-02-11 07:55:33,452 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:33,452 - Current_ids shape: torch.Size([1, 108])
2025-02-11 07:55:33,452 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,452 - Step 77: Generated next token
2025-02-11 07:55:33,452 - Step 77: Updated current_ids
2025-02-11 07:55:33,452 - Step 77: Decoded token text:  wall
2025-02-11 07:55:33,452 - Step 77: Updated current_phrase
2025-02-11 07:55:33,453 - Step 77: Created step_acts
2025-02-11 07:55:33,453 - Step 77: Added to generation_acts
2025-02-11 07:55:33,453 - Step 77: Updated recent_tokens
2025-02-11 07:55:33,455 - Step 77: Decoded current text
2025-02-11 07:55:33,455 - Step 77: Reset consecutive_fillers
2025-02-11 07:55:33,455 - Step 77: Calculated unique_ratio: 0.875
2025-02-11 07:55:33,455 - 
Starting step 78
2025-02-11 07:55:33,455 - Current_ids device: cuda:0
2025-02-11 07:55:33,455 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,478 - Model output complete
2025-02-11 07:55:33,478 - Logits shape: torch.Size([1, 109, 151936])
2025-02-11 07:55:33,478 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,478 - Next token logits device: cuda:0
2025-02-11 07:55:33,478 - Entered do_sample
2025-02-11 07:55:33,478 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,485 - Probs max: 0.47509765625
2025-02-11 07:55:33,485 - Pre-cat
2025-02-11 07:55:33,485 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002]], device='cuda:0')
2025-02-11 07:55:33,489 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:55:33,489 - Current_ids shape: torch.Size([1, 109])
2025-02-11 07:55:33,490 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,490 - Step 78: Generated next token
2025-02-11 07:55:33,490 - Step 78: Updated current_ids
2025-02-11 07:55:33,490 - Step 78: Decoded token text: .
2025-02-11 07:55:33,490 - Step 78: Updated current_phrase
2025-02-11 07:55:33,490 - Step 78: Created step_acts
2025-02-11 07:55:33,490 - Step 78: Added to generation_acts
2025-02-11 07:55:33,490 - Step 78: Updated recent_tokens
2025-02-11 07:55:33,492 - Step 78: Found phrase end token
2025-02-11 07:55:33,492 - Step 78: Updated recent_phrases
2025-02-11 07:55:33,492 - Step 78: Calculated similarity: 0.2
2025-02-11 07:55:33,492 - Step 78: Decoded current text
2025-02-11 07:55:33,493 - Step 78: Reset consecutive_fillers
2025-02-11 07:55:33,493 - Step 78: Calculated unique_ratio: 0.875
2025-02-11 07:55:33,493 - 
Starting step 79
2025-02-11 07:55:33,493 - Current_ids device: cuda:0
2025-02-11 07:55:33,493 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,517 - Model output complete
2025-02-11 07:55:33,517 - Logits shape: torch.Size([1, 110, 151936])
2025-02-11 07:55:33,517 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,517 - Next token logits device: cuda:0
2025-02-11 07:55:33,517 - Entered do_sample
2025-02-11 07:55:33,517 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,522 - Probs max: 0.7861328125
2025-02-11 07:55:33,522 - Pre-cat
2025-02-11 07:55:33,523 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13]], device='cuda:0')
2025-02-11 07:55:33,526 - Next token: tensor([[1988]], device='cuda:0')
2025-02-11 07:55:33,526 - Current_ids shape: torch.Size([1, 110])
2025-02-11 07:55:33,526 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,527 - Step 79: Generated next token
2025-02-11 07:55:33,527 - Step 79: Updated current_ids
2025-02-11 07:55:33,527 - Step 79: Decoded token text:  But
2025-02-11 07:55:33,527 - Step 79: Updated current_phrase
2025-02-11 07:55:33,527 - Step 79: Created step_acts
2025-02-11 07:55:33,527 - Step 79: Added to generation_acts
2025-02-11 07:55:33,527 - Step 79: Updated recent_tokens
2025-02-11 07:55:33,529 - Step 79: Decoded current text
2025-02-11 07:55:33,529 - Step 79: Reset consecutive_fillers
2025-02-11 07:55:33,529 - Step 79: Calculated unique_ratio: 0.875
2025-02-11 07:55:33,529 - 
Starting step 80
2025-02-11 07:55:33,530 - Current_ids device: cuda:0
2025-02-11 07:55:33,530 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,553 - Model output complete
2025-02-11 07:55:33,553 - Logits shape: torch.Size([1, 111, 151936])
2025-02-11 07:55:33,553 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,553 - Next token logits device: cuda:0
2025-02-11 07:55:33,553 - Entered do_sample
2025-02-11 07:55:33,553 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,559 - Probs max: 0.564453125
2025-02-11 07:55:33,560 - Pre-cat
2025-02-11 07:55:33,560 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988]], device='cuda:0')
2025-02-11 07:55:33,565 - Next token: tensor([[979]], device='cuda:0')
2025-02-11 07:55:33,565 - Current_ids shape: torch.Size([1, 111])
2025-02-11 07:55:33,565 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,565 - Step 80: Generated next token
2025-02-11 07:55:33,565 - Step 80: Updated current_ids
2025-02-11 07:55:33,565 - Step 80: Decoded token text:  when
2025-02-11 07:55:33,565 - Step 80: Updated current_phrase
2025-02-11 07:55:33,566 - Step 80: Created step_acts
2025-02-11 07:55:33,566 - Step 80: Added to generation_acts
2025-02-11 07:55:33,568 - Step 80: Updated generated_texts
2025-02-11 07:55:33,568 - Step 80: Updated recent_tokens
2025-02-11 07:55:33,568 - Step 80: Decoded current text
2025-02-11 07:55:33,568 - Step 80: Reset consecutive_fillers
2025-02-11 07:55:33,568 - Step 80: Calculated unique_ratio: 0.9375
2025-02-11 07:55:33,568 - 
Starting step 81
2025-02-11 07:55:33,568 - Current_ids device: cuda:0
2025-02-11 07:55:33,568 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,594 - Model output complete
2025-02-11 07:55:33,594 - Logits shape: torch.Size([1, 112, 151936])
2025-02-11 07:55:33,594 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,594 - Next token logits device: cuda:0
2025-02-11 07:55:33,594 - Entered do_sample
2025-02-11 07:55:33,595 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,598 - Probs max: 0.806640625
2025-02-11 07:55:33,599 - Pre-cat
2025-02-11 07:55:33,599 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979]], device='cuda:0')
2025-02-11 07:55:33,603 - Next token: tensor([[432]], device='cuda:0')
2025-02-11 07:55:33,603 - Current_ids shape: torch.Size([1, 112])
2025-02-11 07:55:33,603 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,603 - Step 81: Generated next token
2025-02-11 07:55:33,603 - Step 81: Updated current_ids
2025-02-11 07:55:33,604 - Step 81: Decoded token text:  it
2025-02-11 07:55:33,604 - Step 81: Updated current_phrase
2025-02-11 07:55:33,604 - Step 81: Created step_acts
2025-02-11 07:55:33,604 - Step 81: Added to generation_acts
2025-02-11 07:55:33,604 - Step 81: Updated recent_tokens
2025-02-11 07:55:33,606 - Step 81: Decoded current text
2025-02-11 07:55:33,606 - Step 81: Reset consecutive_fillers
2025-02-11 07:55:33,606 - Step 81: Calculated unique_ratio: 0.875
2025-02-11 07:55:33,606 - 
Starting step 82
2025-02-11 07:55:33,606 - Current_ids device: cuda:0
2025-02-11 07:55:33,606 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,630 - Model output complete
2025-02-11 07:55:33,630 - Logits shape: torch.Size([1, 113, 151936])
2025-02-11 07:55:33,630 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,630 - Next token logits device: cuda:0
2025-02-11 07:55:33,630 - Entered do_sample
2025-02-11 07:55:33,630 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,637 - Probs max: 0.9951171875
2025-02-11 07:55:33,638 - Pre-cat
2025-02-11 07:55:33,638 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432]], device='cuda:0')
2025-02-11 07:55:33,642 - Next token: tensor([[594]], device='cuda:0')
2025-02-11 07:55:33,642 - Current_ids shape: torch.Size([1, 113])
2025-02-11 07:55:33,642 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,642 - Step 82: Generated next token
2025-02-11 07:55:33,643 - Step 82: Updated current_ids
2025-02-11 07:55:33,643 - Step 82: Decoded token text: 's
2025-02-11 07:55:33,643 - Step 82: Updated current_phrase
2025-02-11 07:55:33,643 - Step 82: Created step_acts
2025-02-11 07:55:33,643 - Step 82: Added to generation_acts
2025-02-11 07:55:33,643 - Step 82: Updated recent_tokens
2025-02-11 07:55:33,645 - Step 82: Decoded current text
2025-02-11 07:55:33,645 - Step 82: Reset consecutive_fillers
2025-02-11 07:55:33,645 - Step 82: Calculated unique_ratio: 0.875
2025-02-11 07:55:33,645 - 
Starting step 83
2025-02-11 07:55:33,645 - Current_ids device: cuda:0
2025-02-11 07:55:33,646 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,669 - Model output complete
2025-02-11 07:55:33,669 - Logits shape: torch.Size([1, 114, 151936])
2025-02-11 07:55:33,669 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,669 - Next token logits device: cuda:0
2025-02-11 07:55:33,669 - Entered do_sample
2025-02-11 07:55:33,669 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,675 - Probs max: 0.98681640625
2025-02-11 07:55:33,676 - Pre-cat
2025-02-11 07:55:33,676 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594]], device='cuda:0')
2025-02-11 07:55:33,684 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:33,685 - Current_ids shape: torch.Size([1, 114])
2025-02-11 07:55:33,685 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,685 - Step 83: Generated next token
2025-02-11 07:55:33,685 - Step 83: Updated current_ids
2025-02-11 07:55:33,685 - Step 83: Decoded token text:  thrown
2025-02-11 07:55:33,685 - Step 83: Updated current_phrase
2025-02-11 07:55:33,686 - Step 83: Created step_acts
2025-02-11 07:55:33,686 - Step 83: Added to generation_acts
2025-02-11 07:55:33,686 - Step 83: Updated recent_tokens
2025-02-11 07:55:33,688 - Step 83: Decoded current text
2025-02-11 07:55:33,688 - Step 83: Reset consecutive_fillers
2025-02-11 07:55:33,688 - Step 83: Calculated unique_ratio: 0.875
2025-02-11 07:55:33,688 - 
Starting step 84
2025-02-11 07:55:33,688 - Current_ids device: cuda:0
2025-02-11 07:55:33,688 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,713 - Model output complete
2025-02-11 07:55:33,714 - Logits shape: torch.Size([1, 115, 151936])
2025-02-11 07:55:33,714 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,714 - Next token logits device: cuda:0
2025-02-11 07:55:33,714 - Entered do_sample
2025-02-11 07:55:33,714 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,718 - Probs max: 0.45458984375
2025-02-11 07:55:33,720 - Pre-cat
2025-02-11 07:55:33,720 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989]],
       device='cuda:0')
2025-02-11 07:55:33,729 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:33,730 - Current_ids shape: torch.Size([1, 115])
2025-02-11 07:55:33,730 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,730 - Step 84: Generated next token
2025-02-11 07:55:33,730 - Step 84: Updated current_ids
2025-02-11 07:55:33,730 - Step 84: Decoded token text:  very
2025-02-11 07:55:33,730 - Step 84: Updated current_phrase
2025-02-11 07:55:33,731 - Step 84: Created step_acts
2025-02-11 07:55:33,731 - Step 84: Added to generation_acts
2025-02-11 07:55:33,731 - Step 84: Updated recent_tokens
2025-02-11 07:55:33,733 - Step 84: Decoded current text
2025-02-11 07:55:33,733 - Step 84: Reset consecutive_fillers
2025-02-11 07:55:33,734 - Step 84: Calculated unique_ratio: 0.875
2025-02-11 07:55:33,734 - 
Starting step 85
2025-02-11 07:55:33,734 - Current_ids device: cuda:0
2025-02-11 07:55:33,734 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,806 - Model output complete
2025-02-11 07:55:33,806 - Logits shape: torch.Size([1, 116, 151936])
2025-02-11 07:55:33,806 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,806 - Next token logits device: cuda:0
2025-02-11 07:55:33,806 - Entered do_sample
2025-02-11 07:55:33,806 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,808 - Probs max: 0.98779296875
2025-02-11 07:55:33,810 - Pre-cat
2025-02-11 07:55:33,811 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602]],
       device='cuda:0')
2025-02-11 07:55:33,819 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:55:33,820 - Current_ids shape: torch.Size([1, 116])
2025-02-11 07:55:33,820 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,820 - Step 85: Generated next token
2025-02-11 07:55:33,820 - Step 85: Updated current_ids
2025-02-11 07:55:33,820 - Step 85: Decoded token text:  fast
2025-02-11 07:55:33,821 - Step 85: Updated current_phrase
2025-02-11 07:55:33,821 - Step 85: Created step_acts
2025-02-11 07:55:33,821 - Step 85: Added to generation_acts
2025-02-11 07:55:33,823 - Step 85: Updated generated_texts
2025-02-11 07:55:33,823 - Step 85: Updated recent_tokens
2025-02-11 07:55:33,824 - Step 85: Decoded current text
2025-02-11 07:55:33,824 - Step 85: Reset consecutive_fillers
2025-02-11 07:55:33,824 - Step 85: Calculated unique_ratio: 0.875
2025-02-11 07:55:33,824 - 
Starting step 86
2025-02-11 07:55:33,824 - Current_ids device: cuda:0
2025-02-11 07:55:33,824 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,847 - Model output complete
2025-02-11 07:55:33,847 - Logits shape: torch.Size([1, 117, 151936])
2025-02-11 07:55:33,847 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,847 - Next token logits device: cuda:0
2025-02-11 07:55:33,847 - Entered do_sample
2025-02-11 07:55:33,848 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,855 - Probs max: 0.998046875
2025-02-11 07:55:33,856 - Pre-cat
2025-02-11 07:55:33,856 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937]],
       device='cuda:0')
2025-02-11 07:55:33,861 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:33,861 - Current_ids shape: torch.Size([1, 117])
2025-02-11 07:55:33,861 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,861 - Step 86: Generated next token
2025-02-11 07:55:33,861 - Step 86: Updated current_ids
2025-02-11 07:55:33,861 - Step 86: Decoded token text: ,
2025-02-11 07:55:33,861 - Step 86: Updated current_phrase
2025-02-11 07:55:33,862 - Step 86: Created step_acts
2025-02-11 07:55:33,862 - Step 86: Added to generation_acts
2025-02-11 07:55:33,862 - Step 86: Updated recent_tokens
2025-02-11 07:55:33,864 - Step 86: Found phrase end token
2025-02-11 07:55:33,864 - Step 86: Updated recent_phrases
2025-02-11 07:55:33,864 - Step 86: Calculated similarity: 0.0
2025-02-11 07:55:33,864 - Step 86: Decoded current text
2025-02-11 07:55:33,864 - Step 86: Reset consecutive_fillers
2025-02-11 07:55:33,864 - Step 86: Calculated unique_ratio: 0.875
2025-02-11 07:55:33,864 - 
Starting step 87
2025-02-11 07:55:33,864 - Current_ids device: cuda:0
2025-02-11 07:55:33,865 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,893 - Model output complete
2025-02-11 07:55:33,893 - Logits shape: torch.Size([1, 118, 151936])
2025-02-11 07:55:33,893 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,893 - Next token logits device: cuda:0
2025-02-11 07:55:33,893 - Entered do_sample
2025-02-11 07:55:33,893 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,899 - Probs max: 0.3818359375
2025-02-11 07:55:33,900 - Pre-cat
2025-02-11 07:55:33,900 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937,
             11]], device='cuda:0')
2025-02-11 07:55:33,905 - Next token: tensor([[7196]], device='cuda:0')
2025-02-11 07:55:33,905 - Current_ids shape: torch.Size([1, 118])
2025-02-11 07:55:33,905 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,905 - Step 87: Generated next token
2025-02-11 07:55:33,905 - Step 87: Updated current_ids
2025-02-11 07:55:33,905 - Step 87: Decoded token text:  maybe
2025-02-11 07:55:33,906 - Step 87: Updated current_phrase
2025-02-11 07:55:33,906 - Step 87: Created step_acts
2025-02-11 07:55:33,906 - Step 87: Added to generation_acts
2025-02-11 07:55:33,906 - Step 87: Updated recent_tokens
2025-02-11 07:55:33,908 - Step 87: Decoded current text
2025-02-11 07:55:33,908 - Step 87: Reset consecutive_fillers
2025-02-11 07:55:33,908 - Step 87: Calculated unique_ratio: 0.9375
2025-02-11 07:55:33,908 - 
Starting step 88
2025-02-11 07:55:33,908 - Current_ids device: cuda:0
2025-02-11 07:55:33,908 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,935 - Model output complete
2025-02-11 07:55:33,935 - Logits shape: torch.Size([1, 119, 151936])
2025-02-11 07:55:33,936 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,936 - Next token logits device: cuda:0
2025-02-11 07:55:33,936 - Entered do_sample
2025-02-11 07:55:33,936 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,938 - Probs max: 0.8056640625
2025-02-11 07:55:33,940 - Pre-cat
2025-02-11 07:55:33,940 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937,
             11,   7196]], device='cuda:0')
2025-02-11 07:55:33,946 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:33,947 - Current_ids shape: torch.Size([1, 119])
2025-02-11 07:55:33,947 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,947 - Step 88: Generated next token
2025-02-11 07:55:33,947 - Step 88: Updated current_ids
2025-02-11 07:55:33,947 - Step 88: Decoded token text:  the
2025-02-11 07:55:33,947 - Step 88: Updated current_phrase
2025-02-11 07:55:33,948 - Step 88: Created step_acts
2025-02-11 07:55:33,948 - Step 88: Added to generation_acts
2025-02-11 07:55:33,948 - Step 88: Updated recent_tokens
2025-02-11 07:55:33,950 - Step 88: Decoded current text
2025-02-11 07:55:33,950 - Step 88: Reset consecutive_fillers
2025-02-11 07:55:33,950 - Step 88: Calculated unique_ratio: 0.9375
2025-02-11 07:55:33,950 - 
Starting step 89
2025-02-11 07:55:33,950 - Current_ids device: cuda:0
2025-02-11 07:55:33,950 - Current_ids dtype: torch.int64
2025-02-11 07:55:33,974 - Model output complete
2025-02-11 07:55:33,974 - Logits shape: torch.Size([1, 120, 151936])
2025-02-11 07:55:33,974 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,974 - Next token logits device: cuda:0
2025-02-11 07:55:33,974 - Entered do_sample
2025-02-11 07:55:33,974 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:33,980 - Probs max: 0.548828125
2025-02-11 07:55:33,981 - Pre-cat
2025-02-11 07:55:33,981 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937,
             11,   7196,    279]], device='cuda:0')
2025-02-11 07:55:33,985 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:33,985 - Current_ids shape: torch.Size([1, 120])
2025-02-11 07:55:33,985 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:33,985 - Step 89: Generated next token
2025-02-11 07:55:33,985 - Step 89: Updated current_ids
2025-02-11 07:55:33,986 - Step 89: Decoded token text:  wall
2025-02-11 07:55:33,986 - Step 89: Updated current_phrase
2025-02-11 07:55:33,986 - Step 89: Created step_acts
2025-02-11 07:55:33,986 - Step 89: Added to generation_acts
2025-02-11 07:55:33,986 - Step 89: Updated recent_tokens
2025-02-11 07:55:33,988 - Step 89: Decoded current text
2025-02-11 07:55:33,988 - Step 89: Reset consecutive_fillers
2025-02-11 07:55:33,988 - Step 89: Calculated unique_ratio: 0.875
2025-02-11 07:55:33,988 - 
Starting step 90
2025-02-11 07:55:33,988 - Current_ids device: cuda:0
2025-02-11 07:55:33,988 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,018 - Model output complete
2025-02-11 07:55:34,018 - Logits shape: torch.Size([1, 121, 151936])
2025-02-11 07:55:34,018 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,018 - Next token logits device: cuda:0
2025-02-11 07:55:34,018 - Entered do_sample
2025-02-11 07:55:34,019 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,021 - Probs max: 0.8779296875
2025-02-11 07:55:34,023 - Pre-cat
2025-02-11 07:55:34,023 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937,
             11,   7196,    279,   7002]], device='cuda:0')
2025-02-11 07:55:34,032 - Next token: tensor([[3171]], device='cuda:0')
2025-02-11 07:55:34,032 - Current_ids shape: torch.Size([1, 121])
2025-02-11 07:55:34,032 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,032 - Step 90: Generated next token
2025-02-11 07:55:34,033 - Step 90: Updated current_ids
2025-02-11 07:55:34,033 - Step 90: Decoded token text:  doesn
2025-02-11 07:55:34,033 - Step 90: Updated current_phrase
2025-02-11 07:55:34,034 - Step 90: Created step_acts
2025-02-11 07:55:34,034 - Step 90: Added to generation_acts
2025-02-11 07:55:34,035 - Step 90: Updated generated_texts
2025-02-11 07:55:34,036 - Step 90: Updated recent_tokens
2025-02-11 07:55:34,036 - Step 90: Decoded current text
2025-02-11 07:55:34,036 - Step 90: Reset consecutive_fillers
2025-02-11 07:55:34,036 - Step 90: Calculated unique_ratio: 0.875
2025-02-11 07:55:34,036 - 
Starting step 91
2025-02-11 07:55:34,036 - Current_ids device: cuda:0
2025-02-11 07:55:34,037 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,102 - Model output complete
2025-02-11 07:55:34,102 - Logits shape: torch.Size([1, 122, 151936])
2025-02-11 07:55:34,102 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,102 - Next token logits device: cuda:0
2025-02-11 07:55:34,103 - Entered do_sample
2025-02-11 07:55:34,103 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,106 - Probs max: 0.998046875
2025-02-11 07:55:34,106 - Pre-cat
2025-02-11 07:55:34,106 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937,
             11,   7196,    279,   7002,   3171]], device='cuda:0')
2025-02-11 07:55:34,111 - Next token: tensor([[944]], device='cuda:0')
2025-02-11 07:55:34,111 - Current_ids shape: torch.Size([1, 122])
2025-02-11 07:55:34,111 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,111 - Step 91: Generated next token
2025-02-11 07:55:34,112 - Step 91: Updated current_ids
2025-02-11 07:55:34,112 - Step 91: Decoded token text: 't
2025-02-11 07:55:34,112 - Step 91: Updated current_phrase
2025-02-11 07:55:34,112 - Step 91: Created step_acts
2025-02-11 07:55:34,112 - Step 91: Added to generation_acts
2025-02-11 07:55:34,112 - Step 91: Updated recent_tokens
2025-02-11 07:55:34,114 - Step 91: Decoded current text
2025-02-11 07:55:34,114 - Step 91: Reset consecutive_fillers
2025-02-11 07:55:34,115 - Step 91: Calculated unique_ratio: 0.875
2025-02-11 07:55:34,115 - 
Starting step 92
2025-02-11 07:55:34,115 - Current_ids device: cuda:0
2025-02-11 07:55:34,115 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,145 - Model output complete
2025-02-11 07:55:34,145 - Logits shape: torch.Size([1, 123, 151936])
2025-02-11 07:55:34,146 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,146 - Next token logits device: cuda:0
2025-02-11 07:55:34,146 - Entered do_sample
2025-02-11 07:55:34,146 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,148 - Probs max: 0.6513671875
2025-02-11 07:55:34,150 - Pre-cat
2025-02-11 07:55:34,151 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937,
             11,   7196,    279,   7002,   3171,    944]], device='cuda:0')
2025-02-11 07:55:34,160 - Next token: tensor([[13767]], device='cuda:0')
2025-02-11 07:55:34,160 - Current_ids shape: torch.Size([1, 123])
2025-02-11 07:55:34,160 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,160 - Step 92: Generated next token
2025-02-11 07:55:34,160 - Step 92: Updated current_ids
2025-02-11 07:55:34,161 - Step 92: Decoded token text:  react
2025-02-11 07:55:34,161 - Step 92: Updated current_phrase
2025-02-11 07:55:34,161 - Step 92: Created step_acts
2025-02-11 07:55:34,161 - Step 92: Added to generation_acts
2025-02-11 07:55:34,161 - Step 92: Updated recent_tokens
2025-02-11 07:55:34,163 - Step 92: Decoded current text
2025-02-11 07:55:34,163 - Step 92: Reset consecutive_fillers
2025-02-11 07:55:34,164 - Step 92: Calculated unique_ratio: 0.9375
2025-02-11 07:55:34,164 - 
Starting step 93
2025-02-11 07:55:34,164 - Current_ids device: cuda:0
2025-02-11 07:55:34,164 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,204 - Model output complete
2025-02-11 07:55:34,204 - Logits shape: torch.Size([1, 124, 151936])
2025-02-11 07:55:34,204 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,204 - Next token logits device: cuda:0
2025-02-11 07:55:34,204 - Entered do_sample
2025-02-11 07:55:34,204 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,207 - Probs max: 0.44287109375
2025-02-11 07:55:34,209 - Pre-cat
2025-02-11 07:55:34,209 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937,
             11,   7196,    279,   7002,   3171,    944,  13767]],
       device='cuda:0')
2025-02-11 07:55:34,218 - Next token: tensor([[438]], device='cuda:0')
2025-02-11 07:55:34,218 - Current_ids shape: torch.Size([1, 124])
2025-02-11 07:55:34,218 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,218 - Step 93: Generated next token
2025-02-11 07:55:34,219 - Step 93: Updated current_ids
2025-02-11 07:55:34,219 - Step 93: Decoded token text:  as
2025-02-11 07:55:34,219 - Step 93: Updated current_phrase
2025-02-11 07:55:34,220 - Step 93: Created step_acts
2025-02-11 07:55:34,220 - Step 93: Added to generation_acts
2025-02-11 07:55:34,220 - Step 93: Updated recent_tokens
2025-02-11 07:55:34,222 - Step 93: Decoded current text
2025-02-11 07:55:34,222 - Step 93: Reset consecutive_fillers
2025-02-11 07:55:34,223 - Step 93: Calculated unique_ratio: 1.0
2025-02-11 07:55:34,223 - 
Starting step 94
2025-02-11 07:55:34,223 - Current_ids device: cuda:0
2025-02-11 07:55:34,223 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,281 - Model output complete
2025-02-11 07:55:34,281 - Logits shape: torch.Size([1, 125, 151936])
2025-02-11 07:55:34,281 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,281 - Next token logits device: cuda:0
2025-02-11 07:55:34,281 - Entered do_sample
2025-02-11 07:55:34,281 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,286 - Probs max: 0.68798828125
2025-02-11 07:55:34,287 - Pre-cat
2025-02-11 07:55:34,287 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937,
             11,   7196,    279,   7002,   3171,    944,  13767,    438]],
       device='cuda:0')
2025-02-11 07:55:34,291 - Next token: tensor([[1753]], device='cuda:0')
2025-02-11 07:55:34,291 - Current_ids shape: torch.Size([1, 125])
2025-02-11 07:55:34,291 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,291 - Step 94: Generated next token
2025-02-11 07:55:34,291 - Step 94: Updated current_ids
2025-02-11 07:55:34,292 - Step 94: Decoded token text:  much
2025-02-11 07:55:34,292 - Step 94: Updated current_phrase
2025-02-11 07:55:34,292 - Step 94: Created step_acts
2025-02-11 07:55:34,292 - Step 94: Added to generation_acts
2025-02-11 07:55:34,292 - Step 94: Updated recent_tokens
2025-02-11 07:55:34,294 - Step 94: Decoded current text
2025-02-11 07:55:34,294 - Step 94: Reset consecutive_fillers
2025-02-11 07:55:34,294 - Step 94: Calculated unique_ratio: 1.0
2025-02-11 07:55:34,294 - 
Starting step 95
2025-02-11 07:55:34,294 - Current_ids device: cuda:0
2025-02-11 07:55:34,294 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,318 - Model output complete
2025-02-11 07:55:34,318 - Logits shape: torch.Size([1, 126, 151936])
2025-02-11 07:55:34,318 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,318 - Next token logits device: cuda:0
2025-02-11 07:55:34,318 - Entered do_sample
2025-02-11 07:55:34,318 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,325 - Probs max: 0.393798828125
2025-02-11 07:55:34,325 - Pre-cat
2025-02-11 07:55:34,325 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937,
             11,   7196,    279,   7002,   3171,    944,  13767,    438,   1753]],
       device='cuda:0')
2025-02-11 07:55:34,330 - Next token: tensor([[13]], device='cuda:0')
2025-02-11 07:55:34,330 - Current_ids shape: torch.Size([1, 126])
2025-02-11 07:55:34,330 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,330 - Step 95: Generated next token
2025-02-11 07:55:34,330 - Step 95: Updated current_ids
2025-02-11 07:55:34,330 - Step 95: Decoded token text: .
2025-02-11 07:55:34,330 - Step 95: Updated current_phrase
2025-02-11 07:55:34,331 - Step 95: Created step_acts
2025-02-11 07:55:34,331 - Step 95: Added to generation_acts
2025-02-11 07:55:34,333 - Step 95: Updated generated_texts
2025-02-11 07:55:34,333 - Step 95: Updated recent_tokens
2025-02-11 07:55:34,333 - Step 95: Found phrase end token
2025-02-11 07:55:34,333 - Step 95: Updated recent_phrases
2025-02-11 07:55:34,333 - Step 95: Calculated similarity: 0.0
2025-02-11 07:55:34,333 - Step 95: Decoded current text
2025-02-11 07:55:34,333 - Step 95: Reset consecutive_fillers
2025-02-11 07:55:34,334 - Step 95: Calculated unique_ratio: 1.0
2025-02-11 07:55:34,334 - 
Starting step 96
2025-02-11 07:55:34,334 - Current_ids device: cuda:0
2025-02-11 07:55:34,334 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,364 - Model output complete
2025-02-11 07:55:34,365 - Logits shape: torch.Size([1, 127, 151936])
2025-02-11 07:55:34,365 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,365 - Next token logits device: cuda:0
2025-02-11 07:55:34,365 - Entered do_sample
2025-02-11 07:55:34,365 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,367 - Probs max: 0.62353515625
2025-02-11 07:55:34,369 - Pre-cat
2025-02-11 07:55:34,369 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937,
             11,   7196,    279,   7002,   3171,    944,  13767,    438,   1753,
             13]], device='cuda:0')
2025-02-11 07:55:34,374 - Next token: tensor([[2521]], device='cuda:0')
2025-02-11 07:55:34,375 - Current_ids shape: torch.Size([1, 127])
2025-02-11 07:55:34,375 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,375 - Step 96: Generated next token
2025-02-11 07:55:34,375 - Step 96: Updated current_ids
2025-02-11 07:55:34,375 - Step 96: Decoded token text:  Or
2025-02-11 07:55:34,376 - Step 96: Updated current_phrase
2025-02-11 07:55:34,376 - Step 96: Created step_acts
2025-02-11 07:55:34,376 - Step 96: Added to generation_acts
2025-02-11 07:55:34,376 - Step 96: Updated recent_tokens
2025-02-11 07:55:34,378 - Step 96: Decoded current text
2025-02-11 07:55:34,378 - Step 96: Reset consecutive_fillers
2025-02-11 07:55:34,379 - Step 96: Calculated unique_ratio: 1.0
2025-02-11 07:55:34,379 - 
Starting step 97
2025-02-11 07:55:34,379 - Current_ids device: cuda:0
2025-02-11 07:55:34,379 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,412 - Model output complete
2025-02-11 07:55:34,413 - Logits shape: torch.Size([1, 128, 151936])
2025-02-11 07:55:34,413 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,413 - Next token logits device: cuda:0
2025-02-11 07:55:34,413 - Entered do_sample
2025-02-11 07:55:34,413 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,416 - Probs max: 0.806640625
2025-02-11 07:55:34,417 - Pre-cat
2025-02-11 07:55:34,417 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937,
             11,   7196,    279,   7002,   3171,    944,  13767,    438,   1753,
             13,   2521]], device='cuda:0')
2025-02-11 07:55:34,423 - Next token: tensor([[8365]], device='cuda:0')
2025-02-11 07:55:34,423 - Current_ids shape: torch.Size([1, 128])
2025-02-11 07:55:34,423 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,423 - Step 97: Generated next token
2025-02-11 07:55:34,423 - Step 97: Updated current_ids
2025-02-11 07:55:34,423 - Step 97: Decoded token text:  perhaps
2025-02-11 07:55:34,423 - Step 97: Updated current_phrase
2025-02-11 07:55:34,424 - Step 97: Created step_acts
2025-02-11 07:55:34,424 - Step 97: Added to generation_acts
2025-02-11 07:55:34,424 - Step 97: Updated recent_tokens
2025-02-11 07:55:34,426 - Step 97: Decoded current text
2025-02-11 07:55:34,426 - Step 97: Reset consecutive_fillers
2025-02-11 07:55:34,426 - Step 97: Calculated unique_ratio: 1.0
2025-02-11 07:55:34,426 - 
Starting step 98
2025-02-11 07:55:34,426 - Current_ids device: cuda:0
2025-02-11 07:55:34,426 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,458 - Model output complete
2025-02-11 07:55:34,458 - Logits shape: torch.Size([1, 129, 151936])
2025-02-11 07:55:34,458 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,459 - Next token logits device: cuda:0
2025-02-11 07:55:34,459 - Entered do_sample
2025-02-11 07:55:34,459 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,464 - Probs max: 0.75048828125
2025-02-11 07:55:34,466 - Pre-cat
2025-02-11 07:55:34,466 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937,
             11,   7196,    279,   7002,   3171,    944,  13767,    438,   1753,
             13,   2521,   8365]], device='cuda:0')
2025-02-11 07:55:34,475 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:34,476 - Current_ids shape: torch.Size([1, 129])
2025-02-11 07:55:34,476 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,476 - Step 98: Generated next token
2025-02-11 07:55:34,476 - Step 98: Updated current_ids
2025-02-11 07:55:34,476 - Step 98: Decoded token text:  the
2025-02-11 07:55:34,476 - Step 98: Updated current_phrase
2025-02-11 07:55:34,477 - Step 98: Created step_acts
2025-02-11 07:55:34,477 - Step 98: Added to generation_acts
2025-02-11 07:55:34,477 - Step 98: Updated recent_tokens
2025-02-11 07:55:34,479 - Step 98: Decoded current text
2025-02-11 07:55:34,480 - Step 98: Reset consecutive_fillers
2025-02-11 07:55:34,480 - Step 98: Calculated unique_ratio: 0.9375
2025-02-11 07:55:34,480 - 
Starting step 99
2025-02-11 07:55:34,480 - Current_ids device: cuda:0
2025-02-11 07:55:34,480 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,513 - Model output complete
2025-02-11 07:55:34,514 - Logits shape: torch.Size([1, 130, 151936])
2025-02-11 07:55:34,514 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,514 - Next token logits device: cuda:0
2025-02-11 07:55:34,514 - Entered do_sample
2025-02-11 07:55:34,514 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,527 - Probs max: 0.740234375
2025-02-11 07:55:34,527 - Pre-cat
2025-02-11 07:55:34,528 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,   1416,    279,   4935,    374,  14989,
           1602,   4937,     11,    279,   7002,    686,    387,   1112,    362,
              8,  12236,    785,   2606,    525,    362,      8,  60353,    425,
              8,  60353,    356,      8,  60353,    422,  39025,   1939,  14190,
             11,    358,   1184,    311,   7071,    700,    279,   4396,   4226,
             13,   2980,    498,  10339,   3170,    304,   1741,    264,  15048,
             11,    279,   7002,  68845,  21303,   1091,    304,   4622,   4682,
           1939,     40,   6099,    429,    979,    264,   4935,    374,  14989,
           6974,    264,   7002,     11,    432,    293,  29944,   1007,    279,
           7002,     13,   1988,    979,    432,    594,  14989,   1602,   4937,
             11,   7196,    279,   7002,   3171,    944,  13767,    438,   1753,
             13,   2521,   8365,    279]], device='cuda:0')
2025-02-11 07:55:34,533 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:34,533 - Current_ids shape: torch.Size([1, 130])
2025-02-11 07:55:34,534 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,534 - Step 99: Generated next token
2025-02-11 07:55:34,534 - Step 99: Updated current_ids
2025-02-11 07:55:34,534 - Step 99: Decoded token text:  ball
2025-02-11 07:55:34,534 - Step 99: Updated current_phrase
2025-02-11 07:55:34,534 - Step 99: Created step_acts
2025-02-11 07:55:34,534 - Step 99: Added to generation_acts
2025-02-11 07:55:34,537 - Step 99: Updated generated_texts
2025-02-11 07:55:34,537 - Step 99: Updated recent_tokens
2025-02-11 07:55:34,537 - Step 99: Decoded current text
2025-02-11 07:55:34,537 - Step 99: Reset consecutive_fillers
2025-02-11 07:55:34,537 - Step 99: Calculated unique_ratio: 0.9375
2025-02-11 07:55:34,647 - 
Starting step 0
2025-02-11 07:55:34,647 - Current_ids device: cuda:0
2025-02-11 07:55:34,647 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,674 - Model output complete
2025-02-11 07:55:34,675 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:34,675 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,675 - Next token logits device: cuda:0
2025-02-11 07:55:34,675 - Entered do_sample
2025-02-11 07:55:34,675 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,677 - Probs max: 0.50341796875
2025-02-11 07:55:34,678 - Pre-cat
2025-02-11 07:55:34,678 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:34,680 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:34,680 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:34,680 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,680 - Step 0: Generated next token
2025-02-11 07:55:34,680 - Step 0: Updated current_ids
2025-02-11 07:55:34,681 - Step 0: Decoded token text:  The
2025-02-11 07:55:34,681 - Step 0: Updated current_phrase
2025-02-11 07:55:34,681 - Step 0: Created step_acts
2025-02-11 07:55:34,681 - Step 0: Added to generation_acts
2025-02-11 07:55:34,682 - Step 0: Updated generated_texts
2025-02-11 07:55:34,683 - Step 0: Updated recent_tokens
2025-02-11 07:55:34,683 - Step 0: Decoded current text
2025-02-11 07:55:34,683 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:34,683 - 
Starting step 1
2025-02-11 07:55:34,683 - Current_ids device: cuda:0
2025-02-11 07:55:34,683 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,709 - Model output complete
2025-02-11 07:55:34,709 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:34,709 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,709 - Next token logits device: cuda:0
2025-02-11 07:55:34,709 - Entered do_sample
2025-02-11 07:55:34,709 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,712 - Probs max: 0.83837890625
2025-02-11 07:55:34,713 - Pre-cat
2025-02-11 07:55:34,713 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:55:34,715 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:34,716 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:34,716 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,716 - Step 1: Generated next token
2025-02-11 07:55:34,716 - Step 1: Updated current_ids
2025-02-11 07:55:34,716 - Step 1: Decoded token text:  ball
2025-02-11 07:55:34,717 - Step 1: Updated current_phrase
2025-02-11 07:55:34,717 - Step 1: Created step_acts
2025-02-11 07:55:34,717 - Step 1: Added to generation_acts
2025-02-11 07:55:34,717 - Step 1: Updated recent_tokens
2025-02-11 07:55:34,719 - Step 1: Decoded current text
2025-02-11 07:55:34,719 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:34,719 - 
Starting step 2
2025-02-11 07:55:34,719 - Current_ids device: cuda:0
2025-02-11 07:55:34,719 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,812 - Model output complete
2025-02-11 07:55:34,812 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:34,812 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,812 - Next token logits device: cuda:0
2025-02-11 07:55:34,812 - Entered do_sample
2025-02-11 07:55:34,813 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,815 - Probs max: 0.583984375
2025-02-11 07:55:34,817 - Pre-cat
2025-02-11 07:55:34,817 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:55:34,820 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:34,820 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:34,821 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,821 - Step 2: Generated next token
2025-02-11 07:55:34,821 - Step 2: Updated current_ids
2025-02-11 07:55:34,821 - Step 2: Decoded token text:  is
2025-02-11 07:55:34,821 - Step 2: Updated current_phrase
2025-02-11 07:55:34,821 - Step 2: Created step_acts
2025-02-11 07:55:34,821 - Step 2: Added to generation_acts
2025-02-11 07:55:34,822 - Step 2: Updated recent_tokens
2025-02-11 07:55:34,823 - Step 2: Decoded current text
2025-02-11 07:55:34,823 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:34,823 - 
Starting step 3
2025-02-11 07:55:34,823 - Current_ids device: cuda:0
2025-02-11 07:55:34,823 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,863 - Model output complete
2025-02-11 07:55:34,863 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:34,863 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,863 - Next token logits device: cuda:0
2025-02-11 07:55:34,863 - Entered do_sample
2025-02-11 07:55:34,863 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,866 - Probs max: 0.59521484375
2025-02-11 07:55:34,867 - Pre-cat
2025-02-11 07:55:34,867 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:34,868 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:34,869 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:34,869 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,869 - Step 3: Generated next token
2025-02-11 07:55:34,869 - Step 3: Updated current_ids
2025-02-11 07:55:34,869 - Step 3: Decoded token text:  thrown
2025-02-11 07:55:34,869 - Step 3: Updated current_phrase
2025-02-11 07:55:34,870 - Step 3: Created step_acts
2025-02-11 07:55:34,870 - Step 3: Added to generation_acts
2025-02-11 07:55:34,870 - Step 3: Updated recent_tokens
2025-02-11 07:55:34,871 - Step 3: Decoded current text
2025-02-11 07:55:34,871 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:34,871 - 
Starting step 4
2025-02-11 07:55:34,871 - Current_ids device: cuda:0
2025-02-11 07:55:34,871 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,896 - Model output complete
2025-02-11 07:55:34,896 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:34,896 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,896 - Next token logits device: cuda:0
2025-02-11 07:55:34,896 - Entered do_sample
2025-02-11 07:55:34,896 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,898 - Probs max: 0.67431640625
2025-02-11 07:55:34,899 - Pre-cat
2025-02-11 07:55:34,899 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:55:34,901 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:55:34,901 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:34,901 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,901 - Step 4: Generated next token
2025-02-11 07:55:34,901 - Step 4: Updated current_ids
2025-02-11 07:55:34,901 - Step 4: Decoded token text:  at
2025-02-11 07:55:34,902 - Step 4: Updated current_phrase
2025-02-11 07:55:34,902 - Step 4: Created step_acts
2025-02-11 07:55:34,902 - Step 4: Added to generation_acts
2025-02-11 07:55:34,902 - Step 4: Updated recent_tokens
2025-02-11 07:55:34,903 - Step 4: Decoded current text
2025-02-11 07:55:34,903 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:34,903 - 
Starting step 5
2025-02-11 07:55:34,903 - Current_ids device: cuda:0
2025-02-11 07:55:34,903 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,927 - Model output complete
2025-02-11 07:55:34,928 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:34,928 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,928 - Next token logits device: cuda:0
2025-02-11 07:55:34,928 - Entered do_sample
2025-02-11 07:55:34,928 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:34,930 - Probs max: 0.58251953125
2025-02-11 07:55:34,931 - Pre-cat
2025-02-11 07:55:34,931 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518]],
       device='cuda:0')
2025-02-11 07:55:34,934 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:34,935 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:34,935 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:34,935 - Step 5: Generated next token
2025-02-11 07:55:34,936 - Step 5: Updated current_ids
2025-02-11 07:55:34,936 - Step 5: Decoded token text:  the
2025-02-11 07:55:34,936 - Step 5: Updated current_phrase
2025-02-11 07:55:34,937 - Step 5: Created step_acts
2025-02-11 07:55:34,937 - Step 5: Added to generation_acts
2025-02-11 07:55:34,938 - Step 5: Updated generated_texts
2025-02-11 07:55:34,938 - Step 5: Updated recent_tokens
2025-02-11 07:55:34,939 - Step 5: Decoded current text
2025-02-11 07:55:34,939 - Step 5: Incremented consecutive_fillers to 1
2025-02-11 07:55:34,939 - 
Starting step 6
2025-02-11 07:55:34,939 - Current_ids device: cuda:0
2025-02-11 07:55:34,939 - Current_ids dtype: torch.int64
2025-02-11 07:55:34,999 - Model output complete
2025-02-11 07:55:34,999 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:35,000 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,000 - Next token logits device: cuda:0
2025-02-11 07:55:35,000 - Entered do_sample
2025-02-11 07:55:35,000 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,002 - Probs max: 0.98828125
2025-02-11 07:55:35,002 - Pre-cat
2025-02-11 07:55:35,002 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            279]], device='cuda:0')
2025-02-11 07:55:35,005 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:35,005 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:35,005 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,005 - Step 6: Generated next token
2025-02-11 07:55:35,005 - Step 6: Updated current_ids
2025-02-11 07:55:35,005 - Step 6: Decoded token text:  wall
2025-02-11 07:55:35,005 - Step 6: Updated current_phrase
2025-02-11 07:55:35,006 - Step 6: Created step_acts
2025-02-11 07:55:35,006 - Step 6: Added to generation_acts
2025-02-11 07:55:35,006 - Step 6: Updated recent_tokens
2025-02-11 07:55:35,007 - Step 6: Decoded current text
2025-02-11 07:55:35,007 - Step 6: Incremented consecutive_fillers to 2
2025-02-11 07:55:35,008 - 
Starting step 7
2025-02-11 07:55:35,008 - Current_ids device: cuda:0
2025-02-11 07:55:35,008 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,034 - Model output complete
2025-02-11 07:55:35,034 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:35,034 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,034 - Next token logits device: cuda:0
2025-02-11 07:55:35,034 - Entered do_sample
2025-02-11 07:55:35,034 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,037 - Probs max: 0.90087890625
2025-02-11 07:55:35,038 - Pre-cat
2025-02-11 07:55:35,038 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            279,   7002]], device='cuda:0')
2025-02-11 07:55:35,039 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:35,040 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:35,040 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,040 - Step 7: Generated next token
2025-02-11 07:55:35,040 - Step 7: Updated current_ids
2025-02-11 07:55:35,040 - Step 7: Decoded token text:  very
2025-02-11 07:55:35,040 - Step 7: Updated current_phrase
2025-02-11 07:55:35,040 - Step 7: Created step_acts
2025-02-11 07:55:35,041 - Step 7: Added to generation_acts
2025-02-11 07:55:35,041 - Step 7: Updated recent_tokens
2025-02-11 07:55:35,042 - Step 7: Decoded current text
2025-02-11 07:55:35,042 - Step 7: Incremented consecutive_fillers to 3
2025-02-11 07:55:35,151 - 
Starting step 0
2025-02-11 07:55:35,152 - Current_ids device: cuda:0
2025-02-11 07:55:35,152 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,179 - Model output complete
2025-02-11 07:55:35,179 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:35,179 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,179 - Next token logits device: cuda:0
2025-02-11 07:55:35,179 - Entered do_sample
2025-02-11 07:55:35,179 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,182 - Probs max: 0.50341796875
2025-02-11 07:55:35,187 - Pre-cat
2025-02-11 07:55:35,187 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:35,190 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:35,191 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:35,191 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,191 - Step 0: Generated next token
2025-02-11 07:55:35,191 - Step 0: Updated current_ids
2025-02-11 07:55:35,191 - Step 0: Decoded token text:  The
2025-02-11 07:55:35,191 - Step 0: Updated current_phrase
2025-02-11 07:55:35,192 - Step 0: Created step_acts
2025-02-11 07:55:35,192 - Step 0: Added to generation_acts
2025-02-11 07:55:35,193 - Step 0: Updated generated_texts
2025-02-11 07:55:35,193 - Step 0: Updated recent_tokens
2025-02-11 07:55:35,194 - Step 0: Decoded current text
2025-02-11 07:55:35,194 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:35,194 - 
Starting step 1
2025-02-11 07:55:35,194 - Current_ids device: cuda:0
2025-02-11 07:55:35,194 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,220 - Model output complete
2025-02-11 07:55:35,220 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:35,220 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,220 - Next token logits device: cuda:0
2025-02-11 07:55:35,220 - Entered do_sample
2025-02-11 07:55:35,221 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,223 - Probs max: 0.83837890625
2025-02-11 07:55:35,224 - Pre-cat
2025-02-11 07:55:35,224 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:55:35,225 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:35,225 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:35,225 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,226 - Step 1: Generated next token
2025-02-11 07:55:35,226 - Step 1: Updated current_ids
2025-02-11 07:55:35,226 - Step 1: Decoded token text:  ball
2025-02-11 07:55:35,226 - Step 1: Updated current_phrase
2025-02-11 07:55:35,226 - Step 1: Created step_acts
2025-02-11 07:55:35,226 - Step 1: Added to generation_acts
2025-02-11 07:55:35,226 - Step 1: Updated recent_tokens
2025-02-11 07:55:35,228 - Step 1: Decoded current text
2025-02-11 07:55:35,228 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:35,228 - 
Starting step 2
2025-02-11 07:55:35,228 - Current_ids device: cuda:0
2025-02-11 07:55:35,228 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,255 - Model output complete
2025-02-11 07:55:35,255 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:35,255 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,255 - Next token logits device: cuda:0
2025-02-11 07:55:35,256 - Entered do_sample
2025-02-11 07:55:35,256 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,259 - Probs max: 0.583984375
2025-02-11 07:55:35,259 - Pre-cat
2025-02-11 07:55:35,259 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:55:35,261 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:35,261 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:35,261 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,261 - Step 2: Generated next token
2025-02-11 07:55:35,261 - Step 2: Updated current_ids
2025-02-11 07:55:35,261 - Step 2: Decoded token text:  is
2025-02-11 07:55:35,261 - Step 2: Updated current_phrase
2025-02-11 07:55:35,262 - Step 2: Created step_acts
2025-02-11 07:55:35,262 - Step 2: Added to generation_acts
2025-02-11 07:55:35,262 - Step 2: Updated recent_tokens
2025-02-11 07:55:35,263 - Step 2: Decoded current text
2025-02-11 07:55:35,263 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:35,263 - 
Starting step 3
2025-02-11 07:55:35,263 - Current_ids device: cuda:0
2025-02-11 07:55:35,263 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,287 - Model output complete
2025-02-11 07:55:35,287 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:35,287 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,287 - Next token logits device: cuda:0
2025-02-11 07:55:35,287 - Entered do_sample
2025-02-11 07:55:35,288 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,290 - Probs max: 0.59521484375
2025-02-11 07:55:35,290 - Pre-cat
2025-02-11 07:55:35,290 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:35,292 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:35,292 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:35,292 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,292 - Step 3: Generated next token
2025-02-11 07:55:35,292 - Step 3: Updated current_ids
2025-02-11 07:55:35,292 - Step 3: Decoded token text:  thrown
2025-02-11 07:55:35,292 - Step 3: Updated current_phrase
2025-02-11 07:55:35,293 - Step 3: Created step_acts
2025-02-11 07:55:35,293 - Step 3: Added to generation_acts
2025-02-11 07:55:35,293 - Step 3: Updated recent_tokens
2025-02-11 07:55:35,294 - Step 3: Decoded current text
2025-02-11 07:55:35,294 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:35,294 - 
Starting step 4
2025-02-11 07:55:35,294 - Current_ids device: cuda:0
2025-02-11 07:55:35,294 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,318 - Model output complete
2025-02-11 07:55:35,318 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:35,319 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,319 - Next token logits device: cuda:0
2025-02-11 07:55:35,319 - Entered do_sample
2025-02-11 07:55:35,319 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,321 - Probs max: 0.67431640625
2025-02-11 07:55:35,322 - Pre-cat
2025-02-11 07:55:35,322 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:55:35,323 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:55:35,323 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:35,323 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,323 - Step 4: Generated next token
2025-02-11 07:55:35,324 - Step 4: Updated current_ids
2025-02-11 07:55:35,324 - Step 4: Decoded token text:  at
2025-02-11 07:55:35,324 - Step 4: Updated current_phrase
2025-02-11 07:55:35,324 - Step 4: Created step_acts
2025-02-11 07:55:35,324 - Step 4: Added to generation_acts
2025-02-11 07:55:35,324 - Step 4: Updated recent_tokens
2025-02-11 07:55:35,326 - Step 4: Decoded current text
2025-02-11 07:55:35,326 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:35,326 - 
Starting step 5
2025-02-11 07:55:35,326 - Current_ids device: cuda:0
2025-02-11 07:55:35,326 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,379 - Model output complete
2025-02-11 07:55:35,379 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:35,379 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,379 - Next token logits device: cuda:0
2025-02-11 07:55:35,379 - Entered do_sample
2025-02-11 07:55:35,379 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,381 - Probs max: 0.58251953125
2025-02-11 07:55:35,383 - Pre-cat
2025-02-11 07:55:35,383 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518]],
       device='cuda:0')
2025-02-11 07:55:35,387 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:35,388 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:35,388 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,388 - Step 5: Generated next token
2025-02-11 07:55:35,388 - Step 5: Updated current_ids
2025-02-11 07:55:35,389 - Step 5: Decoded token text:  the
2025-02-11 07:55:35,389 - Step 5: Updated current_phrase
2025-02-11 07:55:35,389 - Step 5: Created step_acts
2025-02-11 07:55:35,389 - Step 5: Added to generation_acts
2025-02-11 07:55:35,391 - Step 5: Updated generated_texts
2025-02-11 07:55:35,391 - Step 5: Updated recent_tokens
2025-02-11 07:55:35,391 - Step 5: Decoded current text
2025-02-11 07:55:35,391 - Step 5: Incremented consecutive_fillers to 1
2025-02-11 07:55:35,391 - 
Starting step 6
2025-02-11 07:55:35,392 - Current_ids device: cuda:0
2025-02-11 07:55:35,392 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,447 - Model output complete
2025-02-11 07:55:35,447 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:35,447 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,447 - Next token logits device: cuda:0
2025-02-11 07:55:35,447 - Entered do_sample
2025-02-11 07:55:35,447 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,449 - Probs max: 0.98828125
2025-02-11 07:55:35,452 - Pre-cat
2025-02-11 07:55:35,452 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            279]], device='cuda:0')
2025-02-11 07:55:35,455 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:35,456 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:35,456 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,456 - Step 6: Generated next token
2025-02-11 07:55:35,456 - Step 6: Updated current_ids
2025-02-11 07:55:35,456 - Step 6: Decoded token text:  wall
2025-02-11 07:55:35,456 - Step 6: Updated current_phrase
2025-02-11 07:55:35,457 - Step 6: Created step_acts
2025-02-11 07:55:35,457 - Step 6: Added to generation_acts
2025-02-11 07:55:35,457 - Step 6: Updated recent_tokens
2025-02-11 07:55:35,459 - Step 6: Decoded current text
2025-02-11 07:55:35,459 - Step 6: Incremented consecutive_fillers to 2
2025-02-11 07:55:35,459 - 
Starting step 7
2025-02-11 07:55:35,459 - Current_ids device: cuda:0
2025-02-11 07:55:35,459 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,488 - Model output complete
2025-02-11 07:55:35,489 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:35,489 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,489 - Next token logits device: cuda:0
2025-02-11 07:55:35,489 - Entered do_sample
2025-02-11 07:55:35,489 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,491 - Probs max: 0.90087890625
2025-02-11 07:55:35,492 - Pre-cat
2025-02-11 07:55:35,492 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            279,   7002]], device='cuda:0')
2025-02-11 07:55:35,493 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:35,494 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:35,494 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,494 - Step 7: Generated next token
2025-02-11 07:55:35,494 - Step 7: Updated current_ids
2025-02-11 07:55:35,494 - Step 7: Decoded token text:  very
2025-02-11 07:55:35,494 - Step 7: Updated current_phrase
2025-02-11 07:55:35,495 - Step 7: Created step_acts
2025-02-11 07:55:35,495 - Step 7: Added to generation_acts
2025-02-11 07:55:35,495 - Step 7: Updated recent_tokens
2025-02-11 07:55:35,496 - Step 7: Decoded current text
2025-02-11 07:55:35,496 - Step 7: Incremented consecutive_fillers to 3
2025-02-11 07:55:35,598 - 
Starting step 0
2025-02-11 07:55:35,598 - Current_ids device: cuda:0
2025-02-11 07:55:35,598 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,628 - Model output complete
2025-02-11 07:55:35,628 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:35,628 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,628 - Next token logits device: cuda:0
2025-02-11 07:55:35,628 - Entered do_sample
2025-02-11 07:55:35,628 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,631 - Probs max: 0.50341796875
2025-02-11 07:55:35,632 - Pre-cat
2025-02-11 07:55:35,632 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:35,635 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:35,636 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:35,636 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,636 - Step 0: Generated next token
2025-02-11 07:55:35,636 - Step 0: Updated current_ids
2025-02-11 07:55:35,636 - Step 0: Decoded token text:  The
2025-02-11 07:55:35,636 - Step 0: Updated current_phrase
2025-02-11 07:55:35,637 - Step 0: Created step_acts
2025-02-11 07:55:35,637 - Step 0: Added to generation_acts
2025-02-11 07:55:35,638 - Step 0: Updated generated_texts
2025-02-11 07:55:35,638 - Step 0: Updated recent_tokens
2025-02-11 07:55:35,638 - Step 0: Decoded current text
2025-02-11 07:55:35,639 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:35,639 - 
Starting step 1
2025-02-11 07:55:35,639 - Current_ids device: cuda:0
2025-02-11 07:55:35,639 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,677 - Model output complete
2025-02-11 07:55:35,677 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:35,677 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,677 - Next token logits device: cuda:0
2025-02-11 07:55:35,678 - Entered do_sample
2025-02-11 07:55:35,678 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,680 - Probs max: 0.83837890625
2025-02-11 07:55:35,681 - Pre-cat
2025-02-11 07:55:35,681 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:55:35,683 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:35,683 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:35,683 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,683 - Step 1: Generated next token
2025-02-11 07:55:35,683 - Step 1: Updated current_ids
2025-02-11 07:55:35,684 - Step 1: Decoded token text:  ball
2025-02-11 07:55:35,684 - Step 1: Updated current_phrase
2025-02-11 07:55:35,684 - Step 1: Created step_acts
2025-02-11 07:55:35,684 - Step 1: Added to generation_acts
2025-02-11 07:55:35,684 - Step 1: Updated recent_tokens
2025-02-11 07:55:35,685 - Step 1: Decoded current text
2025-02-11 07:55:35,686 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:35,686 - 
Starting step 2
2025-02-11 07:55:35,686 - Current_ids device: cuda:0
2025-02-11 07:55:35,686 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,743 - Model output complete
2025-02-11 07:55:35,743 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:35,743 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,743 - Next token logits device: cuda:0
2025-02-11 07:55:35,743 - Entered do_sample
2025-02-11 07:55:35,743 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,746 - Probs max: 0.583984375
2025-02-11 07:55:35,747 - Pre-cat
2025-02-11 07:55:35,747 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:55:35,748 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:35,749 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:35,749 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,749 - Step 2: Generated next token
2025-02-11 07:55:35,749 - Step 2: Updated current_ids
2025-02-11 07:55:35,749 - Step 2: Decoded token text:  is
2025-02-11 07:55:35,749 - Step 2: Updated current_phrase
2025-02-11 07:55:35,749 - Step 2: Created step_acts
2025-02-11 07:55:35,749 - Step 2: Added to generation_acts
2025-02-11 07:55:35,750 - Step 2: Updated recent_tokens
2025-02-11 07:55:35,751 - Step 2: Decoded current text
2025-02-11 07:55:35,751 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:35,751 - 
Starting step 3
2025-02-11 07:55:35,751 - Current_ids device: cuda:0
2025-02-11 07:55:35,751 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,774 - Model output complete
2025-02-11 07:55:35,774 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:35,774 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,775 - Next token logits device: cuda:0
2025-02-11 07:55:35,775 - Entered do_sample
2025-02-11 07:55:35,775 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,777 - Probs max: 0.59521484375
2025-02-11 07:55:35,778 - Pre-cat
2025-02-11 07:55:35,778 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:35,779 - Next token: tensor([[14989]], device='cuda:0')
2025-02-11 07:55:35,779 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:35,779 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,780 - Step 3: Generated next token
2025-02-11 07:55:35,780 - Step 3: Updated current_ids
2025-02-11 07:55:35,780 - Step 3: Decoded token text:  thrown
2025-02-11 07:55:35,780 - Step 3: Updated current_phrase
2025-02-11 07:55:35,780 - Step 3: Created step_acts
2025-02-11 07:55:35,780 - Step 3: Added to generation_acts
2025-02-11 07:55:35,780 - Step 3: Updated recent_tokens
2025-02-11 07:55:35,782 - Step 3: Decoded current text
2025-02-11 07:55:35,782 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:35,782 - 
Starting step 4
2025-02-11 07:55:35,782 - Current_ids device: cuda:0
2025-02-11 07:55:35,782 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,806 - Model output complete
2025-02-11 07:55:35,806 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:35,806 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,807 - Next token logits device: cuda:0
2025-02-11 07:55:35,807 - Entered do_sample
2025-02-11 07:55:35,807 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,809 - Probs max: 0.67431640625
2025-02-11 07:55:35,810 - Pre-cat
2025-02-11 07:55:35,810 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989]],
       device='cuda:0')
2025-02-11 07:55:35,811 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:55:35,812 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:35,812 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,812 - Step 4: Generated next token
2025-02-11 07:55:35,812 - Step 4: Updated current_ids
2025-02-11 07:55:35,812 - Step 4: Decoded token text:  at
2025-02-11 07:55:35,812 - Step 4: Updated current_phrase
2025-02-11 07:55:35,812 - Step 4: Created step_acts
2025-02-11 07:55:35,812 - Step 4: Added to generation_acts
2025-02-11 07:55:35,812 - Step 4: Updated recent_tokens
2025-02-11 07:55:35,814 - Step 4: Decoded current text
2025-02-11 07:55:35,814 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:35,814 - 
Starting step 5
2025-02-11 07:55:35,814 - Current_ids device: cuda:0
2025-02-11 07:55:35,814 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,852 - Model output complete
2025-02-11 07:55:35,852 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:35,852 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,852 - Next token logits device: cuda:0
2025-02-11 07:55:35,852 - Entered do_sample
2025-02-11 07:55:35,852 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,854 - Probs max: 0.58251953125
2025-02-11 07:55:35,855 - Pre-cat
2025-02-11 07:55:35,855 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518]],
       device='cuda:0')
2025-02-11 07:55:35,857 - Next token: tensor([[264]], device='cuda:0')
2025-02-11 07:55:35,858 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:35,858 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,858 - Step 5: Generated next token
2025-02-11 07:55:35,858 - Step 5: Updated current_ids
2025-02-11 07:55:35,858 - Step 5: Decoded token text:  a
2025-02-11 07:55:35,858 - Step 5: Updated current_phrase
2025-02-11 07:55:35,859 - Step 5: Created step_acts
2025-02-11 07:55:35,859 - Step 5: Added to generation_acts
2025-02-11 07:55:35,860 - Step 5: Updated generated_texts
2025-02-11 07:55:35,860 - Step 5: Updated recent_tokens
2025-02-11 07:55:35,860 - Step 5: Decoded current text
2025-02-11 07:55:35,861 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:35,861 - 
Starting step 6
2025-02-11 07:55:35,861 - Current_ids device: cuda:0
2025-02-11 07:55:35,861 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,890 - Model output complete
2025-02-11 07:55:35,890 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:35,890 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,890 - Next token logits device: cuda:0
2025-02-11 07:55:35,890 - Entered do_sample
2025-02-11 07:55:35,890 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,893 - Probs max: 0.9208984375
2025-02-11 07:55:35,893 - Pre-cat
2025-02-11 07:55:35,893 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264]], device='cuda:0')
2025-02-11 07:55:35,895 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:35,896 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:35,896 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,896 - Step 6: Generated next token
2025-02-11 07:55:35,896 - Step 6: Updated current_ids
2025-02-11 07:55:35,896 - Step 6: Decoded token text:  wall
2025-02-11 07:55:35,896 - Step 6: Updated current_phrase
2025-02-11 07:55:35,896 - Step 6: Created step_acts
2025-02-11 07:55:35,896 - Step 6: Added to generation_acts
2025-02-11 07:55:35,897 - Step 6: Updated recent_tokens
2025-02-11 07:55:35,898 - Step 6: Decoded current text
2025-02-11 07:55:35,898 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:35,898 - 
Starting step 7
2025-02-11 07:55:35,898 - Current_ids device: cuda:0
2025-02-11 07:55:35,898 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,920 - Model output complete
2025-02-11 07:55:35,921 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:35,921 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,921 - Next token logits device: cuda:0
2025-02-11 07:55:35,921 - Entered do_sample
2025-02-11 07:55:35,921 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,924 - Probs max: 0.96826171875
2025-02-11 07:55:35,924 - Pre-cat
2025-02-11 07:55:35,924 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002]], device='cuda:0')
2025-02-11 07:55:35,926 - Next token: tensor([[1602]], device='cuda:0')
2025-02-11 07:55:35,926 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:35,926 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,926 - Step 7: Generated next token
2025-02-11 07:55:35,927 - Step 7: Updated current_ids
2025-02-11 07:55:35,927 - Step 7: Decoded token text:  very
2025-02-11 07:55:35,927 - Step 7: Updated current_phrase
2025-02-11 07:55:35,927 - Step 7: Created step_acts
2025-02-11 07:55:35,927 - Step 7: Added to generation_acts
2025-02-11 07:55:35,927 - Step 7: Updated recent_tokens
2025-02-11 07:55:35,929 - Step 7: Decoded current text
2025-02-11 07:55:35,929 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:35,929 - 
Starting step 8
2025-02-11 07:55:35,929 - Current_ids device: cuda:0
2025-02-11 07:55:35,929 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,952 - Model output complete
2025-02-11 07:55:35,952 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:35,952 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,952 - Next token logits device: cuda:0
2025-02-11 07:55:35,952 - Entered do_sample
2025-02-11 07:55:35,953 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,955 - Probs max: 0.99609375
2025-02-11 07:55:35,956 - Pre-cat
2025-02-11 07:55:35,956 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602]], device='cuda:0')
2025-02-11 07:55:35,958 - Next token: tensor([[4937]], device='cuda:0')
2025-02-11 07:55:35,958 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:35,958 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,958 - Step 8: Generated next token
2025-02-11 07:55:35,959 - Step 8: Updated current_ids
2025-02-11 07:55:35,959 - Step 8: Decoded token text:  fast
2025-02-11 07:55:35,959 - Step 8: Updated current_phrase
2025-02-11 07:55:35,959 - Step 8: Created step_acts
2025-02-11 07:55:35,959 - Step 8: Added to generation_acts
2025-02-11 07:55:35,959 - Step 8: Updated recent_tokens
2025-02-11 07:55:35,961 - Step 8: Decoded current text
2025-02-11 07:55:35,961 - Step 8: Reset consecutive_fillers
2025-02-11 07:55:35,961 - 
Starting step 9
2025-02-11 07:55:35,961 - Current_ids device: cuda:0
2025-02-11 07:55:35,961 - Current_ids dtype: torch.int64
2025-02-11 07:55:35,983 - Model output complete
2025-02-11 07:55:35,983 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:35,983 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,983 - Next token logits device: cuda:0
2025-02-11 07:55:35,983 - Entered do_sample
2025-02-11 07:55:35,983 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:35,986 - Probs max: 0.544921875
2025-02-11 07:55:35,987 - Pre-cat
2025-02-11 07:55:35,987 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937]], device='cuda:0')
2025-02-11 07:55:35,989 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:35,989 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:35,989 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:35,989 - Step 9: Generated next token
2025-02-11 07:55:35,989 - Step 9: Updated current_ids
2025-02-11 07:55:35,989 - Step 9: Decoded token text: ,
2025-02-11 07:55:35,989 - Step 9: Updated current_phrase
2025-02-11 07:55:35,990 - Step 9: Created step_acts
2025-02-11 07:55:35,990 - Step 9: Added to generation_acts
2025-02-11 07:55:35,990 - Step 9: Updated recent_tokens
2025-02-11 07:55:35,991 - Step 9: Found phrase end token
2025-02-11 07:55:35,991 - Step 9: Updated recent_phrases
2025-02-11 07:55:35,991 - Step 9: Decoded current text
2025-02-11 07:55:35,991 - Step 9: Reset consecutive_fillers
2025-02-11 07:55:35,992 - 
Starting step 10
2025-02-11 07:55:35,992 - Current_ids device: cuda:0
2025-02-11 07:55:35,992 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,015 - Model output complete
2025-02-11 07:55:36,015 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:36,015 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,015 - Next token logits device: cuda:0
2025-02-11 07:55:36,015 - Entered do_sample
2025-02-11 07:55:36,015 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,018 - Probs max: 0.469970703125
2025-02-11 07:55:36,018 - Pre-cat
2025-02-11 07:55:36,018 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     11]], device='cuda:0')
2025-02-11 07:55:36,020 - Next token: tensor([[773]], device='cuda:0')
2025-02-11 07:55:36,020 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:36,020 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,020 - Step 10: Generated next token
2025-02-11 07:55:36,020 - Step 10: Updated current_ids
2025-02-11 07:55:36,021 - Step 10: Decoded token text:  so
2025-02-11 07:55:36,021 - Step 10: Updated current_phrase
2025-02-11 07:55:36,021 - Step 10: Created step_acts
2025-02-11 07:55:36,021 - Step 10: Added to generation_acts
2025-02-11 07:55:36,022 - Step 10: Updated generated_texts
2025-02-11 07:55:36,022 - Step 10: Updated recent_tokens
2025-02-11 07:55:36,023 - Step 10: Decoded current text
2025-02-11 07:55:36,023 - Step 10: Reset consecutive_fillers
2025-02-11 07:55:36,023 - 
Starting step 11
2025-02-11 07:55:36,023 - Current_ids device: cuda:0
2025-02-11 07:55:36,023 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,046 - Model output complete
2025-02-11 07:55:36,046 - Logits shape: torch.Size([1, 42, 151936])
2025-02-11 07:55:36,046 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,046 - Next token logits device: cuda:0
2025-02-11 07:55:36,046 - Entered do_sample
2025-02-11 07:55:36,047 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,049 - Probs max: 0.9150390625
2025-02-11 07:55:36,050 - Pre-cat
2025-02-11 07:55:36,050 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     11,    773]], device='cuda:0')
2025-02-11 07:55:36,052 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:36,052 - Current_ids shape: torch.Size([1, 42])
2025-02-11 07:55:36,052 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,052 - Step 11: Generated next token
2025-02-11 07:55:36,052 - Step 11: Updated current_ids
2025-02-11 07:55:36,052 - Step 11: Decoded token text:  the
2025-02-11 07:55:36,052 - Step 11: Updated current_phrase
2025-02-11 07:55:36,053 - Step 11: Created step_acts
2025-02-11 07:55:36,053 - Step 11: Added to generation_acts
2025-02-11 07:55:36,053 - Step 11: Updated recent_tokens
2025-02-11 07:55:36,054 - Step 11: Decoded current text
2025-02-11 07:55:36,054 - Step 11: Incremented consecutive_fillers to 1
2025-02-11 07:55:36,054 - 
Starting step 12
2025-02-11 07:55:36,054 - Current_ids device: cuda:0
2025-02-11 07:55:36,054 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,078 - Model output complete
2025-02-11 07:55:36,078 - Logits shape: torch.Size([1, 43, 151936])
2025-02-11 07:55:36,078 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,078 - Next token logits device: cuda:0
2025-02-11 07:55:36,078 - Entered do_sample
2025-02-11 07:55:36,078 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,080 - Probs max: 0.52978515625
2025-02-11 07:55:36,081 - Pre-cat
2025-02-11 07:55:36,081 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     11,    773,    279]],
       device='cuda:0')
2025-02-11 07:55:36,082 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:36,083 - Current_ids shape: torch.Size([1, 43])
2025-02-11 07:55:36,083 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,083 - Step 12: Generated next token
2025-02-11 07:55:36,083 - Step 12: Updated current_ids
2025-02-11 07:55:36,083 - Step 12: Decoded token text:  wall
2025-02-11 07:55:36,083 - Step 12: Updated current_phrase
2025-02-11 07:55:36,083 - Step 12: Created step_acts
2025-02-11 07:55:36,083 - Step 12: Added to generation_acts
2025-02-11 07:55:36,083 - Step 12: Updated recent_tokens
2025-02-11 07:55:36,085 - Step 12: Decoded current text
2025-02-11 07:55:36,085 - Step 12: Incremented consecutive_fillers to 2
2025-02-11 07:55:36,085 - 
Starting step 13
2025-02-11 07:55:36,085 - Current_ids device: cuda:0
2025-02-11 07:55:36,085 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,108 - Model output complete
2025-02-11 07:55:36,108 - Logits shape: torch.Size([1, 44, 151936])
2025-02-11 07:55:36,108 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,108 - Next token logits device: cuda:0
2025-02-11 07:55:36,108 - Entered do_sample
2025-02-11 07:55:36,108 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,112 - Probs max: 0.5751953125
2025-02-11 07:55:36,112 - Pre-cat
2025-02-11 07:55:36,112 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,  14989,    518,
            264,   7002,   1602,   4937,     11,    773,    279,   7002]],
       device='cuda:0')
2025-02-11 07:55:36,114 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:36,114 - Current_ids shape: torch.Size([1, 44])
2025-02-11 07:55:36,114 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,114 - Step 13: Generated next token
2025-02-11 07:55:36,114 - Step 13: Updated current_ids
2025-02-11 07:55:36,114 - Step 13: Decoded token text:  is
2025-02-11 07:55:36,114 - Step 13: Updated current_phrase
2025-02-11 07:55:36,115 - Step 13: Created step_acts
2025-02-11 07:55:36,115 - Step 13: Added to generation_acts
2025-02-11 07:55:36,115 - Step 13: Updated recent_tokens
2025-02-11 07:55:36,116 - Step 13: Decoded current text
2025-02-11 07:55:36,116 - Step 13: Incremented consecutive_fillers to 3
2025-02-11 07:55:36,215 - 
Starting step 0
2025-02-11 07:55:36,215 - Current_ids device: cuda:0
2025-02-11 07:55:36,216 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,241 - Model output complete
2025-02-11 07:55:36,241 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:36,241 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,241 - Next token logits device: cuda:0
2025-02-11 07:55:36,241 - Entered do_sample
2025-02-11 07:55:36,241 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,245 - Probs max: 0.50341796875
2025-02-11 07:55:36,246 - Pre-cat
2025-02-11 07:55:36,246 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:36,249 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:36,249 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:36,249 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,249 - Step 0: Generated next token
2025-02-11 07:55:36,249 - Step 0: Updated current_ids
2025-02-11 07:55:36,250 - Step 0: Decoded token text:  The
2025-02-11 07:55:36,250 - Step 0: Updated current_phrase
2025-02-11 07:55:36,250 - Step 0: Created step_acts
2025-02-11 07:55:36,251 - Step 0: Added to generation_acts
2025-02-11 07:55:36,252 - Step 0: Updated generated_texts
2025-02-11 07:55:36,252 - Step 0: Updated recent_tokens
2025-02-11 07:55:36,252 - Step 0: Decoded current text
2025-02-11 07:55:36,252 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:36,252 - 
Starting step 1
2025-02-11 07:55:36,253 - Current_ids device: cuda:0
2025-02-11 07:55:36,253 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,279 - Model output complete
2025-02-11 07:55:36,279 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:36,280 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,280 - Next token logits device: cuda:0
2025-02-11 07:55:36,280 - Entered do_sample
2025-02-11 07:55:36,280 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,282 - Probs max: 0.83837890625
2025-02-11 07:55:36,283 - Pre-cat
2025-02-11 07:55:36,283 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:55:36,284 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:36,284 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:36,284 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,284 - Step 1: Generated next token
2025-02-11 07:55:36,284 - Step 1: Updated current_ids
2025-02-11 07:55:36,284 - Step 1: Decoded token text:  ball
2025-02-11 07:55:36,284 - Step 1: Updated current_phrase
2025-02-11 07:55:36,285 - Step 1: Created step_acts
2025-02-11 07:55:36,285 - Step 1: Added to generation_acts
2025-02-11 07:55:36,285 - Step 1: Updated recent_tokens
2025-02-11 07:55:36,286 - Step 1: Decoded current text
2025-02-11 07:55:36,287 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:36,287 - 
Starting step 2
2025-02-11 07:55:36,287 - Current_ids device: cuda:0
2025-02-11 07:55:36,287 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,312 - Model output complete
2025-02-11 07:55:36,312 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:36,312 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,312 - Next token logits device: cuda:0
2025-02-11 07:55:36,312 - Entered do_sample
2025-02-11 07:55:36,312 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,315 - Probs max: 0.583984375
2025-02-11 07:55:36,315 - Pre-cat
2025-02-11 07:55:36,316 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:55:36,317 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:36,317 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:36,317 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,317 - Step 2: Generated next token
2025-02-11 07:55:36,317 - Step 2: Updated current_ids
2025-02-11 07:55:36,317 - Step 2: Decoded token text:  will
2025-02-11 07:55:36,318 - Step 2: Updated current_phrase
2025-02-11 07:55:36,318 - Step 2: Created step_acts
2025-02-11 07:55:36,318 - Step 2: Added to generation_acts
2025-02-11 07:55:36,318 - Step 2: Updated recent_tokens
2025-02-11 07:55:36,319 - Step 2: Decoded current text
2025-02-11 07:55:36,319 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:36,319 - 
Starting step 3
2025-02-11 07:55:36,319 - Current_ids device: cuda:0
2025-02-11 07:55:36,320 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,342 - Model output complete
2025-02-11 07:55:36,343 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:36,343 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,343 - Next token logits device: cuda:0
2025-02-11 07:55:36,343 - Entered do_sample
2025-02-11 07:55:36,343 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,346 - Probs max: 0.56884765625
2025-02-11 07:55:36,347 - Pre-cat
2025-02-11 07:55:36,347 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686]],
       device='cuda:0')
2025-02-11 07:55:36,349 - Next token: tensor([[4201]], device='cuda:0')
2025-02-11 07:55:36,349 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:36,349 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,349 - Step 3: Generated next token
2025-02-11 07:55:36,349 - Step 3: Updated current_ids
2025-02-11 07:55:36,349 - Step 3: Decoded token text:  hit
2025-02-11 07:55:36,349 - Step 3: Updated current_phrase
2025-02-11 07:55:36,350 - Step 3: Created step_acts
2025-02-11 07:55:36,350 - Step 3: Added to generation_acts
2025-02-11 07:55:36,350 - Step 3: Updated recent_tokens
2025-02-11 07:55:36,351 - Step 3: Decoded current text
2025-02-11 07:55:36,351 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:36,351 - 
Starting step 4
2025-02-11 07:55:36,351 - Current_ids device: cuda:0
2025-02-11 07:55:36,351 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,374 - Model output complete
2025-02-11 07:55:36,374 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:36,374 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,374 - Next token logits device: cuda:0
2025-02-11 07:55:36,375 - Entered do_sample
2025-02-11 07:55:36,375 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,377 - Probs max: 0.99609375
2025-02-11 07:55:36,377 - Pre-cat
2025-02-11 07:55:36,378 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201]],
       device='cuda:0')
2025-02-11 07:55:36,379 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:36,379 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:36,379 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,379 - Step 4: Generated next token
2025-02-11 07:55:36,379 - Step 4: Updated current_ids
2025-02-11 07:55:36,380 - Step 4: Decoded token text:  the
2025-02-11 07:55:36,380 - Step 4: Updated current_phrase
2025-02-11 07:55:36,380 - Step 4: Created step_acts
2025-02-11 07:55:36,380 - Step 4: Added to generation_acts
2025-02-11 07:55:36,380 - Step 4: Updated recent_tokens
2025-02-11 07:55:36,381 - Step 4: Decoded current text
2025-02-11 07:55:36,381 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:36,381 - 
Starting step 5
2025-02-11 07:55:36,382 - Current_ids device: cuda:0
2025-02-11 07:55:36,382 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,420 - Model output complete
2025-02-11 07:55:36,420 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:36,420 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,420 - Next token logits device: cuda:0
2025-02-11 07:55:36,420 - Entered do_sample
2025-02-11 07:55:36,420 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,422 - Probs max: 0.99853515625
2025-02-11 07:55:36,424 - Pre-cat
2025-02-11 07:55:36,424 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279]],
       device='cuda:0')
2025-02-11 07:55:36,427 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:36,428 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:36,428 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,428 - Step 5: Generated next token
2025-02-11 07:55:36,428 - Step 5: Updated current_ids
2025-02-11 07:55:36,428 - Step 5: Decoded token text:  wall
2025-02-11 07:55:36,429 - Step 5: Updated current_phrase
2025-02-11 07:55:36,429 - Step 5: Created step_acts
2025-02-11 07:55:36,429 - Step 5: Added to generation_acts
2025-02-11 07:55:36,430 - Step 5: Updated generated_texts
2025-02-11 07:55:36,431 - Step 5: Updated recent_tokens
2025-02-11 07:55:36,431 - Step 5: Decoded current text
2025-02-11 07:55:36,431 - Step 5: Reset consecutive_fillers
2025-02-11 07:55:36,431 - 
Starting step 6
2025-02-11 07:55:36,431 - Current_ids device: cuda:0
2025-02-11 07:55:36,431 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,491 - Model output complete
2025-02-11 07:55:36,491 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:36,491 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,491 - Next token logits device: cuda:0
2025-02-11 07:55:36,491 - Entered do_sample
2025-02-11 07:55:36,491 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,493 - Probs max: 0.376708984375
2025-02-11 07:55:36,494 - Pre-cat
2025-02-11 07:55:36,494 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002]], device='cuda:0')
2025-02-11 07:55:36,496 - Next token: tensor([[11]], device='cuda:0')
2025-02-11 07:55:36,496 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:36,497 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,497 - Step 6: Generated next token
2025-02-11 07:55:36,497 - Step 6: Updated current_ids
2025-02-11 07:55:36,497 - Step 6: Decoded token text: ,
2025-02-11 07:55:36,497 - Step 6: Updated current_phrase
2025-02-11 07:55:36,497 - Step 6: Created step_acts
2025-02-11 07:55:36,497 - Step 6: Added to generation_acts
2025-02-11 07:55:36,498 - Step 6: Updated recent_tokens
2025-02-11 07:55:36,499 - Step 6: Found phrase end token
2025-02-11 07:55:36,499 - Step 6: Updated recent_phrases
2025-02-11 07:55:36,499 - Step 6: Decoded current text
2025-02-11 07:55:36,499 - Step 6: Reset consecutive_fillers
2025-02-11 07:55:36,499 - 
Starting step 7
2025-02-11 07:55:36,499 - Current_ids device: cuda:0
2025-02-11 07:55:36,499 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,525 - Model output complete
2025-02-11 07:55:36,525 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:36,525 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,525 - Next token logits device: cuda:0
2025-02-11 07:55:36,525 - Entered do_sample
2025-02-11 07:55:36,525 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,528 - Probs max: 0.4912109375
2025-02-11 07:55:36,528 - Pre-cat
2025-02-11 07:55:36,528 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,     11]], device='cuda:0')
2025-02-11 07:55:36,530 - Next token: tensor([[323]], device='cuda:0')
2025-02-11 07:55:36,530 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:36,530 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,531 - Step 7: Generated next token
2025-02-11 07:55:36,531 - Step 7: Updated current_ids
2025-02-11 07:55:36,531 - Step 7: Decoded token text:  and
2025-02-11 07:55:36,531 - Step 7: Updated current_phrase
2025-02-11 07:55:36,531 - Step 7: Created step_acts
2025-02-11 07:55:36,531 - Step 7: Added to generation_acts
2025-02-11 07:55:36,531 - Step 7: Updated recent_tokens
2025-02-11 07:55:36,533 - Step 7: Decoded current text
2025-02-11 07:55:36,533 - Step 7: Reset consecutive_fillers
2025-02-11 07:55:36,533 - 
Starting step 8
2025-02-11 07:55:36,533 - Current_ids device: cuda:0
2025-02-11 07:55:36,533 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,559 - Model output complete
2025-02-11 07:55:36,559 - Logits shape: torch.Size([1, 39, 151936])
2025-02-11 07:55:36,559 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,559 - Next token logits device: cuda:0
2025-02-11 07:55:36,559 - Entered do_sample
2025-02-11 07:55:36,559 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,561 - Probs max: 0.6806640625
2025-02-11 07:55:36,562 - Pre-cat
2025-02-11 07:55:36,562 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,     11,    323]], device='cuda:0')
2025-02-11 07:55:36,564 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:36,565 - Current_ids shape: torch.Size([1, 39])
2025-02-11 07:55:36,565 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,565 - Step 8: Generated next token
2025-02-11 07:55:36,565 - Step 8: Updated current_ids
2025-02-11 07:55:36,566 - Step 8: Decoded token text:  the
2025-02-11 07:55:36,566 - Step 8: Updated current_phrase
2025-02-11 07:55:36,566 - Step 8: Created step_acts
2025-02-11 07:55:36,566 - Step 8: Added to generation_acts
2025-02-11 07:55:36,566 - Step 8: Updated recent_tokens
2025-02-11 07:55:36,568 - Step 8: Decoded current text
2025-02-11 07:55:36,568 - Step 8: Incremented consecutive_fillers to 1
2025-02-11 07:55:36,568 - 
Starting step 9
2025-02-11 07:55:36,568 - Current_ids device: cuda:0
2025-02-11 07:55:36,568 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,604 - Model output complete
2025-02-11 07:55:36,604 - Logits shape: torch.Size([1, 40, 151936])
2025-02-11 07:55:36,604 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,604 - Next token logits device: cuda:0
2025-02-11 07:55:36,604 - Entered do_sample
2025-02-11 07:55:36,604 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,607 - Probs max: 0.94140625
2025-02-11 07:55:36,612 - Pre-cat
2025-02-11 07:55:36,613 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,     11,    323,    279]], device='cuda:0')
2025-02-11 07:55:36,616 - Next token: tensor([[7002]], device='cuda:0')
2025-02-11 07:55:36,616 - Current_ids shape: torch.Size([1, 40])
2025-02-11 07:55:36,616 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,616 - Step 9: Generated next token
2025-02-11 07:55:36,617 - Step 9: Updated current_ids
2025-02-11 07:55:36,617 - Step 9: Decoded token text:  wall
2025-02-11 07:55:36,617 - Step 9: Updated current_phrase
2025-02-11 07:55:36,617 - Step 9: Created step_acts
2025-02-11 07:55:36,617 - Step 9: Added to generation_acts
2025-02-11 07:55:36,618 - Step 9: Updated recent_tokens
2025-02-11 07:55:36,619 - Step 9: Decoded current text
2025-02-11 07:55:36,619 - Step 9: Incremented consecutive_fillers to 2
2025-02-11 07:55:36,619 - 
Starting step 10
2025-02-11 07:55:36,619 - Current_ids device: cuda:0
2025-02-11 07:55:36,619 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,667 - Model output complete
2025-02-11 07:55:36,667 - Logits shape: torch.Size([1, 41, 151936])
2025-02-11 07:55:36,668 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,668 - Next token logits device: cuda:0
2025-02-11 07:55:36,668 - Entered do_sample
2025-02-11 07:55:36,668 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,669 - Probs max: 0.9951171875
2025-02-11 07:55:36,671 - Pre-cat
2025-02-11 07:55:36,671 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    686,   4201,    279,
           7002,     11,    323,    279,   7002]], device='cuda:0')
2025-02-11 07:55:36,675 - Next token: tensor([[686]], device='cuda:0')
2025-02-11 07:55:36,675 - Current_ids shape: torch.Size([1, 41])
2025-02-11 07:55:36,675 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,675 - Step 10: Generated next token
2025-02-11 07:55:36,675 - Step 10: Updated current_ids
2025-02-11 07:55:36,676 - Step 10: Decoded token text:  will
2025-02-11 07:55:36,676 - Step 10: Updated current_phrase
2025-02-11 07:55:36,676 - Step 10: Created step_acts
2025-02-11 07:55:36,676 - Step 10: Added to generation_acts
2025-02-11 07:55:36,678 - Step 10: Updated generated_texts
2025-02-11 07:55:36,678 - Step 10: Updated recent_tokens
2025-02-11 07:55:36,678 - Step 10: Decoded current text
2025-02-11 07:55:36,678 - Step 10: Incremented consecutive_fillers to 3
2025-02-11 07:55:36,796 - 
Starting step 0
2025-02-11 07:55:36,796 - Current_ids device: cuda:0
2025-02-11 07:55:36,796 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,828 - Model output complete
2025-02-11 07:55:36,828 - Logits shape: torch.Size([1, 31, 151936])
2025-02-11 07:55:36,829 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,829 - Next token logits device: cuda:0
2025-02-11 07:55:36,829 - Entered do_sample
2025-02-11 07:55:36,829 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,831 - Probs max: 0.50341796875
2025-02-11 07:55:36,834 - Pre-cat
2025-02-11 07:55:36,834 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25]], device='cuda:0')
2025-02-11 07:55:36,837 - Next token: tensor([[576]], device='cuda:0')
2025-02-11 07:55:36,837 - Current_ids shape: torch.Size([1, 31])
2025-02-11 07:55:36,837 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,837 - Step 0: Generated next token
2025-02-11 07:55:36,837 - Step 0: Updated current_ids
2025-02-11 07:55:36,838 - Step 0: Decoded token text:  The
2025-02-11 07:55:36,838 - Step 0: Updated current_phrase
2025-02-11 07:55:36,838 - Step 0: Created step_acts
2025-02-11 07:55:36,838 - Step 0: Added to generation_acts
2025-02-11 07:55:36,839 - Step 0: Updated generated_texts
2025-02-11 07:55:36,839 - Step 0: Updated recent_tokens
2025-02-11 07:55:36,840 - Step 0: Decoded current text
2025-02-11 07:55:36,840 - Step 0: Reset consecutive_fillers
2025-02-11 07:55:36,840 - 
Starting step 1
2025-02-11 07:55:36,840 - Current_ids device: cuda:0
2025-02-11 07:55:36,840 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,903 - Model output complete
2025-02-11 07:55:36,904 - Logits shape: torch.Size([1, 32, 151936])
2025-02-11 07:55:36,904 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,904 - Next token logits device: cuda:0
2025-02-11 07:55:36,904 - Entered do_sample
2025-02-11 07:55:36,904 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,906 - Probs max: 0.83837890625
2025-02-11 07:55:36,907 - Pre-cat
2025-02-11 07:55:36,907 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576]], device='cuda:0')
2025-02-11 07:55:36,909 - Next token: tensor([[4935]], device='cuda:0')
2025-02-11 07:55:36,910 - Current_ids shape: torch.Size([1, 32])
2025-02-11 07:55:36,910 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,910 - Step 1: Generated next token
2025-02-11 07:55:36,910 - Step 1: Updated current_ids
2025-02-11 07:55:36,910 - Step 1: Decoded token text:  ball
2025-02-11 07:55:36,910 - Step 1: Updated current_phrase
2025-02-11 07:55:36,910 - Step 1: Created step_acts
2025-02-11 07:55:36,910 - Step 1: Added to generation_acts
2025-02-11 07:55:36,911 - Step 1: Updated recent_tokens
2025-02-11 07:55:36,912 - Step 1: Decoded current text
2025-02-11 07:55:36,912 - Step 1: Reset consecutive_fillers
2025-02-11 07:55:36,912 - 
Starting step 2
2025-02-11 07:55:36,912 - Current_ids device: cuda:0
2025-02-11 07:55:36,912 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,938 - Model output complete
2025-02-11 07:55:36,938 - Logits shape: torch.Size([1, 33, 151936])
2025-02-11 07:55:36,938 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,938 - Next token logits device: cuda:0
2025-02-11 07:55:36,938 - Entered do_sample
2025-02-11 07:55:36,938 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,940 - Probs max: 0.583984375
2025-02-11 07:55:36,941 - Pre-cat
2025-02-11 07:55:36,941 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935]], device='cuda:0')
2025-02-11 07:55:36,943 - Next token: tensor([[374]], device='cuda:0')
2025-02-11 07:55:36,944 - Current_ids shape: torch.Size([1, 33])
2025-02-11 07:55:36,944 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,944 - Step 2: Generated next token
2025-02-11 07:55:36,944 - Step 2: Updated current_ids
2025-02-11 07:55:36,944 - Step 2: Decoded token text:  is
2025-02-11 07:55:36,944 - Step 2: Updated current_phrase
2025-02-11 07:55:36,945 - Step 2: Created step_acts
2025-02-11 07:55:36,945 - Step 2: Added to generation_acts
2025-02-11 07:55:36,945 - Step 2: Updated recent_tokens
2025-02-11 07:55:36,946 - Step 2: Decoded current text
2025-02-11 07:55:36,946 - Step 2: Reset consecutive_fillers
2025-02-11 07:55:36,946 - 
Starting step 3
2025-02-11 07:55:36,946 - Current_ids device: cuda:0
2025-02-11 07:55:36,946 - Current_ids dtype: torch.int64
2025-02-11 07:55:36,974 - Model output complete
2025-02-11 07:55:36,975 - Logits shape: torch.Size([1, 34, 151936])
2025-02-11 07:55:36,975 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,975 - Next token logits device: cuda:0
2025-02-11 07:55:36,975 - Entered do_sample
2025-02-11 07:55:36,975 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:36,978 - Probs max: 0.59521484375
2025-02-11 07:55:36,979 - Pre-cat
2025-02-11 07:55:36,980 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374]],
       device='cuda:0')
2025-02-11 07:55:36,983 - Next token: tensor([[7218]], device='cuda:0')
2025-02-11 07:55:36,984 - Current_ids shape: torch.Size([1, 34])
2025-02-11 07:55:36,984 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:36,984 - Step 3: Generated next token
2025-02-11 07:55:36,984 - Step 3: Updated current_ids
2025-02-11 07:55:36,985 - Step 3: Decoded token text:  moving
2025-02-11 07:55:36,985 - Step 3: Updated current_phrase
2025-02-11 07:55:36,985 - Step 3: Created step_acts
2025-02-11 07:55:36,985 - Step 3: Added to generation_acts
2025-02-11 07:55:36,985 - Step 3: Updated recent_tokens
2025-02-11 07:55:36,987 - Step 3: Decoded current text
2025-02-11 07:55:36,987 - Step 3: Reset consecutive_fillers
2025-02-11 07:55:36,987 - 
Starting step 4
2025-02-11 07:55:36,987 - Current_ids device: cuda:0
2025-02-11 07:55:36,987 - Current_ids dtype: torch.int64
2025-02-11 07:55:37,044 - Model output complete
2025-02-11 07:55:37,044 - Logits shape: torch.Size([1, 35, 151936])
2025-02-11 07:55:37,044 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:37,044 - Next token logits device: cuda:0
2025-02-11 07:55:37,044 - Entered do_sample
2025-02-11 07:55:37,044 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:37,046 - Probs max: 0.677734375
2025-02-11 07:55:37,047 - Pre-cat
2025-02-11 07:55:37,047 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218]],
       device='cuda:0')
2025-02-11 07:55:37,049 - Next token: tensor([[518]], device='cuda:0')
2025-02-11 07:55:37,049 - Current_ids shape: torch.Size([1, 35])
2025-02-11 07:55:37,049 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:37,049 - Step 4: Generated next token
2025-02-11 07:55:37,049 - Step 4: Updated current_ids
2025-02-11 07:55:37,049 - Step 4: Decoded token text:  at
2025-02-11 07:55:37,050 - Step 4: Updated current_phrase
2025-02-11 07:55:37,050 - Step 4: Created step_acts
2025-02-11 07:55:37,050 - Step 4: Added to generation_acts
2025-02-11 07:55:37,050 - Step 4: Updated recent_tokens
2025-02-11 07:55:37,051 - Step 4: Decoded current text
2025-02-11 07:55:37,051 - Step 4: Reset consecutive_fillers
2025-02-11 07:55:37,051 - 
Starting step 5
2025-02-11 07:55:37,051 - Current_ids device: cuda:0
2025-02-11 07:55:37,052 - Current_ids dtype: torch.int64
2025-02-11 07:55:37,087 - Model output complete
2025-02-11 07:55:37,088 - Logits shape: torch.Size([1, 36, 151936])
2025-02-11 07:55:37,088 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:37,088 - Next token logits device: cuda:0
2025-02-11 07:55:37,088 - Entered do_sample
2025-02-11 07:55:37,088 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:37,090 - Probs max: 0.50146484375
2025-02-11 07:55:37,091 - Pre-cat
2025-02-11 07:55:37,092 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518]],
       device='cuda:0')
2025-02-11 07:55:37,093 - Next token: tensor([[279]], device='cuda:0')
2025-02-11 07:55:37,093 - Current_ids shape: torch.Size([1, 36])
2025-02-11 07:55:37,094 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:37,094 - Step 5: Generated next token
2025-02-11 07:55:37,094 - Step 5: Updated current_ids
2025-02-11 07:55:37,094 - Step 5: Decoded token text:  the
2025-02-11 07:55:37,094 - Step 5: Updated current_phrase
2025-02-11 07:55:37,094 - Step 5: Created step_acts
2025-02-11 07:55:37,094 - Step 5: Added to generation_acts
2025-02-11 07:55:37,096 - Step 5: Updated generated_texts
2025-02-11 07:55:37,096 - Step 5: Updated recent_tokens
2025-02-11 07:55:37,096 - Step 5: Decoded current text
2025-02-11 07:55:37,096 - Step 5: Incremented consecutive_fillers to 1
2025-02-11 07:55:37,096 - 
Starting step 6
2025-02-11 07:55:37,096 - Current_ids device: cuda:0
2025-02-11 07:55:37,096 - Current_ids dtype: torch.int64
2025-02-11 07:55:37,120 - Model output complete
2025-02-11 07:55:37,120 - Logits shape: torch.Size([1, 37, 151936])
2025-02-11 07:55:37,120 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:37,121 - Next token logits device: cuda:0
2025-02-11 07:55:37,121 - Entered do_sample
2025-02-11 07:55:37,121 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:37,123 - Probs max: 0.78466796875
2025-02-11 07:55:37,124 - Pre-cat
2025-02-11 07:55:37,124 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            279]], device='cuda:0')
2025-02-11 07:55:37,126 - Next token: tensor([[4628]], device='cuda:0')
2025-02-11 07:55:37,126 - Current_ids shape: torch.Size([1, 37])
2025-02-11 07:55:37,126 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:37,126 - Step 6: Generated next token
2025-02-11 07:55:37,126 - Step 6: Updated current_ids
2025-02-11 07:55:37,127 - Step 6: Decoded token text:  speed
2025-02-11 07:55:37,127 - Step 6: Updated current_phrase
2025-02-11 07:55:37,127 - Step 6: Created step_acts
2025-02-11 07:55:37,127 - Step 6: Added to generation_acts
2025-02-11 07:55:37,127 - Step 6: Updated recent_tokens
2025-02-11 07:55:37,130 - Step 6: Decoded current text
2025-02-11 07:55:37,131 - Step 6: Incremented consecutive_fillers to 2
2025-02-11 07:55:37,131 - 
Starting step 7
2025-02-11 07:55:37,131 - Current_ids device: cuda:0
2025-02-11 07:55:37,132 - Current_ids dtype: torch.int64
2025-02-11 07:55:37,172 - Model output complete
2025-02-11 07:55:37,173 - Logits shape: torch.Size([1, 38, 151936])
2025-02-11 07:55:37,173 - Next token logits shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:37,173 - Next token logits device: cuda:0
2025-02-11 07:55:37,173 - Entered do_sample
2025-02-11 07:55:37,173 - Probs shape: torch.Size([1, 1, 151936])
2025-02-11 07:55:37,175 - Probs max: 0.99365234375
2025-02-11 07:55:37,177 - Pre-cat
2025-02-11 07:55:37,177 - Current ids: tensor([[151646,  16141,    279,   2701,   3405,    323,   3410,    697,  32711,
            369,    279,   4226,     25,   1207,     25,   3555,    686,   3537,
            421,    264,   4935,    374,  14989,    518,    264,   7002,   1602,
           4937,     30,    362,     25,    576,   4935,    374,   7218,    518,
            279,   4628]], device='cuda:0')
2025-02-11 07:55:37,180 - Next token: tensor([[315]], device='cuda:0')
2025-02-11 07:55:37,181 - Current_ids shape: torch.Size([1, 38])
2025-02-11 07:55:37,181 - Next token shape: torch.Size([1, 1])
2025-02-11 07:55:37,181 - Step 7: Generated next token
2025-02-11 07:55:37,181 - Step 7: Updated current_ids
2025-02-11 07:55:37,181 - Step 7: Decoded token text:  of
2025-02-11 07:55:37,181 - Step 7: Updated current_phrase
2025-02-11 07:55:37,182 - Step 7: Created step_acts
2025-02-11 07:55:37,182 - Step 7: Added to generation_acts
2025-02-11 07:55:37,182 - Step 7: Updated recent_tokens
2025-02-11 07:55:37,183 - Step 7: Decoded current text
2025-02-11 07:55:37,183 - Step 7: Incremented consecutive_fillers to 3
